q_id,doc_id,old_question,answer,doc_path,documents,rationale,reference_figure,paper_title,paper_abstract,caption,question
spiqa_0,1805.01216v3,How does Figure 9 in the paper *Disentangling Language and Knowledge in Task-Oriented Dialogs* illustrate the transformation of point-of-interest properties between the original and pre-processed SMD Navigate datasets?," 

The pre-processed SMD Navigate data combines all the properties (such as distance, address) of a point of interest (POI) into a single subject with the object being ""poi"". The original data had separate entries for each property. ",1805.01216v3.pdf,"['1805.01216v3.pdf', '1705.09296v2.pdf', '1705.02946v3.pdf', '1803.04572v2.pdf', '1707.06320v2.pdf', '1809.00263v5.pdf', '1804.07849v4.pdf', '1709.02755v5.pdf', '1812.06589v2.pdf', '1710.01507v4.pdf', '1708.05239v3.pdf', '1705.02798v6.pdf', '1812.00108v4.pdf', '1708.03797v1.pdf']"," 

The figure shows two tables. The top table is the original SMD Navigate data, and the bottom table is the pre-processed data. In the original data, each property of a POI has its own entry in the table. For example, the POI ""the_westin"" has three entries: one for its distance, one for its traffic information, and one for its address. In the pre-processed data, all of these properties are combined into a single entry with the subject ""2 miles moderate_traffic rest_stop"" and the object ""poi"".",1805.01216v3-Figure9-1.png,Disentangling Language and Knowledge in Task-Oriented Dialogs,"The Knowledge Base (KB) used for real-world applications, such as booking a
movie or restaurant reservation, keeps changing over time. End-to-end neural
networks trained for these task-oriented dialogs are expected to be immune to
any changes in the KB. However, existing approaches breakdown when asked to
handle such changes. We propose an encoder-decoder architecture (BoSsNet) with
a novel Bag-of-Sequences (BoSs) memory, which facilitates the disentangled
learning of the response's language model and its knowledge incorporation.
Consequently, the KB can be modified with new knowledge without a drop in
interpretability. We find that BoSsNet outperforms state-of-the-art models,
with considerable improvements (> 10\%) on bAbI OOV test sets and other
human-human datasets. We also systematically modify existing datasets to
measure disentanglement and show BoSsNet to be robust to KB modifications.",Figure 9: Pre-processing of SMD Navigate data used in Mem2Seq paper," 

What is the difference between the original and pre-processed SMD Navigate data? "
spiqa_1,1805.07567v2,"Referring to the figure that illustrates the impact of increasing β2 on precision and recall in the salient object detection model optimized by the FLoss function, how does increasing β2 influence the trade-off between precision and recall?"," 

Increasing the value of β2 decreases the precision and increases the recall of the model.",1805.07567v2.pdf,"['1805.07567v2.pdf', '1802.07222v1.pdf', '1811.02553v4.pdf', '1705.07384v2.pdf', '1707.08608v3.pdf', '1703.10730v2.pdf', '1710.01507v4.pdf', '1804.07849v4.pdf', '1811.08481v2.pdf', '1804.05995v2.pdf']"," 

The figure shows that the precision curve (red) slopes downward as β2 increases, while the recall curve (green) slopes upward. This indicates that increasing β2 leads to a decrease in precision and an increase in recall.",1805.07567v2-Figure6-1.png,Optimizing the F-measure for Threshold-free Salient Object Detection,"Current CNN-based solutions to salient object detection (SOD) mainly rely on
the optimization of cross-entropy loss (CELoss). Then the quality of detected
saliency maps is often evaluated in terms of F-measure. In this paper, we
investigate an interesting issue: can we consistently use the F-measure
formulation in both training and evaluation for SOD? By reformulating the
standard F-measure we propose the relaxed F-measure which is differentiable
w.r.t the posterior and can be easily appended to the back of CNNs as the loss
function. Compared to the conventional cross-entropy loss of which the
gradients decrease dramatically in the saturated area, our loss function, named
FLoss, holds considerable gradients even when the activation approaches the
target. Consequently, the FLoss can continuously force the network to produce
polarized activations. Comprehensive benchmarks on several popular datasets
show that FLoss outperforms the state-of-the-art with a considerable margin.
More specifically, due to the polarized predictions, our method is able to
obtain high-quality saliency maps without carefully tuning the optimal
threshold, showing significant advantages in real-world applications.","Precision, Recall, F-measure of model trained under different β2 (Eq. 1). The precision decreases with the growing of β2 whereas recall increases. This characteristic gives us much flexibility to adjust the balance between recall and precision: use larger β2 in a recall-first application and lower β2 otherwise."," 

What is the effect of increasing the value of β2 on the precision and recall of the model? "
spiqa_2,1603.00286v5,"How does Figure 1 in the paper illustrate the difference between 2-D and 1-D cake division, specifically showing how geometric constraints in 2-D (like rectangularity) can result in unallocated portions of the cake, unlike in 1-D division?"," 

In 2-D division, there may be unallocated cake even when there are geometric constraints on the pieces, as shown in Figure 1. This is not the case in 1-D division, where the entire cake can always be allocated.",1603.00286v5.pdf,"['1603.00286v5.pdf', '1705.07164v8.pdf', '1811.02721v3.pdf', '1901.00056v2.pdf', '1811.06635v1.pdf', '1701.03077v10.pdf', '1703.02507v3.pdf', '1811.07073v3.pdf', '1612.02803v5.pdf', '1706.04284v3.pdf', '1707.01917v2.pdf', '1703.00060v2.pdf', '1803.02750v3.pdf']"," 

Figure 1 shows an example of a 2-D cake where there is unallocated cake even though the pieces must be rectangles. This is because the geometric constraints on the pieces prevent them from being placed in such a way that the entire cake is allocated. This qualitative difference between 2-D and 1-D division is important to note, as it can lead to paradoxes that are not present in 1-D division.",1603.00286v5-Figure1-1.png,Redividing the Cake,"The paper considers fair allocation of resources that are already allocated
in an unfair way. This setting requires a careful balance between the fairness
considerations and the rights of the present owners.
  The paper presents re-division algorithms that attain various trade-off
points between fairness and ownership rights, in various settings differing in
the geometric constraints on the allotments: (a) no geometric constraints; (b)
connectivity -- the cake is a one-dimensional interval and each piece must be a
contiguous interval; (c) rectangularity -- the cake is a two-dimensional
rectangle or rectilinear polygon and the pieces should be rectangles; (d)
convexity -- the cake is a two-dimensional convex polygon and the pieces should
be convex.
  These re-division algorithms have implications on another problem: the
price-of-fairness -- the loss of social welfare caused by fairness
requirements. Each algorithm implies an upper bound on the price-of-fairness
with the respective geometric constraints.","Fig. 1 With geometric constraints, an efficient allocation might leave some cake unallocated. All figures were made with GeoGebra 5 (Hohenwarter et al., 2013)."," 

Why does the author state that there is a qualitative difference between 2-D and 1-D division?"
spiqa_3,1805.06431v4,"How does ChoiceNet's performance under symmetric noise (20% and 50%) settings compare to its performance under the Pair-45% asymmetric noise setting, as shown in Table 2, and what does this reveal about its strengths and specific limitations when handling different types of noise?","ChoiceNet generally performs well compared to other methods, achieving the highest accuracy on both symmetric noise settings (sym-50% and sym-20%). However, it falls to second place under the Pair-45% asymmetric noise setting, indicating a weakness in handling this specific type of noise.",1805.06431v4.pdf,"['1805.06431v4.pdf', '1906.10843v1.pdf', '1805.08751v2.pdf', '1812.10735v2.pdf']","Table 2 presents the test accuracies of various methods under different noise conditions. By comparing the values in the table, we can see that ChoiceNet outperforms all other methods on the two symmetric noise settings, demonstrating its strength in handling such noise. However, under the Pair-45% asymmetric noise setting, ChoiceNet is surpassed by Co-teaching. This suggests that ChoiceNet may struggle with accurately inferring label distributions when noise patterns become more complex and asymmetric. The passage further clarifies this weakness, explaining that the ""Cholesky Block"" component of ChoiceNet struggles under Pair-45% noise due to the specific way this noise type assigns labels.",1805.06431v4-Table2-1.png,Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks,"In this paper, we focus on weakly supervised learning with noisy training
data for both classification and regression problems.We assume that the
training outputs are collected from a mixture of a target and correlated noise
distributions.Our proposed method simultaneously estimates the target
distribution and the quality of each data which is defined as the correlation
between the target and data generating distributions.The cornerstone of the
proposed method is a Cholesky Block that enables modeling dependencies among
mixture distributions in a differentiable manner where we maintain the
distribution over the network weights.We first provide illustrative examples in
both regression and classification tasks to show the effectiveness of the
proposed method.Then, the proposed method is extensively evaluated in a number
of experiments where we show that it constantly shows comparable or superior
performances compared to existing baseline methods in the handling of noisy
data.",Table 2: Test accuracies on the CIFAR-10 dataset with by symmetric and asymmetric noises., How does the performance of ChoiceNet compare to other methods under different noise settings? Briefly explain the strengths and weaknesses of ChoiceNet.
spiqa_4,1802.07459v2,"Based on the data breakdown provided in Table 1 of the paper ""Matching Article Pairs with Graphical Decomposition and Convolutions,"" how many negative samples were allocated to the training set of the CNSE dataset, following the 60% split used in the experiments?","There are approximately 9,719 negative samples in the training set of the CNSE dataset.",1802.07459v2.pdf,"['1802.07459v2.pdf', '1812.06589v2.pdf', '1710.06177v2.pdf', '1901.00056v2.pdf', '1809.01246v1.pdf', '1705.07164v8.pdf', '1704.07121v2.pdf', '1704.00774v3.pdf', '1805.08751v2.pdf', '1705.09882v2.pdf', '1805.06447v3.pdf']","Table 1 shows that the CNSE dataset has a total of 16,198 negative samples. The passage states that 60% of all samples are used for training. Therefore, to find the number of negative samples in the training set, we can calculate: 

0.6 * 16,198 = 9,718.8 

Since we cannot have fractions of samples, we round this number to the nearest whole number, which is 9,719.",1802.07459v2-Table1-1.png,Matching Article Pairs with Graphical Decomposition and Convolutions,"Identifying the relationship between two articles, e.g., whether two articles
published from different sources describe the same breaking news, is critical
to many document understanding tasks. Existing approaches for modeling and
matching sentence pairs do not perform well in matching longer documents, which
embody more complex interactions between the enclosed entities than a sentence
does. To model article pairs, we propose the Concept Interaction Graph to
represent an article as a graph of concepts. We then match a pair of articles
by comparing the sentences that enclose the same concept vertex through a
series of encoding techniques, and aggregate the matching signals through a
graph convolutional network. To facilitate the evaluation of long article
matching, we have created two datasets, each consisting of about 30K pairs of
breaking news articles covering diverse topics in the open domain. Extensive
evaluations of the proposed methods on the two datasets demonstrate significant
improvements over a wide range of state-of-the-art methods for natural language
matching.",Table 1: Description of evaluation datasets., How many negative samples are there in the training set of the CNSE dataset?
spiqa_5,1811.09393v4,"According to the figure illustrating the conditional VSR \(D_s,t\), how do the warped triplets leverage motion information to improve frame alignment and achieve more accurate video super-resolution results in this research paper?"," The warped triplets provide additional information about the motion and appearance of the scene, which helps the VSR Ds,t to generate more accurate and realistic results.",1811.09393v4.pdf,"['1811.09393v4.pdf', '1706.03847v3.pdf', '1705.10667v4.pdf', '1611.03780v2.pdf', '1705.07384v2.pdf']"," The warped triplets are created by warping the original triplets using the estimated motion information. This warping process aligns the corresponding pixels in the different frames, which allows the VSR Ds,t to better understand the motion and appearance of the scene. ",1811.09393v4-Figure4-1.png,Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation,"Our work explores temporal self-supervision for GAN-based video generation
tasks. While adversarial training successfully yields generative models for a
variety of areas, temporal relationships in the generated data are much less
explored. Natural temporal changes are crucial for sequential generation tasks,
e.g. video super-resolution and unpaired video translation. For the former,
state-of-the-art methods often favor simpler norm losses such as $L^2$ over
adversarial training. However, their averaging nature easily leads to
temporally smooth results with an undesirable lack of spatial detail. For
unpaired video translation, existing approaches modify the generator networks
to form spatio-temporal cycle consistencies. In contrast, we focus on improving
learning objectives and propose a temporally self-supervised algorithm. For
both tasks, we show that temporal adversarial learning is key to achieving
temporally coherent solutions without sacrificing spatial detail. We also
propose a novel Ping-Pong loss to improve the long-term temporal consistency.
It effectively prevents recurrent networks from accumulating artifacts
temporally without depressing detailed features. Additionally, we propose a
first set of metrics to quantitatively evaluate the accuracy as well as the
perceptual quality of the temporal evolution. A series of user studies confirm
the rankings computed with these metrics. Code, data, models, and results are
provided at https://github.com/thunil/TecoGAN. The project page
https://ge.in.tum.de/publications/2019-tecogan-chu/ contains supplemental
materials.","a) The frame-recurrent VSR Generator. b) Conditional VSR Ds,t ."," What is the role of the warped triplets in the conditional VSR Ds,t?"
spiqa_6,1706.04284v3,"In the figure from the paper illustrating semantic segmentation examples from Pascal VOC 2012, which denoiser produces the most accurate segmentation of the sheep, and what specific details in the associated segmentation label map support this conclusion?"," The denoiser trained with the classification network and evaluated for semantic segmentation performs the best on the sheep image. This is because the segmentation label map for this denoiser is the most accurate, and it correctly identifies the sheep's body and legs. ",1706.04284v3.pdf,"['1706.04284v3.pdf', '1611.04363v2.pdf', '1802.07459v2.pdf', '1703.02507v3.pdf', '1704.04539v2.pdf', '1705.09882v2.pdf', '1811.02721v3.pdf', '1611.07718v2.pdf', '1710.05654v2.pdf', '1704.00774v3.pdf', '1803.01128v3.pdf', '1704.07854v4.pdf', '1708.05239v3.pdf', '1803.04383v2.pdf']"," The figure shows the ground truth image and the denoised images using different denoisers. The segmentation label maps below each image show how well each denoiser performs. The denoiser trained with the classification network and evaluated for semantic segmentation has the most accurate segmentation label map, indicating that it performs the best.",1706.04284v3-Figure5-1.png,When Image Denoising Meets High-Level Vision Tasks: A Deep Learning Approach,"Conventionally, image denoising and high-level vision tasks are handled
separately in computer vision. In this paper, we cope with the two jointly and
explore the mutual influence between them. First we propose a convolutional
neural network for image denoising which achieves the state-of-the-art
performance. Second we propose a deep neural network solution that cascades two
modules for image denoising and various high-level tasks, respectively, and use
the joint loss for updating only the denoising network via back-propagation. We
demonstrate that on one hand, the proposed denoiser has the generality to
overcome the performance degradation of different high-level vision tasks. On
the other hand, with the guidance of high-level vision information, the
denoising network can generate more visually appealing results. To the best of
our knowledge, this is the first work investigating the benefit of exploiting
image semantics simultaneously for image denoising and high-level vision tasks
via deep learning. The code is available online
https://github.com/Ding-Liu/DeepDenoising.","Two semantic segmentation examples from Pascal VOC 2012 validation set. From left to right: (a) the ground truth image, the denoised image using (b) the separately trained denoiser, (c) the denoiser trained with the reconstruction and segmentation joint loss, and (d) the denoiser trained with the classification network and evaluated for semantic segmentation. Their corresponding segmentation label maps are shown below. The zoom-in region which generates inaccurate segmentation in (b) is displayed in the red box."," Which denoiser performs the best on the sheep image, and how can you tell?"
spiqa_7,1901.00398v2,"Based on Figure 2 in the paper ""Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation,"" did human evaluators demonstrate higher accuracy in identifying human-written reviews compared to machine-generated reviews, considering both the individual (H1) and majority (H2) voting criteria?",The human evaluators were more accurate at identifying human-written reviews than machine-generated reviews.,1901.00398v2.pdf,"['1901.00398v2.pdf', '1805.07567v2.pdf', '1603.03833v4.pdf', '1703.02507v3.pdf', '1803.01128v3.pdf', '1803.02750v3.pdf', '1606.07384v2.pdf', '1705.09966v2.pdf', '1811.07073v3.pdf', '1709.02755v5.pdf', '1812.00281v3.pdf', '1703.00060v2.pdf', '1809.01989v2.pdf', '1805.08751v2.pdf']"," The figure shows that the accuracy of the human evaluators was higher for human-written reviews than for machine-generated reviews. For example, the accuracy of H1 for human-written reviews predicted as human-written was 80%, while the accuracy for machine-generated reviews predicted as human-written was only 40%.",1901.00398v2-Figure2-1.png,Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation,"We conduct a large-scale, systematic study to evaluate the existing
evaluation methods for natural language generation in the context of generating
online product reviews. We compare human-based evaluators with a variety of
automated evaluation procedures, including discriminative evaluators that
measure how well machine-generated text can be distinguished from human-written
text, as well as word overlap metrics that assess how similar the generated
text compares to human-written references. We determine to what extent these
different evaluators agree on the ranking of a dozen of state-of-the-art
generators for online product reviews. We find that human evaluators do not
correlate well with discriminative evaluators, leaving a bigger question of
whether adversarial accuracy is the correct objective for natural language
generation. In general, distinguishing machine-generated text is challenging
even for human evaluators, and human decisions correlate better with lexical
overlaps. We find lexical diversity an intriguing metric that is indicative of
the assessments of different evaluators. A post-experiment survey of
participants provides insights into how to evaluate and improve the quality of
natural language generation systems.",Figure 2: Accuracy of human evaluators on individual reviews: H1 - individual votes; H2 - majority votes.," Which type of review was more accurately identified by the human evaluators, human-written or machine-generated? "
spiqa_8,1709.02755v5,"Based on Table 1 and the passage, how does the SRU model compare to the cuDNN-optimized LSTM in terms of exact match (EM), F1 scores, and the more than 5x speed-up in training on the SQuAD dataset?",The SRU model outperforms the LSTM model in both accuracy and training speed on the SQuAD dataset.,1709.02755v5.pdf,"['1709.02755v5.pdf', '1804.01429v3.pdf', '1805.07567v2.pdf', '1706.00827v2.pdf', '1708.01425v4.pdf', '1906.06589v3.pdf', '1704.07854v4.pdf', '1803.04383v2.pdf', '1705.09296v2.pdf', '1709.00139v4.pdf', '1703.07015v3.pdf']","Table 1 shows that the SRU model achieves higher EM and F1 scores compared to the LSTM model. Additionally, the passage states that SRU exhibits over 5x speed-up over LSTM and 53-63% reduction in total training time. This indicates that SRU not only achieves better accuracy but also trains significantly faster than the LSTM model.",1709.02755v5-Table2-1.png,Simple Recurrent Units for Highly Parallelizable Recurrence,"Common recurrent neural architectures scale poorly due to the intrinsic
difficulty in parallelizing their state computations. In this work, we propose
the Simple Recurrent Unit (SRU), a light recurrent unit that balances model
capacity and scalability. SRU is designed to provide expressive recurrence,
enable highly parallelized implementation, and comes with careful
initialization to facilitate training of deep models. We demonstrate the
effectiveness of SRU on multiple NLP tasks. SRU achieves 5--9x speed-up over
cuDNN-optimized LSTM on classification and question answering datasets, and
delivers stronger results than LSTM and convolutional models. We also obtain an
average of 0.7 BLEU improvement over the Transformer model on translation by
incorporating SRU into the architecture.","Table 2: Exact match (EM) and F1 scores of various models on SQuAD (Section 4.2). We also report the total processing time per epoch and the time spent in RNN computations. SRU outperforms other models, and is more than five times faster than cuDNN LSTM.","According to Table 1 and the passage, how does the performance of the SRU model compare to the LSTM model in terms of both accuracy and training speed on the SQuAD dataset?"
spiqa_9,1804.07849v4,"Referring to the ablation experiments in Table 2 on the Penn WSJ dataset, which feature's removal caused the largest drop in accuracy, from 80.1% to 65.6%, highlighting its critical role in the model's performance?",Morphological modeling with LSTMs contributes the most to the best model's performance compared to the baseline model.,1804.07849v4.pdf,"['1804.07849v4.pdf', '1703.07015v3.pdf', '1811.02553v4.pdf', '1611.07718v2.pdf']","The table shows that ""No character encoding,"" which effectively removes the morphological modeling with LSTMs, results in the largest drop in accuracy (from 80.1% to 65.6%). This suggests that this feature is crucial for the model's performance. While other factors like context size and initialization also affect accuracy, their impact is smaller compared to the absence of morphological modeling.",1804.07849v4-Table2-1.png,Mutual Information Maximization for Simple and Accurate Part-Of-Speech Induction,"We address part-of-speech (POS) induction by maximizing the mutual
information between the induced label and its context. We focus on two training
objectives that are amenable to stochastic gradient descent (SGD): a novel
generalization of the classical Brown clustering objective and a recently
proposed variational lower bound. While both objectives are subject to noise in
gradient updates, we show through analysis and experiments that the variational
lower bound is robust whereas the generalized Brown objective is vulnerable. We
obtain competitive performance on a multitude of datasets and languages with a
simple architecture that encodes morphology and context.",Table 2: Ablation of the best model on Penn WSJ.,"According to the ablation experiments, which factor contributes the most to the best model's performance compared to the baseline model?"
spiqa_10,1811.02721v3,Table 1,"TCP performs poorly on IEEE 802.15.4 networks because the Maximum Transmission Unit (MTU) for these networks is significantly smaller than other network types. This small MTU size results in a high percentage of overhead due to the TCP/IP headers, exceeding 50%. ",1811.02721v3.pdf,"['1811.02721v3.pdf', '1708.00160v2.pdf', '1612.02803v5.pdf', '1703.10730v2.pdf', '1809.04276v2.pdf', '1805.04687v2.pdf', '1705.02798v6.pdf']","Table 1 shows that the MTU for 802.15.4 networks is only 104-116 bytes, while other networks like Fast Ethernet and Wi-Fi have an MTU of 1500 bytes. The passage explains that TCP/IP headers consume a significant portion of the available MTU in 802.15.4 frames. This high overhead percentage leads to inefficient data transmission and consequently, poor TCP performance.",1811.02721v3-Table4-1.png,Performant TCP for Low-Power Wireless Networks,"Low-power and lossy networks (LLNs) enable diverse applications integrating
many resource-constrained embedded devices, often requiring interconnectivity
with existing TCP/IP networks as part of the Internet of Things. But TCP has
received little attention in LLNs due to concerns about its overhead and
performance, leading to LLN-specific protocols that require specialized
gateways for interoperability. We present a systematic study of a well-designed
TCP stack in IEEE 802.15.4-based LLNs, based on the TCP protocol logic in
FreeBSD. Through careful implementation and extensive experiments, we show that
modern low-power sensor platforms are capable of running full-scale TCP and
that TCP, counter to common belief, performs well despite the lossy nature of
LLNs. By carefully studying the interaction between the transport and link
layers, we identify subtle but important modifications to both, achieving TCP
goodput within 25% of an upper bound (5-40x higher than prior results) and
low-power operation commensurate to CoAP in a practical LLN application
scenario. This suggests that a TCP-based transport layer, seamlessly
interoperable with existing TCP/IP networks, is viable and performant in LLNs.",Table 4: Comparison of TCP/IP links,"Based on Table 1 and the passage, why does TCP perform poorly on IEEE 802.15.4 networks compared to other network types listed? "
spiqa_11,1805.04687v2,"Based on the data presented in Table 1 of the ""BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning"" paper, which object category has the highest combined total of bounding box and instance track annotations in the BDD100K MOT dataset?",Cars have the largest total number of annotations.,1805.04687v2.pdf,"['1805.04687v2.pdf', '1708.00160v2.pdf', '1901.00398v2.pdf', '1706.03847v3.pdf', '1809.01246v1.pdf', '1804.05995v2.pdf', '1706.00633v4.pdf', '1611.03780v2.pdf', '1812.06589v2.pdf']","Table 1 shows the number of annotations for different categories in the BDD100K MOT dataset, including both bounding boxes (""Boxes"") and instance tracks (""Tracks""). By adding the values for ""car"" in both rows, we find that cars have a total of approximately 2.7 million annotations (97K tracks + 2.6M boxes), which is greater than any other category.",1805.04687v2-Table11-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.",Table 11: Annotations of the BDD100K MOT dataset by category.,"Based on Table 1, which category of objects has the largest total number of annotations in the BDD100K MOT dataset, considering both bounding boxes and instance tracks? "
spiqa_12,1709.08294v3,Table 1,"The Yelp P. dataset has the largest vocabulary size with 25,709 unique words. This is significantly larger than the average number of words per document in the dataset, which is 138.",1709.08294v3.pdf,"['1709.08294v3.pdf', '1705.02946v3.pdf', '1804.04786v3.pdf', '1811.02721v3.pdf', '1611.03780v2.pdf', '1710.05654v2.pdf', '1703.04887v4.pdf', '1707.06320v2.pdf', '1804.07931v2.pdf', '1605.07496v3.pdf', '1611.02654v2.pdf', '1603.00286v5.pdf']","The table provides information about the vocabulary size and average number of words for each dataset. By comparing these values for each dataset, we can see that the Yelp P. dataset has the largest vocabulary size, indicating a wider variety of unique words used in the documents. However, the average document length is relatively short, suggesting that individual documents may not utilize the full range of vocabulary available. This discrepancy could be due to factors such as frequent use of stop words or a high degree of variation in document length within the dataset.",1709.08294v3-Table1-1.png,Learning Context-Sensitive Convolutional Filters for Text Processing,"Convolutional neural networks (CNNs) have recently emerged as a popular
building block for natural language processing (NLP). Despite their success,
most existing CNN models employed in NLP share the same learned (and static)
set of filters for all input sentences. In this paper, we consider an approach
of using a small meta network to learn context-sensitive convolutional filters
for text processing. The role of meta network is to abstract the contextual
information of a sentence or document into a set of input-aware filters. We
further generalize this framework to model sentence pairs, where a
bidirectional filter generation mechanism is introduced to encapsulate
co-dependent sentence representations. In our benchmarks on four different
tasks, including ontology classification, sentiment analysis, answer sentence
selection, and paraphrase identification, our proposed model, a modified CNN
with context-sensitive filters, consistently outperforms the standard CNN and
attention-based CNN baselines. By visualizing the learned context-sensitive
filters, we further validate and rationalize the effectiveness of proposed
framework.",Table 1: Dataset statistics.,"Based on Table 1, which dataset has the largest vocabulary size and how does this compare to the average number of words per document in that dataset?"
spiqa_13,1802.07351v2,"Based on the ablation study results in Table 1 of the Devon paper, which specific architectural modification led to the largest increase in end-point error, rising to 15.64, on the KITTI 2015 dataset?",Removing the normalization in the relation modules had the most significant negative impact on performance for the KITTI 2015 dataset.,1802.07351v2.pdf,"['1802.07351v2.pdf', '1702.08694v3.pdf', '1606.07384v2.pdf', '1805.00912v4.pdf', '1705.09296v2.pdf', '1704.08615v2.pdf', '1708.05239v3.pdf', '1707.00189v3.pdf']","Table 1 shows the results of various modifications to the Devon model architecture on different datasets. For the KITTI 2015 dataset, the ""Without norm"" configuration resulted in the highest end-point error of 15.64, which is considerably higher than the baseline model's error of 13.25. This indicates that removing the normalization significantly reduces the model's accuracy on this particular dataset.",1802.07351v2-Table5-1.png,Devon: Deformable Volume Network for Learning Optical Flow,"State-of-the-art neural network models estimate large displacement optical
flow in multi-resolution and use warping to propagate the estimation between
two resolutions. Despite their impressive results, it is known that there are
two problems with the approach. First, the multi-resolution estimation of
optical flow fails in situations where small objects move fast. Second, warping
creates artifacts when occlusion or dis-occlusion happens. In this paper, we
propose a new neural network module, Deformable Cost Volume, which alleviates
the two problems. Based on this module, we designed the Deformable Volume
Network (Devon) which can estimate multi-scale optical flow in a single high
resolution. Experiments show Devon is more suitable in handling small objects
moving fast and achieves comparable results to the state-of-the-art methods in
public benchmarks.",Table 5. Results of ablation experiments after training on FlyingChairs (end-point error).,"Based on the ablation study, which modification to the Devon model architecture had the most significant negative impact on performance for the KITTI 2015 dataset?"
spiqa_14,1709.08294v3,"Looking at the performance results in Figure (b), which specific question type within the WikiQA dataset does the ACNN model achieve its highest accuracy compared to all others?","ACNN performs best on ""Who"" questions.",1709.08294v3.pdf,"['1709.08294v3.pdf', '1605.07496v3.pdf', '1805.01216v3.pdf', '1809.00458v1.pdf', '1901.00398v2.pdf', '1812.00108v4.pdf', '1809.03550v3.pdf', '1804.05938v2.pdf', '1706.04269v2.pdf', '1705.02946v3.pdf', '1901.00056v2.pdf', '1805.02349v2.pdf', '1802.07459v2.pdf']","Figure (b) shows the performance of ACNN and AdaQA on different question types. We can see that ACNN performs better than AdaQA on all question types, and it performs the best on ""Who"" questions.",1709.08294v3-Figure3-1.png,Learning Context-Sensitive Convolutional Filters for Text Processing,"Convolutional neural networks (CNNs) have recently emerged as a popular
building block for natural language processing (NLP). Despite their success,
most existing CNN models employed in NLP share the same learned (and static)
set of filters for all input sentences. In this paper, we consider an approach
of using a small meta network to learn context-sensitive convolutional filters
for text processing. The role of meta network is to abstract the contextual
information of a sentence or document into a set of input-aware filters. We
further generalize this framework to model sentence pairs, where a
bidirectional filter generation mechanism is introduced to encapsulate
co-dependent sentence representations. In our benchmarks on four different
tasks, including ontology classification, sentiment analysis, answer sentence
selection, and paraphrase identification, our proposed model, a modified CNN
with context-sensitive filters, consistently outperforms the standard CNN and
attention-based CNN baselines. By visualizing the learned context-sensitive
filters, we further validate and rationalize the effectiveness of proposed
framework.","Comprehensive study of the proposed ACNN framework, including (a) the number of filters (Yelp dataset), and (b) performance vs question types (WikiQA dataset), and (c) t-SNE visualization of learned filter weights (DBpedia dataset).","Based on the figure, which type of question does ACNN perform the best on?"
spiqa_15,1709.02755v5,"How does the SRU model's translation training configuration, as detailed in Table 6, address the challenges of large vocabulary sizes through techniques like token-based batching, shared embedding, and positional encoding?","The training process uses several techniques to handle large vocabulary sizes. These include:

1. **Token-based batching:** Instead of grouping sentences of similar lengths together, the training process batches together a fixed number of tokens (5120 tokens per batch). This approach ensures that the model sees a consistent amount of vocabulary regardless of sentence length variation.
2. **Shared embedding:** This technique maps both source and target words to the same embedding space, effectively reducing the memory footprint needed to store word representations. 
3. **Positional encoding:** This method injects information about the position of words in a sentence into the model, helping it better understand long-range dependencies within the text. ",1709.02755v5.pdf,"['1709.02755v5.pdf', '1701.03077v10.pdf', '1809.03449v3.pdf', '1611.05742v3.pdf', '1710.05654v2.pdf', '1612.02803v5.pdf', '1805.01216v3.pdf', '1802.07222v1.pdf', '1707.00189v3.pdf', '1906.10843v1.pdf', '1804.01429v3.pdf', '1805.07567v2.pdf']","The table provides various hyperparameters used for training the translation model. By analyzing specific parameters like `batch_type`, `share_embedding`, and `position_encoding`, we can understand how the model is designed to cope with large vocabularies. These specific settings enable efficient processing and representation of large vocabulary sizes within the model's memory and architecture.",1709.02755v5-Table6-1.png,Simple Recurrent Units for Highly Parallelizable Recurrence,"Common recurrent neural architectures scale poorly due to the intrinsic
difficulty in parallelizing their state computations. In this work, we propose
the Simple Recurrent Unit (SRU), a light recurrent unit that balances model
capacity and scalability. SRU is designed to provide expressive recurrence,
enable highly parallelized implementation, and comes with careful
initialization to facilitate training of deep models. We demonstrate the
effectiveness of SRU on multiple NLP tasks. SRU achieves 5--9x speed-up over
cuDNN-optimized LSTM on classification and question answering datasets, and
delivers stronger results than LSTM and convolutional models. We also obtain an
average of 0.7 BLEU improvement over the Transformer model on translation by
incorporating SRU into the architecture.",Table 6: Translation training configuration.,"Based on the table, how does the training process handle large vocabulary sizes?"
spiqa_16,1805.04687v2,"Based on Table 8 in the BDD100K paper, which multitask learning approach for semantic segmentation achieved the highest mean IoU, and how does its performance compare to the baseline Sem-Seg model in terms of the improvement in mean IoU?","The **Sem-Seg + Det** approach achieved the highest mean IoU of 58.3, which is an improvement of 1.4 points compared to the baseline Sem-Seg model with a mean IoU of 56.9.",1805.04687v2.pdf,"['1805.04687v2.pdf', '1804.05938v2.pdf', '1704.07121v2.pdf', '1804.05936v2.pdf', '1811.06635v1.pdf', '1611.05742v3.pdf', '1705.02798v6.pdf', '1708.03797v1.pdf', '1805.04609v3.pdf', '1710.01507v4.pdf', '1709.00139v4.pdf', '1709.02755v5.pdf', '1803.02750v3.pdf']","The table shows the mean IoU scores for different semantic segmentation approaches. We can see that the ""Sem-Seg + Det"" model has the highest score in the ""mean IoU"" column, indicating it achieved the best overall performance. Additionally, by comparing this score to the ""Sem-Seg"" model, we can quantify the improvement achieved by adding object detection to the training process.",1805.04687v2-Table8-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.","Table 8: Evaluation results for semantic segmentation. We explore segmentation joint-training with different tasks. Detection can improve the overall accuracy of segmentation, although their output structures are different. However, although Lane and Drivable area improve the segmentation of road and sidewalk, the overall accuracy drops.","Based on the table, which approach achieved the highest mean IoU for semantic segmentation, and how did it perform compared to the baseline Sem-Seg model?"
spiqa_17,1706.00633v4,"Based on Table 5, when using the Cross-Entropy (CE) objective function and evaluating adversarial example crafting on the ResNet-32 model across both MNIST and CIFAR-10 datasets, which attack method is the fastest, and how many seconds faster is it compared to the slowest method, the C&W-wb attack?","The FGSM attack is the most efficient, requiring approximately 1.9 milliseconds to craft an adversarial example with the CE objective function. This is roughly **55,000 times faster** than the slowest method, C&W-wb, which takes about 700 seconds for the same objective function.",1706.00633v4.pdf,"['1706.00633v4.pdf', '1802.07222v1.pdf', '1709.02755v5.pdf', '1805.02349v2.pdf']","The table presents the average time costs for different attack methods on both MNIST and CIFAR-10 datasets with the ResNet-32 model. By comparing the values in the ""Time"" column, we can identify the fastest and slowest methods. The table also specifies the objective function used (CE or RCE), allowing us to compare methods under the same conditions.",1706.00633v4-Table5-1.png,Towards Robust Detection of Adversarial Examples,"Although the recent progress is substantial, deep learning methods can be
vulnerable to the maliciously generated adversarial examples. In this paper, we
present a novel training procedure and a thresholding test strategy, towards
robust detection of adversarial examples. In training, we propose to minimize
the reverse cross-entropy (RCE), which encourages a deep network to learn
latent representations that better distinguish adversarial examples from normal
ones. In testing, we propose to use a thresholding strategy as the detector to
filter out adversarial examples for reliable predictions. Our method is simple
to implement using standard algorithms, with little extra training cost
compared to the common cross-entropy minimization. We apply our method to
defend various attacking methods on the widely used MNIST and CIFAR-10
datasets, and achieve significant improvements on robust predictions under all
the threat models in the adversarial setting.",Table 5: The average time costs (s) on crafting each adversarial example via different attacks. The values are also the average values between MNIST and CIFAR-10. The models is Resnet-32.,"Based on the table, which attack method is the most efficient in terms of time taken to craft an adversarial example, and how much faster is it compared to the slowest method for the same objective function?"
spiqa_18,1802.07351v2,"In Table 2 of the Devon paper, which method achieves the lowest end-point error on the Sintel ""Final"" test set, and how does this performance compare to Devon (ft)'s error of 6.35?","PWC-Net (ft) performs best on the Sintel ""Final"" test set with an error of 5.04. Devon (ft) has a higher error of 6.35 on the same set. ",1802.07351v2.pdf,"['1802.07351v2.pdf', '1706.04269v2.pdf', '1704.07121v2.pdf', '1710.06177v2.pdf', '1703.04887v4.pdf']","The table shows the end-point error for various methods on both the ""Clean"" and ""Final"" versions of the Sintel test set. The lowest error value indicates the best performance. Comparing the values in the ""Final"" column, we can see PWC-Net (ft) has the lowest error (5.04), while Devon (ft) has a higher error of 6.35.",1802.07351v2-Table2-1.png,Devon: Deformable Volume Network for Learning Optical Flow,"State-of-the-art neural network models estimate large displacement optical
flow in multi-resolution and use warping to propagate the estimation between
two resolutions. Despite their impressive results, it is known that there are
two problems with the approach. First, the multi-resolution estimation of
optical flow fails in situations where small objects move fast. Second, warping
creates artifacts when occlusion or dis-occlusion happens. In this paper, we
propose a new neural network module, Deformable Cost Volume, which alleviates
the two problems. Based on this module, we designed the Deformable Volume
Network (Devon) which can estimate multi-scale optical flow in a single high
resolution. Experiments show Devon is more suitable in handling small objects
moving fast and achieves comparable results to the state-of-the-art methods in
public benchmarks.",Table 2. Results on Sintel (end-point error). (ft) denotes the finetuning.,"Based on the table, which method performs best on the Sintel ""Final"" test set, and how does its performance compare to Devon (ft) on the same set? "
spiqa_19,1802.07351v2,"Based on Table 4's results for the KITTI 2015 test set, which fine-tuned model achieved the best F1-all score, and how does this score compare to the F1-all score of Devon (ft) on the same test set?","PWC-Net (ft) achieved the best performance on the KITTI 2015 test set with an F1-all score of 9.16%. This is significantly better than Devon (ft), which achieved an F1-all score of 14.31% on the same dataset. ",1802.07351v2.pdf,"['1802.07351v2.pdf', '1710.01507v4.pdf', '1705.09296v2.pdf', '1704.07121v2.pdf', '1701.06171v4.pdf', '1812.00108v4.pdf', '1805.02349v2.pdf', '1803.04572v2.pdf', '1805.06431v4.pdf', '1707.01917v2.pdf', '1703.10730v2.pdf', '1708.01425v4.pdf']","The table shows the performance of different models on the KITTI 2015 dataset, including both training and testing sets. The F1-all score is listed for each model on the test set. By comparing the F1-all scores, we can see that PWC-Net (ft) has the lowest score, indicating the best performance. Additionally, we can directly compare the F1-all scores of PWC-Net (ft) and Devon (ft) to see that PWC-Net (ft) performs significantly better.",1802.07351v2-Table4-1.png,Devon: Deformable Volume Network for Learning Optical Flow,"State-of-the-art neural network models estimate large displacement optical
flow in multi-resolution and use warping to propagate the estimation between
two resolutions. Despite their impressive results, it is known that there are
two problems with the approach. First, the multi-resolution estimation of
optical flow fails in situations where small objects move fast. Second, warping
creates artifacts when occlusion or dis-occlusion happens. In this paper, we
propose a new neural network module, Deformable Cost Volume, which alleviates
the two problems. Based on this module, we designed the Deformable Volume
Network (Devon) which can estimate multi-scale optical flow in a single high
resolution. Experiments show Devon is more suitable in handling small objects
moving fast and achieves comparable results to the state-of-the-art methods in
public benchmarks.",Table 4. Results on KITTI. (ft) denotes the fine-tuning. EPE denotes end-point error. Fl-all denotes the ratio of pixels where the flow estimate is incorrect by both ≥ 3 pixels and ≥ 5%.,"Based on the table, which model achieved the best performance on the KITTI 2015 test set in terms of F1-all score, and how does its performance compare to Devon (ft) on the same dataset?"
spiqa_20,1605.07496v3,"Based on the figure comparing the runtime performance for FSRE test functions in the paper ""Alternating Optimisation and Quadrature for Robust Control"", does One Step ALOQ show superior runtime efficiency over WSN for both F-SRE1 and F-SRE2?",ALOQ is significantly more efficient than WSN.,1605.07496v3.pdf,"['1605.07496v3.pdf', '1704.05426v4.pdf', '1703.04887v4.pdf', '1702.08694v3.pdf', '1708.03797v1.pdf', '1803.03467v4.pdf', '1809.03550v3.pdf', '1611.03780v2.pdf']",The figure shows that the One Step ALOQ method has the lowest runtime for both F-SRE1 and F-SRE2.,1605.07496v3-Figure11-1.png,Alternating Optimisation and Quadrature for Robust Control,"Bayesian optimisation has been successfully applied to a variety of
reinforcement learning problems. However, the traditional approach for learning
optimal policies in simulators does not utilise the opportunity to improve
learning by adjusting certain environment variables: state features that are
unobservable and randomly determined by the environment in a physical setting
but are controllable in a simulator. This paper considers the problem of
finding a robust policy while taking into account the impact of environment
variables. We present Alternating Optimisation and Quadrature (ALOQ), which
uses Bayesian optimisation and Bayesian quadrature to address such settings.
ALOQ is robust to the presence of significant rare events, which may not be
observable under random sampling, but play a substantial role in determining
the optimal policy. Experimental results across different domains show that
ALOQ can learn more efficiently and robustly than existing methods.",Comparison of runtime of all methods on the FSRE test functions.,"Between WSN and ALOQ, which method is the most efficient in terms of runtime for both F-SRE1 and F-SRE2?"
spiqa_21,1704.05958v2,"Based on the data provided in Table 1 of the paper ""Global Relation Embedding for Relation Extraction,"" what is the approximate percentage of entity pairs in the NYT training set that are associated with a corresponding relational fact in the knowledge base (KB)?",Approximately 6.66%.,1704.05958v2.pdf,"['1704.05958v2.pdf', '1708.02153v2.pdf', '1804.07931v2.pdf', '1805.08751v2.pdf', '1703.00899v2.pdf', '1803.01128v3.pdf', '1708.06832v3.pdf', '1705.02798v6.pdf', '1803.06506v3.pdf', '1804.04786v3.pdf', '1901.00398v2.pdf', '1803.05776v2.pdf', '1704.07121v2.pdf', '1605.07496v3.pdf']","The table shows that the training set contains 291,699 entity pairs and 19,429 relational facts from the KB. To find the percentage of entity pairs with a corresponding KB fact, we divide the number of facts by the number of entity pairs and multiply by 100: 

(19,429 / 291,699) * 100 ≈ 6.66%. 

This indicates that only a small fraction of entity pairs in the training data have an explicitly defined relationship in the KB.",1704.05958v2-Table1-1.png,Global Relation Embedding for Relation Extraction,"We study the problem of textual relation embedding with distant supervision.
To combat the wrong labeling problem of distant supervision, we propose to
embed textual relations with global statistics of relations, i.e., the
co-occurrence statistics of textual and knowledge base relations collected from
the entire corpus. This approach turns out to be more robust to the training
noise introduced by distant supervision. On a popular relation extraction
dataset, we show that the learned textual relation embedding can be used to
augment existing relation extraction models and significantly improve their
performance. Most remarkably, for the top 1,000 relational facts discovered by
the best existing model, the precision can be improved from 83.9% to 89.3%.",Table 1: Statistics of the NYT dataset.,Can you estimate the percentage of entity pairs in the NYT training set that have a corresponding relational fact in the Knowledge Base (KB)?
spiqa_22,1805.01216v3,"How does BoSsNet’s multi-hop encoder, as demonstrated in Table 14, enhance performance on bAbI tasks 3 (restaurant sorting by rating) and 5 (preference-based restaurant recommendation), and how do the tasks’ reliance on multi-step reasoning and inferencing over multiple KB entries contribute to this improvement?","The multi-hop encoder performs better on bAbI tasks 3 and 5 because these tasks specifically require inferencing over multiple KB tuples. In other words, the model needs to ""hop"" between different pieces of information in the knowledge base to make the correct inferences and recommendations.

Task 3 involves sorting restaurants by rating, and task 5 requires recommending a restaurant based on user preferences. Both tasks necessitate the model to consider various restaurant attributes and their relationships, which the multi-hop encoder facilitates by capturing longer-range dependencies within the knowledge base.",1805.01216v3.pdf,"['1805.01216v3.pdf', '1707.01922v5.pdf', '1901.00056v2.pdf', '1705.02946v3.pdf', '1811.02721v3.pdf', '1811.02553v4.pdf', '1803.03467v4.pdf', '1705.08016v3.pdf', '1812.00281v3.pdf', '1811.09393v4.pdf', '1704.05958v2.pdf']","The table shows that for tasks 3 and 5, the multi-hop encoder achieves higher accuracy compared to the 1-hop encoder. This improvement aligns with the nature of these tasks, which require multi-step reasoning and inference over multiple KB entries. The passage further clarifies this connection by highlighting the need for sorting and recommendation based on various restaurant attributes, achievable through the multi-hop encoder's ability to capture complex relationships within the knowledge base.",1805.01216v3-Table14-1.png,Disentangling Language and Knowledge in Task-Oriented Dialogs,"The Knowledge Base (KB) used for real-world applications, such as booking a
movie or restaurant reservation, keeps changing over time. End-to-end neural
networks trained for these task-oriented dialogs are expected to be immune to
any changes in the KB. However, existing approaches breakdown when asked to
handle such changes. We propose an encoder-decoder architecture (BoSsNet) with
a novel Bag-of-Sequences (BoSs) memory, which facilitates the disentangled
learning of the response's language model and its knowledge incorporation.
Consequently, the KB can be modified with new knowledge without a drop in
interpretability. We find that BoSsNet outperforms state-of-the-art models,
with considerable improvements (> 10\%) on bAbI OOV test sets and other
human-human datasets. We also systematically modify existing datasets to
measure disentanglement and show BoSsNet to be robust to KB modifications.",Table 14: Ablation study: impact of hops in BOSSNET encoder,"Can you explain why the BOSSNET with multi-hop encoder performs better on bAbI tasks 3 and 5 compared to the 1-hop encoder, and how this relates to the tasks themselves?"
spiqa_23,1709.08294v3,"In the context of Table 2, how do the authors demonstrate that their S-ACNN model with a single filter achieves better performance than the S-CNN on the Yelp P. and DBpedia datasets, and how do they justify the claim that the adaptive filter mechanism makes S-ACNN more expressive in capturing sentence-specific features, despite not achieving the lowest overall test error rates?","The authors claim that S-ACNN is more expressive than S-CNN because, despite having only one filter, it significantly outperforms S-CNN on both datasets. This suggests that the filter-generation module in ACNN allows for greater flexibility and adaptability, enabling the model to better capture the specific features of each sentence.",1709.08294v3.pdf,"['1709.08294v3.pdf', '1809.03149v2.pdf', '1606.07384v2.pdf', '1805.04687v2.pdf', '1812.00108v4.pdf']","Table~\ref{tab:topic} shows the test error rates for different models on the Yelp P. and DBpedia datasets. While S-ACNN doesn't achieve the lowest error rate overall, it shows a substantial improvement over S-CNN, which also uses a single filter. This relative improvement suggests that the adaptivity of the filter in S-ACNN allows it to generate more relevant and informative representations for different sentences, leading to better performance even with limited modeling capacity.",1709.08294v3-Table2-1.png,Learning Context-Sensitive Convolutional Filters for Text Processing,"Convolutional neural networks (CNNs) have recently emerged as a popular
building block for natural language processing (NLP). Despite their success,
most existing CNN models employed in NLP share the same learned (and static)
set of filters for all input sentences. In this paper, we consider an approach
of using a small meta network to learn context-sensitive convolutional filters
for text processing. The role of meta network is to abstract the contextual
information of a sentence or document into a set of input-aware filters. We
further generalize this framework to model sentence pairs, where a
bidirectional filter generation mechanism is introduced to encapsulate
co-dependent sentence representations. In our benchmarks on four different
tasks, including ontology classification, sentiment analysis, answer sentence
selection, and paraphrase identification, our proposed model, a modified CNN
with context-sensitive filters, consistently outperforms the standard CNN and
attention-based CNN baselines. By visualizing the learned context-sensitive
filters, we further validate and rationalize the effectiveness of proposed
framework.","Table 2: Test error rates on document classification tasks (in percentages). S-model indicates that the model has one single convolutional filter, while M-model indicates that the model has multiple convolutional filters. Results marked with ∗ are reported by (Zhang et al., 2015), † are reported by (Conneau et al., 2016), and ‡ are reported by (Lin et al., 2017).","Can you explain why the authors claim that their S-ACNN model with a single filter is ""much more expressive"" than the basic S-CNN model, even though it doesn't achieve the best overall performance on either dataset?"
spiqa_24,1710.06177v2,"Why does Table 2 in the ""Learning to Learn Image Classifiers with Visual Analogy"" paper show that the transfer learning-based VAGER model underperforms compared to LR specifically for the ""Bubble"" class in the 1-shot binary classification setting, while it demonstrates superior accuracy across the other novel classes?","VAGER leverages transfer learning, while LR does not. This means VAGER attempts to apply knowledge from other classes to improve its performance on new classes. For nine out of the ten novel classes, this strategy seems to be successful, as VAGER consistently outperforms LR. However, for the ""Bubble"" class, the transfer learning approach seems to have a negative impact, causing VAGER to perform worse than LR.",1710.06177v2.pdf,"['1710.06177v2.pdf', '1611.07718v2.pdf', '1705.10667v4.pdf', '1708.05239v3.pdf', '1612.02803v5.pdf', '1704.00774v3.pdf', '1603.00286v5.pdf']","Table 1 directly compares the performance of VAGER and LR for each class. While VAGER shows higher accuracy for most classes, the ""Bubble"" class is a clear exception, with LR achieving a higher accuracy. This observation, combined with the passage's information about VAGER's use of transfer learning, suggests that the transferred knowledge might be detrimental for classifying ""Bubble"" images in this specific scenario.",1710.06177v2-Table2-1.png,Learning to Learn Image Classifiers with Visual Analogy,"Humans are far better learners who can learn a new concept very fast with
only a few samples compared with machines. The plausible mystery making the
difference is two fundamental learning mechanisms: learning to learn and
learning by analogy. In this paper, we attempt to investigate a new human-like
learning method by organically combining these two mechanisms. In particular,
we study how to generalize the classification parameters from previously
learned concepts to a new concept. we first propose a novel Visual Analogy
Graph Embedded Regression (VAGER) model to jointly learn a low-dimensional
embedding space and a linear mapping function from the embedding space to
classification parameters for base classes. We then propose an out-of-sample
embedding method to learn the embedding of a new class represented by a few
samples through its visual analogy with base classes and derive the
classification parameters for the new class. We conduct extensive experiments
on ImageNet dataset and the results show that our method could consistently and
significantly outperform state-of-the-art baselines.",Table 2. Comparison of VAGER and LR over novel classes with 1-shot binary classification setting,"Can you explain why the performance of VAGER is worse than LR for the ""Bubble"" class in the 1-shot binary classification setting, while it performs better for the other nine classes?"
spiqa_25,1805.06447v3,"In Table 7 of the ""Resisting Large Data Variations via Introspective Transformation Network"" paper, how does the error rate of ITN (B-CNN) on the MNIST dataset change as the update threshold (Tu) increases, specifically from 1e-3 to 1e-1?",The performance of ITN (B-CNN) on the MNIST dataset decreases as the update threshold (Tu) increases. This is evident from the increasing ITN error percentages as Tu goes from 1e-3 to 1e-1.,1805.06447v3.pdf,"['1805.06447v3.pdf', '1703.00060v2.pdf', '1706.04284v3.pdf', '1611.02654v2.pdf', '1701.06171v4.pdf', '1703.07015v3.pdf']","Table 1 explicitly shows the ITN error for various Tu values. As we move down the table, Tu increases, and correspondingly, the ITN error also increases. This trend indicates an inverse relationship between Tu and the performance of ITN. The passage further explains that this performance drop is due to the decrease in the quality of generated samples when the threshold is increased.",1805.06447v3-Table7-1.png,Resisting Large Data Variations via Introspective Transformation Network,"Training deep networks that generalize to a wide range of variations in test
data is essential to building accurate and robust image classifiers. One
standard strategy is to apply data augmentation to synthetically enlarge the
training set. However, data augmentation is essentially a brute-force method
which generates uniform samples from some pre-defined set of transformations.
In this paper, we propose a principled approach to train networks with
significantly improved resistance to large variations between training and
testing data. This is achieved by embedding a learnable transformation module
into the introspective network, which is a convolutional neural network (CNN)
classifier empowered with generative capabilities. Our approach alternates
between synthesizing pseudo-negative samples and transformed positive examples
based on the current model, and optimizing model predictions on these
synthesized samples. Experimental results verify that our approach
significantly improves the ability of deep networks to resist large variations
between training and testing data and achieves classification accuracy
improvements on several benchmark datasets, including MNIST, affNIST, SVHN,
CIFAR-10 and miniImageNet.",Table 7. Testing errors of ITN (B-CNN) with various thresholds on MNIST.,Describe the relationship between the update threshold (Tu) and the performance of ITN (B-CNN) on the MNIST dataset.
spiqa_26,1803.01128v3,"Referring to the perplexity scores displayed in Table 6, do the adversarial examples generated with the 2-keyword constraint significantly deviate from the original syntactic structure as inferred from the language model's predictability?","No, adversarial examples generated with the 2-keyword constraint deviate significantly from the original syntactic structure.",1803.01128v3.pdf,"['1803.01128v3.pdf', '1707.01922v5.pdf', '1706.04284v3.pdf', '1804.04410v2.pdf', '1803.04572v2.pdf', '1708.05239v3.pdf', '1809.00263v5.pdf', '1709.08294v3.pdf', '1809.04276v2.pdf', '1701.06171v4.pdf', '1805.00912v4.pdf']","The table shows the perplexity scores of different types of adversarial examples. Perplexity measures how well a language model predicts a given text, with lower perplexity indicating better prediction and thus a more natural and expected syntactic structure. While the original sentences and those with non-overlap or 1-keyword constraints have relatively low perplexity scores, the 2-keyword examples show a dramatic increase in perplexity. This suggests that these examples deviate significantly from the original syntactic structure and are less predictable by the language model.",1803.01128v3-Table6-1.png,Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples,"Crafting adversarial examples has become an important technique to evaluate
the robustness of deep neural networks (DNNs). However, most existing works
focus on attacking the image classification problem since its input space is
continuous and output space is finite.
  In this paper, we study the much more challenging problem of crafting
adversarial examples for sequence-to-sequence (seq2seq) models, whose inputs
are discrete text strings and outputs have an almost infinite number of
possibilities. To address the challenges caused by the discrete input space, we
propose a projected gradient method combined with group lasso and gradient
regularization. To handle the almost infinite output space, we design some
novel loss functions to conduct non-overlapping attack and targeted keyword
attack. We apply our algorithm to machine translation and text summarization
tasks, and verify the effectiveness of the proposed algorithm: by changing less
than 3 words, we can make seq2seq model to produce desired outputs with high
success rates. On the other hand, we recognize that, compared with the
well-evaluated CNN-based classifiers, seq2seq models are intrinsically more
robust to adversarial attacks.",Table 6: Perplexity score for adversarial example,Do adversarial examples generated with the 2-keyword constraint maintain a similar syntactic structure to the original sentences?
spiqa_27,1708.01425v4,"Does the figure in the paper on the argument reasoning comprehension task indicate that formal training in reasoning, logic, or argumentation leads to a statistically significant improvement in accuracy for participants with graduate degrees?","No, it does not appear to have a significant effect.",1708.01425v4.pdf,"['1708.01425v4.pdf', '1706.00827v2.pdf', '1705.02798v6.pdf', '1804.05995v2.pdf', '1702.03584v3.pdf', '1708.02153v2.pdf', '1706.03847v3.pdf', '1705.02946v3.pdf', '1804.05938v2.pdf', '1805.07567v2.pdf']","The figure shows that the accuracy for people with graduate degrees who have no training, some training, and extensive training is all around 80%. The error bars for these groups also overlap, indicating that the differences between the groups are not statistically significant.",1708.01425v4-Figure4-1.png,The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants,"Reasoning is a crucial part of natural language argumentation. To comprehend
an argument, one must analyze its warrant, which explains why its claim follows
from its premises. As arguments are highly contextualized, warrants are usually
presupposed and left implicit. Thus, the comprehension does not only require
language understanding and logic skills, but also depends on common sense. In
this paper we develop a methodology for reconstructing warrants systematically.
We operationalize it in a scalable crowdsourcing process, resulting in a freely
licensed dataset with warrants for 2k authentic arguments from news comments.
On this basis, we present a new challenging task, the argument reasoning
comprehension task. Given an argument with a claim and a premise, the goal is
to choose the correct implicit warrant from two options. Both warrants are
plausible and lexically close, but lead to contradicting claims. A solution to
this task will define a substantial step towards automatic warrant
reconstruction. However, experiments with several neural attention and language
models reveal that current approaches do not suffice.","Human upper bounds on the argument reasoning comprehension task with respect to education and formal training in reasoning, logic, or argumentation. For each configuration, the mean values are displayed together with the number of participants (above the bar) and with their standard deviations (error bars).","Does formal training in reasoning, logic, or argumentation seem to have a significant effect on argument reasoning comprehension accuracy for people with graduate degrees?"
spiqa_28,1611.07718v2,"Based on the figure comparing merge-and-run mappings and identity mappings, does sharing the first convolutional layer and the last fully connected layer lead to better accuracy for CIFAR-10 in the merge-and-run network?",Yes.,1611.07718v2.pdf,"['1611.07718v2.pdf', '1707.08608v3.pdf', '1703.07015v3.pdf', '1809.00263v5.pdf', '1709.02418v2.pdf', '1804.07707v2.pdf', '1705.07384v2.pdf', '1804.05936v2.pdf', '1811.10673v1.pdf']","The table shows that the accuracy of the merge-and-run mapping is higher when the first convolutional layer and the last fully connected layer are shared. For example, on CIFAR-10, the accuracy of the merge-and-run mapping is 4.99% with sharing and 4.41% without sharing.",1611.07718v2-Table5-1.png,Deep Convolutional Neural Networks with Merge-and-Run Mappings,"A deep residual network, built by stacking a sequence of residual blocks, is
easy to train, because identity mappings skip residual branches and thus
improve information flow. To further reduce the training difficulty, we present
a simple network architecture, deep merge-and-run neural networks. The novelty
lies in a modularized building block, merge-and-run block, which assembles
residual branches in parallel through a merge-and-run mapping: Average the
inputs of these residual branches (Merge), and add the average to the output of
each residual branch as the input of the subsequent residual branch (Run),
respectively. We show that the merge-and-run mapping is a linear idempotent
function in which the transformation matrix is idempotent, and thus improves
information flow, making training easy. In comparison to residual networks, our
networks enjoy compelling advantages: they contain much shorter paths, and the
width, i.e., the number of channels, is increased. We evaluate the performance
on the standard recognition tasks. Our approach demonstrates consistent
improvements over ResNets with the comparable setup, and achieves competitive
results (e.g., $3.57\%$ testing error on CIFAR-$10$, $19.00\%$ on CIFAR-$100$,
$1.51\%$ on SVHN).",Comparison between merge-and-run mappings and identity mappings. Sharing = share the first conv. and the last FC.,Does sharing the first convolutional layer and the last fully connected layer improve the accuracy of the merge-and-run mapping?
spiqa_29,1803.04383v2,"Based on the empirical payback rates shown in Figure 4 of the ""Delayed Impact of Fair Machine Learning"" paper, how does the probability of debt repayment change with varying credit scores for both black and white groups in the TransUnion TransRisk dataset?",The probability of repaying a debt increases with credit score.,1803.04383v2.pdf,"['1803.04383v2.pdf', '1709.02755v5.pdf', '1811.02553v4.pdf', '1701.06171v4.pdf', '1804.07849v4.pdf', '1611.02654v2.pdf', '1708.00160v2.pdf', '1811.08481v2.pdf', '1809.01989v2.pdf']",The figure shows that the empirical payback rates for both black and white groups increase with credit score. This is because individuals with higher credit scores are more likely to be able to repay their debts than individuals with lower credit scores.,1803.04383v2-Figure4-1.png,Delayed Impact of Fair Machine Learning,"Fairness in machine learning has predominantly been studied in static
classification settings without concern for how decisions change the underlying
population over time. Conventional wisdom suggests that fairness criteria
promote the long-term well-being of those groups they aim to protect.
  We study how static fairness criteria interact with temporal indicators of
well-being, such as long-term improvement, stagnation, and decline in a
variable of interest. We demonstrate that even in a one-step feedback model,
common fairness criteria in general do not promote improvement over time, and
may in fact cause harm in cases where an unconstrained objective would not.
  We completely characterize the delayed impact of three standard criteria,
contrasting the regimes in which these exhibit qualitatively different
behavior. In addition, we find that a natural form of measurement error
broadens the regime in which fairness criteria perform favorably.
  Our results highlight the importance of measurement and temporal modeling in
the evaluation of fairness criteria, suggesting a range of new challenges and
trade-offs.",Figure 4: The empirical payback rates as a function of credit score and CDF for both groups from the TransUnion TransRisk dataset.,Does the probability of repaying a debt increase or decrease with credit score?
spiqa_30,1802.07222v1,"According to the figure titled ""Number of network related reboots in a day"" in the paper *""007: Democratically Finding The Cause of Packet Drops,""* which specific time window experienced the highest number of network-related reboots during the deployment in the tier-1 datacenter?",The most network-related reboots occurred between 18:00 and 20:00.,1802.07222v1.pdf,"['1802.07222v1.pdf', '1705.02946v3.pdf', '1611.04363v2.pdf', '1706.08146v3.pdf']","The plot shows the number of network-related reboots as a function of the hour of the day. The peak of the plot occurs between 18:00 and 20:00, indicating that the most reboots occurred during this time period.",1802.07222v1-Figure14-1.png,007: Democratically Finding The Cause of Packet Drops,"Network failures continue to plague datacenter operators as their symptoms
may not have direct correlation with where or why they occur. We introduce 007,
a lightweight, always-on diagnosis application that can find problematic links
and also pinpoint problems for each TCP connection. 007 is completely contained
within the end host. During its two month deployment in a tier-1 datacenter, it
detected every problem found by previously deployed monitoring tools while also
finding the sources of other problems previously undetected.",Number of network related reboots in a day.,During which hours of the day did the most network-related reboots occur?
spiqa_31,1804.05938v2,"Based on the feature descriptions in Table 2, how do the TF-IDF and BM25 features differ in their approach to estimating document relevance in the context of the ""Unbiased Learning to Rank with Unbiased Propensity Estimation"" framework, and what implications might these differences have for the experiments conducted in this paper?","Both TF-IDF and BM25 are features used to estimate the relevance of a document to a query. However, they differ in their underlying calculations.

TF-IDF: This feature represents the average product of term frequency (TF) and inverse document frequency (IDF) for each query term within different document sections (URL, title, content, and whole document). TF measures how often a term appears in a specific document section, while IDF measures how important that term is across the entire document collection.

BM25: This feature utilizes the BM25 ranking function, which is a probabilistic model that considers term frequency, document length, and average document length to estimate relevance. While it also considers term frequency like TF-IDF, it incorporates additional factors to improve the weighting scheme.",1804.05938v2.pdf,"['1804.05938v2.pdf', '1706.04284v3.pdf', '1708.03797v1.pdf', '1707.01922v5.pdf', '1703.10730v2.pdf', '1708.05239v3.pdf']","Table 1 provides a summary of the ranking features extracted for the experiments. It explicitly mentions that TF-IDF is based on the average TF*IDF value, while BM25 utilizes the BM25 ranking function. This distinction highlights the different approaches used by each feature to assess document relevance.",1804.05938v2-Table2-1.png,Unbiased Learning to Rank with Unbiased Propensity Estimation,"Learning to rank with biased click data is a well-known challenge. A variety
of methods has been explored to debias click data for learning to rank such as
click models, result interleaving and, more recently, the unbiased
learning-to-rank framework based on inverse propensity weighting. Despite their
differences, most existing studies separate the estimation of click bias
(namely the \textit{propensity model}) from the learning of ranking algorithms.
To estimate click propensities, they either conduct online result
randomization, which can negatively affect the user experience, or offline
parameter estimation, which has special requirements for click data and is
optimized for objectives (e.g. click likelihood) that are not directly related
to the ranking performance of the system. In this work, we address those
problems by unifying the learning of propensity models and ranking models. We
find that the problem of estimating a propensity model from click data is a
dual problem of unbiased learning to rank. Based on this observation, we
propose a Dual Learning Algorithm (DLA) that jointly learns an unbiased ranker
and an \textit{unbiased propensity model}. DLA is an automatic unbiased
learning-to-rank framework as it directly learns unbiased ranking models from
biased click data without any preprocessing. It can adapt to the change of bias
distributions and is applicable to online learning. Our empirical experiments
with synthetic and real-world data show that the models trained with DLA
significantly outperformed the unbiased learning-to-rank algorithms based on
result randomization and the models trained with relevance signals extracted by
click models.",Table 2: A summary of the ranking features extracted for our real-world experiments.,"Explain the difference between the features ""TF-IDF"" and ""BM25""."
spiqa_32,1809.00263v5,"In Table 1 of the *Stochastic Dynamics for Video Infilling* paper, what are the specific missing loss terms in the ""SDVI loss term 1&3"" model that could explain its inferior PSNR and SSIM performance across all datasets compared to the full SDVI model, and how do these missing components impact video frame regeneration quality?","The ""SDVI loss term 1&3"" model only uses the pixel reconstruction loss and the inclusive KL divergence loss, while the full SDVI model additionally incorporates the pixel prediction loss and the exclusive KL divergence loss. According to the passage, the exclusive KL divergence term encourages the inference distribution to be more accurate, while the pixel prediction loss further improves video quality during inference. Therefore, the absence of these terms in the ""SDVI loss term 1&3"" model likely explains its inferior performance compared to the full SDVI model.",1809.00263v5.pdf,"['1809.00263v5.pdf', '1603.03833v4.pdf', '1802.07351v2.pdf', '1707.01917v2.pdf', '1611.05742v3.pdf']","Table 1 shows that the full SDVI model consistently achieves higher PSNR and SSIM values than the ""SDVI loss term 1\&3"" model across all datasets. This observation suggests that the additional loss terms in the full model contribute to improved reconstruction quality and video quality during inference. The passage provides the theoretical justification for including these additional terms, specifically highlighting their roles in promoting accuracy and diversity in the inference distribution and enhancing video quality. By comparing the performance of the two models and referencing the detailed explanation of the loss function in the passage, we can understand how the different loss terms contribute to the overall performance of the SDVI model.",1809.00263v5-Table1-1.png,Stochastic Dynamics for Video Infilling,"In this paper, we introduce a stochastic dynamics video infilling (SDVI)
framework to generate frames between long intervals in a video. Our task
differs from video interpolation which aims to produce transitional frames for
a short interval between every two frames and increase the temporal resolution.
Our task, namely video infilling, however, aims to infill long intervals with
plausible frame sequences. Our framework models the infilling as a constrained
stochastic generation process and sequentially samples dynamics from the
inferred distribution. SDVI consists of two parts: (1) a bi-directional
constraint propagation module to guarantee the spatial-temporal coherence among
frames, (2) a stochastic sampling process to generate dynamics from the
inferred distributions. Experimental results show that SDVI can generate clear
frame sequences with varying contents. Moreover, motions in the generated
sequence are realistic and able to transfer smoothly from the given start frame
to the terminal frame. Our project site is
https://xharlie.github.io/projects/project_sites/SDVI/video_results.html",Table 1: Metrics averaging over all 7 intermediate frames. We report the scores of the best-sampled sequences for SDVI.,"Explain the likely reason why the ""SDVI loss term 1&3"" model performs worse than the full SDVI model in terms of PSNR and SSIM across all datasets."
spiqa_33,1708.02153v2,"In the context of Table 4's results and the MIM influence measurement framework discussed in the ""*Axiomatic Characterization of Data-Driven Influence Measures for Classification*,"" how can the ""Last contact"" feature exhibit a high positive influence on the SSL score, even though it is not directly utilized by the SSL algorithm?","The ""Last contact"" feature shows a strong positive influence on the SSL score because it is likely correlated with other features that are used by the algorithm. As the passage mentions, data-driven methods like MIM can assign influence to features that are not directly used by the model if they are correlated with other influential features. In this case, a recent ""Last contact"" date might be correlated with a higher number of recent offenses or other factors that contribute to a higher SSL score.",1708.02153v2.pdf,"['1708.02153v2.pdf', '1812.00281v3.pdf', '1707.01917v2.pdf', '1705.09966v2.pdf', '1906.10843v1.pdf', '1703.10730v2.pdf']","Table 1 shows the ""Infl."" value for ""Last contact"" is positive and relatively large in both examples (45.32 and 46.2), indicating a significant positive influence on the SSL score. However, the passage clarifies that the SSL algorithm itself does not directly use this feature. This apparent contradiction can be explained by the fact that ""Last contact"" might be correlated with other features that are used by the algorithm and contribute to a higher score.",1708.02153v2-Table4-1.png,Axiomatic Characterization of Data-Driven Influence Measures for Classification,"We study the following problem: given a labeled dataset and a specific
datapoint x, how did the i-th feature influence the classification for x? We
identify a family of numerical influence measures - functions that, given a
datapoint x, assign a numeric value phi_i(x) to every feature i, corresponding
to how altering i's value would influence the outcome for x. This family, which
we term monotone influence measures (MIM), is uniquely derived from a set of
desirable properties, or axioms. The MIM family constitutes a provably sound
methodology for measuring feature influence in classification domains; the
values generated by MIM are based on the dataset alone, and do not make any
queries to the classifier. While this requirement naturally limits the scope of
our framework, we demonstrate its effectiveness on data.","Table 4: Two example results of MIM influence measurement. Age is given in decades, features 5, 11, 12 and 13 are binary.","Explain why the ""Last contact"" feature has a significant positive influence on the SSL score in both examples, even though it is not directly used by the SSL algorithm."
spiqa_34,1805.04687v2,"Referring to Table 7 in the BDD100K paper, why does the model trained on both the detection and MOT sets (""MOT + Det"") exhibit a higher number of identity switches (IDS), despite improving in other tracking metrics like AP, MOTA, and MOTP?","While the model trained on both MOT and detection sets shows improved performance in detection and tracking metrics (AP, MOTA, MOTP), it also exhibits a higher number of identity switches (IDS). This can be attributed to the increased diversity of instances introduced by the detection set. Although the MOT set provides a larger number of bounding boxes for training, the detection set adds varied examples that may lead to more frequent identity switches during tracking, even as it improves the model's overall performance.",1805.04687v2.pdf,"['1805.04687v2.pdf', '1705.02798v6.pdf', '1708.06832v3.pdf', '1705.09882v2.pdf', '1708.05239v3.pdf', '1804.05936v2.pdf', '1708.03797v1.pdf', '1809.01246v1.pdf', '1611.04684v1.pdf', '1709.02755v5.pdf', '1811.08257v1.pdf', '1803.02750v3.pdf', '1706.00827v2.pdf', '1611.05742v3.pdf', '1705.09296v2.pdf']","The table shows that the model trained on both MOT and detection sets (""MOT + Det"") has a higher IDS value (9098) compared to the model trained only on the MOT set (8386). This might seem contradictory at first, given the improvement in other metrics. However, the passage provides the key insight: the detection set introduces more diverse examples. This diversity can lead to situations where the model struggles to maintain consistent identities across frames, resulting in more identity switches.",1805.04687v2-Table7-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.","Table 7: Evaluation results for multiple object tracking cascaded with object detection. AP is the detection metric. Even though the tracking set has much more boxes, the model can still benefit from the diverse instance examples in the detection set.","Explain why the model trained on both MOT and detection sets has a higher number of identity switches (IDS) compared to the model trained only on the MOT set, even though it achieves better performance in terms of AP, MOTA, and MOTP?"
spiqa_35,1702.08694v3,"According to Figure 4 of the paper, how do the upper bound of the KL divergence (left panel) and the corresponding p-value (right panel) behave as $a$ increases when $b = 0.3$ and $N = 100$, and what insights can be drawn regarding the statistical significance of higher-order feature interactions from this behavior?","The maximum achievable KL divergence initially increases with increasing values of $a$ until it reaches a peak. Then, it decreases with increasing values of $a$. The minimum achievable p-value initially decreases with increasing values of $a$ until it reaches a minimum. Then, it increases with increasing values of $a$.",1702.08694v3.pdf,"['1702.08694v3.pdf', '1809.03550v3.pdf', '1703.10730v2.pdf', '1804.05938v2.pdf', '1901.00056v2.pdf', '1705.07164v8.pdf', '1804.04410v2.pdf', '1705.09882v2.pdf']","The left panel of the figure shows the maximum achievable KL divergence as a function of $a$ for a fixed value of $b$. The right panel shows the corresponding minimum achievable p-value. The KL divergence initially increases and then decreases with increasing values of $a$, while the p-value initially decreases and then increases. This behavior is consistent with the analysis in the passage, which shows that the KL divergence is monotonically increasing for $a < b$ and monotonically decreasing for $b < a < 1/2$.",1702.08694v3-Figure4-1.png,Finding Statistically Significant Interactions between Continuous Features,"The search for higher-order feature interactions that are statistically
significantly associated with a class variable is of high relevance in fields
such as Genetics or Healthcare, but the combinatorial explosion of the
candidate space makes this problem extremely challenging in terms of
computational efficiency and proper correction for multiple testing. While
recent progress has been made regarding this challenge for binary features, we
here present the first solution for continuous features. We propose an
algorithm which overcomes the combinatorial explosion of the search space of
higher-order interactions by deriving a lower bound on the p-value for each
interaction, which enables us to massively prune interactions that can never
reach significance and to thereby gain more statistical power. In our
experiments, our approach efficiently detects all significant interactions in a
variety of synthetic and real-world datasets.","Figure 4: The upper bound B(a, b) of the KL divergence (left) and the corresponding p-value (right) with N = 100 with respect to changes in a when b = 0.3.","For a fixed value of $b$, how does the maximum achievable KL divergence and the corresponding minimum achievable p-value change with increasing values of $a$?"
spiqa_36,1804.04410v2,"How does the telescoping architecture in Bing’s retrieval system, as illustrated in Figure 1, implement the rank-and-prune process across stages L1 and L2, after documents are initially matched in stage L0 with a pre-defined match plan?","Documents are first matched using a pre-defined match plan. Then, they are passed through additional rank-and-prune stages, which are implemented as a cascade of machine learning models.",1804.04410v2.pdf,"['1804.04410v2.pdf', '1901.00056v2.pdf', '1809.04276v2.pdf', '1707.08608v3.pdf', '1811.08481v2.pdf', '1803.02750v3.pdf', '1802.07351v2.pdf', '1812.00281v3.pdf', '1804.04786v3.pdf', '1706.04269v2.pdf', '1812.10735v2.pdf', '1611.07718v2.pdf', '1708.00160v2.pdf', '1809.03149v2.pdf']","The figure shows the telescoping architecture, with the matching stage (L0) at the bottom and the rank-and-prune stages (L1, L2) above it. The arrows indicate the flow of documents through the system.",1804.04410v2-Figure1-1.png,Optimizing Query Evaluations using Reinforcement Learning for Web Search,"In web search, typically a candidate generation step selects a small set of
documents---from collections containing as many as billions of web pages---that
are subsequently ranked and pruned before being presented to the user. In Bing,
the candidate generation involves scanning the index using statically designed
match plans that prescribe sequences of different match criteria and stopping
conditions. In this work, we pose match planning as a reinforcement learning
task and observe up to 20% reduction in index blocks accessed, with small or no
degradation in the quality of the candidate sets.",Figure 1: A telescoping architecture employed in Bing’s retrieval system. Documents are scanned using a pre-defined match plan. Matched documents are passed through additional rank-and-prune stages.,How are documents ranked and pruned in the telescoping architecture?
spiqa_37,1704.07121v2,How does Figure 1 in the paper illustrate how the QoU and IoU procedures remedy selection bias in the Visual7W dataset by creating alternative decoys that force the model to equally consider both the image and the question?,"The shortcuts in the Visual7W dataset can be remedied by creating alternative decoys that are more likely to be correct, based on either the image or the question alone. This forces the machine to consider all of the information together in order to select the correct answer.",1704.07121v2.pdf,"['1704.07121v2.pdf', '1804.00863v3.pdf', '1811.07073v3.pdf', '1704.07854v4.pdf', '1805.06431v4.pdf', '1708.03797v1.pdf', '1802.07222v1.pdf', '1706.00633v4.pdf', '1705.09882v2.pdf', '1703.04887v4.pdf', '1704.05426v4.pdf']","The figure shows an example of how the shortcuts in the Visual7W dataset can be remedied. In the original dataset, the correct answer ""A train"" is much more likely to be chosen than the other decoy answers. However, by using the QU and IU procedures, alternative decoys can be created that are just as likely to be correct as the original answer. This forces the machine to consider all of the information together in order to select the correct answer. 

Figure type: Schematic",1704.07121v2-Figure1-1.png,Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets,"Visual question answering (Visual QA) has attracted a lot of attention
lately, seen essentially as a form of (visual) Turing test that artificial
intelligence should strive to achieve. In this paper, we study a crucial
component of this task: how can we design good datasets for the task? We focus
on the design of multiple-choice based datasets where the learner has to select
the right answer from a set of candidate ones including the target (\ie the
correct one) and the decoys (\ie the incorrect ones). Through careful analysis
of the results attained by state-of-the-art learning models and human
annotators on existing datasets, we show that the design of the decoy answers
has a significant impact on how and what the learning models learn from the
datasets. In particular, the resulting learner can ignore the visual
information, the question, or both while still doing well on the task. Inspired
by this, we propose automatic procedures to remedy such design deficiencies. We
apply the procedures to re-construct decoy answers for two popular Visual QA
datasets as well as to create a new Visual QA dataset from the Visual Genome
project, resulting in the largest dataset for this task. Extensive empirical
studies show that the design deficiencies have been alleviated in the remedied
datasets and the performance on them is likely a more faithful indicator of the
difference among learning models. The datasets are released and publicly
available via http://www.teds.usc.edu/website_vqa/.","Figure 1: An illustration of how the shortcuts in the Visual7W dataset (Zhu et al., 2016) should be remedied. In the original dataset, the correct answer “A train” is easily selected by a machine as it is far often used as the correct answer than the other decoy (negative) answers. (The numbers in the brackets are probability scores computed using eq. (2)). Our two procedures — QoU and IoU (cf. Sect. 4) — create alternative decoys such that both the correct answer and the decoys are highly likely by examining either the image or the question alone. In these cases, machines make mistakes unless they consider all information together. Thus, the alternative decoys suggested our procedures are better designed to gauge how well a learning algorithm can understand all information equally well.",How can the shortcuts in the Visual7W dataset be remedied?
spiqa_38,1707.08608v3,"In the sequence transduction example depicted in the figure, where errors are highlighted in red, how did the model's accuracy improve from 66.7% to 100% as iterations progressed through the application of gradient-based inference to enforce output constraints?",The accuracy of the model increased from 66.7% to 100% as the iterations progressed.,1707.08608v3.pdf,"['1707.08608v3.pdf', '1708.00160v2.pdf', '1901.00398v2.pdf', '1603.03833v4.pdf']",The table shows that the accuracy of the model increased from 66.7% to 100% as the iterations progressed.,1707.08608v3-Table9-1.png,Gradient-based Inference for Networks with Output Constraints,"Practitioners apply neural networks to increasingly complex problems in
natural language processing, such as syntactic parsing and semantic role
labeling that have rich output structures. Many such structured-prediction
problems require deterministic constraints on the output values; for example,
in sequence-to-sequence syntactic parsing, we require that the sequential
outputs encode valid trees. While hidden units might capture such properties,
the network is not always able to learn such constraints from the training data
alone, and practitioners must then resort to post-processing. In this paper, we
present an inference method for neural networks that enforces deterministic
constraints on outputs without performing rule-based post-processing or
expensive discrete search. Instead, in the spirit of gradient-based training,
we enforce constraints with gradient-based inference (GBI): for each input at
test-time, we nudge continuous model weights until the network's unconstrained
inference procedure generates an output that satisfies the constraints. We
study the efficacy of GBI on three tasks with hard constraints: semantic role
labeling, syntactic parsing, and sequence transduction. In each case, the
algorithm not only satisfies constraints but improves accuracy, even when the
underlying network is state-of-the-art.",A sequence transduction example for which enforcing the constraints improves accuracy. Red indicates errors.,How did the accuracy of the model change as the iterations progressed?
spiqa_39,1811.02553v4,"Based on the cosine similarity analysis in the figure, how do TRPO and PPO differ in their ability to estimate the true gradient with fewer state-action pairs, and what does this imply about their convergence behavior?",TRPO generally converges faster to the true gradient than PPO.,1811.02553v4.pdf,"['1811.02553v4.pdf', '1906.06589v3.pdf', '1704.07854v4.pdf', '1709.08294v3.pdf', '1706.00633v4.pdf', '1803.01128v3.pdf', '1703.02507v3.pdf']","The figure shows that the cosine similarity between the true gradient and the gradient estimates for TRPO is generally higher than that for PPO, for a given number of state-action pairs. This indicates that TRPO is able to more accurately estimate the true gradient with fewer samples.",1811.02553v4-Figure10-1.png,A Closer Look at Deep Policy Gradients,"We study how the behavior of deep policy gradient algorithms reflects the
conceptual framework motivating their development. To this end, we propose a
fine-grained analysis of state-of-the-art methods based on key elements of this
framework: gradient estimation, value prediction, and optimization landscapes.
Our results show that the behavior of deep policy gradient algorithms often
deviates from what their motivating framework would predict: the surrogate
objective does not match the true reward landscape, learned value estimators
fail to fit the true value function, and gradient estimates poorly correlate
with the ""true"" gradient. The mismatch between predicted and empirical behavior
we uncover highlights our poor understanding of current methods, and indicates
the need to move beyond current benchmark-centric evaluation methods.","Convergence of gradient estimates to the “true” expected gradient (c.f. (1)). We measure the cosine similarity between the true gradient (approximated using around 1M samples) and gradient estimates, as a function of number of state-action pairs used to obtain the later. For a particular policy and state-action pair count, we obtain multiple estimates of this cosine similarity and then report the average, along with the 95% confidence intervals (shaded). Each of the colored lines (for a specific algorithm) represents a particular trained agent (we perform multiple trials with the same hyperparameter configurations but different random seeds). The dotted vertical black line (at 2K) indicates the sample regime used for gradient estimation in standard practical implementations of policy gradient methods.",How do TRPO and PPO compare in terms of convergence to the true gradient?
spiqa_40,1812.06589v2,"""According to the ablation figure in the Arbitrary Talking Face Generation paper, how does the realism of the generated faces differ between the baseline method and the proposed methods, particularly in terms of visual clarity and facial detail?""","The baseline method generates faces that are blurry and unrealistic, while the other methods generate faces that are more realistic.",1812.06589v2.pdf,"['1812.06589v2.pdf', '1611.03780v2.pdf', '1704.08615v2.pdf', '1611.04363v2.pdf', '1704.00774v3.pdf', '1803.06506v3.pdf', '1805.08751v2.pdf']","The figure shows examples of faces generated by the different methods. The baseline method generates faces that are blurry and unrealistic, while the other methods generate faces that are more realistic.",1812.06589v2-Figure6-1.png,Arbitrary Talking Face Generation via Attentional Audio-Visual Coherence Learning,"Talking face generation aims to synthesize a face video with precise lip
synchronization as well as a smooth transition of facial motion over the entire
video via the given speech clip and facial image. Most existing methods mainly
focus on either disentangling the information in a single image or learning
temporal information between frames. However, cross-modality coherence between
audio and video information has not been well addressed during synthesis. In
this paper, we propose a novel arbitrary talking face generation framework by
discovering the audio-visual coherence via the proposed Asymmetric Mutual
Information Estimator (AMIE). In addition, we propose a Dynamic Attention (DA)
block by selectively focusing the lip area of the input image during the
training stage, to further enhance lip synchronization. Experimental results on
benchmark LRW dataset and GRID dataset transcend the state-of-the-art methods
on prevalent metrics with robust high-resolution synthesizing on gender and
pose variations.",Qualitative results of ablation.,How do the different methods compare in terms of their ability to generate realistic faces?
spiqa_41,1704.07854v4,"Can you explain how the two-dimensional parameter space, as shown in the left image of the figure, represents the initial condition variations of the liquid drop in terms of its position (α1) and size (α2)?",The initial conditions of the simulations vary in two dimensions: the position of the liquid drop along the x-axis (α1) and the size of the drop (α2).,1704.07854v4.pdf,"['1704.07854v4.pdf', '1611.04684v1.pdf', '1803.04572v2.pdf', '1704.05426v4.pdf', '1811.09393v4.pdf', '1811.06635v1.pdf', '1803.04383v2.pdf', '1812.10735v2.pdf', '1901.00398v2.pdf']","The left image shows a grid of simulations with different initial conditions. The x-axis represents the position of the drop, and the y-axis represents the size of the drop.",1704.07854v4-Figure13-1.png,Generating Liquid Simulations with Deformation-aware Neural Networks,"We propose a novel approach for deformation-aware neural networks that learn
the weighting and synthesis of dense volumetric deformation fields. Our method
specifically targets the space-time representation of physical surfaces from
liquid simulations. Liquids exhibit highly complex, non-linear behavior under
changing simulation conditions such as different initial conditions. Our
algorithm captures these complex phenomena in two stages: a first neural
network computes a weighting function for a set of pre-computed deformations,
while a second network directly generates a deformation field for refining the
surface. Key for successful training runs in this setting is a suitable loss
function that encodes the effect of the deformations, and a robust calculation
of the corresponding gradients. To demonstrate the effectiveness of our
approach, we showcase our method with several complex examples of flowing
liquids with topology changes. Our representation makes it possible to rapidly
generate the desired implicit surfaces. We have implemented a mobile
application to demonstrate that real-time interactions with complex liquid
effects are possible with our approach.","The left image illustrates the initial conditions of our two dimensional parameter space setup. It consists of a set of two-dimensional liquid simulations, which vary the position of the liquid drop along x as α1, and its size as α2. The right half shows the data used for training at t = 30. Note the significant amount of variance in positions of small scale features such as the thin sheets. Both images show only a subset of the whole data.",How do the initial conditions of the simulations vary?
spiqa_42,1704.07854v4,"Based on Figure 12 of the paper, how does the simple two-layer parameter network, responsible for applying long-range, non-linear deformation fields, compare in complexity and function to the more intricate deformation network, which uses de-convolutional layers to generate dense deformation fields for refining liquid surfaces?","The parameter network is a simple structure with two fully connected layers, while the deformation network is more complex and contains two fully connected layers followed by two or more four-dimensional de-convolution layers. The parameter network learns how to apply multiple long-range, non-linear deformation fields, while the deformation network learns to generate dense deformation fields to refine the final surface.",1704.07854v4.pdf,"['1704.07854v4.pdf', '1811.08481v2.pdf', '1605.07496v3.pdf', '1811.02721v3.pdf', '1703.02507v3.pdf', '1811.02553v4.pdf', '1611.04363v2.pdf', '1708.05239v3.pdf', '1707.00189v3.pdf', '1809.01246v1.pdf', '1704.04539v2.pdf']",The figure shows the two networks side-by-side. The parameter network is on the left and is shown as a simple structure with two layers. The deformation network is on the right and is shown as a more complex structure with several layers. The figure also includes annotations that describe the function of each network.,1704.07854v4-Figure12-1.png,Generating Liquid Simulations with Deformation-aware Neural Networks,"We propose a novel approach for deformation-aware neural networks that learn
the weighting and synthesis of dense volumetric deformation fields. Our method
specifically targets the space-time representation of physical surfaces from
liquid simulations. Liquids exhibit highly complex, non-linear behavior under
changing simulation conditions such as different initial conditions. Our
algorithm captures these complex phenomena in two stages: a first neural
network computes a weighting function for a set of pre-computed deformations,
while a second network directly generates a deformation field for refining the
surface. Key for successful training runs in this setting is a suitable loss
function that encodes the effect of the deformations, and a robust calculation
of the corresponding gradients. To demonstrate the effectiveness of our
approach, we showcase our method with several complex examples of flowing
liquids with topology changes. Our representation makes it possible to rapidly
generate the desired implicit surfaces. We have implemented a mobile
application to demonstrate that real-time interactions with complex liquid
effects are possible with our approach.","Figure 12: Overview of our two neural networks. While the parameter network (left) is simple, consisting of two fully connected layers, its cost functions allows it to learn how to apply multiple long-range, non-linear deformation fields. The deformation network (right), which makes use of several de-convolutional layers, instead learns to generate dense deformation fields to refine the final surface.",How do the parameter network and the deformation network differ in terms of complexity and function?
spiqa_43,1804.00863v3,"Based on the figure that illustrates multiple materials under various illuminations and viewpoints, how closely do the reconstructions align with the original samples, particularly when comparing the bottom row of reconstructions to the top row of real-world photographs?",The reconstructions are very similar to the original samples.,1804.00863v3.pdf,"['1804.00863v3.pdf', '1803.01128v3.pdf', '1706.00633v4.pdf', '1709.02418v2.pdf', '1705.10667v4.pdf', '1603.00286v5.pdf', '1611.05742v3.pdf', '1704.07121v2.pdf', '1805.06431v4.pdf', '1805.08751v2.pdf', '1704.05426v4.pdf', '1812.06589v2.pdf', '1811.08257v1.pdf', '1809.01246v1.pdf']","The figure shows the original samples in the top row and the reconstructions in the bottom row. The reconstructions are very close to the original samples, indicating that the method is able to accurately reconstruct the objects.",1804.00863v3-Figure11-1.png,Deep Appearance Maps,"We propose a deep representation of appearance, i. e., the relation of color,
surface orientation, viewer position, material and illumination. Previous
approaches have useddeep learning to extract classic appearance
representationsrelating to reflectance model parameters (e. g., Phong)
orillumination (e. g., HDR environment maps). We suggest todirectly represent
appearance itself as a network we call aDeep Appearance Map (DAM). This is a 4D
generalizationover 2D reflectance maps, which held the view direction fixed.
First, we show how a DAM can be learned from images or video frames and later
be used to synthesize appearance, given new surface orientations and viewer
positions. Second, we demonstrate how another network can be used to map from
an image or video frames to a DAM network to reproduce this appearance, without
using a lengthy optimization such as stochastic gradient descent
(learning-to-learn). Finally, we show the example of an appearance
estimation-and-segmentation task, mapping from an image showingmultiple
materials to multiple deep appearance maps.",Real-world photo data and our reconstruction (from other views) of multiple materials (denoted M) in multiple illumination (L) from multiple views (V).,How do the reconstructions compare to the original samples?
spiqa_44,1803.04572v2,"Based on the figure comparing COPA, Helwig, and SPARTan in the ""COPA: Constrained PARAFAC2 for Sparse & Large Datasets"" paper, what specific differences in the temporal patterns of phenotype magnitude, shape, and periodicity can be observed between the sickle cell anemia patient and the leukemia patient?","The temporal patterns of phenotype magnitude differ between sickle cell anemia and leukemia patients in terms of both shape and magnitude. For sickle cell anemia patients, the patterns are generally smoother and more periodic, with lower overall magnitude. For leukemia patients, the patterns are more erratic and have higher overall magnitude.",1803.04572v2.pdf,"['1803.04572v2.pdf', '1805.01216v3.pdf', '1701.03077v10.pdf', '1706.08146v3.pdf', '1811.02721v3.pdf', '1704.05426v4.pdf', '1603.00286v5.pdf', '1812.00108v4.pdf', '1707.06320v2.pdf', '1608.02784v2.pdf', '1709.08294v3.pdf', '1605.07496v3.pdf', '1803.04383v2.pdf', '1708.03797v1.pdf', '1901.00056v2.pdf']","The figure shows the temporal patterns of phenotype magnitude for two patients, one with sickle cell anemia and one with leukemia. The patterns for the sickle cell anemia patient are smoother and more periodic, with lower overall magnitude, while the patterns for the leukemia patient are more erratic and have higher overall magnitude.",1803.04572v2-Figure8-1.png,COPA: Constrained PARAFAC2 for Sparse & Large Datasets,"PARAFAC2 has demonstrated success in modeling irregular tensors, where the
tensor dimensions vary across one of the modes. An example scenario is modeling
treatments across a set of patients with the varying number of medical
encounters over time. Despite recent improvements on unconstrained PARAFAC2,
its model factors are usually dense and sensitive to noise which limits their
interpretability. As a result, the following open challenges remain: a) various
modeling constraints, such as temporal smoothness, sparsity and non-negativity,
are needed to be imposed for interpretable temporal modeling and b) a scalable
approach is required to support those constraints efficiently for large
datasets. To tackle these challenges, we propose a {\it CO}nstrained {\it
PA}RAFAC2 (COPA) method, which carefully incorporates optimization constraints
such as temporal smoothness, sparsity, and non-negativity in the resulting
factors. To efficiently support all those constraints, COPA adopts a hybrid
optimization framework using alternating optimization and alternating direction
method of multiplier (AO-ADMM). As evaluated on large electronic health record
(EHR) datasets with hundreds of thousands of patients, COPA achieves
significant speedups (up to 36 times faster) over prior PARAFAC2 approaches
that only attempt to handle a subset of the constraints that COPA enables.
Overall, our method outperforms all the baselines attempting to handle a subset
of the constraints in terms of speed, while achieving the same level of
accuracy. Through a case study on temporal phenotyping of medically complex
children, we demonstrate how the constraints imposed by COPA reveal concise
phenotypes and meaningful temporal profiles of patients. The clinical
interpretation of both the phenotypes and the temporal profiles was confirmed
by a medical expert.","The temporal patterns extracted for two patients by COPA , Helwig, and SPARTan. The first row is associated with a patient who has sickle cell anemia while the second row is for a patient with Leukemia.",How do the temporal patterns of phenotype magnitude differ between sickle cell anemia and leukemia patients?
spiqa_45,1707.06320v2,"Based on Table 5 of the ""Learning Visually Grounded Sentence Representations"" paper, how do the Spearman correlation scores of the Cap2Img model's word embeddings compare to those of GloVe embeddings across the four semantic similarity benchmarks?",The word embeddings learned by the Cap2Img model outperform the original GloVe embeddings in terms of semantic similarity.,1707.06320v2.pdf,"['1707.06320v2.pdf', '1803.04383v2.pdf', '1603.00286v5.pdf', '1809.04276v2.pdf', '1703.10730v2.pdf']","Table 1 shows the Spearman correlation scores for different word embedding models on four standard semantic similarity benchmarks. A higher correlation score indicates better performance in capturing semantic similarity. Comparing the scores of Cap2Img and GloVe across all four benchmarks, we see that Cap2Img consistently achieves higher scores. This suggests that the grounded word projections learned by the Cap2Img model lead to word embeddings that better capture semantic relationships compared to the original GloVe embeddings.",1707.06320v2-Table5-1.png,Learning Visually Grounded Sentence Representations,"We introduce a variety of models, trained on a supervised image captioning
corpus to predict the image features for a given caption, to perform sentence
representation grounding. We train a grounded sentence encoder that achieves
good performance on COCO caption and image retrieval and subsequently show that
this encoder can successfully be transferred to various NLP tasks, with
improved performance over text-only models. Lastly, we analyze the contribution
of grounding, and show that word embeddings learned by this system outperform
non-grounded ones.",Table 5: Spearman ρs correlation on four standard semantic similarity evaluation benchmarks.,How do the word embeddings learned by the Cap2Img model compare to the original GloVe embeddings in terms of semantic similarity?
spiqa_46,1706.04269v2,"Referring to the figure depicting successful and failed action spotting attempts in the AVA and THUMOS14 videos, how does Action Search leverage temporal context from previously and subsequently searched frames to predict where the target action is most likely to occur next, especially in cases where it fails to pinpoint the exact action location?",Action Search uses temporal context to reason about where to search next by looking at the frames before and after the current frame. This allows the model to learn the temporal patterns of actions and to predict where the action is most likely to occur in the next frame.,1706.04269v2.pdf,"['1706.04269v2.pdf', '1611.05742v3.pdf', '1811.09393v4.pdf', '1812.00108v4.pdf', '1705.02946v3.pdf', '1809.00458v1.pdf', '1605.07496v3.pdf', '1811.08257v1.pdf', '1812.10735v2.pdf', '1803.01128v3.pdf', '1705.02798v6.pdf', '1703.04887v4.pdf', '1809.01246v1.pdf', '1803.04572v2.pdf']"," The figure shows the search sequences produced by Action Search for different videos. The x-axis shows the model search step, and the y-axis shows the video time. The blue dots represent the frames that the model has searched, and the green and red boxes represent the ground-truth action locations. The figure shows that the model is able to use temporal context to successfully spot the target action location in some cases, but it also fails in some cases. In the failure cases, the model often oscillates around the action without spotting frames within the exact temporal location.",1706.04269v2-Figure5-1.png,Action Search: Spotting Actions in Videos and Its Application to Temporal Action Localization,"State-of-the-art temporal action detectors inefficiently search the entire
video for specific actions. Despite the encouraging progress these methods
achieve, it is crucial to design automated approaches that only explore parts
of the video which are the most relevant to the actions being searched for. To
address this need, we propose the new problem of action spotting in video,
which we define as finding a specific action in a video while observing a small
portion of that video. Inspired by the observation that humans are extremely
efficient and accurate in spotting and finding action instances in video, we
propose Action Search, a novel Recurrent Neural Network approach that mimics
the way humans spot actions. Moreover, to address the absence of data recording
the behavior of human annotators, we put forward the Human Searches dataset,
which compiles the search sequences employed by human annotators spotting
actions in the AVA and THUMOS14 datasets. We consider temporal action
localization as an application of the action spotting problem. Experiments on
the THUMOS14 dataset reveal that our model is not only able to explore the
video efficiently (observing on average 17.3% of the video) but it also
accurately finds human activities with 30.8% mAP.","Qualitative search sequences produced by Action Search. The left column corresponds to AVA [21] testing videos, and the right column corresponds to THUMOS14 [24] testing videos. The top two rows depict examples when our model successfully spots the target action location (in green). The last row illustrate failure cases, i.e. when the action location (in red) is not spotted exactly. We observe that Action Search uses temporal context to reason about where to search next. In failure cases, we notice that our model often oscillates around actions without spotting frames within the exact temporal location.",How does Action Search use temporal context to reason about where to search next?
spiqa_47,1707.08608v3,"In Table 1 of the paper ""Gradient-based Inference for Networks with Output Constraints,"" how does GBI’s reduction in average disagreement rate for the failure set of the SRL-100 network compare to that of A*, and what are the specific percentages of reduction?","GBI is more effective than A* in reducing the disagreement rate on the SRL-100 network's failure set. After applying GBI, the average disagreement rate drops to 24.92%, while A* only reduces it to 33.91%. This represents an 19.93% greater reduction in disagreement rate when using GBI compared to A*.",1707.08608v3.pdf,"['1707.08608v3.pdf', '1612.02803v5.pdf', '1809.00263v5.pdf', '1809.00458v1.pdf', '1705.02798v6.pdf', '1704.07854v4.pdf', '1804.05938v2.pdf', '1703.02507v3.pdf', '1809.03449v3.pdf', '1702.03584v3.pdf', '1710.05654v2.pdf', '1704.05426v4.pdf', '1710.06177v2.pdf', '1709.02755v5.pdf', '1803.03467v4.pdf']","Table 1 provides the average disagreement rate on the failure set for both GBI and A* across different SRL networks. Looking specifically at the SRL-100 network, we can see that the ""after"" values under the ""Average Disagreement"" column show a larger decrease for GBI (44.85% to 24.92%) compared to A* (44.85% to 33.91%). This indicates that GBI is more successful in reducing the number of predicted spans that disagree with the true syntactic parse.",1707.08608v3-Table1-1.png,Gradient-based Inference for Networks with Output Constraints,"Practitioners apply neural networks to increasingly complex problems in
natural language processing, such as syntactic parsing and semantic role
labeling that have rich output structures. Many such structured-prediction
problems require deterministic constraints on the output values; for example,
in sequence-to-sequence syntactic parsing, we require that the sequential
outputs encode valid trees. While hidden units might capture such properties,
the network is not always able to learn such constraints from the training data
alone, and practitioners must then resort to post-processing. In this paper, we
present an inference method for neural networks that enforces deterministic
constraints on outputs without performing rule-based post-processing or
expensive discrete search. Instead, in the spirit of gradient-based training,
we enforce constraints with gradient-based inference (GBI): for each input at
test-time, we nudge continuous model weights until the network's unconstrained
inference procedure generates an output that satisfies the constraints. We
study the efficacy of GBI on three tasks with hard constraints: semantic role
labeling, syntactic parsing, and sequence transduction. In each case, the
algorithm not only satisfies constraints but improves accuracy, even when the
underlying network is state-of-the-art.","Table 1: Comparison of the GBI vs. A*inference procedure for SRL. We report the avg. disagreement rate, F1-scores and exact match for the failure set (columns 5-10) and F1-score for the whole test set (last 2 columns). Also, we report performances on a wide range of reference models SRL-X, where X denotes % of dataset used for training. We employ Viterbi decoding as a base inference strategy (before) and apply GBI (after) in combination with Viterbi.",How does GBI compare to A* in terms of reducing disagreement rate on the SRL-100 network's failure set?
spiqa_48,1611.03780v2,"According to the figure reporting B-metrics with a multiplicative constant of 100 in the ""Randomized Experimental Design via Geographic Clustering"" paper, how does GeoCUTS perform relative to DMA and Grid clustering methods for highly active users?",GeoCUTS performs comparably to other clusterings for highly active users.,1611.03780v2.pdf,"['1611.03780v2.pdf', '1803.03467v4.pdf', '1706.03847v3.pdf', '1906.10843v1.pdf', '1804.05995v2.pdf', '1608.02784v2.pdf', '1809.00458v1.pdf', '1805.04609v3.pdf', '1809.01246v1.pdf', '1809.02731v3.pdf', '1708.01425v4.pdf', '1803.05776v2.pdf', '1709.02755v5.pdf', '1605.07496v3.pdf']",The table shows that the B-metrics for GeoCUTS are similar to the B-metrics for DMA and Grid for highly active users in the US and France.,1611.03780v2-Table2-1.png,Randomized Experimental Design via Geographic Clustering,"Web-based services often run randomized experiments to improve their
products. A popular way to run these experiments is to use geographical regions
as units of experimentation, since this does not require tracking of individual
users or browser cookies. Since users may issue queries from multiple
geographical locations, geo-regions cannot be considered independent and
interference may be present in the experiment. In this paper, we study this
problem, and first present GeoCUTS, a novel algorithm that forms geographical
clusters to minimize interference while preserving balance in cluster size. We
use a random sample of anonymized traffic from Google Search to form a graph
representing user movements, then construct a geographically coherent
clustering of the graph. Our main technical contribution is a statistical
framework to measure the effectiveness of clusterings. Furthermore, we perform
empirical evaluations showing that the performance of GeoCUTS is comparable to
hand-crafted geo-regions with respect to both novel and existing metrics.","B-metrics across clusterings, reported with a multiplicative constant of 100. We see that GeoCUTS performs comparably to other clusterings for highly active users, and somewhat better for highly mobile users.",How does GeoCUTS perform compared to other clusterings for highly active users?
spiqa_49,1812.00281v3,"Referring to the figure, how does HUMBI’s use of 772 distinctive subjects with diverse demographics, including ethnicity, gender, age, clothing style, and physical condition, contribute to capturing a wide range of human body expressions?","HUMBI includes 772 distinctive subjects across gender, ethnicity, age, clothing style, and physical condition, which generates diverse appearance of human expressions.",1812.00281v3.pdf,"['1812.00281v3.pdf', '1705.07384v2.pdf', '1704.04539v2.pdf', '1803.03467v4.pdf', '1703.02507v3.pdf', '1804.05938v2.pdf']","The figure shows that HUMBI includes people of different genders, ethnicities, ages, clothing styles, and physical conditions. This diversity helps to capture a wide range of human expressions.",1812.00281v3-Figure3-1.png,HUMBI: A Large Multiview Dataset of Human Body Expressions,"This paper presents a new large multiview dataset called HUMBI for human body
expressions with natural clothing. The goal of HUMBI is to facilitate modeling
view-specific appearance and geometry of gaze, face, hand, body, and garment
from assorted people. 107 synchronized HD cameras are used to capture 772
distinctive subjects across gender, ethnicity, age, and physical condition.
With the multiview image streams, we reconstruct high fidelity body expressions
using 3D mesh models, which allows representing view-specific appearance using
their canonical atlas. We demonstrate that HUMBI is highly effective in
learning and reconstructing a complete human model and is complementary to the
existing datasets of human body expressions with limited views and subjects
such as MPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets.","(Top and bottom) HUMBI includes 772 distinctive subjects across gender, ethnicity, age, clothing style, and physical condition, which generates diverse appearance of human expressions. (Middle) For each subject, 107 HD cameras capture her/his expressions including gaze, face, hand, body, and garment.",How does HUMBI capture diverse appearance of human expressions?
spiqa_50,1812.00281v3,"Based on the scatter plot presented in the figure comparing views and subjects across various human body expression datasets, where does HUMBI rank in terms of the number of subjects compared to MPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio?",HUMBI has the highest number of subjects compared to the other datasets.,1812.00281v3.pdf,"['1812.00281v3.pdf', '1805.06447v3.pdf', '1707.00189v3.pdf', '1707.06320v2.pdf', '1703.10730v2.pdf', '1705.07384v2.pdf', '1804.07931v2.pdf', '1709.02418v2.pdf', '1805.04609v3.pdf', '1705.02798v6.pdf', '1611.05742v3.pdf', '1803.04383v2.pdf', '1804.00863v3.pdf', '1805.04687v2.pdf', '1709.08294v3.pdf']","The figure shows a scatter plot of the number of views vs. the number of subjects for several datasets. HUMBI is located in the upper right corner of the plot, indicating that it has the highest number of views and subjects.",1812.00281v3-Figure2-1.png,HUMBI: A Large Multiview Dataset of Human Body Expressions,"This paper presents a new large multiview dataset called HUMBI for human body
expressions with natural clothing. The goal of HUMBI is to facilitate modeling
view-specific appearance and geometry of gaze, face, hand, body, and garment
from assorted people. 107 synchronized HD cameras are used to capture 772
distinctive subjects across gender, ethnicity, age, and physical condition.
With the multiview image streams, we reconstruct high fidelity body expressions
using 3D mesh models, which allows representing view-specific appearance using
their canonical atlas. We demonstrate that HUMBI is highly effective in
learning and reconstructing a complete human model and is complementary to the
existing datasets of human body expressions with limited views and subjects
such as MPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets.",We present HUMBI that pushes towards two extremes: views and subjects. The view-specific appearance measured by 107 HD cameras regarding five elementary body expressions for 772 distinctive subjects.,How does HUMBI compare to other datasets in terms of the number of subjects?
spiqa_51,1705.08016v3,"In Figure 3 of the paper, how does Pairwise Confusion improve the localization performance of the VGG-16 model, as demonstrated by the tighter and more accurate focus on the target objects in the Grad-CAM heatmaps, compared to the baseline model?",PC improves the localization ability of a CNN.,1705.08016v3.pdf,"['1705.08016v3.pdf', '1811.08257v1.pdf', '1611.07718v2.pdf', '1811.07073v3.pdf', '1805.06431v4.pdf']","Figure 4 shows Grad-CAM heatmaps of images from the CUB-200-2011 dataset, with and without PC. The heatmaps show that PC-trained models provide tighter, more accurate localization around the target object, whereas baseline models sometimes have localization driven by image artifacts. For example, in Figure 4(a), the baseline VGG-16 network pays significant attention to a cartoon bird in the background, even though it makes the correct prediction. With PC, the attention is limited almost exclusively to the correct object.",1705.08016v3-Figure3-1.png,Pairwise Confusion for Fine-Grained Visual Classification,"Fine-Grained Visual Classification (FGVC) datasets contain small sample
sizes, along with significant intra-class variation and inter-class similarity.
While prior work has addressed intra-class variation using localization and
segmentation techniques, inter-class similarity may also affect feature
learning and reduce classification performance. In this work, we address this
problem using a novel optimization procedure for the end-to-end neural network
training on FGVC tasks. Our procedure, called Pairwise Confusion (PC) reduces
overfitting by intentionally {introducing confusion} in the activations. With
PC regularization, we obtain state-of-the-art performance on six of the most
widely-used FGVC datasets and demonstrate improved localization ability. {PC}
is easy to implement, does not need excessive hyperparameter tuning during
training, and does not add significant overhead during test time.","Fig. 3. Pairwise Confusion (PC) obtains improved localization performance, as demonstrated here with Grad-CAM heatmaps of the CUB-200-2011 dataset images (left) with a VGGNet-16 model trained without PC (middle) and with PC (right). The objects in (a) and (b) are correctly classified by both networks, and (c) and (d) are correctly classified by PC, but not the baseline network (VGG-16). For all cases, we consistently observe a tighter and more accurate localization with PC, whereas the baseline VGG-16 network often latches on to artifacts, even while making correct predictions.",How does Pairwise Confusion (PC) affect the localization ability of a CNN?
spiqa_52,1811.02721v3,"Based on the figure detailing the effect of batching on power consumption in LLN experiments with TCP, how does batching influence the radio duty cycle and CPU duty cycle?",Batching reduces both the radio duty cycle and CPU duty cycle.,1811.02721v3.pdf,"['1811.02721v3.pdf', '1809.04276v2.pdf', '1805.07567v2.pdf', '1805.02349v2.pdf', '1812.10735v2.pdf', '1802.07222v1.pdf', '1812.06589v2.pdf', '1702.03584v3.pdf', '1708.06832v3.pdf', '1703.10730v2.pdf', '1603.00286v5.pdf', '1812.00281v3.pdf']","The figure shows that the radio duty cycle and CPU duty cycle are both lower when batching is used, compared to when no batching is used. This is because batching allows the device to send and receive data in larger chunks, which reduces the number of times the radio and CPU need to be activated.",1811.02721v3-Figure9-1.png,Performant TCP for Low-Power Wireless Networks,"Low-power and lossy networks (LLNs) enable diverse applications integrating
many resource-constrained embedded devices, often requiring interconnectivity
with existing TCP/IP networks as part of the Internet of Things. But TCP has
received little attention in LLNs due to concerns about its overhead and
performance, leading to LLN-specific protocols that require specialized
gateways for interoperability. We present a systematic study of a well-designed
TCP stack in IEEE 802.15.4-based LLNs, based on the TCP protocol logic in
FreeBSD. Through careful implementation and extensive experiments, we show that
modern low-power sensor platforms are capable of running full-scale TCP and
that TCP, counter to common belief, performs well despite the lossy nature of
LLNs. By carefully studying the interaction between the transport and link
layers, we identify subtle but important modifications to both, achieving TCP
goodput within 25% of an upper bound (5-40x higher than prior results) and
low-power operation commensurate to CoAP in a practical LLN application
scenario. This suggests that a TCP-based transport layer, seamlessly
interoperable with existing TCP/IP networks, is viable and performant in LLNs.",Effect of batching on power consumption,How does batching affect the radio duty cycle and CPU duty cycle?
spiqa_53,1804.07707v2,"In the context of the findings displayed in Table 2, how does explicitly modeling meaning-preserving syntactic invariances improve the model’s ability to generate multiple valid and acceptable paraphrases from the same AMR graph compared to the baseline model?",Explicitly modeling meaning-preserving invariances leads to the generation of better paraphrases.,1804.07707v2.pdf,"['1804.07707v2.pdf', '1809.03550v3.pdf', '1805.08751v2.pdf', '1611.04363v2.pdf', '1705.09296v2.pdf', '1606.07384v2.pdf', '1605.07496v3.pdf', '1809.02731v3.pdf', '1811.08257v1.pdf', '1704.08615v2.pdf', '1804.04786v3.pdf', '1809.03449v3.pdf', '1704.05426v4.pdf', '1706.03847v3.pdf']","Table 1 shows that the syntax-aware model, which explicitly models these invariances, produces a significantly higher average number of acceptable realisations (1.52) compared to the baseline model (1.19). This difference is statistically significant with p < 0.001. The passage further explains that this improvement is due to the explicit modeling of meaning-preserving invariances, allowing the model to generate more paraphrases that retain the same meaning as the reference realization.",1804.07707v2-Table2-1.png,Factorising AMR generation through syntax,"Generating from Abstract Meaning Representation (AMR) is an underspecified
problem, as many syntactic decisions are not constrained by the semantic graph.
To explicitly account for this underspecification, we break down generating
from AMR into two steps: first generate a syntactic structure, and then
generate the surface form. We show that decomposing the generation process this
way leads to state-of-the-art single model performance generating from AMR
without additional unlabelled data. We also demonstrate that we can generate
meaning-preserving syntactic paraphrases of the same AMR graph, as judged by
humans.",Table 2: Average number of acceptable realisations out of 3. The difference is significant with p < 0.001.,How does explicitly modeling meaning-preserving invariances impact the generation of paraphrases?
spiqa_54,1708.02153v2,"In Table 2 of the paper ""Axiomatic Characterization of Data-Driven Influence Measures for Classification,"" how does increasing the parameter values (ρ for LIME with Euclidean distance, μ for LIME with cosine similarity, and σ for Parzen) affect the smoothness and noisiness of the influence vectors across different methods?","As the parameter value increases, the influence vectors generally become smoother and less noisy.",1708.02153v2.pdf,"['1708.02153v2.pdf', '1805.04609v3.pdf', '1804.07707v2.pdf', '1809.01989v2.pdf', '1809.04276v2.pdf', '1811.02553v4.pdf', '1805.01216v3.pdf', '1811.08481v2.pdf', '1812.06589v2.pdf', '1805.07567v2.pdf', '1704.04539v2.pdf', '1701.06171v4.pdf', '1805.08751v2.pdf']","Table 2 showcases the influence vectors and shifted POIs for different parameter values within each method (LIME with Euclidean distance, LIME with cosine similarity, and Parzen). By visually comparing the ""Influence"" images across increasing parameter values within each section, we can see a general trend of the influence vectors becoming less jagged and exhibiting smoother transitions. This suggests that higher parameter values lead to smoother and less noisy influence vectors.",1708.02153v2-Table3-1.png,Axiomatic Characterization of Data-Driven Influence Measures for Classification,"We study the following problem: given a labeled dataset and a specific
datapoint x, how did the i-th feature influence the classification for x? We
identify a family of numerical influence measures - functions that, given a
datapoint x, assign a numeric value phi_i(x) to every feature i, corresponding
to how altering i's value would influence the outcome for x. This family, which
we term monotone influence measures (MIM), is uniquely derived from a set of
desirable properties, or axioms. The MIM family constitutes a provably sound
methodology for measuring feature influence in classification domains; the
values generated by MIM are based on the dataset alone, and do not make any
queries to the classifier. While this requirement naturally limits the scope of
our framework, we demonstrate its effectiveness on data.",Table 3: The effect of different parameters and different distance measures.,"How does increasing the parameter value (ρ for LIME with Euclidean distance, μ for LIME with cosine similarity, and σ for Parzen) seem to affect the influence vectors?"
spiqa_55,1805.04687v2,"Based on the results in Table 5 of the BDD100K paper, how does scaling the training set size from 10K to 70K images influence the ODS-F for lane marking and IoU for drivable area segmentation, and what performance trends can be observed?",Increasing the training set size generally leads to improved performance for both lane marking and drivable area segmentation tasks.,1805.04687v2.pdf,"['1805.04687v2.pdf', '1906.10843v1.pdf', '1603.03833v4.pdf', '1603.00286v5.pdf', '1701.03077v10.pdf', '1611.07718v2.pdf', '1703.02507v3.pdf', '1608.02784v2.pdf', '1702.03584v3.pdf', '1804.05936v2.pdf', '1901.00398v2.pdf', '1804.04410v2.pdf', '1709.02418v2.pdf', '1707.00524v2.pdf']","The table shows the evaluation results for models trained on different sized datasets (10K, 20K, and 70K images). For both lane marking (ODS-F metric) and drivable area segmentation (IoU metric), the performance generally improves as the training set size increases. This is evident when comparing the ""mean"" values across the different training set sizes for each task. 

For example, the mean ODS-F for lane marking increases from 45.41% with 10K images to 54.36% with 20K images and further to 54.48% with 70K images. Similarly, the mean IoU for drivable area segmentation increases from 64.23% with 10K images to 71.13% with 20K images and 71.37% with 70K images. 

This trend suggests that having more training data allows the model to learn more effectively and generalize better, resulting in improved performance on both tasks.",1805.04687v2-Table5-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.","Table 5: Evaluation results of homogeneous multitask learning on lane marking and drivable area segmentation. We train lane marking, drivable area segmentation and the joint training of both on training splits with 10K, 20K, and the full 70K images.",How does increasing the training set size affect the performance of the lane marking and drivable area segmentation tasks?
spiqa_56,1705.02798v6,"How does the reattention mechanism presented in this paper, specifically as measured by the KL divergence in Table 7, impact attention redundancy and deficiency across different blocks on the SQuAD dataset, and why is the improvement more pronounced between the first two blocks compared to later blocks?","This paper shows that reattention helps alleviate both redundancy and deficiency in attention distributions.

Redundancy: Reattention increases the KL divergence between adjacent attention blocks, indicating that the attention distributions across blocks become more distinct and less redundant.
Deficiency: Reattention reduces the KL divergence between the normalized attention distribution ($E^t$) and the ideal uniform distribution (${E^t}^*$), suggesting that the attention becomes more balanced and closer to the desired distribution.
However, the improvement in redundancy is more pronounced between the first two blocks ($E^1$ to $E^2$) than the last two blocks ($B^2$ to $B^3$). This suggests that the first reattention is more effective in capturing word pair similarities using the original word representations. In contrast, the later reattention might be negatively impacted by the highly non-linear word representations generated in the previous layers.",1705.02798v6.pdf,"['1705.02798v6.pdf', '1811.07073v3.pdf', '1809.02731v3.pdf', '1704.08615v2.pdf', '1811.09393v4.pdf', '1702.03584v3.pdf', '1809.04276v2.pdf', '1803.04572v2.pdf', '1804.05938v2.pdf', '1611.04684v1.pdf', '1710.01507v4.pdf', '1809.01989v2.pdf']",The KL divergence values in Table 7 quantify the differences between attention distributions. Comparing the values with and without reattention allows us to assess the impact of reattention on redundancy and deficiency. The passage further clarifies the observed differences in the effectiveness of reattention across different blocks.,1705.02798v6-Table5-1.png,Reinforced Mnemonic Reader for Machine Reading Comprehension,"In this paper, we introduce the Reinforced Mnemonic Reader for machine
reading comprehension tasks, which enhances previous attentive readers in two
aspects. First, a reattention mechanism is proposed to refine current
attentions by directly accessing to past attentions that are temporally
memorized in a multi-round alignment architecture, so as to avoid the problems
of attention redundancy and attention deficiency. Second, a new optimization
approach, called dynamic-critical reinforcement learning, is introduced to
extend the standard supervised method. It always encourages to predict a more
acceptable answer so as to address the convergence suppression problem occurred
in traditional reinforcement learning algorithms. Extensive experiments on the
Stanford Question Answering Dataset (SQuAD) show that our model achieves
state-of-the-art results. Meanwhile, our model outperforms previous systems by
over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD
datasets.",Table 5: Comparison of KL diverfence on different attention distributions on SQuAD dev set.,How does reattention affect the redundancy and deficiency of attention distributions? Can you explain the observed differences in the impact of reattention on different blocks?
spiqa_57,1709.02755v5,"Referring to Figure 2 in the ""Simple Recurrent Units for Highly Parallelizable Recurrence"" paper, how does scaling correction influence the training loss and convergence speed of SRU models, particularly for deeper architectures like the 20-layer model?","Scaling correction improves the training progress of SRU models, especially for deeper models with many stacked layers.",1709.02755v5.pdf,"['1709.02755v5.pdf', '1704.05958v2.pdf', '1702.03584v3.pdf', '1704.00774v3.pdf', '1707.00189v3.pdf', '1703.10730v2.pdf']",The figure shows the training curves of SRU models with and without scaling correction. The x-axis is the number of training steps and the y-axis is the training loss. The curves with scaling correction (red) have a lower loss and converge faster than the curves without scaling correction (black). This is especially noticeable for the 20-layer model.,1709.02755v5-Figure2-1.png,Simple Recurrent Units for Highly Parallelizable Recurrence,"Common recurrent neural architectures scale poorly due to the intrinsic
difficulty in parallelizing their state computations. In this work, we propose
the Simple Recurrent Unit (SRU), a light recurrent unit that balances model
capacity and scalability. SRU is designed to provide expressive recurrence,
enable highly parallelized implementation, and comes with careful
initialization to facilitate training of deep models. We demonstrate the
effectiveness of SRU on multiple NLP tasks. SRU achieves 5--9x speed-up over
cuDNN-optimized LSTM on classification and question answering datasets, and
delivers stronger results than LSTM and convolutional models. We also obtain an
average of 0.7 BLEU improvement over the Transformer model on translation by
incorporating SRU into the architecture.","Figure 2: Training curves of SRU on classification. The x-axis is the number of training steps and the y-axis is the training loss. Scaling correction improves the training progress, especially for deeper models with many stacked layers.",How does scaling correction affect the training of SRU models?
spiqa_58,1709.08294v3,"Based on Figure 1 and Section 3.3, how does the interaction between the filter generation module and the convolution module allow the ACNN framework to produce context-sensitive filters, and how does this process adapt to different input sentences?","The ACNN framework learns context-sensitive filters through two modules: the filter generation module and the adaptive convolution module. The filter generation module produces a set of filters conditioned on the input sentence, while the adaptive convolution module applies the generated filters to an input sentence. The two modules are jointly differentiable, and the overall architecture can be trained in an end-to-end manner.",1709.08294v3.pdf,"['1709.08294v3.pdf', '1805.07567v2.pdf', '1705.10667v4.pdf', '1811.08257v1.pdf', '1704.08615v2.pdf', '1703.00060v2.pdf', '1703.10730v2.pdf']","The figure shows the general ACNN framework, with the filter generation module on the left and the convolution module on the right. The arrows between the modules indicate the flow of information, with the filter generation module producing filters based on the input sentence and the convolution module applying those filters to the input sentence.",1709.08294v3-Figure1-1.png,Learning Context-Sensitive Convolutional Filters for Text Processing,"Convolutional neural networks (CNNs) have recently emerged as a popular
building block for natural language processing (NLP). Despite their success,
most existing CNN models employed in NLP share the same learned (and static)
set of filters for all input sentences. In this paper, we consider an approach
of using a small meta network to learn context-sensitive convolutional filters
for text processing. The role of meta network is to abstract the contextual
information of a sentence or document into a set of input-aware filters. We
further generalize this framework to model sentence pairs, where a
bidirectional filter generation mechanism is introduced to encapsulate
co-dependent sentence representations. In our benchmarks on four different
tasks, including ontology classification, sentiment analysis, answer sentence
selection, and paraphrase identification, our proposed model, a modified CNN
with context-sensitive filters, consistently outperforms the standard CNN and
attention-based CNN baselines. By visualizing the learned context-sensitive
filters, we further validate and rationalize the effectiveness of proposed
framework.","Figure 1: The general ACNN framework. Notably, the input sentences to filter generating module and convolution module could be different (see Section 3.3).",How does the ACNN framework learn context-sensitive filters?
spiqa_59,1709.08294v3,"In the context of Figure 2, how does the filter generation module in the AdaQA model produce context-sensitive filters based on the embeddings of the question and answer pair?",The AdaQA model generates context-aware filters through the filter generation module. This module takes the question and answer as input and outputs a set of filters that are specific to the question and answer pair.,1709.08294v3.pdf,"['1709.08294v3.pdf', '1811.02721v3.pdf', '1611.04684v1.pdf', '1705.07384v2.pdf', '1805.04609v3.pdf', '1708.01425v4.pdf', '1708.02153v2.pdf']","The figure shows that the filter generation module is located below the question and answer embedding modules. The question and answer embeddings are fed into the filter generation module, which then outputs the context-aware filters. These filters are then used by the convolution module to encode the question and answer.",1709.08294v3-Figure2-1.png,Learning Context-Sensitive Convolutional Filters for Text Processing,"Convolutional neural networks (CNNs) have recently emerged as a popular
building block for natural language processing (NLP). Despite their success,
most existing CNN models employed in NLP share the same learned (and static)
set of filters for all input sentences. In this paper, we consider an approach
of using a small meta network to learn context-sensitive convolutional filters
for text processing. The role of meta network is to abstract the contextual
information of a sentence or document into a set of input-aware filters. We
further generalize this framework to model sentence pairs, where a
bidirectional filter generation mechanism is introduced to encapsulate
co-dependent sentence representations. In our benchmarks on four different
tasks, including ontology classification, sentiment analysis, answer sentence
selection, and paraphrase identification, our proposed model, a modified CNN
with context-sensitive filters, consistently outperforms the standard CNN and
attention-based CNN baselines. By visualizing the learned context-sensitive
filters, we further validate and rationalize the effectiveness of proposed
framework.",Figure 2: Schematic description of Adaptive Question Answering (AdaQA) model.,How does the Adaptive Question Answering (AdaQA) model generate context-aware filters?
spiqa_60,1809.01246v1,"In the figure titled ""Average Relative Error of Node Queries"" from the ""Fast and Accurate Graph Stream Summarization"" paper, how does increasing the width parameter affect the ARE for different configurations of GSS and TCM across the five datasets?","The ARE of node queries generally decreases as the width increases for all configurations of GSS and TCM. However, there are some fluctuations in the ARE for some configurations.",1809.01246v1.pdf,"['1809.01246v1.pdf', '1705.09296v2.pdf', '1709.02418v2.pdf', '1811.06635v1.pdf', '1804.00863v3.pdf', '1707.08608v3.pdf', '1812.00281v3.pdf', '1703.04887v4.pdf', '1611.04684v1.pdf', '1703.07015v3.pdf', '1811.07073v3.pdf', '1706.04284v3.pdf', '1703.10730v2.pdf', '1804.05936v2.pdf', '1703.00060v2.pdf']","The figure shows the ARE of node queries for different configurations of GSS and TCM on five different datasets. The x-axis of each plot represents the width, and the y-axis represents the ARE. The lines in each plot represent the different configurations of GSS and TCM. The figure shows that the ARE generally decreases as the width increases, which indicates that the accuracy of the node queries improves as the width increases.",1809.01246v1-Figure11-1.png,Fast and Accurate Graph Stream Summarization,"A graph stream is a continuous sequence of data items, in which each item
indicates an edge, including its two endpoints and edge weight. It forms a
dynamic graph that changes with every item in the stream. Graph streams play
important roles in cyber security, social networks, cloud troubleshooting
systems and other fields. Due to the vast volume and high update speed of graph
streams, traditional data structures for graph storage such as the adjacency
matrix and the adjacency list are no longer sufficient. However, prior art of
graph stream summarization, like CM sketches, gSketches, TCM and gMatrix,
either supports limited kinds of queries or suffers from poor accuracy of query
results. In this paper, we propose a novel Graph Stream Sketch (GSS for short)
to summarize the graph streams, which has the linear space cost (O(|E|), E is
the edge set of the graph) and the constant update time complexity (O(1)) and
supports all kinds of queries over graph streams with the controllable errors.
Both theoretical analysis and experiment results confirm the superiority of our
solution with regard to the time/space complexity and query results' precision
compared with the state-of-the-art.",Average Relative Error of Node Queries,How does the Average Relative Error (ARE) of node queries change as the width increases for different configurations of GSS and TCM?
spiqa_61,1805.04687v2,"Based on the dataset comparison presented in Table 2, how does the scale and diversity of the BDD100K dataset—specifically in terms of frames, sequences, and tracked identities—highlight its advantages over KITTI and MOT17 for multiple object tracking tasks?","The BDD100K dataset is significantly larger and more complex than both the KITTI and MOT17 datasets. It contains roughly 40 times more frames, 16 times more sequences, and 13 times more identities than KITTI. Compared to MOT17, BDD100K has about 10 times more frames, 80 times more sequences, and 8 times more identities. This increase in size and complexity makes BDD100K a more challenging and comprehensive benchmark for multiple object tracking algorithms. ",1805.04687v2.pdf,"['1805.04687v2.pdf', '1707.01917v2.pdf', '1705.10667v4.pdf', '1708.02153v2.pdf', '1709.08294v3.pdf', '1611.03780v2.pdf', '1803.04572v2.pdf', '1612.02803v5.pdf', '1705.09882v2.pdf', '1906.06589v3.pdf', '1611.04684v1.pdf', '1805.06447v3.pdf', '1804.04786v3.pdf']","Table~\ref{tab:box_tracking_label_comp} provides a direct comparison of these datasets, showing the number of frames, sequences, identities, and bounding boxes for each. By analyzing these numbers, we can see that BDD100K significantly surpasses the other two datasets in terms of scale and diversity of tracked objects.",1805.04687v2-Table2-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.","Table 2: MOT datasets statistics of training and validation sets. Our dataset has more sequences, frames, identities as well as more box annotations.",How does the BDD100K dataset compare to the KITTI and MOT17 datasets in terms of size and complexity?
spiqa_62,1702.08694v3,"Based on Figure 3, how does the performance of the C-Tarone method compare to the binarization method in terms of precision, recall, F-measure, and running time across the real-world datasets used in the study?",The C-Tarone method has higher precision and F-measure than the binarization method in all datasets. The C-Tarone method has better or competitive recall than the binarization method. The running time of the C-Tarone method is competitive with the binarization method.,1702.08694v3.pdf,"['1702.08694v3.pdf', '1706.00633v4.pdf', '1708.03797v1.pdf', '1706.00827v2.pdf']","The figure shows the precision, recall, F-measure, and running time of the C-Tarone method and the binarization method for different datasets. The bars for the C-Tarone method are higher than the bars for the binarization method for precision and F-measure, which means that the C-Tarone method performs better than the binarization method in terms of precision and F-measure. The bars for the C-Tarone method are similar to the bars for the binarization method for recall, which means that the C-Tarone method performs similarly to the binarization method in terms of recall. The bars for the C-Tarone method are similar to the bars for the binarization method for running time, which means that the C-Tarone method performs similarly to the binarization method in terms of running time.",1702.08694v3-Figure3-1.png,Finding Statistically Significant Interactions between Continuous Features,"The search for higher-order feature interactions that are statistically
significantly associated with a class variable is of high relevance in fields
such as Genetics or Healthcare, but the combinatorial explosion of the
candidate space makes this problem extremely challenging in terms of
computational efficiency and proper correction for multiple testing. While
recent progress has been made regarding this challenge for binary features, we
here present the first solution for continuous features. We propose an
algorithm which overcomes the combinatorial explosion of the search space of
higher-order interactions by deriving a lower bound on the p-value for each
interaction, which enables us to massively prune interactions that can never
reach significance and to thereby gain more statistical power. In our
experiments, our approach efficiently detects all significant interactions in a
variety of synthetic and real-world datasets.","Figure 3: Results on real data. Regarding the scale of precision and F-measure, see the comment at the last paragraph just before Section 3. The y-axis is in logarithmic scale. C-Tarone is shown in red and the binarization approach is shown in blue. Higher (taller) is better in precision, recall, and F-measure, while lower is better in running time.","How does the C-Tarone method compare to the binarization method in terms of precision, recall, F-measure, and running time?"
spiqa_63,1803.02750v3,"Based on the figure displaying the impact of Zipf coefficients on CPU overhead in the ""Efficient Synchronization of State-based CRDTs"" paper, how does the increase in CPU overhead differ between classic delta-based and delta-based BP+RR synchronization approaches?",The CPU overhead of classic delta-based is consistently higher than that of delta-based BP+RR as the Zipf coefficient increases.,1803.02750v3.pdf,"['1803.02750v3.pdf', '1809.03449v3.pdf', '1809.04276v2.pdf', '1705.09296v2.pdf', '1809.03149v2.pdf', '1804.04410v2.pdf', '1805.06447v3.pdf', '1608.02784v2.pdf', '1812.00281v3.pdf', '1709.02755v5.pdf', '1704.00774v3.pdf']","The plot shows the CPU overhead of both classic delta-based and delta-based BP+RR for different Zipf coefficients. The green line represents classic delta-based, and the blue line represents delta-based BP+RR. As the Zipf coefficient increases, the CPU overhead of both algorithms increases, but the CPU overhead of classic delta-based increases at a faster rate.",1803.02750v3-Figure12-1.png,Efficient Synchronization of State-based CRDTs,"To ensure high availability in large scale distributed systems, Conflict-free
Replicated Data Types (CRDTs) relax consistency by allowing immediate query and
update operations at the local replica, with no need for remote
synchronization. State-based CRDTs synchronize replicas by periodically sending
their full state to other replicas, which can become extremely costly as the
CRDT state grows. Delta-based CRDTs address this problem by producing small
incremental states (deltas) to be used in synchronization instead of the full
state. However, current synchronisation algorithms for Delta-based CRDTs induce
redundant wasteful delta propagation, performing worse than expected, and
surprisingly, no better than State-based. In this paper we: 1) identify two
sources of inefficiency in current synchronization algorithms for delta-based
CRDTs; 2) bring the concept of join decomposition to state-based CRDTs; 3)
exploit join decompositions to obtain optimal deltas and 4) improve the
efficiency of synchronization algorithms; and finally, 5) evaluate the improved
algorithms.",CPU overhead of classic delta-based when compared to delta-based BP+RR.,How does the CPU overhead of classic delta-based compare to delta-based BP+RR as the Zipf coefficient increases?
spiqa_64,1805.06431v4,"How effectively does the ChoiceNet model fit datasets with uniform corruptions, as evidenced by the fitting results in Figure (c) and the correlations with the ground truth in Figure (d)?",The ChoiceNet model performs poorly on datasets with uniform corruptions.,1805.06431v4.pdf,"['1805.06431v4.pdf', '1804.05995v2.pdf', '1704.07854v4.pdf', '1803.03467v4.pdf', '1703.00899v2.pdf', '1811.08257v1.pdf', '1704.00774v3.pdf', '1608.02784v2.pdf', '1705.02946v3.pdf', '1805.02349v2.pdf', '1708.03797v1.pdf']"," The figure shows the fitting results and correlations of the ChoiceNet model on datasets with flipped functions and uniform corruptions. In (c), the ChoiceNet model is unable to accurately fit the data with uniform corruptions. This is further supported by the correlations in (d), which show that the two components of the ChoiceNet model have low correlations with the ground truth. ",1805.06431v4-Figure4-1.png,Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks,"In this paper, we focus on weakly supervised learning with noisy training
data for both classification and regression problems.We assume that the
training outputs are collected from a mixture of a target and correlated noise
distributions.Our proposed method simultaneously estimates the target
distribution and the quality of each data which is defined as the correlation
between the target and data generating distributions.The cornerstone of the
proposed method is a Cholesky Block that enables modeling dependencies among
mixture distributions in a differentiable manner where we maintain the
distribution over the network weights.We first provide illustrative examples in
both regression and classification tasks to show the effectiveness of the
proposed method.Then, the proposed method is extensively evaluated in a number
of experiments where we show that it constantly shows comparable or superior
performances compared to existing baseline methods in the handling of noisy
data.",Fitting results on datasets with (a) flipped function and (c) uniform corruptions. Resulting correlations of two components with (b) flipped function and (d) uniform corruptions.,How does the ChoiceNet model perform on datasets with uniform corruptions?
spiqa_65,1805.06447v3,"In the ITN framework's training process, as illustrated in Figure 1 of the ""Resisting Large Data Variations via Introspective Transformation Network"" paper, how are pseudo-negative samples generated from transformed positive examples to enhance the CNN classifier?",The ITN framework generates pseudo-negative samples by applying learned transformations to positive samples.,1805.06447v3.pdf,"['1805.06447v3.pdf', '1805.02349v2.pdf', '1804.07931v2.pdf', '1802.07459v2.pdf']"," The figure shows that the ITN framework consists of a transformation module and a CNN classifier. The transformation module learns to transform positive samples in a way that maximizes their variation from the original training samples. These transformed positive samples are then used as pseudo-negative samples to train the CNN classifier.

**Figure type:** Schematic",1805.06447v3-Figure1-1.png,Resisting Large Data Variations via Introspective Transformation Network,"Training deep networks that generalize to a wide range of variations in test
data is essential to building accurate and robust image classifiers. One
standard strategy is to apply data augmentation to synthetically enlarge the
training set. However, data augmentation is essentially a brute-force method
which generates uniform samples from some pre-defined set of transformations.
In this paper, we propose a principled approach to train networks with
significantly improved resistance to large variations between training and
testing data. This is achieved by embedding a learnable transformation module
into the introspective network, which is a convolutional neural network (CNN)
classifier empowered with generative capabilities. Our approach alternates
between synthesizing pseudo-negative samples and transformed positive examples
based on the current model, and optimizing model predictions on these
synthesized samples. Experimental results verify that our approach
significantly improves the ability of deep networks to resist large variations
between training and testing data and achieves classification accuracy
improvements on several benchmark datasets, including MNIST, affNIST, SVHN,
CIFAR-10 and miniImageNet.",Figure 1. Illustration of the intuition of our ITN framework. ITN enhances the discriminator by generating additional pseudo-negative samples in the training step.,How does the ITN framework generate pseudo-negative samples?
spiqa_66,1804.01429v3,"In the LIVR framework, as shown in Figure 2, how are bitmaps used to mask the video representations and spatially decompose semantic features into distinct place-based regions?",The LIVR framework decomposes semantic features into different places by utilizing bitmaps encoded with the semantic labels of places. This decomposition encourages the network to learn features of generic place-based motion patterns that are independent of scene layouts.,1804.01429v3.pdf,"['1804.01429v3.pdf', '1805.00912v4.pdf', '1804.04786v3.pdf', '1811.08257v1.pdf', '1612.02803v5.pdf', '1809.03550v3.pdf', '1906.06589v3.pdf', '1805.01216v3.pdf', '1703.10730v2.pdf', '1811.02721v3.pdf', '1707.00524v2.pdf', '1812.06589v2.pdf']","The figure shows how the semantic features are decomposed into different places using bitmaps. The bitmaps are used to mask the video representations, which results in place-based feature descriptions.",1804.01429v3-Figure2-1.png,Layout-induced Video Representation for Recognizing Agent-in-Place Actions,"We address the recognition of agent-in-place actions, which are associated
with agents who perform them and places where they occur, in the context of
outdoor home surveillance. We introduce a representation of the geometry and
topology of scene layouts so that a network can generalize from the layouts
observed in the training set to unseen layouts in the test set. This
Layout-Induced Video Representation (LIVR) abstracts away low-level appearance
variance and encodes geometric and topological relationships of places in a
specific scene layout. LIVR partitions the semantic features of a video clip
into different places to force the network to learn place-based feature
descriptions; to predict the confidence of each action, LIVR aggregates
features from the place associated with an action and its adjacent places on
the scene layout. We introduce the Agent-in-Place Action dataset to show that
our method allows neural network models to generalize significantly better to
unseen scenes.","Figure 2. Framework of LIVR. Given the segmentation map, we decompose the semantic features into different places and extract place-based feature descriptions individually, then dynamically aggregate them at inference time according to the topology of the scene. denotes the masking operation for spatial decomposition. ""NN"" stands for neural network.",How does the LIVR framework decompose semantic features into different places?
spiqa_67,1706.00827v2,"Referring to the right panel of the figure, where 1000 line instances are shown with the true parameters in red and modes in green, how does the Mean-Shift algorithm maintain robustness to the presence of 50 outliers during the process of density-based mode-seeking for line fitting?",The Mean-Shift algorithm is robust to outliers.,1706.00827v2.pdf,"['1706.00827v2.pdf', '1611.07718v2.pdf', '1803.04572v2.pdf', '1809.00458v1.pdf', '1709.02418v2.pdf', '1811.08257v1.pdf']","The left panel of the figure shows three lines with outliers. The right panel shows the results of the Mean-Shift algorithm, which has correctly identified the three lines despite the presence of outliers. This is because the Mean-Shift algorithm is based on the density of points, and outliers do not significantly affect the density.",1706.00827v2-Figure2-1.png,Multi-Class Model Fitting by Energy Minimization and Mode-Seeking,"We propose a general formulation, called Multi-X, for multi-class
multi-instance model fitting - the problem of interpreting the input data as a
mixture of noisy observations originating from multiple instances of multiple
classes. We extend the commonly used alpha-expansion-based technique with a new
move in the label space. The move replaces a set of labels with the
corresponding density mode in the model parameter domain, thus achieving fast
and robust optimization. Key optimization parameters like the bandwidth of the
mode seeking are set automatically within the algorithm. Considering that a
group of outliers may form spatially coherent structures in the data, we
propose a cross-validation-based technique removing statistically insignificant
instances. Multi-X outperforms significantly the state-of-the-art on publicly
available datasets for diverse problems: multiple plane and rigid motion
detection; motion segmentation; simultaneous plane and cylinder fitting; circle
and line fitting.","(Left) Three lines each generating 100 points with zero-mean Gaussian noise added, plus 50 outliers. (Right) 1000 line instances generated from random point pairs, the ground truth instance parameters (red dots) and the modes (green) provided by Mean-Shift shown in the model parameter domain: α angle – vertical, offset – horizontal axis.",How does the Mean-Shift algorithm perform in the presence of outliers?
spiqa_68,1812.00108v4,"Referring to Figure 2 in the ""Multi-Stream Dynamic Video Summarization"" paper, how does the Multi-DPP module utilize determinantal point processes to select a diverse set of time-steps across multiple views, ensuring temporal and spatial diversity in the summary?",The Multi-DPP module increases diversity within the selected time-steps by using a determinantal point process (DPP) to select a subset of diverse time-steps from the input sequence.,1812.00108v4.pdf,"['1812.00108v4.pdf', '1811.06635v1.pdf', '1703.00899v2.pdf', '1701.06171v4.pdf', '1708.05239v3.pdf', '1603.03833v4.pdf', '1608.02784v2.pdf', '1811.08257v1.pdf', '1704.04539v2.pdf', '1804.05936v2.pdf', '1701.03077v10.pdf', '1809.03550v3.pdf']",The figure shows that the Multi-DPP module takes as input the feature vectors of all frames at each view and outputs a subset of diverse time-steps. This is done by using a DPP to select a subset of time-steps that are maximally dissimilar to each other.,1812.00108v4-Figure2-1.png,Multi-Stream Dynamic Video Summarization,"With vast amounts of video content being uploaded to the Internet every
minute, video summarization becomes critical for efficient browsing, searching,
and indexing of visual content. Nonetheless, the spread of social and
egocentric cameras creates an abundance of sparse scenarios captured by several
devices, and ultimately required to be jointly summarized. In this paper, we
discuss the problem of summarizing videos recorded independently by several
dynamic cameras that intermittently share the field of view. We present a
robust framework that (a) identifies a diverse set of important events among
moving cameras that often are not capturing the same scene, and (b) selects the
most representative view(s) at each event to be included in a universal
summary. Due to the lack of an applicable alternative, we collected a new
multi-view egocentric dataset, Multi-Ego. Our dataset is recorded
simultaneously by three cameras, covering a wide variety of real-life
scenarios. The footage is annotated by multiple individuals under various
summarization configurations, with a consensus analysis ensuring a reliable
ground truth. We conduct extensive experiments on the compiled dataset in
addition to three other standard benchmarks that show the robustness and the
advantage of our approach in both supervised and unsupervised settings.
Additionally, we show that our approach learns collectively from data of varied
number-of-views and orthogonal to other summarization methods, deeming it
scalable and generic.","Figure 2: Multi-DPP is applied to increase diversity within the selected time-steps. When view labels are available, we also use cross-entropy to learn representative view(s) at each time-step.",How does the Multi-DPP module increase diversity within the selected time-steps?
spiqa_69,1804.05936v2,"In the context of Figure 3 of the ""Learning a Deep Listwise Context Model for Ranking Refinement"" paper, how does the NegPair reduction change as the number of perfect documents in a query increases, and what trend does this reveal about the model's effectiveness?",The NegPair reduction generally increases as the number of perfect results in a query increases.,1804.05936v2.pdf,"['1804.05936v2.pdf', '1805.08751v2.pdf', '1707.01922v5.pdf', '1708.05239v3.pdf', '1709.00139v4.pdf', '1705.02946v3.pdf', '1707.01917v2.pdf', '1709.08294v3.pdf', '1710.05654v2.pdf', '1804.05995v2.pdf', '1811.02553v4.pdf', '1811.10673v1.pdf', '1805.02349v2.pdf', '1603.00286v5.pdf']","The figure shows that the average NegPair reduction is higher for queries with more perfect results. For example, the NegPair reduction is 0.99 for queries with one perfect result, but it is 2.64 for queries with four perfect results. This suggests that DLCMs are more effective at reducing the number of negative pairs for queries with more perfect results.",1804.05936v2-Figure3-1.png,Learning a Deep Listwise Context Model for Ranking Refinement,"Learning to rank has been intensively studied and widely applied in
information retrieval. Typically, a global ranking function is learned from a
set of labeled data, which can achieve good performance on average but may be
suboptimal for individual queries by ignoring the fact that relevant documents
for different queries may have different distributions in the feature space.
Inspired by the idea of pseudo relevance feedback where top ranked documents,
which we refer as the \textit{local ranking context}, can provide important
information about the query's characteristics, we propose to use the inherent
feature distributions of the top results to learn a Deep Listwise Context Model
that helps us fine tune the initial ranked list. Specifically, we employ a
recurrent neural network to sequentially encode the top results using their
feature vectors, learn a local context model and use it to re-rank the top
results. There are three merits with our model: (1) Our model can capture the
local ranking context based on the complex interactions between top results
using a deep neural network; (2) Our model can be built upon existing
learning-to-rank methods by directly using their extracted feature vectors; (3)
Our model is trained with an attention-based loss function, which is more
effective and efficient than many existing listwise methods. Experimental
results show that the proposed model can significantly improve the
state-of-the-art learning to rank methods on benchmark retrieval corpora.",Figure 3: TheNegPair reduction and corresponding improvement proportion for queries with different number of perfect documents.,How does the NegPair reduction vary with the number of perfect results in a query?
spiqa_70,1811.09393v4,"In the figure illustrating the Ping-Pong (PP) loss mechanism, how does the loss function reduce the L2 distance between corresponding frames, thereby improving temporal coherence and reducing drifting artifacts during video generation?",The PP loss constrains the output sequence to be symmetric by reducing the L2 distance between corresponding frames in the forward and backward passes. This helps to reduce drifting artifacts and improve temporal coherence.,1811.09393v4.pdf,"['1811.09393v4.pdf', '1702.08694v3.pdf', '1701.06171v4.pdf', '1611.02654v2.pdf', '1708.03797v1.pdf', '1811.08481v2.pdf', '1804.05936v2.pdf', '1704.05426v4.pdf', '1804.07707v2.pdf']"," The figure shows how the PP loss works. The forward pass (Ping) and the backward pass (Pong) are shown in the figure. The PP loss reduces the L2 distance between corresponding frames in the forward and backward passes, which is shown by the red circles with a minus sign. This helps to reduce drifting artifacts and improve temporal coherence.

**Figure type:** Schematic",1811.09393v4-Figure3-1.png,Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation,"Our work explores temporal self-supervision for GAN-based video generation
tasks. While adversarial training successfully yields generative models for a
variety of areas, temporal relationships in the generated data are much less
explored. Natural temporal changes are crucial for sequential generation tasks,
e.g. video super-resolution and unpaired video translation. For the former,
state-of-the-art methods often favor simpler norm losses such as $L^2$ over
adversarial training. However, their averaging nature easily leads to
temporally smooth results with an undesirable lack of spatial detail. For
unpaired video translation, existing approaches modify the generator networks
to form spatio-temporal cycle consistencies. In contrast, we focus on improving
learning objectives and propose a temporally self-supervised algorithm. For
both tasks, we show that temporal adversarial learning is key to achieving
temporally coherent solutions without sacrificing spatial detail. We also
propose a novel Ping-Pong loss to improve the long-term temporal consistency.
It effectively prevents recurrent networks from accumulating artifacts
temporally without depressing detailed features. Additionally, we propose a
first set of metrics to quantitatively evaluate the accuracy as well as the
perceptual quality of the temporal evolution. A series of user studies confirm
the rankings computed with these metrics. Code, data, models, and results are
provided at https://github.com/thunil/TecoGAN. The project page
https://ge.in.tum.de/publications/2019-tecogan-chu/ contains supplemental
materials.","a) Result without PP loss. The VSR network is trained with a recurrent frame-length of 10. When inference on long sequences, frame 15 and latter frames of the foliage scene show the drifting artifacts. b) Result trained with PP loss. These artifacts are removed successfully for the latter. c) When inferring a symmetric PP sequence with a forward pass (Ping) and its backward counterpart (Pong), our PP loss constrains the output sequence to be symmetric. It reduces the L2 distance between дt and д′t , the corresponding frames in the forward and backward passes, shown via red circles with a minus sign. The PP loss reduces drifting artifacts and improves temporal coherence.",How does the PP loss improve the temporal coherence of the video sequence?
spiqa_71,1804.04410v2,"Referring to Figure 2 of the paper, how does the learned RL policy impact the number of index blocks accessed compared to the baseline for CAT2 queries on the weighted set?",The RL policy accesses fewer index blocks than the baseline.,1804.04410v2.pdf,"['1804.04410v2.pdf', '1805.04687v2.pdf', '1809.03550v3.pdf', '1804.07931v2.pdf', '1704.07854v4.pdf', '1803.06506v3.pdf', '1708.06832v3.pdf', '1706.08146v3.pdf', '1703.02507v3.pdf', '1707.06320v2.pdf', '1701.06171v4.pdf', '1703.10730v2.pdf', '1706.00827v2.pdf', '1811.02553v4.pdf', '1805.02349v2.pdf']","The figure shows that the RL policy line is always below the baseline line, which means that the RL policy accesses fewer index blocks than the baseline for all queries.",1804.04410v2-Figure2-1.png,Optimizing Query Evaluations using Reinforcement Learning for Web Search,"In web search, typically a candidate generation step selects a small set of
documents---from collections containing as many as billions of web pages---that
are subsequently ranked and pruned before being presented to the user. In Bing,
the candidate generation involves scanning the index using statically designed
match plans that prescribe sequences of different match criteria and stopping
conditions. In this work, we pose match planning as a reinforcement learning
task and observe up to 20% reduction in index blocks accessed, with small or no
degradation in the quality of the candidate sets.",Figure 2: The reduction in index blocks accessed from the learned policy for CAT2 queries on the weighted set. We intentionally leave out the actual page access numbers on the y-axis because of confidentiality. The queries on the x-axis are sorted by page access independently for each treatment.,How does the RL policy compare to the baseline in terms of index blocks accessed?
spiqa_72,1805.06431v4,"Referring to the figure depicting the results of the random shuffle experiments on the MNIST dataset, how does the accuracy of the Mixup method vary as the level of noise increases from 50% to 95%, and what trend is observed?",The accuracy of the Mixup method decreases as the level of random shuffle increases.,1805.06431v4.pdf,"['1805.06431v4.pdf', '1611.05742v3.pdf', '1606.07384v2.pdf', '1709.08294v3.pdf', '1805.02349v2.pdf', '1804.01429v3.pdf', '1708.02153v2.pdf', '1709.00139v4.pdf', '1702.08694v3.pdf', '1804.05936v2.pdf']",The figure shows that the Mixup method achieves the highest accuracy with 50% random shuffle and the lowest accuracy with 95% random shuffle. This suggests that the Mixup method is more sensitive to the level of noise in the data than the other methods.,1805.06431v4-Figure12-1.png,Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks,"In this paper, we focus on weakly supervised learning with noisy training
data for both classification and regression problems.We assume that the
training outputs are collected from a mixture of a target and correlated noise
distributions.Our proposed method simultaneously estimates the target
distribution and the quality of each data which is defined as the correlation
between the target and data generating distributions.The cornerstone of the
proposed method is a Cholesky Block that enables modeling dependencies among
mixture distributions in a differentiable manner where we maintain the
distribution over the network weights.We first provide illustrative examples in
both regression and classification tasks to show the effectiveness of the
proposed method.Then, the proposed method is extensively evaluated in a number
of experiments where we show that it constantly shows comparable or superior
performances compared to existing baseline methods in the handling of noisy
data.",Learning curves of compared methods on random shuffle experiments using MNIST with different noise levels.,How does the accuracy of the Mixup method change as the level of random shuffle increases?
spiqa_73,1805.04609v3,"Based on Figure 2 of the *Textual Membership Queries* paper, how does the accuracy of the US-BS-MQ method compare to that of the S-MQ method when more SST dataset examples are incorporated?",The US-BS-MQ method achieves higher accuracy than the S-MQ method when adding SST examples.,1805.04609v3.pdf,"['1805.04609v3.pdf', '1709.02755v5.pdf', '1705.09296v2.pdf', '1706.04284v3.pdf', '1704.07121v2.pdf', '1703.00899v2.pdf']",The figure shows that the red line (US-BS-MQ) is consistently above the blue line (S-MQ) for the SST dataset.,1805.04609v3-Figure2-1.png,Textual Membership Queries,"Human labeling of data can be very time-consuming and expensive, yet, in many
cases it is critical for the success of the learning process. In order to
minimize human labeling efforts, we propose a novel active learning solution
that does not rely on existing sources of unlabeled data. It uses a small
amount of labeled data as the core set for the synthesis of useful membership
queries (MQs) - unlabeled instances generated by an algorithm for human
labeling. Our solution uses modification operators, functions that modify
instances to some extent. We apply the operators on a small set of instances
(core set), creating a set of new membership queries. Using this framework, we
look at the instance space as a search space and apply search algorithms in
order to generate new examples highly relevant to the learner. We implement
this framework in the textual domain and test it on several text classification
tasks and show improved classifier performance as more MQs are labeled and
incorporated into the training set. To the best of our knowledge, this is the
first work on membership queries in the textual domain.",Figure 2: Comparison of accuracy achieved by the different methods,How does the accuracy of the US-BS-MQ method compare to that of the S-MQ method when adding SST examples?
spiqa_74,1805.06431v4,"Referencing the learning curves shown in the figure for the CIFAR-10 dataset with 50% random shuffle, which model, WideResNet or ChoiceNet, demonstrates superior accuracy?",The WideResNet model has higher accuracy than the ChoiceNet model on the CIFAR-10 dataset with 50% random shuffle.,1805.06431v4.pdf,"['1805.06431v4.pdf', '1611.05742v3.pdf', '1705.02798v6.pdf', '1703.10730v2.pdf']","The figure shows the learning curves of the WideResNet and ChoiceNet models on the CIFAR-10 dataset with 50% random shuffle. The WideResNet model's learning curve is higher than the ChoiceNet model's learning curve, indicating that the WideResNet model has higher accuracy.",1805.06431v4-Figure14-1.png,Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks,"In this paper, we focus on weakly supervised learning with noisy training
data for both classification and regression problems.We assume that the
training outputs are collected from a mixture of a target and correlated noise
distributions.Our proposed method simultaneously estimates the target
distribution and the quality of each data which is defined as the correlation
between the target and data generating distributions.The cornerstone of the
proposed method is a Cholesky Block that enables modeling dependencies among
mixture distributions in a differentiable manner where we maintain the
distribution over the network weights.We first provide illustrative examples in
both regression and classification tasks to show the effectiveness of the
proposed method.Then, the proposed method is extensively evaluated in a number
of experiments where we show that it constantly shows comparable or superior
performances compared to existing baseline methods in the handling of noisy
data.",Learning curves of compared methods on CIFAR-10 experiments with different noise levels.,How does the accuracy of the WideResNet model compare to the ChoiceNet model on the CIFAR-10 dataset with 50% random shuffle?
spiqa_75,1706.00633v4,"In the figure illustrating the performance of the ResNet-32 model on CIFAR-10 in the context of adversarial detection, how does the model's accuracy respond to increasing values of the hyperparameter c?",The accuracy of the model decreases as the value of c increases.,1706.00633v4.pdf,"['1706.00633v4.pdf', '1901.00398v2.pdf', '1703.00060v2.pdf', '1805.06447v3.pdf']",The figure shows that the accuracy of the model is highest when c is small and decreases as c increases.,1706.00633v4-Figure7-1.png,Towards Robust Detection of Adversarial Examples,"Although the recent progress is substantial, deep learning methods can be
vulnerable to the maliciously generated adversarial examples. In this paper, we
present a novel training procedure and a thresholding test strategy, towards
robust detection of adversarial examples. In training, we propose to minimize
the reverse cross-entropy (RCE), which encourages a deep network to learn
latent representations that better distinguish adversarial examples from normal
ones. In testing, we propose to use a thresholding strategy as the detector to
filter out adversarial examples for reliable predictions. Our method is simple
to implement using standard algorithms, with little extra training cost
compared to the common cross-entropy minimization. We apply our method to
defend various attacking methods on the widely used MNIST and CIFAR-10
datasets, and achieve significant improvements on robust predictions under all
the threat models in the adversarial setting.","The network is Resnet-32, the dataset is CIFAR-10.",How does the accuracy of the model change as the value of c increases?
spiqa_76,1706.03847v3,"Based on the figure comparing the median negative gradients of BPR and BPR-max with respect to the target score, how does the addition of 2048 negative samples, as illustrated in the center and right panels, affect the gradient behavior compared to using only minibatch samples in the context of session-based recommendations?",The addition of negative samples increases the gradient of BPR and BPR-max with respect to the target score.,1706.03847v3.pdf,"['1706.03847v3.pdf', '1703.10730v2.pdf', '1701.03077v10.pdf', '1802.07459v2.pdf', '1805.00912v4.pdf']",The plots in the center and right panels of the figure show that the gradients of BPR and BPR-max are higher when 2048 additional negative samples are added to the minibatch samples (center and right panels) than when only minibatch samples are used (left panel).,1706.03847v3-Figure2-1.png,Recurrent Neural Networks with Top-k Gains for Session-based Recommendations,"RNNs have been shown to be excellent models for sequential data and in
particular for data that is generated by users in an session-based manner. The
use of RNNs provides impressive performance benefits over classical methods in
session-based recommendations. In this work we introduce novel ranking loss
functions tailored to RNNs in the recommendation setting. The improved
performance of these losses over alternatives, along with further tricks and
refinements described in this work, allow for an overall improvement of up to
35% in terms of MRR and Recall@20 over previous session-based RNN solutions and
up to 53% over classical collaborative filtering approaches. Unlike data
augmentation-based improvements, our method does not increase training times
significantly. We further demonstrate the performance gain of the RNN over
baselines in an online A/B test.","Median negative gradients of BPR and BPR-max w.r.t. the target score against the rank of the target item. Left: only minibatch samples are used (minibatch size: 32); Center: 2048 additional negative samples were added to the minibatch samples; Right: same setting as the center, focusing on ranks 0-200.",How does the addition of negative samples affect the gradient of BPR and BPR-max with respect to the target score?
spiqa_77,1809.03149v2,"Based on the figure of per-hour advertising rates within one day from the psCMDP experiment, how does the fixed 0.35 rate in the ""Fix"" curve compare to the dynamically optimized advertising rate in the ""Oracle"" curve at hour 14?","The advertising rate for the ""Fix"" curve is lower than the ""Oracle"" curve at hour 14.",1809.03149v2.pdf,"['1809.03149v2.pdf', '1811.02553v4.pdf', '1701.03077v10.pdf', '1803.02750v3.pdf', '1805.04609v3.pdf']","The ""Fix"" curve is shown in blue, and the ""Oracle"" curve is shown in red. At hour 14, the red curve is higher than the blue curve, indicating that the advertising rate for the ""Oracle"" curve is higher.",1809.03149v2-Figure8-1.png,Learning Adaptive Display Exposure for Real-Time Advertising,"In E-commerce advertising, where product recommendations and product ads are
presented to users simultaneously, the traditional setting is to display ads at
fixed positions. However, under such a setting, the advertising system loses
the flexibility to control the number and positions of ads, resulting in
sub-optimal platform revenue and user experience. Consequently, major
e-commerce platforms (e.g., Taobao.com) have begun to consider more flexible
ways to display ads. In this paper, we investigate the problem of advertising
with adaptive exposure: can we dynamically determine the number and positions
of ads for each user visit under certain business constraints so that the
platform revenue can be increased? More specifically, we consider two types of
constraints: request-level constraint ensures user experience for each user
visit, and platform-level constraint controls the overall platform monetization
rate. We model this problem as a Constrained Markov Decision Process with
per-state constraint (psCMDP) and propose a constrained two-level reinforcement
learning approach to decompose the original problem into two relatively
independent sub-problems. To accelerate policy learning, we also devise a
constrained hindsight experience replay mechanism. Experimental evaluations on
industry-scale real-world datasets demonstrate the merits of our approach in
both obtaining higher revenue under the constraints and the effectiveness of
the constrained hindsight experience replay mechanism.","The changing curves of per-hour advertising rates with one day. Fix: The proportion of ads exposed on each request is fixed and set to 0.35; Oracle: After one day, we could figure out the best dynamical advertising rate available for each hour under conditions that satisfy the daily constraint: PVR = 0.35 through data analysis.","How does the advertising rate for the ""Fix"" curve compare to the ""Oracle"" curve at hour 14?"
spiqa_78,1809.03149v2,"Based on the system structure depicted in the figure, how does the adaptive display ad system generate, shuffle, and score the candidate set of items to dynamically select the most relevant ads for optimal user experience and platform revenue under specific business constraints?","The advertising system selects the best items to show to the user by first generating a candidate set of items from the recommender system. This candidate set is then shuffled and sorted by their score, which is determined by the network. The network takes into account the features of the items and the scoring factors, which are likely based on the user's past behavior and preferences.",1809.03149v2.pdf,"['1809.03149v2.pdf', '1811.10673v1.pdf', '1703.00060v2.pdf', '1707.00524v2.pdf', '1705.07164v8.pdf', '1704.08615v2.pdf', '1805.01216v3.pdf', '1805.00912v4.pdf', '1704.00774v3.pdf', '1703.07015v3.pdf', '1706.03847v3.pdf', '1710.01507v4.pdf']","The figure shows the overall structure of the advertising system. It shows how the candidate set of items is generated, how the items are scored, and how the final set of items is selected.",1809.03149v2-Figure1-1.png,Learning Adaptive Display Exposure for Real-Time Advertising,"In E-commerce advertising, where product recommendations and product ads are
presented to users simultaneously, the traditional setting is to display ads at
fixed positions. However, under such a setting, the advertising system loses
the flexibility to control the number and positions of ads, resulting in
sub-optimal platform revenue and user experience. Consequently, major
e-commerce platforms (e.g., Taobao.com) have begun to consider more flexible
ways to display ads. In this paper, we investigate the problem of advertising
with adaptive exposure: can we dynamically determine the number and positions
of ads for each user visit under certain business constraints so that the
platform revenue can be increased? More specifically, we consider two types of
constraints: request-level constraint ensures user experience for each user
visit, and platform-level constraint controls the overall platform monetization
rate. We model this problem as a Constrained Markov Decision Process with
per-state constraint (psCMDP) and propose a constrained two-level reinforcement
learning approach to decompose the original problem into two relatively
independent sub-problems. To accelerate policy learning, we also devise a
constrained hindsight experience replay mechanism. Experimental evaluations on
industry-scale real-world datasets demonstrate the merits of our approach in
both obtaining higher revenue under the constraints and the effectiveness of
the constrained hindsight experience replay mechanism.",Advertising with Adaptive Exposure and Our System Structure.,How does the advertising system select the best items to show to the user?
spiqa_79,1706.03847v3,New question:,"The alpha parameter generally increases recommendation accuracy as the sample size increases. However, the effect of alpha varies depending on the loss function used. For cross-entropy loss, alpha has a relatively small effect on accuracy. For TOP1-max loss, alpha has a larger effect on accuracy, especially for smaller sample sizes. For BPR-max loss, alpha has a very large effect on accuracy, with higher values of alpha leading to much higher accuracy.",1706.03847v3.pdf,"['1706.03847v3.pdf', '1706.04269v2.pdf', '1608.02784v2.pdf', '1809.01989v2.pdf', '1708.06832v3.pdf']","The figure shows the recall of the recommendation system for different values of alpha and different sample sizes. The recall is a measure of how well the system recommends relevant items. The figure shows that the recall generally increases as the sample size increases, and that the effect of alpha varies depending on the loss function used.",1706.03847v3-Figure5-1.png,Recurrent Neural Networks with Top-k Gains for Session-based Recommendations,"RNNs have been shown to be excellent models for sequential data and in
particular for data that is generated by users in an session-based manner. The
use of RNNs provides impressive performance benefits over classical methods in
session-based recommendations. In this work we introduce novel ranking loss
functions tailored to RNNs in the recommendation setting. The improved
performance of these losses over alternatives, along with further tricks and
refinements described in this work, allow for an overall improvement of up to
35% in terms of MRR and Recall@20 over previous session-based RNN solutions and
up to 53% over classical collaborative filtering approaches. Unlike data
augmentation-based improvements, our method does not increase training times
significantly. We further demonstrate the performance gain of the RNN over
baselines in an online A/B test.",The effect of the alpha parameter on recommendation accuracy at different sample sizes on the CLASS dataset. Left: cross-entropy loss; Middle: TOP1-max loss; Right: BPR-max loss.,How does the alpha parameter affect recommendation accuracy for different sample sizes on the CLASS dataset?
spiqa_80,1809.03449v3,"According to the figure evaluating the effect of κ in the data enrichment method within the ""Explicit Utilization of General Knowledge in Machine Reading Comprehension"" paper, how does the average number of inter-word semantic connections per word change as κ increases from 0 to 5?",The average number of inter-word semantic connections per word increases as the value of κ increases.,1809.03449v3.pdf,"['1809.03449v3.pdf', '1705.10667v4.pdf', '1803.06506v3.pdf', '1805.04609v3.pdf', '1805.07567v2.pdf', '1703.04887v4.pdf', '1707.06320v2.pdf', '1707.01917v2.pdf', '1809.00263v5.pdf', '1703.00899v2.pdf', '1707.00524v2.pdf', '1704.07121v2.pdf', '1804.07707v2.pdf', '1706.00633v4.pdf']",The table shows that the average number of inter-word semantic connections per word increases from 0.39 to 5.58 as the value of κ increases from 0 to 5. This suggests that increasing the value of κ leads to a greater number of semantic connections between words.,1809.03449v3-Table3-1.png,Explicit Utilization of General Knowledge in Machine Reading Comprehension,"To bridge the gap between Machine Reading Comprehension (MRC) models and
human beings, which is mainly reflected in the hunger for data and the
robustness to noise, in this paper, we explore how to integrate the neural
networks of MRC models with the general knowledge of human beings. On the one
hand, we propose a data enrichment method, which uses WordNet to extract
inter-word semantic connections as general knowledge from each given
passage-question pair. On the other hand, we propose an end-to-end MRC model
named as Knowledge Aided Reader (KAR), which explicitly uses the above
extracted general knowledge to assist its attention mechanisms. Based on the
data enrichment method, KAR is comparable in performance with the
state-of-the-art MRC models, and significantly more robust to noise than them.
When only a subset (20%-80%) of the training examples are available, KAR
outperforms the state-of-the-art MRC models by a large margin, and is still
reasonably robust to noise.","With κ set to different values in the data enrichment method, we calculate the average number of inter-word semantic connections per word as an estimation of the amount of general knowledge, and evaluate the performance of KAR on the development set.",How does the average number of inter-word semantic connections per word change as the value of κ increases?
spiqa_81,1809.01246v1,"Referring to the figure captioned ""Average Precision of 1-hop Precursor Queries,"" how does the average precision of TCM(256*memory) for the email-EuAll dataset compare to the performance of the other two algorithms in this paper?",The average precision of TCM(256*memory) is lower than the other two algorithms in the email-EuAll dataset.,1809.01246v1.pdf,"['1809.01246v1.pdf', '1703.10730v2.pdf', '1811.08481v2.pdf', '1802.07351v2.pdf', '1705.02798v6.pdf', '1804.07849v4.pdf', '1605.07496v3.pdf', '1811.07073v3.pdf', '1703.04887v4.pdf', '1710.05654v2.pdf', '1804.05938v2.pdf', '1809.03550v3.pdf']","The figure shows the average precision of three algorithms for five different datasets. The average precision of TCM(256*memory) is shown as a blue line. In the email-EuAll dataset, the blue line is below the other two lines, indicating that the average precision of TCM(256*memory) is lower than the other two algorithms.",1809.01246v1-Figure9-1.png,Fast and Accurate Graph Stream Summarization,"A graph stream is a continuous sequence of data items, in which each item
indicates an edge, including its two endpoints and edge weight. It forms a
dynamic graph that changes with every item in the stream. Graph streams play
important roles in cyber security, social networks, cloud troubleshooting
systems and other fields. Due to the vast volume and high update speed of graph
streams, traditional data structures for graph storage such as the adjacency
matrix and the adjacency list are no longer sufficient. However, prior art of
graph stream summarization, like CM sketches, gSketches, TCM and gMatrix,
either supports limited kinds of queries or suffers from poor accuracy of query
results. In this paper, we propose a novel Graph Stream Sketch (GSS for short)
to summarize the graph streams, which has the linear space cost (O(|E|), E is
the edge set of the graph) and the constant update time complexity (O(1)) and
supports all kinds of queries over graph streams with the controllable errors.
Both theoretical analysis and experiment results confirm the superiority of our
solution with regard to the time/space complexity and query results' precision
compared with the state-of-the-art.",Average Precision of 1-hop Precursor Queries,How does the average precision of TCM(256*memory) compare to the other two algorithms in the email-EuAll dataset?
spiqa_82,1811.07073v3,"How does the bounding box encoder's attention map, as illustrated in Figure 2 of the ""Semi-Supervised Semantic Image Segmentation with Self-correcting Networks"" paper, interact with multi-scale feature maps from the encoder, and how does this fusion affect the segmentation process before the decoder stage?",The bounding box encoder network embeds bounding box information at different scales and outputs attention maps that are used to fuse with feature maps from the encoder before being passed to the decoder.,1811.07073v3.pdf,"['1811.07073v3.pdf', '1710.06177v2.pdf', '1709.02755v5.pdf', '1707.00524v2.pdf', '1811.08481v2.pdf', '1611.04363v2.pdf']",The figure shows the bounding box encoder network as a separate branch that receives the bounding box information as input. The output of this network is then used to modify the feature maps from the encoder before they are passed to the decoder.,1811.07073v3-Figure2-1.png,Semi-Supervised Semantic Image Segmentation with Self-correcting Networks,"Building a large image dataset with high-quality object masks for semantic
segmentation is costly and time consuming. In this paper, we introduce a
principled semi-supervised framework that only uses a small set of fully
supervised images (having semantic segmentation labels and box labels) and a
set of images with only object bounding box labels (we call it the weak set).
Our framework trains the primary segmentation model with the aid of an
ancillary model that generates initial segmentation labels for the weak set and
a self-correction module that improves the generated labels during training
using the increasingly accurate primary model. We introduce two variants of the
self-correction module using either linear or convolutional functions.
Experiments on the PASCAL VOC 2012 and Cityscape datasets show that our models
trained with a small fully supervised set perform similar to, or better than,
models trained with a large fully supervised set while requiring ~7x less
annotation effort.","Figure 2: An overview of the ancillary segmentation model. We modify an existing encoder-decoder segmentation model by introducing a bounding box encoder that embeds the box information. The output of the bounding box encoder after passing through a sigmoid activation acts as an attention map. Feature maps at different scales from the encoder are fused (using element-wise-multiplication) with attention maps, then passed to the decoder.",How does the bounding box encoder network influence the segmentation process?
spiqa_83,1701.03077v10,"According to the figure comparing variational autoencoder reconstructions across different image representations, how do the reconstructions using general distributions differ in sharpness and level of detail from those using normal distributions, especially for the DCT + YUV and wavelet representations?",Reconstructions from models using general distributions tend to be sharper and more detailed than reconstructions from the corresponding model that uses normal distributions.,1701.03077v10.pdf,"['1701.03077v10.pdf', '1803.06506v3.pdf', '1704.00774v3.pdf', '1811.08481v2.pdf', '1804.04410v2.pdf', '1805.01216v3.pdf', '1710.06177v2.pdf', '1701.06171v4.pdf', '1805.00912v4.pdf', '1803.04572v2.pdf', '1809.02731v3.pdf', '1703.07015v3.pdf']","This can be seen in the figure by comparing the reconstructions in the ""Ours"" columns to the reconstructions in the ""Normal"" columns. For example, the reconstructions in the ""Ours"" column for the DCT + YUV representation are much sharper and more detailed than the reconstructions in the ""Normal"" column.",1701.03077v10-Figure15-1.png,A General and Adaptive Robust Loss Function,"We present a generalization of the Cauchy/Lorentzian, Geman-McClure,
Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2
loss functions. By introducing robustness as a continuous parameter, our loss
function allows algorithms built around robust loss minimization to be
generalized, which improves performance on basic vision tasks such as
registration and clustering. Interpreting our loss as the negative log of a
univariate density yields a general probability distribution that includes
normal and Cauchy distributions as special cases. This probabilistic
interpretation enables the training of neural networks in which the robustness
of the loss automatically adapts itself during training, which improves
performance on learning-based tasks such as generative image synthesis and
unsupervised monocular depth estimation, without requiring any manual parameter
tuning.","Reconstructions from our family of trained variational autoencoders, in which we use one of three different image representations for modeling images (super-columns) and use either normal, Cauchy, Student’s t, or our general distributions for modeling the coefficients of each representation (sub-columns). The leftmost column shows the images which are used as input to each autoencoder. Reconstructions from models using general distributions tend to be sharper and more detailed than reconstructions from the corresponding model that uses normal distributions, particularly for the DCT or wavelet representations, though this difference is less pronounced than what is seen when comparing samples from these models. The DCT and wavelet models trained with Cauchy distributions or Student’s t-distributions systematically fail to preserve the background of the input image, as was noted when observing samples from these distributions.",How does the choice of distribution affect the quality of the reconstructions?
spiqa_84,1611.07718v2,"How does the classification error change with increasing average path length in residual networks on CIFAR-10, as demonstrated in the figure comparing networks with 3 and 6 residual blocks?",The classification error of a residual network generally increases as the average path length increases.,1611.07718v2.pdf,"['1611.07718v2.pdf', '1611.04684v1.pdf', '1705.07164v8.pdf', '1705.09882v2.pdf', '1703.02507v3.pdf', '1803.01128v3.pdf', '1804.05938v2.pdf', '1809.01246v1.pdf', '1811.02721v3.pdf', '1709.08294v3.pdf', '1709.02755v5.pdf', '1812.06589v2.pdf', '1805.06447v3.pdf', '1804.07849v4.pdf']","The figure shows two lines, one for a residual network with 3 residual blocks and one for a residual network with 6 residual blocks. Both lines show an overall increase in classification error as the average path length increases. This suggests that as the network becomes deeper, it becomes more difficult for it to learn the underlying patterns in the data.",1611.07718v2-Figure6-1.png,Deep Convolutional Neural Networks with Merge-and-Run Mappings,"A deep residual network, built by stacking a sequence of residual blocks, is
easy to train, because identity mappings skip residual branches and thus
improve information flow. To further reduce the training difficulty, we present
a simple network architecture, deep merge-and-run neural networks. The novelty
lies in a modularized building block, merge-and-run block, which assembles
residual branches in parallel through a merge-and-run mapping: Average the
inputs of these residual branches (Merge), and add the average to the output of
each residual branch as the input of the subsequent residual branch (Run),
respectively. We show that the merge-and-run mapping is a linear idempotent
function in which the transformation matrix is idempotent, and thus improves
information flow, making training easy. In comparison to residual networks, our
networks enjoy compelling advantages: they contain much shorter paths, and the
width, i.e., the number of channels, is increased. We evaluate the performance
on the standard recognition tasks. Our approach demonstrates consistent
improvements over ResNets with the comparable setup, and achieves competitive
results (e.g., $3.57\%$ testing error on CIFAR-$10$, $19.00\%$ on CIFAR-$100$,
$1.51\%$ on SVHN).",Illustrating how the testing errors of residual networks change as the average path length increases. The results are reported on CIFAR-10.,How does the classification error of a residual network change as the average path length increases?
spiqa_85,1803.01128v3,"Referring to Table 4 of the Seq2Sick paper, how does the success rate of a targeted keyword attack on text summarization tasks change as the number of targeted keywords increases from 1 or 2 to 3, and what challenges arise from this increase?",The difficulty of performing a successful targeted keywords attack increases as the number of targeted keywords increases.,1803.01128v3.pdf,"['1803.01128v3.pdf', '1802.07222v1.pdf', '1612.02803v5.pdf', '1707.01917v2.pdf', '1702.03584v3.pdf']","The table shows that the success rate of the attack drops significantly when the number of targeted keywords ($|K|$) goes from 2 to 3, for all datasets. When $|K|=1$ or $|K|=2$, the success rate is very high (above 87%). However, when $|K|=3$, the success rate falls to around 40%. This indicates that it is much harder to manipulate the summarization process to include three specific keywords compared to one or two.",1803.01128v3-Table4-1.png,Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples,"Crafting adversarial examples has become an important technique to evaluate
the robustness of deep neural networks (DNNs). However, most existing works
focus on attacking the image classification problem since its input space is
continuous and output space is finite.
  In this paper, we study the much more challenging problem of crafting
adversarial examples for sequence-to-sequence (seq2seq) models, whose inputs
are discrete text strings and outputs have an almost infinite number of
possibilities. To address the challenges caused by the discrete input space, we
propose a projected gradient method combined with group lasso and gradient
regularization. To handle the almost infinite output space, we design some
novel loss functions to conduct non-overlapping attack and targeted keyword
attack. We apply our algorithm to machine translation and text summarization
tasks, and verify the effectiveness of the proposed algorithm: by changing less
than 3 words, we can make seq2seq model to produce desired outputs with high
success rates. On the other hand, we recognize that, compared with the
well-evaluated CNN-based classifiers, seq2seq models are intrinsically more
robust to adversarial attacks.","Table 4: Results of targeted keywords attack in text summarization. |K| is the number of keywords. We found that our method can make the summarization include 1 or 2 target keywords with a high success rate, while the changes made to the input sentences are relatively small, as indicated by the high BLEU scores and low average number of changed words. When |K| = 3, this task becomes more challenging, but our algorithm can still find many adversarial examples.",How does the difficulty of performing a successful targeted keywords attack change as the number of targeted keywords increases?
spiqa_86,1803.03467v4,"Based on the parameter sensitivity analysis shown in the figure of the RippleNet paper, how does the AUC change as the embedding dimension increases for the MovieLens-1M dataset, and at what point does the AUC reach its peak?",The AUC of RippleNet first increases and then decreases with the increase of the dimension of embedding.,1803.03467v4.pdf,"['1803.03467v4.pdf', '1611.03780v2.pdf', '1803.06506v3.pdf', '1706.04284v3.pdf', '1809.01246v1.pdf', '1804.07849v4.pdf', '1708.01425v4.pdf']","The figure shows that the AUC reaches its peak when the dimension of embedding is 8, and then starts to decrease.",1803.03467v4-Figure9-1.png,RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems,"To address the sparsity and cold start problem of collaborative filtering,
researchers usually make use of side information, such as social networks or
item attributes, to improve recommendation performance. This paper considers
the knowledge graph as the source of side information. To address the
limitations of existing embedding-based and path-based methods for
knowledge-graph-aware recommendation, we propose Ripple Network, an end-to-end
framework that naturally incorporates the knowledge graph into recommender
systems. Similar to actual ripples propagating on the surface of water, Ripple
Network stimulates the propagation of user preferences over the set of
knowledge entities by automatically and iteratively extending a user's
potential interests along links in the knowledge graph. The multiple ""ripples""
activated by a user's historically clicked items are thus superposed to form
the preference distribution of the user with respect to a candidate item, which
could be used for predicting the final clicking probability. Through extensive
experiments on real-world datasets, we demonstrate that Ripple Network achieves
substantial gains in a variety of scenarios, including movie, book and news
recommendation, over several state-of-the-art baselines.",Parameter sensitivity of RippleNet.,How does the dimension of embedding affect the AUC of RippleNet on MovieLens-1M?
spiqa_87,1703.00060v2,"Based on the discrimination values in the ""DE_M_h*"" columns of Table 2 for a sample size of 2000, how does the prediction discrimination of the two-phase framework (MSG) compare to DI, with and without classifier tweaking, and what does this reveal about the effectiveness of the methods?","When the sample size is 2000, the two-phase framework (MSG) achieves lower discrimination in prediction compared to DI, both with and without classifier tweaking.

With classifier tweaking: MSG achieves a discrimination level of 0.016 ± 5.3E-4, while DI shows a significantly higher level of 0.095 ± 1.6E-3.
Without classifier tweaking: MSG still demonstrates lower discrimination with 0.067 ± 4.3E-3 compared to DI's 0.095 ± 1.6E-3.

This indicates that the two-phase framework is more effective in removing discrimination from predictions than DI, regardless of whether classifier tweaking is applied.",1703.00060v2.pdf,"['1703.00060v2.pdf', '1802.07351v2.pdf', '1603.00286v5.pdf', '1707.00189v3.pdf', '1708.05239v3.pdf', '1705.10667v4.pdf', '1802.07222v1.pdf', '1704.07121v2.pdf', '1802.07459v2.pdf', '1707.06320v2.pdf', '1805.06431v4.pdf', '1706.00633v4.pdf', '1703.02507v3.pdf', '1706.04284v3.pdf', '1804.07707v2.pdf']","Table 2 presents the measured discrimination after discrimination removal for both MSG and DI methods. The rows correspond to different sample sizes, and the columns represent different discrimination measures. By comparing the values in the ""DE_M_h*"" columns for both methods at the row where the sample size is 2000, we can directly assess the difference in their prediction discrimination levels. The lower values for MSG in both scenarios (with and without tweaking) demonstrate its superior performance in eliminating prediction discrimination.",1703.00060v2-Table2-1.png,Achieving non-discrimination in prediction,"Discrimination-aware classification is receiving an increasing attention in
data science fields. The pre-process methods for constructing a
discrimination-free classifier first remove discrimination from the training
data, and then learn the classifier from the cleaned data. However, they lack a
theoretical guarantee for the potential discrimination when the classifier is
deployed for prediction. In this paper, we fill this gap by mathematically
bounding the probability of the discrimination in prediction being within a
given interval in terms of the training data and classifier. We adopt the
causal model for modeling the data generation mechanism, and formally defining
discrimination in population, in a dataset, and in prediction. We obtain two
important theoretical results: (1) the discrimination in prediction can still
exist even if the discrimination in the training data is completely removed;
and (2) not all pre-process methods can ensure non-discrimination in prediction
even though they can achieve non-discrimination in the modified training data.
Based on the results, we develop a two-phase framework for constructing a
discrimination-free classifier with a theoretical guarantee. The experiments
demonstrate the theoretical results and show the effectiveness of our two-phase
framework.",Table 2: Measured discrimination after discrimination removal (decision tree as the classifier).,"How does the discrimination in the prediction of the two-phase framework (MSG) compare to that of DI, both with and without classifier tweaking, when the sample size is 2000?"
spiqa_88,1812.06589v2,"In Figure 2 of the ""Attentional Audio-Visual Coherence Learning"" paper, how does the dynamic attention block improve lip synchronization and video transition for arbitrary talking face generation by selectively focusing on the lip area while maintaining identity consistency?","The dynamic attention block decouples the lip-related and identity-related information, allowing the network to focus on the most important area for generating realistic talking faces.",1812.06589v2.pdf,"['1812.06589v2.pdf', '1706.04269v2.pdf', '1809.03550v3.pdf', '1804.05936v2.pdf', '1805.04609v3.pdf', '1805.06447v3.pdf', '1705.09296v2.pdf', '1703.02507v3.pdf', '1706.00827v2.pdf', '1811.07073v3.pdf', '1809.03149v2.pdf', '1804.00863v3.pdf', '1706.08146v3.pdf', '1707.08608v3.pdf']","Figure 2 shows how the dynamic attention block works. The block takes as input the previous generated frame and the current audio frame. It then uses a convolutional neural network to compute a set of attention maps, which are used to weight the different parts of the input frame. The attention maps are then used to generate the next frame. The figure shows that the attention maps focus on the lip area, which is the most important area for generating realistic talking faces.",1812.06589v2-Figure4-1.png,Arbitrary Talking Face Generation via Attentional Audio-Visual Coherence Learning,"Talking face generation aims to synthesize a face video with precise lip
synchronization as well as a smooth transition of facial motion over the entire
video via the given speech clip and facial image. Most existing methods mainly
focus on either disentangling the information in a single image or learning
temporal information between frames. However, cross-modality coherence between
audio and video information has not been well addressed during synthesis. In
this paper, we propose a novel arbitrary talking face generation framework by
discovering the audio-visual coherence via the proposed Asymmetric Mutual
Information Estimator (AMIE). In addition, we propose a Dynamic Attention (DA)
block by selectively focusing the lip area of the input image during the
training stage, to further enhance lip synchronization. Experimental results on
benchmark LRW dataset and GRID dataset transcend the state-of-the-art methods
on prevalent metrics with robust high-resolution synthesizing on gender and
pose variations.",Figure 4: The illustration of the proposed dynamic attention.,How does the dynamic attention block improve the transition of generated video for arbitrary identities?
spiqa_89,1805.01216v3,"Based on Figure 2 of the BoSsNet paper, how does the encoder specifically utilize the memory cell representations of the dialog history and KB tuples to process and understand the final user utterance in a task-oriented dialog?",The encoder understands the last user utterance by using the memory cell representations of the dialog history and KB tuples.,1805.01216v3.pdf,"['1805.01216v3.pdf', '1901.00056v2.pdf', '1709.02418v2.pdf', '1708.06832v3.pdf', '1901.00398v2.pdf', '1708.03797v1.pdf', '1805.07567v2.pdf', '1703.00060v2.pdf', '1704.07854v4.pdf', '1707.08608v3.pdf']",The figure shows that the encoder receives input from the memory cell representations of the dialog history and KB tuples. This suggests that the encoder uses these representations to understand the context of the conversation and generate a response.,1805.01216v3-Figure2-1.png,Disentangling Language and Knowledge in Task-Oriented Dialogs,"The Knowledge Base (KB) used for real-world applications, such as booking a
movie or restaurant reservation, keeps changing over time. End-to-end neural
networks trained for these task-oriented dialogs are expected to be immune to
any changes in the KB. However, existing approaches breakdown when asked to
handle such changes. We propose an encoder-decoder architecture (BoSsNet) with
a novel Bag-of-Sequences (BoSs) memory, which facilitates the disentangled
learning of the response's language model and its knowledge incorporation.
Consequently, the KB can be modified with new knowledge without a drop in
interpretability. We find that BoSsNet outperforms state-of-the-art models,
with considerable improvements (> 10\%) on bAbI OOV test sets and other
human-human datasets. We also systematically modify existing datasets to
measure disentanglement and show BoSsNet to be robust to KB modifications.",Figure 2: The dialog history and KB tuples stored in the memory have memory cell representations and token representations. The encoder understands the last user utterance using only the memory cell representations. The decoder generates the next response using both representations.,How does the encoder understand the last user utterance?
spiqa_90,1704.07854v4,"In the context of Figure b from your paper, how does the water flow change when the central wall obstacle is shifted to the right in the liquid simulation of the stairs configuration?",The flow of water increases as the central wall obstacle is shifted to the right.,1704.07854v4.pdf,"['1704.07854v4.pdf', '1805.02349v2.pdf', '1906.10843v1.pdf', '1704.05958v2.pdf', '1802.07222v1.pdf', '1705.09966v2.pdf', '1706.00633v4.pdf', '1703.10730v2.pdf', '1705.02946v3.pdf', '1707.01917v2.pdf', '1811.08481v2.pdf', '1611.05742v3.pdf', '1612.02803v5.pdf']","The figure shows that the flow of water is more constricted when the central wall obstacle is in the center of the stairs. As the wall is shifted to the right, the flow of water has more space to move through, which results in an increase in flow.",1704.07854v4-Figure9-1.png,Generating Liquid Simulations with Deformation-aware Neural Networks,"We propose a novel approach for deformation-aware neural networks that learn
the weighting and synthesis of dense volumetric deformation fields. Our method
specifically targets the space-time representation of physical surfaces from
liquid simulations. Liquids exhibit highly complex, non-linear behavior under
changing simulation conditions such as different initial conditions. Our
algorithm captures these complex phenomena in two stages: a first neural
network computes a weighting function for a set of pre-computed deformations,
while a second network directly generates a deformation field for refining the
surface. Key for successful training runs in this setting is a suitable loss
function that encodes the effect of the deformations, and a robust calculation
of the corresponding gradients. To demonstrate the effectiveness of our
approach, we showcase our method with several complex examples of flowing
liquids with topology changes. Our representation makes it possible to rapidly
generate the desired implicit surfaces. We have implemented a mobile
application to demonstrate that real-time interactions with complex liquid
effects are possible with our approach.","a) Three example configurations from our stairs data set. b) The interactive version of the stair setup shown in the demo app. Notice how the flow around the central wall obstacle changes. As the wall is shifted right, the flow increases corresonpondingly.",How does the flow of water change as the central wall obstacle is shifted to the right?
spiqa_91,1603.03833v4,"What is the effect of downsampling the original trajectory from 33Hz to 4Hz, as shown in Figures 0 and 2, on the creation of multiple manipulation trajectories with different characteristics?"," The frequency reduction process takes a high-frequency trajectory and samples it at a lower frequency, resulting in multiple trajectories with different starting and ending points. ",1603.03833v4.pdf,"['1603.03833v4.pdf', '1804.01429v3.pdf', '1708.00160v2.pdf', '1608.02784v2.pdf', '1705.02798v6.pdf']",Figure 0 shows how the original trajectory (recorded at 33Hz) is sampled at a lower frequency (4Hz) to generate 8 different trajectories. The waypoints are used to guide the generation of the new trajectories.,1603.03833v4-Figure2-1.png,From virtual demonstration to real-world manipulation using LSTM and MDN,"Robots assisting the disabled or elderly must perform complex manipulation
tasks and must adapt to the home environment and preferences of their user.
Learning from demonstration is a promising choice, that would allow the
non-technical user to teach the robot different tasks. However, collecting
demonstrations in the home environment of a disabled user is time consuming,
disruptive to the comfort of the user, and presents safety challenges. It would
be desirable to perform the demonstrations in a virtual environment. In this
paper we describe a solution to the challenging problem of behavior transfer
from virtual demonstration to a physical robot. The virtual demonstrations are
used to train a deep neural network based controller, which is using a Long
Short Term Memory (LSTM) recurrent neural network to generate trajectories. The
training process uses a Mixture Density Network (MDN) to calculate an error
signal suitable for the multimodal nature of demonstrations. The controller
learned in the virtual environment is transferred to a physical robot (a
Rethink Robotics Baxter). An off-the-shelf vision component is used to
substitute for geometric knowledge available in the simulation and an inverse
kinematics module is used to allow the Baxter to enact the trajectory. Our
experimental studies validate the three contributions of the paper: (1) the
controller learned from virtual demonstrations can be used to successfully
perform the manipulation tasks on a physical robot, (2) the LSTM+MDN
architectural choice outperforms other choices, such as the use of feedforward
networks and mean-squared error based training signals and (3) allowing
imperfect demonstrations in the training set also allows the controller to
learn how to correct its manipulation mistakes.",Figure 2: Creating multiple trajectories from a demonstration recorded at a higher frequency.,How does the frequency reduction process create multiple trajectories from a single demonstration?
spiqa_92,1710.05654v2,"What does the figure in the ""Large Scale Graph Learning from Smooth Signals"" paper reveal about the change in graph diameter with increasing average degree for both the small spherical and word2vec datasets, and how does this relate to the quality of manifold recovery?","The graph diameter generally decreases with increasing average degree for all methods and datasets. However, the rate of decrease and the final diameter value vary depending on the method and dataset.",1710.05654v2.pdf,"['1710.05654v2.pdf', '1906.06589v3.pdf', '1703.00060v2.pdf', '1803.04572v2.pdf']","The figure shows the graph diameter as a function of the average degree for different methods and datasets. The diameter is a measure of how far apart nodes are in the graph, and a lower diameter indicates a better manifold recovery. The figure shows that the diameter decreases as the average degree increases, which means that the nodes are becoming more connected and the manifold is being recovered more accurately.",1710.05654v2-Figure9-1.png,Large Scale Graph Learning from Smooth Signals,"Graphs are a prevalent tool in data science, as they model the inherent
structure of the data. They have been used successfully in unsupervised and
semi-supervised learning. Typically they are constructed either by connecting
nearest samples, or by learning them from data, solving an optimization
problem. While graph learning does achieve a better quality, it also comes with
a higher computational cost. In particular, the current state-of-the-art model
cost is $\mathcal{O}(n^2)$ for $n$ samples. In this paper, we show how to scale
it, obtaining an approximation with leading cost of $\mathcal{O}(n\log(n))$,
with quality that approaches the exact graph learning model. Our algorithm uses
known approximate nearest neighbor techniques to reduce the number of
variables, and automatically selects the correct parameters of the model,
requiring a single intuitive input: the desired edge density.","Graph diameter measures manifold recovery quality. Left: small spherical data: 4096 nodes, 1920 signals. Middle: Same data, 40 signals. Right: word2vec: 10,000 nodes, 300 features.",How does the graph diameter change with increasing average degree for different methods and datasets?
spiqa_93,1703.04887v4,"How does increasing the initial accuracy of the discriminator affect the BLEU score in the Chinese-English translation task, as illustrated in the figure comparing different accuracy levels in the BR-CSGAN model?",The BLEU score decreases as the initial accuracy of the discriminator increases.,1703.04887v4.pdf,"['1703.04887v4.pdf', '1705.09882v2.pdf', '1603.03833v4.pdf', '1708.02153v2.pdf', '1804.07931v2.pdf']","The plot shows that the BLEU score decreases as the initial accuracy of the discriminator increases. This is likely because a more accurate discriminator is able to better distinguish between real and generated data, which makes it more difficult for the generator to produce realistic data.",1703.04887v4-Figure2-1.png,Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets,"This paper proposes an approach for applying GANs to NMT. We build a
conditional sequence generative adversarial net which comprises of two
adversarial sub models, a generator and a discriminator. The generator aims to
generate sentences which are hard to be discriminated from human-translated
sentences (i.e., the golden target sentences), And the discriminator makes
efforts to discriminate the machine-generated sentences from human-translated
ones. The two sub models play a mini-max game and achieve the win-win situation
when they reach a Nash Equilibrium. Additionally, the static sentence-level
BLEU is utilized as the reinforced objective for the generator, which biases
the generation towards high BLEU points. During training, both the dynamic
discriminator and the static BLEU objective are employed to evaluate the
generated sentences and feedback the evaluations to guide the learning of the
generator. Experimental results show that the proposed model consistently
outperforms the traditional RNNSearch and the newly emerged state-of-the-art
Transformer on English-German and Chinese-English translation tasks.",BLEU score on the development set for the BR-CSGAN where the discriminators have different initial accuracy. ”0.6-acc” means the initial accuracy is 0.6. We report the results on the Chinese-English translation tasks. RNNSearch is taken as the generator.,How does the initial accuracy of the discriminator affect the BLEU score?
spiqa_94,1708.01425v4,"How does the intra-warrant attention mechanism, as shown in Figure 5, utilize BiLSTM-encoded reason and claim information to generate attention vectors that guide the LSTM layers for each warrant?","The intra-warrant attention mechanism uses a BiLSTM to encode the reason and claim, and then provides this encoded information as an attention vector to LSTM layers for each warrant. The attention vector allows the model to focus on specific parts of the reason and claim that are most relevant to each warrant.",1708.01425v4.pdf,"['1708.01425v4.pdf', '1703.00060v2.pdf', '1707.08608v3.pdf', '1812.10735v2.pdf', '1606.07384v2.pdf', '1906.06589v3.pdf', '1703.07015v3.pdf', '1707.00524v2.pdf', '1812.00281v3.pdf', '1811.02553v4.pdf', '1603.03833v4.pdf', '1811.06635v1.pdf']","The figure shows how the intra-warrant attention mechanism works. The reason and claim are first encoded using a BiLSTM. This encoded information is then used to create an attention vector, which is then provided to LSTM layers for each warrant. The attention vector allows the model to focus on specific parts of the reason and claim that are most relevant to each warrant.",1708.01425v4-Figure5-1.png,The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants,"Reasoning is a crucial part of natural language argumentation. To comprehend
an argument, one must analyze its warrant, which explains why its claim follows
from its premises. As arguments are highly contextualized, warrants are usually
presupposed and left implicit. Thus, the comprehension does not only require
language understanding and logic skills, but also depends on common sense. In
this paper we develop a methodology for reconstructing warrants systematically.
We operationalize it in a scalable crowdsourcing process, resulting in a freely
licensed dataset with warrants for 2k authentic arguments from news comments.
On this basis, we present a new challenging task, the argument reasoning
comprehension task. Given an argument with a claim and a premise, the goal is
to choose the correct implicit warrant from two options. Both warrants are
plausible and lexically close, but lead to contradicting claims. A solution to
this task will define a substantial step towards automatic warrant
reconstruction. However, experiments with several neural attention and language
models reveal that current approaches do not suffice.",Figure 5: Intra-warrant attention. Only the attention vector for the warrant W1 is shown; the attention vector for W0 is constructed analogously. Grey areas represent a modification with additional context.,How does the intra-warrant attention mechanism work?
spiqa_95,1811.02553v4,"Based on the figure showing the landscape concentration of the humanoid-v2 PPO policy, how does the concentration evolve as the number of state-action pairs increases, in relation to the paper's analysis of the inaccuracies between the surrogate objective and true reward landscape?",The landscape concentration increases with the number of state-action pairs.,1811.02553v4.pdf,"['1811.02553v4.pdf', '1811.06635v1.pdf', '1804.07931v2.pdf', '1702.03584v3.pdf', '1703.07015v3.pdf', '1710.06177v2.pdf', '1709.02418v2.pdf', '1804.05995v2.pdf', '1704.07121v2.pdf', '1805.08751v2.pdf', '1703.04887v4.pdf', '1708.01425v4.pdf']","The figure shows the landscape concentration of the humanoid-v2 PPO policy at different numbers of state-action pairs. As the number of state-action pairs increases, the landscape concentration becomes more focused, indicating that the policy is becoming more confident in its predictions.",1811.02553v4-Figure20-1.png,A Closer Look at Deep Policy Gradients,"We study how the behavior of deep policy gradient algorithms reflects the
conceptual framework motivating their development. To this end, we propose a
fine-grained analysis of state-of-the-art methods based on key elements of this
framework: gradient estimation, value prediction, and optimization landscapes.
Our results show that the behavior of deep policy gradient algorithms often
deviates from what their motivating framework would predict: the surrogate
objective does not match the true reward landscape, learned value estimators
fail to fit the true value function, and gradient estimates poorly correlate
with the ""true"" gradient. The mismatch between predicted and empirical behavior
we uncover highlights our poor understanding of current methods, and indicates
the need to move beyond current benchmark-centric evaluation methods.",Humanoid-v2 PPO landscape concentration (see Figure 5 for a description).,How does the landscape concentration of the humanoid-v2 PPO policy change with respect to the number of state-action pairs?
spiqa_96,1811.10673v1,"In Figure 4 of *Adversarial Video Compression Guided by Soft Edge Detection*, how does reducing the quantization level \(k\) impact the color clustering and distribution around edges, as visualized through the scatter plots and histograms?","As the quantization level $k$ is decreased, the cardinality of colors co-located with edges decreases.",1811.10673v1.pdf,"['1811.10673v1.pdf', '1704.05958v2.pdf', '1803.06506v3.pdf', '1605.07496v3.pdf', '1803.05776v2.pdf', '1803.03467v4.pdf', '1901.00056v2.pdf', '1703.02507v3.pdf', '1811.02553v4.pdf', '1708.01425v4.pdf', '1812.00108v4.pdf', '1703.10730v2.pdf']","The figure shows that as the quantization level is decreased, the number of colors in the output image decreases. This is because the quantization process reduces the number of possible colors that can be represented.",1811.10673v1-Figure4-1.png,Adversarial Video Compression Guided by Soft Edge Detection,"We propose a video compression framework using conditional Generative
Adversarial Networks (GANs). We rely on two encoders: one that deploys a
standard video codec and another which generates low-level maps via a pipeline
of down-sampling, a newly devised soft edge detector, and a novel lossless
compression scheme. For decoding, we use a standard video decoder as well as a
neural network based one, which is trained using a conditional GAN. Recent
""deep"" approaches to video compression require multiple videos to pre-train
generative networks to conduct interpolation. In contrast to this prior work,
our scheme trains a generative decoder on pairs of a very limited number of key
frames taken from a single video and corresponding low-level maps. The trained
decoder produces reconstructed frames relying on a guidance of low-level maps,
without any interpolation. Experiments on a diverse set of 131 videos
demonstrate that our proposed GAN-based compression engine achieves much higher
quality reconstructions at very low bitrates than prevailing standard codecs
such as H.264 or HEVC.","Figure 4: Outputs of soft edge detector. (a) The left-most frame is a 64 × 64 downsampled frame S(1) from a reconstructed frame XI (1)′ of one video [1]. The right four frames are outputs of the soft edge detector for different levels of quantization k (Qk). (b) Grayscale histograms of Qk. (c) Three dimensional scatter plots (normalized R/G/B axes) of S, where colors visually distinguish the clusters indexed by Qk.",How does the level of quantization affect the output of the soft edge detector?
spiqa_97,1811.02721v3,"Referring to the figure illustrating TCP congestion behavior over IEEE 802.15.4 in low-power and lossy networks, how does the maximum link delay influence the frequency of TCP timeouts and fast retransmissions?",The number of TCP timeouts and fast retransmissions decreases as the maximum link delay increases.,1811.02721v3.pdf,"['1811.02721v3.pdf', '1707.00189v3.pdf', '1809.02731v3.pdf', '1805.06447v3.pdf', '1805.07567v2.pdf', '1805.06431v4.pdf', '1809.04276v2.pdf', '1812.00281v3.pdf', '1804.07849v4.pdf', '1708.01425v4.pdf', '1703.10730v2.pdf', '1805.04609v3.pdf']","The figure shows that the number of timeouts and fast retransmissions decreases as the maximum link delay increases. This is because, with a longer link delay, there is more time for the sender to receive an acknowledgment before retransmitting the packet.",1811.02721v3-Figure6-1.png,Performant TCP for Low-Power Wireless Networks,"Low-power and lossy networks (LLNs) enable diverse applications integrating
many resource-constrained embedded devices, often requiring interconnectivity
with existing TCP/IP networks as part of the Internet of Things. But TCP has
received little attention in LLNs due to concerns about its overhead and
performance, leading to LLN-specific protocols that require specialized
gateways for interoperability. We present a systematic study of a well-designed
TCP stack in IEEE 802.15.4-based LLNs, based on the TCP protocol logic in
FreeBSD. Through careful implementation and extensive experiments, we show that
modern low-power sensor platforms are capable of running full-scale TCP and
that TCP, counter to common belief, performs well despite the lossy nature of
LLNs. By carefully studying the interaction between the transport and link
layers, we identify subtle but important modifications to both, achieving TCP
goodput within 25% of an upper bound (5-40x higher than prior results) and
low-power operation commensurate to CoAP in a practical LLN application
scenario. This suggests that a TCP-based transport layer, seamlessly
interoperable with existing TCP/IP networks, is viable and performant in LLNs.",Congestion behavior of TCP over IEEE 802.15.4,How does the maximum link delay affect the number of TCP timeouts and fast retransmissions?
spiqa_98,1811.02721v3,"In the ""Performant TCP for Low-Power Wireless Networks"" paper, as outlined in Figure (a), how does increasing the maximum link delay between retransmissions impact the segment loss rate and goodput in a one-hop TCP connection over IEEE 802.15.4-based LLNs?","As the maximum link delay increases, the segment loss rate increases and the goodput decreases.",1811.02721v3.pdf,"['1811.02721v3.pdf', '1804.05938v2.pdf', '1706.04269v2.pdf', '1703.07015v3.pdf', '1812.00281v3.pdf', '1707.00189v3.pdf', '1805.00912v4.pdf', '1611.03780v2.pdf', '1812.06589v2.pdf', '1706.00633v4.pdf', '1706.03847v3.pdf', '1705.08016v3.pdf', '1701.06171v4.pdf', '1611.04684v1.pdf', '1708.06832v3.pdf']","In Figure (a), we can see that the segment loss rate increases as the maximum link delay increases. This is because the longer the delay, the more likely it is that a segment will be lost. Additionally, the goodput decreases as the maximum link delay increases. This is because the longer the delay, the less data can be transmitted in a given amount of time.",1811.02721v3-Figure5-1.png,Performant TCP for Low-Power Wireless Networks,"Low-power and lossy networks (LLNs) enable diverse applications integrating
many resource-constrained embedded devices, often requiring interconnectivity
with existing TCP/IP networks as part of the Internet of Things. But TCP has
received little attention in LLNs due to concerns about its overhead and
performance, leading to LLN-specific protocols that require specialized
gateways for interoperability. We present a systematic study of a well-designed
TCP stack in IEEE 802.15.4-based LLNs, based on the TCP protocol logic in
FreeBSD. Through careful implementation and extensive experiments, we show that
modern low-power sensor platforms are capable of running full-scale TCP and
that TCP, counter to common belief, performs well despite the lossy nature of
LLNs. By carefully studying the interaction between the transport and link
layers, we identify subtle but important modifications to both, achieving TCP
goodput within 25% of an upper bound (5-40x higher than prior results) and
low-power operation commensurate to CoAP in a practical LLN application
scenario. This suggests that a TCP-based transport layer, seamlessly
interoperable with existing TCP/IP networks, is viable and performant in LLNs.","Effect of varying time between link-layer retransmissions. Reported “segment loss” is the loss rate of TCP segments, not individual IEEE 802.15.4 frames. It includes only losses not masked by link-layer retries.",How does the maximum link delay affect the segment loss rate and goodput in a TCP connection with one hop?
spiqa_99,1811.02721v3,"In the context of *Performant TCP for Low-Power Wireless Networks*, as shown in Table 3, how does the memory usage of the posix_sockets module compare to that of the combined protocol and socket layers for both active and passive connections, and what are the specific memory values for each?","The posix_sockets module consistently uses less memory than the combined usage of the protocol and socket layer. For an active connection, it requires about 5468 B compared to 19972 B + 6216 B = 26188 B for the other layers. Similarly, for a passive connection, it uses 5468 B compared to 19972 B + 6216 B = 26188 B for the other layers.",1811.02721v3.pdf,"['1811.02721v3.pdf', '1805.02349v2.pdf', '1704.00774v3.pdf', '1708.02153v2.pdf', '1611.04684v1.pdf', '1703.04887v4.pdf', '1608.02784v2.pdf', '1710.05654v2.pdf', '1709.08294v3.pdf', '1705.07164v8.pdf', '1611.04363v2.pdf', '1705.07384v2.pdf', '1611.07718v2.pdf']","Table 1 provides the memory usage breakdown for `sys` on RIOT OS, including the individual contributions of the protocol, socket layer, and `posix_sockets` module. By comparing the values in the table, we can see that the `posix_sockets` module, while providing a Unix-like interface, consumes less memory than the combined total of the protocol and socket layer for both active and passive connections. This information supports the passage's claim that `sys` fits well within available memory despite its advanced features.",1811.02721v3-Table3-1.png,Performant TCP for Low-Power Wireless Networks,"Low-power and lossy networks (LLNs) enable diverse applications integrating
many resource-constrained embedded devices, often requiring interconnectivity
with existing TCP/IP networks as part of the Internet of Things. But TCP has
received little attention in LLNs due to concerns about its overhead and
performance, leading to LLN-specific protocols that require specialized
gateways for interoperability. We present a systematic study of a well-designed
TCP stack in IEEE 802.15.4-based LLNs, based on the TCP protocol logic in
FreeBSD. Through careful implementation and extensive experiments, we show that
modern low-power sensor platforms are capable of running full-scale TCP and
that TCP, counter to common belief, performs well despite the lossy nature of
LLNs. By carefully studying the interaction between the transport and link
layers, we identify subtle but important modifications to both, achieving TCP
goodput within 25% of an upper bound (5-40x higher than prior results) and
low-power operation commensurate to CoAP in a practical LLN application
scenario. This suggests that a TCP-based transport layer, seamlessly
interoperable with existing TCP/IP networks, is viable and performant in LLNs.","Table 3: Memory usage of TCPlp on RIOT OS. We also include RIOT’s posix_sockets module, used by TCPlp to provide a Unix-like interface.","How does the memory usage of the RIOT OS posix_sockets module compare to the memory used by the protocol and socket layer combined, for both active and passive connections?"
spiqa_100,1705.09296v2,"Based on Table 3, how does the model's use of tone as a covariate lead to distinct word associations for anti-immigration and pro-immigration perspectives within the same topics, such as contrasting ""detainees"" versus ""criminal""?","The model captures different perspectives on immigration by highlighting contrasting words associated with the same topic, depending on whether the tone is anti-immigration or pro-immigration.",1705.09296v2.pdf,"['1705.09296v2.pdf', '1707.08608v3.pdf', '1804.05995v2.pdf', '1704.00774v3.pdf', '1710.05654v2.pdf', '1805.01216v3.pdf', '1704.05426v4.pdf', '1811.09393v4.pdf', '1611.07718v2.pdf']","Table 1 displays several base topics related to immigration. For each topic, the table shows the most significant words associated with the anti-immigration and pro-immigration interactions. These words often represent opposing viewpoints on the same issue. For example, the topic ""ice customs agency enforcement homeland"" has contrasting words like ""criminal"" and ""detainees"" for anti-immigration and pro-immigration perspectives, respectively. This pattern suggests that the model can differentiate between different stances on immigration by considering the interaction between tone and topic.",1705.09296v2-Table3-1.png,Neural Models for Documents with Metadata,"Most real-world document collections involve various types of metadata, such
as author, source, and date, and yet the most commonly-used approaches to
modeling text corpora ignore this information. While specialized models have
been developed for particular applications, few are widely used in practice, as
customization typically requires derivation of a custom inference algorithm. In
this paper, we build on recent advances in variational inference methods and
propose a general neural framework, based on topic models, to enable flexible
incorporation of metadata and allow for rapid exploration of alternative
models. Our approach achieves strong performance, with a manageable tradeoff
between perplexity, coherence, and sparsity. Finally, we demonstrate the
potential of our framework through an exploration of a corpus of articles about
US immigration.","Table 3: Top words for topics (left) and the corresponding anti-immigration (middle) and pro-immigration (right) variations when treating tone as a covariate, with interactions.",How does the model capture different perspectives on immigration when considering tone as a covariate?
spiqa_101,1703.10730v2,What insights does Figure 7 provide about the network's shift in focus from generating masks to producing sharper and more realistic images as the training epochs progress in the unsupervised holistic image generation framework?,"The network initially focuses on predicting a good mask. As the epoch increases, the input parts become sharper. Finally, the network concentrates on generating realistic images.",1703.10730v2.pdf,"['1703.10730v2.pdf', '1704.08615v2.pdf', '1709.00139v4.pdf', '1906.06589v3.pdf', '1708.06832v3.pdf', '1804.07931v2.pdf', '1809.03149v2.pdf', '1603.00286v5.pdf', '1805.01216v3.pdf', '1802.07459v2.pdf']","Figure 0 shows how the generated images and masks change as the training epoch increases. In the early epochs, the masks are blurry and the images are unrealistic. However, as the training progresses, the masks become sharper and the images become more realistic. This suggests that the network is learning to focus on different aspects of the image generation process at different stages of training.",1703.10730v2-Figure7-1.png,Unsupervised Holistic Image Generation from Key Local Patches,"We introduce a new problem of generating an image based on a small number of
key local patches without any geometric prior. In this work, key local patches
are defined as informative regions of the target object or scene. This is a
challenging problem since it requires generating realistic images and
predicting locations of parts at the same time. We construct adversarial
networks to tackle this problem. A generator network generates a fake image as
well as a mask based on the encoder-decoder framework. On the other hand, a
discriminator network aims to detect fake images. The network is trained with
three losses to consider spatial, appearance, and adversarial information. The
spatial loss determines whether the locations of predicted parts are correct.
Input patches are restored in the output image without much modification due to
the appearance loss. The adversarial loss ensures output images are realistic.
The proposed network is trained without supervisory signals since no labels of
key parts are required. Experimental results on six datasets demonstrate that
the proposed algorithm performs favorably on challenging objects and scenes.",Figure 7: Sample generated masks and images at different epochs.,How does the network's focus change as the training epoch increases?
spiqa_102,1812.00281v3,"Based on the camera-ablation study figure in the HUMBI dataset paper, how does the accuracy of the garment reconstruction change with varying numbers of cameras?",The accuracy of the garment reconstruction increases as the number of cameras used increases.,1812.00281v3.pdf,"['1812.00281v3.pdf', '1708.05239v3.pdf', '1803.01128v3.pdf', '1803.03467v4.pdf', '1611.02654v2.pdf', '1708.02153v2.pdf', '1705.08016v3.pdf', '1906.10843v1.pdf', '1704.07121v2.pdf', '1611.04684v1.pdf', '1804.04410v2.pdf', '1611.07718v2.pdf', '1803.04383v2.pdf']","The plot on the right shows that the error in the garment reconstruction decreases as the number of cameras used increases. This is because more cameras provide more viewpoints of the garment, which allows the algorithm to better reconstruct the 3D shape of the garment.",1812.00281v3-Figure8-1.png,HUMBI: A Large Multiview Dataset of Human Body Expressions,"This paper presents a new large multiview dataset called HUMBI for human body
expressions with natural clothing. The goal of HUMBI is to facilitate modeling
view-specific appearance and geometry of gaze, face, hand, body, and garment
from assorted people. 107 synchronized HD cameras are used to capture 772
distinctive subjects across gender, ethnicity, age, and physical condition.
With the multiview image streams, we reconstruct high fidelity body expressions
using 3D mesh models, which allows representing view-specific appearance using
their canonical atlas. We demonstrate that HUMBI is highly effective in
learning and reconstructing a complete human model and is complementary to the
existing datasets of human body expressions with limited views and subjects
such as MPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets.",We conduct camera-ablation study to evaluate the accuracy of the garment reconstruction in terms of the density (left) and the accuracy (right).,How does the number of cameras used affect the accuracy of the garment reconstruction?
spiqa_103,1811.08257v1,"Based on the benchmark figure in the FALCON paper, how does increasing the number of output classes affect the setup and online times for the Softmax function in the context of Fourier Transform-based computations with homomorphic encryption?",The setup and online time for the Softmax increases as the number of classes increases.,1811.08257v1.pdf,"['1811.08257v1.pdf', '1812.10735v2.pdf', '1704.00774v3.pdf', '1804.04410v2.pdf', '1809.00458v1.pdf']","The table shows that the setup and online time for the Softmax increases as the number of classes increases. For example, when the number of classes is 10, the setup time is 8.56 ms and the online time is 3.89 ms. When the number of classes is 1000, the setup time is 574.8 ms and the online time is 254.6 ms. This is because the Softmax needs to compute the probability of each class, and the more classes there are, the more computations are needed.",1811.08257v1-Table4-1.png,FALCON: A Fourier Transform Based Approach for Fast and Secure Convolutional Neural Network Predictions,"Machine learning as a service has been widely deployed to utilize deep neural
network models to provide prediction services. However, this raises privacy
concerns since clients need to send sensitive information to servers. In this
paper, we focus on the scenario where clients want to classify private images
with a convolutional neural network model hosted in the server, while both
parties keep their data private. We present FALCON, a fast and secure approach
for CNN predictions based on Fourier Transform. Our solution enables linear
layers of a CNN model to be evaluated simply and efficiently with fully
homomorphic encryption. We also introduce the first efficient and
privacy-preserving protocol for softmax function, which is an indispensable
component in CNNs and has not yet been evaluated in previous works due to its
high complexity. We implemented the FALCON and evaluated the performance on
real-world CNN models. The experimental results show that FALCON outperforms
the best known works in both computation and communication cost.",Benchmarks for the Softmax.,How does the number of classes affect the setup and online time for the Softmax?
spiqa_104,1811.02553v4,"Based on the reward landscapes depicted in the figure for the PPO algorithm in the Humanoid-v2 environment, how does increasing the number of state-action pairs affect the complexity of the optimization landscape, particularly in terms of the appearance of local optima and the difficulty in finding the global optimum?","As the number of state-action pairs increases, the optimization landscape becomes more complex and has more local optima. This makes it more difficult for the PPO algorithm to find the global optimum.",1811.02553v4.pdf,"['1811.02553v4.pdf', '1703.00899v2.pdf', '1704.00774v3.pdf', '1802.07459v2.pdf', '1809.01246v1.pdf']","The figure shows the optimization landscape for the PPO algorithm with different numbers of state-action pairs. The surrogate and true reward landscapes are shown for both few and many state-action pairs. When there are few state-action pairs, the landscape is relatively smooth and has fewer local optima. However, when there are many state-action pairs, the landscape becomes more complex and has more local optima. This is because there are more possible state-action pairs that the algorithm can explore, which leads to a more complex optimization landscape.",1811.02553v4-Figure13-1.png,A Closer Look at Deep Policy Gradients,"We study how the behavior of deep policy gradient algorithms reflects the
conceptual framework motivating their development. To this end, we propose a
fine-grained analysis of state-of-the-art methods based on key elements of this
framework: gradient estimation, value prediction, and optimization landscapes.
Our results show that the behavior of deep policy gradient algorithms often
deviates from what their motivating framework would predict: the surrogate
objective does not match the true reward landscape, learned value estimators
fail to fit the true value function, and gradient estimates poorly correlate
with the ""true"" gradient. The mismatch between predicted and empirical behavior
we uncover highlights our poor understanding of current methods, and indicates
the need to move beyond current benchmark-centric evaluation methods.",Humanoid-v2 – PPO reward landscapes.,How does the number of state-action pairs affect the optimization landscape for the PPO algorithm?
spiqa_105,1811.02553v4,"Based on the figure titled ""Hopper-v2 – PPO reward landscapes"" in the paper, how does an increase in state-action pairs influence the smoothness and accuracy of the reward landscapes for both the surrogate and true reward functions in deep policy gradients?","As the number of state-action pairs increases, the reward landscape for both the surrogate and true reward functions becomes smoother and more accurate.",1811.02553v4.pdf,"['1811.02553v4.pdf', '1809.04276v2.pdf', '1605.07496v3.pdf', '1705.09296v2.pdf', '1703.04887v4.pdf', '1803.05776v2.pdf', '1707.00189v3.pdf', '1805.02349v2.pdf', '1704.08615v2.pdf', '1707.06320v2.pdf', '1611.05742v3.pdf']","The figure shows that the reward landscape for both the surrogate and true reward functions is more jagged and less accurate when there are fewer state-action pairs. This is because the surrogate function is not able to learn the true reward function as well when there is less data. As the number of state-action pairs increases, the surrogate function is able to learn the true reward function more accurately, and the reward landscape becomes smoother and more accurate.",1811.02553v4-Figure17-1.png,A Closer Look at Deep Policy Gradients,"We study how the behavior of deep policy gradient algorithms reflects the
conceptual framework motivating their development. To this end, we propose a
fine-grained analysis of state-of-the-art methods based on key elements of this
framework: gradient estimation, value prediction, and optimization landscapes.
Our results show that the behavior of deep policy gradient algorithms often
deviates from what their motivating framework would predict: the surrogate
objective does not match the true reward landscape, learned value estimators
fail to fit the true value function, and gradient estimates poorly correlate
with the ""true"" gradient. The mismatch between predicted and empirical behavior
we uncover highlights our poor understanding of current methods, and indicates
the need to move beyond current benchmark-centric evaluation methods.",Hopper-v2 – PPO reward landscapes.,How does the number of state-action pairs affect the reward landscape for the surrogate and true reward functions?
spiqa_106,1702.03584v3,"In Figure 1 of your ""Similarity Preserving Representation Learning for Time Series Clustering"" paper, how does the observed error decrease and converge with the true error over CPU time for the UCR Non-Invasive Fetal ECG Thorax1 dataset?","The observed error is initially higher than the underlying true error, but it quickly decreases and converges to the true error as CPU time increases.",1702.03584v3.pdf,"['1702.03584v3.pdf', '1803.05776v2.pdf', '1707.08608v3.pdf', '1803.01128v3.pdf', '1811.08481v2.pdf', '1708.03797v1.pdf', '1805.02349v2.pdf', '1805.00912v4.pdf', '1612.02803v5.pdf', '1811.09393v4.pdf', '1703.07015v3.pdf', '1608.02784v2.pdf', '1709.00139v4.pdf']","The figure shows two lines, one representing the observed error and the other representing the underlying true error. Both lines decrease as CPU time increases, but the observed error line decreases more quickly and eventually converges to the true error line.",1702.03584v3-Figure1-1.png,Similarity Preserving Representation Learning for Time Series Clustering,"A considerable amount of clustering algorithms take instance-feature matrices
as their inputs. As such, they cannot directly analyze time series data due to
its temporal nature, usually unequal lengths, and complex properties. This is a
great pity since many of these algorithms are effective, robust, efficient, and
easy to use. In this paper, we bridge this gap by proposing an efficient
representation learning framework that is able to convert a set of time series
with various lengths to an instance-feature matrix. In particular, we guarantee
that the pairwise similarities between time series are well preserved after the
transformation, thus the learned feature representation is particularly
suitable for the time series clustering task. Given a set of $n$ time series,
we first construct an $n\times n$ partially-observed similarity matrix by
randomly sampling $\mathcal{O}(n \log n)$ pairs of time series and computing
their pairwise similarities. We then propose an efficient algorithm that solves
a non-convex and NP-hard problem to learn new features based on the
partially-observed similarity matrix. By conducting extensive empirical
studies, we show that the proposed framework is more effective, efficient, and
flexible, compared to other state-of-the-art time series clustering methods.",Figure 1: Two error rates as a function of CPU time on UCR Non-Invasive Fetal ECG Thorax1 dataset,How does the observed error compare to the underlying true error as CPU time increases?
spiqa_108,1804.07931v2,"Based on Figure 3 in the ESMM paper, how do ESMM-NS and ESMM compare to other models in terms of AUC performance across different sampling rates for both CVR and CTCVR tasks?",ESMM-NS and ESMM outperform all other models consistently across different training set sizes on both the CVR and CTCVR tasks.,1804.07931v2.pdf,"['1804.07931v2.pdf', '1708.02153v2.pdf', '1708.05239v3.pdf', '1803.06506v3.pdf', '1704.05958v2.pdf', '1706.08146v3.pdf', '1811.08257v1.pdf', '1812.10735v2.pdf', '1704.07121v2.pdf', '1707.01917v2.pdf', '1804.07849v4.pdf', '1705.02946v3.pdf', '1706.00827v2.pdf']","The figure shows the AUC of different models on the CVR and CTCVR tasks with different training set sizes. The lines for ESMM-NS and ESMM are consistently above the lines for the other models, indicating that they have higher AUC values.",1804.07931v2-Figure3-1.png,Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate,"Estimating post-click conversion rate (CVR) accurately is crucial for ranking
systems in industrial applications such as recommendation and advertising.
Conventional CVR modeling applies popular deep learning methods and achieves
state-of-the-art performance. However it encounters several task-specific
problems in practice, making CVR modeling challenging. For example,
conventional CVR models are trained with samples of clicked impressions while
utilized to make inference on the entire space with samples of all impressions.
This causes a sample selection bias problem. Besides, there exists an extreme
data sparsity problem, making the model fitting rather difficult. In this
paper, we model CVR in a brand-new perspective by making good use of sequential
pattern of user actions, i.e., impression -> click -> conversion. The proposed
Entire Space Multi-task Model (ESMM) can eliminate the two problems
simultaneously by i) modeling CVR directly over the entire space, ii) employing
a feature representation transfer learning strategy. Experiments on dataset
gathered from Taobao's recommender system demonstrate that ESMM significantly
outperforms competitive methods. We also release a sampling version of this
dataset to enable future research. To the best of our knowledge, this is the
first public dataset which contains samples with sequential dependence of click
and conversion labels for CVR modeling.",Figure 3: Comparison of different models w.r.t. different sampling rates on Product Dataset.,How does the performance of ESMM compare to other models on the CVR task and CTCVR task with different training set sizes?
spiqa_109,1611.03780v2,How does the performance of GeoCUTS compare to the Grid method in identifying highly mobile clusters across different Q-metric thresholds as the number of geographically clustered regions increases from approximately 25 to 50 in Table 1?,"GeoCUTS consistently outperforms the Grid method in identifying highly mobile clusters, regardless of the number of clusters. However, the performance of both methods decreases as the number of clusters increases.",1611.03780v2.pdf,"['1611.03780v2.pdf', '1708.02153v2.pdf', '1701.03077v10.pdf', '1906.10843v1.pdf', '1710.01507v4.pdf', '1704.04539v2.pdf', '1804.07849v4.pdf', '1709.02418v2.pdf', '1705.09296v2.pdf', '1809.02731v3.pdf', '1811.02721v3.pdf', '1803.05776v2.pdf']","Table 1 shows the percentage of queries from highly mobile clusters identified by each method for different Q-metric thresholds and cluster numbers. In both the ""$\sim25$ clusters"" and ""$\sim50$ clusters"" scenarios, GeoCUTS identifies a higher percentage of highly mobile clusters than the Grid method across all Q-metric thresholds. For example, with approximately 25 clusters and a Q-metric threshold of 0.8, GeoCUTS identifies 56% of highly mobile clusters, while the Grid method only identifies 30%. This trend holds true even when the number of clusters increases to approximately 50. 

The passage also emphasizes that GeoCUTS significantly outperforms the Grid method regardless of the number of clusters, supporting the conclusion drawn from the table.",1611.03780v2-Table4-1.png,Randomized Experimental Design via Geographic Clustering,"Web-based services often run randomized experiments to improve their
products. A popular way to run these experiments is to use geographical regions
as units of experimentation, since this does not require tracking of individual
users or browser cookies. Since users may issue queries from multiple
geographical locations, geo-regions cannot be considered independent and
interference may be present in the experiment. In this paper, we study this
problem, and first present GeoCUTS, a novel algorithm that forms geographical
clusters to minimize interference while preserving balance in cluster size. We
use a random sample of anonymized traffic from Google Search to form a graph
representing user movements, then construct a geographically coherent
clustering of the graph. Our main technical contribution is a statistical
framework to measure the effectiveness of clusterings. Furthermore, we perform
empirical evaluations showing that the performance of GeoCUTS is comparable to
hand-crafted geo-regions with respect to both novel and existing metrics.",Table 4: Percentage of queries from clusters with a Q-metric ≥ x% for different numbers of clusters in France.,How does the performance of GeoCUTS compare to the Grid method in identifying highly mobile clusters when the number of clusters is increased from approximately 25 to 50?
spiqa_110,1703.07015v3,"Based on the figure displaying ablation test results for the Solar-Energy dataset in this paper, how does the performance of the LSTNet-attn model, in terms of RMSE and correlation metrics, change as the forecasting horizon increases?",The performance of LSTNet-attn generally improves as the horizon increases on the Solar-Energy dataset. This is evident from the fact that both the RMSE and correlation values improve with increasing horizon.,1703.07015v3.pdf,"['1703.07015v3.pdf', '1805.04609v3.pdf', '1705.09882v2.pdf', '1906.06589v3.pdf', '1701.03077v10.pdf', '1705.08016v3.pdf', '1605.07496v3.pdf', '1705.07384v2.pdf', '1709.02418v2.pdf', '1811.08257v1.pdf', '1705.09966v2.pdf', '1804.07931v2.pdf', '1701.06171v4.pdf', '1805.02349v2.pdf', '1803.03467v4.pdf']",The figure shows the RMSE and correlation values for different horizons on the Solar-Energy dataset. The LSTNet-attn model is represented by the green bars.,1703.07015v3-Figure5-1.png,Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks,"Multivariate time series forecasting is an important machine learning problem
across many domains, including predictions of solar plant energy output,
electricity consumption, and traffic jam situation. Temporal data arise in
these real-world applications often involves a mixture of long-term and
short-term patterns, for which traditional approaches such as Autoregressive
models and Gaussian Process may fail. In this paper, we proposed a novel deep
learning framework, namely Long- and Short-term Time-series network (LSTNet),
to address this open challenge. LSTNet uses the Convolution Neural Network
(CNN) and the Recurrent Neural Network (RNN) to extract short-term local
dependency patterns among variables and to discover long-term patterns for time
series trends. Furthermore, we leverage traditional autoregressive model to
tackle the scale insensitive problem of the neural network model. In our
evaluation on real-world data with complex mixtures of repetitive patterns,
LSTNet achieved significant performance improvements over that of several
state-of-the-art baseline methods. All the data and experiment codes are
available online.","Results of LSTNet in the ablation tests on the Solar-Energy, Traffic and Electricity dataset",How does the performance of LSTNet-attn vary with the horizon on the Solar-Energy dataset?
spiqa_111,1702.03584v3,"Based on the figure comparing SPIRAL-DTW-kMeans with k-Shape and CLDS, how does SPIRAL-DTW-kMeans perform in terms of clustering accuracy (NMI) across the UCR time series datasets?",SPIRAL-DTW-kMeans performs better than k-Shape and CLDS on most datasets.,1702.03584v3.pdf,"['1702.03584v3.pdf', '1901.00056v2.pdf', '1704.05426v4.pdf', '1804.05936v2.pdf', '1705.09966v2.pdf', '1804.05995v2.pdf']","In Figure (a) and (b), the circles below the diagonal line indicate datasets where SPIRAL-DTW-kMeans yields better clustering performance in terms of NMI than k-Shape and CLDS, respectively.",1702.03584v3-Figure2-1.png,Similarity Preserving Representation Learning for Time Series Clustering,"A considerable amount of clustering algorithms take instance-feature matrices
as their inputs. As such, they cannot directly analyze time series data due to
its temporal nature, usually unequal lengths, and complex properties. This is a
great pity since many of these algorithms are effective, robust, efficient, and
easy to use. In this paper, we bridge this gap by proposing an efficient
representation learning framework that is able to convert a set of time series
with various lengths to an instance-feature matrix. In particular, we guarantee
that the pairwise similarities between time series are well preserved after the
transformation, thus the learned feature representation is particularly
suitable for the time series clustering task. Given a set of $n$ time series,
we first construct an $n\times n$ partially-observed similarity matrix by
randomly sampling $\mathcal{O}(n \log n)$ pairs of time series and computing
their pairwise similarities. We then propose an efficient algorithm that solves
a non-convex and NP-hard problem to learn new features based on the
partially-observed similarity matrix. By conducting extensive empirical
studies, we show that the proposed framework is more effective, efficient, and
flexible, compared to other state-of-the-art time series clustering methods.","Comparison of our method with existing clustering algorithms over all the 85 UCR time series datasets. Top two: comparison between our method SPIRAL-DTW-kMeans with two state-of-the-art methods k-Shape, and CLDS. Bottom two: comparisons between MSM based methods, our method SPIRAL-MSM-kMeans with Laplace-MSM-kMeans and kMedoids-MSM. Circles below the diagonal indicate datasets over which our method yields better clustering performance in terms of NMI.",How does the performance of SPIRAL-DTW-kMeans compare to k-Shape and CLDS?
spiqa_112,1709.02755v5,"Based on Table 1 from the paper ""Simple Recurrent Units for Highly Parallelizable Recurrence,"" how does the SRU model with 8 layers perform in terms of test accuracy on the SUBJ dataset when compared to the best reported results (e.g., Zhao et al., 2015), and what does the comparison of SRU's training time on the SST dataset reveal about its efficiency relative to other baseline models under the same experimental setup?","While SRU with 8 layers achieves high test accuracy within the ""Our setup"" section on the SUBJ dataset (93.7%), it falls slightly short of the best reported result of 95.5% achieved by Zhao et al. (2015). However, SRU's training time of 879 seconds for 100 epochs on the SST dataset is faster than the LSTM model (2409 seconds) but slower than the CNN model (417 seconds) and the QRNN models (345 and 371 seconds).",1709.02755v5.pdf,"['1709.02755v5.pdf', '1702.08694v3.pdf', '1811.02553v4.pdf', '1705.08016v3.pdf', '1812.06589v2.pdf', '1708.03797v1.pdf', '1708.00160v2.pdf']","The table shows the test accuracies for different models on various datasets, including SUBJ. Comparing the values in the corresponding columns allows us to assess the relative performance of SRU with 8 layers against other models. Additionally, the last column provides training time information for the SST dataset, enabling comparison of training efficiency between SRU and the other models in the ""Our setup"" section.",1709.02755v5-Table1-1.png,Simple Recurrent Units for Highly Parallelizable Recurrence,"Common recurrent neural architectures scale poorly due to the intrinsic
difficulty in parallelizing their state computations. In this work, we propose
the Simple Recurrent Unit (SRU), a light recurrent unit that balances model
capacity and scalability. SRU is designed to provide expressive recurrence,
enable highly parallelized implementation, and comes with careful
initialization to facilitate training of deep models. We demonstrate the
effectiveness of SRU on multiple NLP tasks. SRU achieves 5--9x speed-up over
cuDNN-optimized LSTM on classification and question answering datasets, and
delivers stronger results than LSTM and convolutional models. We also obtain an
average of 0.7 BLEU improvement over the Transformer model on translation by
incorporating SRU into the architecture.","Table 1: Test accuracies on classification benchmarks (Section 4.1). The first block presents best reported results of various methods. The second block compares SRU and other baselines given the same setup. For the SST dataset, we report average results of 5 runs. For other datasets, we perform 3 independent trials of 10-fold cross validation (3×10 runs). The last column compares the wall clock time (in seconds) to finish 100 epochs on the SST dataset.","How does the performance of SRU with 8 layers compare to the best reported results on the SUBJ dataset, and how does its training time compare to the other models in the ""Our setup"" section?"
spiqa_113,1710.06177v2,"Based on the binary classification results presented in Table 1 of the ""Learning to Learn Image Classifiers with Visual Analogy"" paper, how does VAGER+Voting's performance in terms of AUC and F1 metrics compare to other VAGER variants, particularly in the 1-shot and 20-shot settings?","VAGER+Voting consistently outperforms all other VAGER variants in both 1-shot and 20-shot settings, achieving the highest AUC and F1 scores.",1710.06177v2.pdf,"['1710.06177v2.pdf', '1804.01429v3.pdf', '1703.10730v2.pdf', '1708.01425v4.pdf', '1901.00398v2.pdf', '1803.02750v3.pdf', '1704.08615v2.pdf']","Looking at the table, we can compare the bolded values for VAGER+Voting with the corresponding values for other VAGER variants. In both the 1-shot and 20-shot columns, VAGER+Voting has the highest AUC and F1 scores, demonstrating its superior performance compared to other variants. This suggests that the Voting strategy effectively combines the strengths of different components within the VAGER framework.",1710.06177v2-Table1-1.png,Learning to Learn Image Classifiers with Visual Analogy,"Humans are far better learners who can learn a new concept very fast with
only a few samples compared with machines. The plausible mystery making the
difference is two fundamental learning mechanisms: learning to learn and
learning by analogy. In this paper, we attempt to investigate a new human-like
learning method by organically combining these two mechanisms. In particular,
we study how to generalize the classification parameters from previously
learned concepts to a new concept. we first propose a novel Visual Analogy
Graph Embedded Regression (VAGER) model to jointly learn a low-dimensional
embedding space and a linear mapping function from the embedding space to
classification parameters for base classes. We then propose an out-of-sample
embedding method to learn the embedding of a new class represented by a few
samples through its visual analogy with base classes and derive the
classification parameters for the new class. We conduct extensive experiments
on ImageNet dataset and the results show that our method could consistently and
significantly outperform state-of-the-art baselines.",Table 1. Performance of different algorithms for k-shot binary classification problem,How does the performance of VAGER+Voting compare to other VAGER variants in the 1-shot and 20-shot settings?
spiqa_114,1701.03077v10,"Based on the figure illustrating gFGR's performance for the task of [41] as a function of the shape parameter α, how do the mean and max RMSE metrics vary when α is increased, particularly compared to FGR (α = -2)?",The performance of gFGR generally improves as the shape parameter α increases.,1701.03077v10.pdf,"['1701.03077v10.pdf', '1803.05776v2.pdf', '1703.02507v3.pdf', '1703.07015v3.pdf', '1809.00458v1.pdf', '1809.01246v1.pdf']","The figure shows that the mean RMSE and max RMSE both decrease as α increases, indicating better performance.",1701.03077v10-Figure5-1.png,A General and Adaptive Robust Loss Function,"We present a generalization of the Cauchy/Lorentzian, Geman-McClure,
Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2
loss functions. By introducing robustness as a continuous parameter, our loss
function allows algorithms built around robust loss minimization to be
generalized, which improves performance on basic vision tasks such as
registration and clustering. Interpreting our loss as the negative log of a
univariate density yields a general probability distribution that includes
normal and Cauchy distributions as special cases. This probabilistic
interpretation enables the training of neural networks in which the robustness
of the loss automatically adapts itself during training, which improves
performance on learning-based tasks such as generative image synthesis and
unsupervised monocular depth estimation, without requiring any manual parameter
tuning.","Performance (lower is better) of our gFGR algorithm on the task of [41] as we vary our shape parameter α, with the lowest-error point indicated by a circle. FGR (equivalent to gFGR with α = −2) is shown as a dashed line and a square, and shapeannealed gFGR for each noise level is shown as a dotted line.",How does the performance of gFGR change as the shape parameter α increases?
spiqa_115,1805.04687v2,"Based on Table 14 of the BDD100K paper, how do ODS-F scores for lane marking detection, including direction, continuity, and category, evolve as the threshold values (τ = 1, 2, 10 pixels) are increased?","As the threshold (τ) increases, the ODS-F scores for direction, continuity, and category generally increase as well. This indicates that the model performs better in detecting lane markings with higher thresholds, meaning it can tolerate larger deviations from the ground truth annotations.",1805.04687v2.pdf,"['1805.04687v2.pdf', '1707.01917v2.pdf', '1803.05776v2.pdf', '1612.02803v5.pdf', '1809.03449v3.pdf', '1706.00633v4.pdf', '1703.00060v2.pdf', '1606.07384v2.pdf', '1805.08751v2.pdf', '1809.04276v2.pdf', '1901.00398v2.pdf', '1805.06447v3.pdf', '1603.03833v4.pdf', '1709.02418v2.pdf']","The table presents ODS-F scores for different thresholds (τ = 1, 2, 10 pixels) across various training sets and lane marking attributes. By comparing the scores across different thresholds within each attribute and training set, we can observe the general trend of increasing performance with increasing thresholds. For example, for Lane 10K training set, the average ODS-F score for direction increases from 28.38 at τ = 1 to 36.19 at τ = 2 and 49.29 at τ = 10. This pattern holds true for most attributes and training sets, indicating that the model performs better with higher thresholds.",1805.04687v2-Table14-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.","Table 14: Full evaluation results of the individual lane marking task and the joint training of lane marking and the drivable area detection. We report the ODS-F scores with different thresholds τ = 1, 2, 10 pixels of direction, continuity as well as each category.","How does the performance of lane marking detection change with different thresholds (τ) for direction, continuity, and category?"
spiqa_116,1705.09882v2,"In the right panel of your figure comparing top-1 re-identification accuracy on DPI-T, how does the proposed RGB-to-Depth transfer method's performance differ from Yosinski et al. [90] when all layers are fine-tuned (x=7)?",The proposed RGB-to-Depth transfer performs slightly better than Yosinski et al. [90] in terms of top-1 accuracy on DPI-T when all layers are fine-tuned.,1705.09882v2.pdf,"['1705.09882v2.pdf', '1706.04269v2.pdf', '1612.02803v5.pdf', '1809.03449v3.pdf', '1811.07073v3.pdf', '1805.01216v3.pdf', '1811.02721v3.pdf', '1812.10735v2.pdf']","The right panel of the figure shows the top-1 re-identification accuracy for different methods when different numbers of layers are fine-tuned. When all layers are fine-tuned (x=7), our RGB-to-Depth transfer achieves a top-1 accuracy of about 75%, while Yosinski et al. [90] achieves a top-1 accuracy of about 73%.",1705.09882v2-Figure5-1.png,Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-Identification,"We address the problem of person re-identification from commodity depth
sensors. One challenge for depth-based recognition is data scarcity. Our first
contribution addresses this problem by introducing split-rate RGB-to-Depth
transfer, which leverages large RGB datasets more effectively than popular
fine-tuning approaches. Our transfer scheme is based on the observation that
the model parameters at the bottom layers of a deep convolutional neural
network can be directly shared between RGB and depth data while the remaining
layers need to be fine-tuned rapidly. Our second contribution enhances
re-identification for video by implementing temporal attention as a
Bernoulli-Sigmoid unit acting upon frame-level features. Since this unit is
stochastic, the temporal attention parameters are trained using reinforcement
learning. Extensive experiments validate the accuracy of our method in person
re-identification from depth sequences. Finally, in a scenario where subjects
wear unseen clothes, we show large performance gains compared to a
state-of-the-art model which relies on RGB data.",Comparison of our RGB-to-Depth transfer with Yosinski et al. [90] in terms of top-1 accuracy on DPI-T. In this ablation study the x axis represents the number of layers whose weights are frozen (left) or fine-tuned (right) starting from the bottom.,How does the performance of our RGB-to-Depth transfer compare to Yosinski et al. [90] in terms of top-1 accuracy on DPI-T when all layers are fine-tuned?
spiqa_117,1811.07073v3,"In Table 4 of the paper, what mIOU score does the ""Conv. Self-Correction"" method achieve under the 30% fully supervised (F=914) and 70% weakly supervised (W=2061) data split on the Cityscapes validation set, and how does this score compare to the other methods listed?","The ""Conv. Self-Correction"" method achieves the highest mIOU score of 79.46 compared to other methods listed in the table under the same data split condition.",1811.07073v3.pdf,"['1811.07073v3.pdf', '1706.04269v2.pdf', '1803.02750v3.pdf', '1605.07496v3.pdf', '1704.07121v2.pdf', '1901.00398v2.pdf', '1812.00108v4.pdf', '1710.06177v2.pdf', '1805.08751v2.pdf', '1805.01216v3.pdf', '1611.02654v2.pdf', '1710.05654v2.pdf']","Table 1 presents the mIOU scores for various methods under different data split scenarios. When focusing on the rows where $F=914$ and $W=2061$ (representing the 30% split), we can directly compare the mIOU scores of all listed methods. The ""Conv. Self-Correction"" method clearly shows the highest score, indicating its superior performance in this specific setting.",1811.07073v3-Table4-1.png,Semi-Supervised Semantic Image Segmentation with Self-correcting Networks,"Building a large image dataset with high-quality object masks for semantic
segmentation is costly and time consuming. In this paper, we introduce a
principled semi-supervised framework that only uses a small set of fully
supervised images (having semantic segmentation labels and box labels) and a
set of images with only object bounding box labels (we call it the weak set).
Our framework trains the primary segmentation model with the aid of an
ancillary model that generates initial segmentation labels for the weak set and
a self-correction module that improves the generated labels during training
using the increasingly accurate primary model. We introduce two variants of the
self-correction module using either linear or convolutional functions.
Experiments on the PASCAL VOC 2012 and Cityscape datasets show that our models
trained with a small fully supervised set perform similar to, or better than,
models trained with a large fully supervised set while requiring ~7x less
annotation effort.","Table 4: Results on Cityscapes validation set. 30% of the training examples is used as F , and the remaining as W .","How does the performance of the ""Conv. Self-Correction"" method compare to other methods when using 30% of the training examples as $\F$ and the remaining as $\W$ on the Cityscapes validation set?"
spiqa_118,1704.05426v4,"Referring to Table 4 in the MultiNLI paper, how does the ESIM model's test set accuracy change on SNLI, as well as on matched and mismatched MNLI genres, when trained on MNLI alone versus when trained on both MNLI and SNLI combined?","When trained on MNLI alone, the ESIM model achieves an accuracy of 60.7% on SNLI, 72.3% on matched genres in MNLI, and 72.1% on mismatched genres in MNLI. However, when trained on both MNLI and SNLI combined, the ESIM model's performance improves across all tasks, reaching 79.7% accuracy on SNLI, 72.4% on matched MNLI genres, and 71.9% on mismatched MNLI genres.",1704.05426v4.pdf,"['1704.05426v4.pdf', '1811.10673v1.pdf', '1709.02755v5.pdf', '1802.07459v2.pdf', '1708.01425v4.pdf', '1710.01507v4.pdf', '1809.03449v3.pdf', '1704.05958v2.pdf', '1906.10843v1.pdf', '1809.04276v2.pdf', '1710.05654v2.pdf']","Table 1 shows the test set accuracies for different models trained on different data sets. By comparing the rows corresponding to ESIM trained on MNLI and ESIM trained on MNLI + SNLI, we can observe the impact of adding SNLI data to the training process. The increased accuracy across all test sets suggests that the combined dataset helps the model generalize better and improve its performance.",1704.05426v4-Table4-1.png,A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference,"This paper introduces the Multi-Genre Natural Language Inference (MultiNLI)
corpus, a dataset designed for use in the development and evaluation of machine
learning models for sentence understanding. In addition to being one of the
largest corpora available for the task of NLI, at 433k examples, this corpus
improves upon available resources in its coverage: it offers data from ten
distinct genres of written and spoken English--making it possible to evaluate
systems on nearly the full complexity of the language--and it offers an
explicit setting for the evaluation of cross-genre domain adaptation.","Table 4: Test set accuracies (%) for all models; Match. represents test set performance on the MultiNLI genres that are also represented in the training set, Mis. represents test set performance on the remaining ones; Most freq. is a trivial ‘most frequent class’ baseline.",How does the performance of the ESIM model differ when trained on MNLI alone versus trained on both MNLI and SNLI combined?
spiqa_119,1708.05239v3,"Based on the results depicted in the figure for the prostate cancer dataset, does the PE-N=5 sampler exhibit a higher log-predictive density on the held-out test data compared to the HMC sampler?",The PE-N=5 sampler performs better than the HMC sampler.,1708.05239v3.pdf,"['1708.05239v3.pdf', '1611.04684v1.pdf', '1709.02755v5.pdf', '1704.08615v2.pdf', '1804.04410v2.pdf', '1702.03584v3.pdf', '1805.08751v2.pdf']","The box plot for the PE-N=5 sampler is higher than the box plot for the HMC sampler, indicating that the PE-N=5 sampler has a higher log-predictive density on held-out test data.",1708.05239v3-Figure10-1.png,Pseudo-extended Markov chain Monte Carlo,"Sampling from posterior distributions using Markov chain Monte Carlo (MCMC)
methods can require an exhaustive number of iterations, particularly when the
posterior is multi-modal as the MCMC sampler can become trapped in a local mode
for a large number of iterations. In this paper, we introduce the
pseudo-extended MCMC method as a simple approach for improving the mixing of
the MCMC sampler for multi-modal posterior distributions. The pseudo-extended
method augments the state-space of the posterior using pseudo-samples as
auxiliary variables. On the extended space, the modes of the posterior are
connected, which allows the MCMC sampler to easily move between well-separated
posterior modes. We demonstrate that the pseudo-extended approach delivers
improved MCMC sampling over the Hamiltonian Monte Carlo algorithm on
multi-modal posteriors, including Boltzmann machines and models with
sparsity-inducing priors.","Log-predictive density on held-out test data (random 20% of full data) for the prostate cancer dataset comparing the HMC and pseudo-extended HMC samplers, with N = 2 and N = 5. For the case of fixed β = [0.25, 0.5, 0.75], the number of pseudo-samples N = 2.",How does the performance of the PE-N=5 sampler compare to the HMC sampler?
spiqa_120,1704.08615v2,"Based on the figure where the performance of SIM saliency maps is evaluated across varying numbers of fixations, what trend is observed in the SIM scores as the number of fixations per sample increases?",The performance of the SIM saliency map increases as the number of fixations increases.,1704.08615v2.pdf,"['1704.08615v2.pdf', '1811.09393v4.pdf', '1805.06431v4.pdf', '1804.04410v2.pdf']","The table in the figure shows the average performance of the SIM saliency map for different numbers of fixations. The performance is measured by the SIM score, which is a measure of how well the saliency map predicts the fixations of human observers. The table shows that the SIM score increases as the number of fixations increases. This means that the SIM saliency map is better at predicting the fixations of human observers when there are more fixations.",1704.08615v2-Figure7-1.png,"Saliency Benchmarking Made Easy: Separating Models, Maps and Metrics","Dozens of new models on fixation prediction are published every year and
compared on open benchmarks such as MIT300 and LSUN. However, progress in the
field can be difficult to judge because models are compared using a variety of
inconsistent metrics. Here we show that no single saliency map can perform well
under all metrics. Instead, we propose a principled approach to solve the
benchmarking problem by separating the notions of saliency models, maps and
metrics. Inspired by Bayesian decision theory, we define a saliency model to be
a probabilistic model of fixation density prediction and a saliency map to be a
metric-specific prediction derived from the model density which maximizes the
expected performance on that metric given the model density. We derive these
optimal saliency maps for the most commonly used saliency metrics (AUC, sAUC,
NSS, CC, SIM, KL-Div) and show that they can be computed analytically or
approximated with high precision. We show that this leads to consistent
rankings in all metrics and avoids the penalties of using one saliency map for
all metrics. Our method allows researchers to have their model compete on many
different metrics with state-of-the-art in those metrics: ""good"" models will
perform well in all metrics.","The optimal SIM saliency map depends on the number of fixations. (a) For a sample density (see Figure 6), we calculated the optimal SIM saliency map for different numbers of fixations per sample (numbers on top) and additionally the mean empirical saliency map (CC). (b) average performance of those saliency maps (rows) when repeatedly sampling a certain number of fixations (columns) and computing SIM. The best performing saliency map for each sampled dataset (columns) is printed in boldface. It’s always the saliency map calculated with the same number of fixations. Note that the CC saliency map – although looking identical – always performs slighly worse",How does the performance of the SIM saliency map change as the number of fixations increases?
spiqa_121,1701.03077v10,"What does Figure 9 reveal about the comparative performance of the adaptive ""Wavelets + YUV"" VAE model and the fixed model across different shape parameters α, specifically in terms of their validation set ELBO?",The adaptive model consistently outperforms the fixed model for all values of α.,1701.03077v10.pdf,"['1701.03077v10.pdf', '1811.02553v4.pdf', '1705.09882v2.pdf', '1612.02803v5.pdf', '1812.06589v2.pdf', '1805.07567v2.pdf', '1704.07121v2.pdf', '1703.02507v3.pdf', '1707.01922v5.pdf', '1805.02349v2.pdf']",The plot shows that the test set ELBO (evidence lower bound) of the adaptive model (blue line) is always higher than the ELBO of the fixed model (red line) for all values of α. A higher ELBO indicates better performance.,1701.03077v10-Figure9-1.png,A General and Adaptive Robust Loss Function,"We present a generalization of the Cauchy/Lorentzian, Geman-McClure,
Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2
loss functions. By introducing robustness as a continuous parameter, our loss
function allows algorithms built around robust loss minimization to be
generalized, which improves performance on basic vision tasks such as
registration and clustering. Interpreting our loss as the negative log of a
univariate density yields a general probability distribution that includes
normal and Cauchy distributions as special cases. This probabilistic
interpretation enables the training of neural networks in which the robustness
of the loss automatically adapts itself during training, which improves
performance on learning-based tasks such as generative image synthesis and
unsupervised monocular depth estimation, without requiring any manual parameter
tuning.",Figure 9. Here we compare the validation set ELBO of our adaptive “Wavelets + YUV” VAE model with the ELBO achieved when setting all wavelet coefficients to have the same fixed shape parameter α. We see that allowing our distribution to individually adapt its shape parameter to each coefficient outperforms any single fixed shape parameter.,How does the performance of the adaptive model compare to the fixed model with different values of α?
spiqa_122,1805.06431v4,"How do the test accuracies of ChoiceNet, ConvNet, and ConvNet+Mixup on the MNIST dataset, as shown in Table 12, change as the label corruption increases from 25% to 47%, and which model demonstrates the most robustness against label corruption?","As the corruption level increases, the performance of all models decreases. However, ChoiceNet consistently outperforms both ConvNet and ConvNet+Mixup across all corruption levels, maintaining high accuracy even when almost half of the labels are incorrect. This suggests that ChoiceNet is significantly more robust to label corruption compared to the other models.",1805.06431v4.pdf,"['1805.06431v4.pdf', '1606.07384v2.pdf', '1605.07496v3.pdf', '1701.03077v10.pdf', '1703.02507v3.pdf', '1805.07567v2.pdf', '1804.04786v3.pdf', '1706.00633v4.pdf', '1705.09966v2.pdf', '1803.04572v2.pdf', '1704.07121v2.pdf', '1704.07854v4.pdf', '1708.00160v2.pdf', '1703.00060v2.pdf', '1804.07931v2.pdf']","The table shows the test accuracies of different models on the MNIST dataset with varying levels of label corruption. By comparing the accuracies across different corruption levels (25%, 40%, 45%, 47%), we can observe the trend of decreasing performance for all models. However, the decline is much less severe for ChoiceNet, which maintains an accuracy above 92% even at the highest corruption level. This observation allows us to conclude that ChoiceNet is more robust to label corruption than the other models.",1805.06431v4-Table12-1.png,Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks,"In this paper, we focus on weakly supervised learning with noisy training
data for both classification and regression problems.We assume that the
training outputs are collected from a mixture of a target and correlated noise
distributions.Our proposed method simultaneously estimates the target
distribution and the quality of each data which is defined as the correlation
between the target and data generating distributions.The cornerstone of the
proposed method is a Cholesky Block that enables modeling dependencies among
mixture distributions in a differentiable manner where we maintain the
distribution over the network weights.We first provide illustrative examples in
both regression and classification tasks to show the effectiveness of the
proposed method.Then, the proposed method is extensively evaluated in a number
of experiments where we show that it constantly shows comparable or superior
performances compared to existing baseline methods in the handling of noisy
data.",Table 12: Test accuracies on the MNIST dataset with randomly permutated label.,How does the performance of the different models change as the corruption level increases? Which model appears to be the most robust to label corruption?
spiqa_123,1809.04276v2,"Based on Table 4, how does the classification accuracy of the discriminator in the REAT method compare to the conventional discriminator in AL, and what specific numerical evidence supports this comparison?",The discriminator in the author's approach achieves higher accuracy (95.72%) compared to the conventional discriminator in AL (94.01%).,1809.04276v2.pdf,"['1809.04276v2.pdf', '1811.08257v1.pdf', '1805.06447v3.pdf', '1811.02553v4.pdf', '1611.07718v2.pdf', '1705.09296v2.pdf', '1703.00899v2.pdf']",Table 1 explicitly shows the classification accuracy for both discriminators. The higher accuracy of the author's discriminator indicates its better ability to distinguish between human-generated and machine-generated responses. This supports the claim in the passage that the N-best response candidates used in the author's approach are helpful for the discriminator.,1809.04276v2-Table4-1.png,Retrieval-Enhanced Adversarial Training for Neural Response Generation,"Dialogue systems are usually built on either generation-based or
retrieval-based approaches, yet they do not benefit from the advantages of
different models. In this paper, we propose a Retrieval-Enhanced Adversarial
Training (REAT) method for neural response generation. Distinct from existing
approaches, the REAT method leverages an encoder-decoder framework in terms of
an adversarial training paradigm, while taking advantage of N-best response
candidates from a retrieval-based system to construct the discriminator. An
empirical study on a large scale public available benchmark dataset shows that
the REAT method significantly outperforms the vanilla Seq2Seq model as well as
the conventional adversarial training approach.",Table 4: Classification accuracy of discriminators in AL and our approach.,How does the performance of the discriminator in the proposed approach compare to the conventional discriminator in AL? What evidence suggests this difference in performance?
spiqa_124,1804.04410v2,"Based on Table 1 of the paper on reinforcement learning for query evaluations in web search, how did the learned policy impact both the relevance (NCG) and efficiency (index blocks accessed) for CAT2 queries compared to the production baseline?","For CAT2 queries, the learned policy shows a slight improvement in relevance (NCG) for the weighted set and a significant reduction in index blocks accessed for both weighted and unweighted sets.",1804.04410v2.pdf,"['1804.04410v2.pdf', '1803.02750v3.pdf', '1809.02731v3.pdf', '1611.04363v2.pdf', '1811.08257v1.pdf']","Table 1 shows the changes in NCG (a measure of relevance) and index blocks accessed (a measure of efficiency) for both CAT1 and CAT2 queries. Comparing the values for CAT2 to the baseline (0%), we see a small positive change in NCG for the weighted set (+0.2%) and a significant decrease in index blocks accessed for both sets (-22.7% for weighted and unspecified for unweighted). This indicates that the learned policy maintains similar relevance while significantly improving efficiency for CAT2 queries.",1804.04410v2-Table1-1.png,Optimizing Query Evaluations using Reinforcement Learning for Web Search,"In web search, typically a candidate generation step selects a small set of
documents---from collections containing as many as billions of web pages---that
are subsequently ranked and pruned before being presented to the user. In Bing,
the candidate generation involves scanning the index using statically designed
match plans that prescribe sequences of different match criteria and stopping
conditions. In this work, we pose match planning as a reinforcement learning
task and observe up to 20% reduction in index blocks accessed, with small or no
degradation in the quality of the candidate sets.","Table 1: Changes in NCG and the index blocks accessed u from our learned policy relative to production baselines. In both categories, we observe significant reduction in index blocks accessed, although at the cost of some loss in relevance in case of CAT1. All the differences in NCG and u are statistically significant (p < 0.01). Coverage of CAT2 queries in the unweighted set is too low to report numbers.",How does the performance of the learned policy compare to the production baseline for CAT2 queries in terms of relevance and efficiency?
spiqa_125,1812.00108v4,"How does the F1-score of the model change when trained and tested on two-view versus three-view data, as demonstrated in the scalability analysis figure showing precision, recall, and F1-score metrics?","The performance of the model generally improves as the number of views increases. For example, when the model is trained and tested on two-view data, the F1-score is 29.67. However, when the model is trained and tested on three-view data, the F1-score increases to 30.2. This suggests that the model is able to learn more effectively from data with more views.",1812.00108v4.pdf,"['1812.00108v4.pdf', '1704.05426v4.pdf', '1709.02418v2.pdf', '1704.00774v3.pdf', '1804.05995v2.pdf', '1805.01216v3.pdf', '1705.02798v6.pdf', '1708.05239v3.pdf', '1809.00458v1.pdf', '1707.08608v3.pdf', '1804.05936v2.pdf', '1705.10667v4.pdf', '1702.08694v3.pdf', '1706.00827v2.pdf']","The table in the figure shows the precision, recall, and F1-score of the model for different numbers of views. The F1-score is a measure of the overall performance of the model, and it is calculated as the harmonic mean of precision and recall.",1812.00108v4-Table3-1.png,Multi-Stream Dynamic Video Summarization,"With vast amounts of video content being uploaded to the Internet every
minute, video summarization becomes critical for efficient browsing, searching,
and indexing of visual content. Nonetheless, the spread of social and
egocentric cameras creates an abundance of sparse scenarios captured by several
devices, and ultimately required to be jointly summarized. In this paper, we
discuss the problem of summarizing videos recorded independently by several
dynamic cameras that intermittently share the field of view. We present a
robust framework that (a) identifies a diverse set of important events among
moving cameras that often are not capturing the same scene, and (b) selects the
most representative view(s) at each event to be included in a universal
summary. Due to the lack of an applicable alternative, we collected a new
multi-view egocentric dataset, Multi-Ego. Our dataset is recorded
simultaneously by three cameras, covering a wide variety of real-life
scenarios. The footage is annotated by multiple individuals under various
summarization configurations, with a consensus analysis ensuring a reliable
ground truth. We conduct extensive experiments on the compiled dataset in
addition to three other standard benchmarks that show the robustness and the
advantage of our approach in both supervised and unsupervised settings.
Additionally, we show that our approach learns collectively from data of varied
number-of-views and orthogonal to other summarization methods, deeming it
scalable and generic.",Scalability Analysis: Our framework can be trained and tested on data of different number-of-views.,How does the performance of the model change as the number of views increases?
spiqa_126,1803.06506v3,"Based on the figure presenting the Pearson correlation coefficients, how does the model's performance in unsupervised visual grounding change with respect to both bounding box area and the similarity of concepts to ImageNet classes?",The performance of the model increases with increasing bounding box area and decreases with increasing similarity of the concept with ImageNet classes.,1803.06506v3.pdf,"['1803.06506v3.pdf', '1811.08481v2.pdf', '1707.01922v5.pdf', '1704.07854v4.pdf', '1901.00398v2.pdf', '1611.04363v2.pdf', '1611.07718v2.pdf', '1811.06635v1.pdf', '1812.00108v4.pdf', '1704.05958v2.pdf', '1705.07384v2.pdf', '1809.03550v3.pdf', '1702.08694v3.pdf', '1812.00281v3.pdf', '1805.04687v2.pdf']","The left plot in the figure shows that the performance of the model is positively correlated with the bounding box area, with a Pearson correlation coefficient of 0.85. This means that as the bounding box area increases, the performance of the model also increases. The right plot in the figure shows that the performance of the model is negatively correlated with the similarity of the concept with ImageNet classes, with a Pearson correlation coefficient of -0.02. This means that as the similarity of the concept with ImageNet classes increases, the performance of the model decreases.",1803.06506v3-Figure3-1.png,Learning Unsupervised Visual Grounding Through Semantic Self-Supervision,"Localizing natural language phrases in images is a challenging problem that
requires joint understanding of both the textual and visual modalities. In the
unsupervised setting, lack of supervisory signals exacerbate this difficulty.
In this paper, we propose a novel framework for unsupervised visual grounding
which uses concept learning as a proxy task to obtain self-supervision. The
simple intuition behind this idea is to encourage the model to localize to
regions which can explain some semantic property in the data, in our case, the
property being the presence of a concept in a set of images. We present
thorough quantitative and qualitative experiments to demonstrate the efficacy
of our approach and show a 5.6% improvement over the current state of the art
on Visual Genome dataset, a 5.8% improvement on the ReferItGame dataset and
comparable to state-of-art performance on the Flickr30k dataset.",Variation of performance with respect to bounding box area and similarity of concept with ImageNet classes.,How does the performance of the model vary with respect to the bounding box area and the similarity of the concept with ImageNet classes?
spiqa_127,1811.07073v3,"Referring to Table 3, how does the inclusion of a convolutional self-correction module affect mIOU scores on the Cityscapes validation set as the size of the fully supervised set F increases, compared to a model without self-correction?",The model with convolutional self-correction consistently outperforms the model with no self-correction as the number of images in set $\mathcal{F}$ increases.,1811.07073v3.pdf,"['1811.07073v3.pdf', '1707.00524v2.pdf', '1709.00139v4.pdf', '1605.07496v3.pdf', '1611.05742v3.pdf', '1709.02755v5.pdf', '1803.05776v2.pdf']","Looking at the last three rows of Table 1, we can compare the mIOU scores for different self-correction methods. For each value of images in $\mathcal{F}$ (200, 450, and 914), the convolutional self-correction model achieves a higher mIOU score than the model with no self-correction. This trend indicates that convolutional self-correction leads to better performance, especially as the size of set $\mathcal{F}$ increases. Additionally, the passage mentions that the same conclusions observed on the PASCAL dataset hold for the Cityscapes dataset, implying that the efficacy of self-correction is consistent across datasets.",1811.07073v3-Table3-1.png,Semi-Supervised Semantic Image Segmentation with Self-correcting Networks,"Building a large image dataset with high-quality object masks for semantic
segmentation is costly and time consuming. In this paper, we introduce a
principled semi-supervised framework that only uses a small set of fully
supervised images (having semantic segmentation labels and box labels) and a
set of images with only object bounding box labels (we call it the weak set).
Our framework trains the primary segmentation model with the aid of an
ancillary model that generates initial segmentation labels for the weak set and
a self-correction module that improves the generated labels during training
using the increasingly accurate primary model. We introduce two variants of the
self-correction module using either linear or convolutional functions.
Experiments on the PASCAL VOC 2012 and Cityscape datasets show that our models
trained with a small fully supervised set perform similar to, or better than,
models trained with a large fully supervised set while requiring ~7x less
annotation effort.","Table 3: Ablation study of our models on Cityscapes validation set using mIOU for different sizes of F . For the last three rows, the remaining images in the training set are used as W , i.e., W + F = 2975.",How does the performance of the model with convolutional self-correction compare to the model with no self-correction as the number of images in set $\mathcal{F}$ increases?
spiqa_128,1804.07707v2,"Based on Table 1 of the ""Factorising AMR generation through syntax"" paper, how does the BLEU score of the proposed syntax-aware model trained on the LDC2017T10 dataset compare with other models, and what does this reveal about the role of incorporating syntactic structures in improving AMR-to-text generation?","When trained on the LDC2017T10 dataset, the proposed model achieves the highest BLEU scores on both Dev and Test sets compared to other models listed in the table. This suggests that incorporating syntax into the model significantly improves its performance in generating text from AMR representations.",1804.07707v2.pdf,"['1804.07707v2.pdf', '1702.08694v3.pdf', '1710.06177v2.pdf', '1805.07567v2.pdf']","Table 1 presents BLEU scores for different models trained on various datasets. Comparing the scores of ""Our model"" to other models trained on LDC2017T10 specifically allows us to isolate the impact of the proposed approach on the same data. The higher BLEU scores achieved by ""Our model"" indicate that incorporating syntax leads to better generation quality compared to models without this feature.",1804.07707v2-Table3-1.png,Factorising AMR generation through syntax,"Generating from Abstract Meaning Representation (AMR) is an underspecified
problem, as many syntactic decisions are not constrained by the semantic graph.
To explicitly account for this underspecification, we break down generating
from AMR into two steps: first generate a syntactic structure, and then
generate the surface form. We show that decomposing the generation process this
way leads to state-of-the-art single model performance generating from AMR
without additional unlabelled data. We also demonstrate that we can generate
meaning-preserving syntactic paraphrases of the same AMR graph, as judged by
humans.",Table 3: BLEU results for generation.,"How does the performance of the proposed model compare to other models when trained on the LDC2017T10 dataset, and what does this suggest about the effectiveness of incorporating syntax into the model?"
spiqa_129,1705.02798v6,"How do the EM and F1 scores of the single R.M-Reader model, as shown in Table 2 of the Reinforced Mnemonic Reader paper, compare specifically to the single model performances of SLQA and Hybrid AoA Reader on the SQuAD test set?","The single R.M-Reader model achieves an EM score of 79.5% and an F1 score of 86.6% on the SQuAD test set. This performance is better than all other single models listed in the table, except for SLQA and Hybrid AoA Reader, which achieve slightly higher F1 scores of 87.0% and 87.3%, respectively.",1705.02798v6.pdf,"['1705.02798v6.pdf', '1804.04410v2.pdf', '1706.04269v2.pdf', '1611.03780v2.pdf', '1811.08257v1.pdf', '1906.06589v3.pdf']","Table 3 presents the performance of different models on the SQuAD dataset, including both single and ensemble models. By comparing the EM and F1 scores of R.M-Reader with those of other models in the ""Single Model"" section, we can assess its relative performance. While SLQA and Hybrid AoA Reader have slightly higher F1 scores, R.M-Reader still outperforms all other single models in terms of EM score and has a competitive F1 score.",1705.02798v6-Table2-1.png,Reinforced Mnemonic Reader for Machine Reading Comprehension,"In this paper, we introduce the Reinforced Mnemonic Reader for machine
reading comprehension tasks, which enhances previous attentive readers in two
aspects. First, a reattention mechanism is proposed to refine current
attentions by directly accessing to past attentions that are temporally
memorized in a multi-round alignment architecture, so as to avoid the problems
of attention redundancy and attention deficiency. Second, a new optimization
approach, called dynamic-critical reinforcement learning, is introduced to
extend the standard supervised method. It always encourages to predict a more
acceptable answer so as to address the convergence suppression problem occurred
in traditional reinforcement learning algorithms. Extensive experiments on the
Stanford Question Answering Dataset (SQuAD) show that our model achieves
state-of-the-art results. Meanwhile, our model outperforms previous systems by
over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD
datasets.","Table 2: The performance of Reinforced Mnemonic Reader and other competing approaches on the SQuAD dataset. The results of test set are extracted on Feb 2, 2018: Rajpurkar et al.[2016]1, Xiong et al.[2017a]2, Huang et al.[2017]3, Liu et al.[2017b]4 and Peters[2018]5. † indicates unpublished works. BSE refers to BiDAF + Self Attention + ELMo.",How does the performance of the single R.M-Reader model compare to the best single models of other approaches on the SQuAD test set?
spiqa_130,1605.07496v3,"Referring to the figure titled ""ALOQ models the return f as a function of (π, θ),"" how does the predicted mean return behave as a function of θ when π is fixed at 1.5, and what notable trend in the uncertainty is observed, particularly near the point where the minimum return occurs around θ = 0.5?","The predicted return decreases as θ increases, with a minimum at around θ = 0.5.",1605.07496v3.pdf,"['1605.07496v3.pdf', '1705.07384v2.pdf', '1611.07718v2.pdf', '1804.05995v2.pdf', '1704.05958v2.pdf', '1805.00912v4.pdf', '1709.02418v2.pdf', '1710.01507v4.pdf', '1706.00633v4.pdf', '1709.08294v3.pdf', '1611.05742v3.pdf', '1706.03847v3.pdf', '1705.08016v3.pdf']","Figure (b) shows the predicted return for different values of θ when π = 1.5. The blue line represents the mean predicted return, and the shaded area represents the uncertainty associated with the prediction. As θ increases, the mean predicted return decreases, and the uncertainty increases.",1605.07496v3-Figure1-1.png,Alternating Optimisation and Quadrature for Robust Control,"Bayesian optimisation has been successfully applied to a variety of
reinforcement learning problems. However, the traditional approach for learning
optimal policies in simulators does not utilise the opportunity to improve
learning by adjusting certain environment variables: state features that are
unobservable and randomly determined by the environment in a physical setting
but are controllable in a simulator. This paper considers the problem of
finding a robust policy while taking into account the impact of environment
variables. We present Alternating Optimisation and Quadrature (ALOQ), which
uses Bayesian optimisation and Bayesian quadrature to address such settings.
ALOQ is robust to the presence of significant rare events, which may not be
observable under random sampling, but play a substantial role in determining
the optimal policy. Experimental results across different domains show that
ALOQ can learn more efficiently and robustly than existing methods.","ALOQ models the return f as a function of (π, θ); (a) the predicted mean based on some observed data; (b) the predicted return of π = 1.5 for different θ, together with the uncertainty associated with them, given p(θ); (c) ALOQ marginalises out θ and computes f̄(π) and its associated uncertainty, which is used to actively select π.",How does the predicted return change as a function of θ for a fixed value of π = 1.5?
spiqa_131,1703.10730v2,"In Figure 9 of the paper ""Unsupervised Holistic Image Generation from Key Local Patches,"" how does the addition of Gaussian noise to the input image affect the generated outputs (Gen 2 and Gen M2) in terms of clarity and realism, and what does this indicate about the robustness of the proposed algorithm?","The presence of noise in the input image can degrade the quality of the generated images, but the proposed algorithm is still able to generate realistic images even with a certain amount of noise.",1703.10730v2.pdf,"['1703.10730v2.pdf', '1704.04539v2.pdf', '1703.04887v4.pdf', '1809.04276v2.pdf', '1805.01216v3.pdf', '1809.03550v3.pdf', '1704.07121v2.pdf', '1608.02784v2.pdf', '1803.05776v2.pdf', '1811.02721v3.pdf', '1705.02798v6.pdf', '1906.10843v1.pdf', '1706.00827v2.pdf']","The figure shows that the generated images with noise (Gen 2 and Gen M2) are still recognizable as faces, even though they are not as clear as the generated images without noise (Gen 1 and Gen M1). This suggests that the proposed algorithm is robust to noise and can still generate realistic images even when the input image is degraded.",1703.10730v2-Figure9-1.png,Unsupervised Holistic Image Generation from Key Local Patches,"We introduce a new problem of generating an image based on a small number of
key local patches without any geometric prior. In this work, key local patches
are defined as informative regions of the target object or scene. This is a
challenging problem since it requires generating realistic images and
predicting locations of parts at the same time. We construct adversarial
networks to tackle this problem. A generator network generates a fake image as
well as a mask based on the encoder-decoder framework. On the other hand, a
discriminator network aims to detect fake images. The network is trained with
three losses to consider spatial, appearance, and adversarial information. The
spatial loss determines whether the locations of predicted parts are correct.
Input patches are restored in the output image without much modification due to
the appearance loss. The adversarial loss ensures output images are realistic.
The proposed network is trained without supervisory signals since no labels of
key parts are required. Experimental results on six datasets demonstrate that
the proposed algorithm performs favorably on challenging objects and scenes.",Figure 9: Examples of generated results when the input image contains noises. We add a Gaussian noise at each pixel of Input 3. Gen 1 and Gen M1 are generated without noises. Gen 2 and Gen M2 are generated with noises.,How does the presence of noise in the input image affect the quality of the generated images?
spiqa_132,1709.02755v5,"Based on Figure 1 of the ""Simple Recurrent Units for Highly Parallelizable Recurrence"" paper, how does SRU's processing time compare to cuDNN LSTM and word-level convolution (with filter widths k=2 and k=3) across different sequence lengths and feature dimensions for a batch of 32 samples?",The processing time of SRU is significantly faster than that of cuDNN LSTM and word-level convolution with filter widths k=2 and k=3.,1709.02755v5.pdf,"['1709.02755v5.pdf', '1704.05426v4.pdf', '1901.00056v2.pdf', '1704.05958v2.pdf', '1805.06447v3.pdf', '1811.09393v4.pdf', '1804.04786v3.pdf', '1809.03550v3.pdf', '1805.08751v2.pdf', '1707.01922v5.pdf', '1611.05742v3.pdf', '1805.07567v2.pdf']","The figure shows the average processing time of a batch of 32 samples using different architectures. The x-axis shows the number of tokens per sequence (l) and feature dimension (d), and the y-axis shows the processing time in milliseconds. The figure shows that SRU has a lower processing time than the other architectures across all values of l and d.",1709.02755v5-Figure1-1.png,Simple Recurrent Units for Highly Parallelizable Recurrence,"Common recurrent neural architectures scale poorly due to the intrinsic
difficulty in parallelizing their state computations. In this work, we propose
the Simple Recurrent Unit (SRU), a light recurrent unit that balances model
capacity and scalability. SRU is designed to provide expressive recurrence,
enable highly parallelized implementation, and comes with careful
initialization to facilitate training of deep models. We demonstrate the
effectiveness of SRU on multiple NLP tasks. SRU achieves 5--9x speed-up over
cuDNN-optimized LSTM on classification and question answering datasets, and
delivers stronger results than LSTM and convolutional models. We also obtain an
average of 0.7 BLEU improvement over the Transformer model on translation by
incorporating SRU into the architecture.","Figure 1: Average processing time in milliseconds of a batch of 32 samples using cuDNN LSTM, wordlevel convolution conv2d (with filter width k = 2 and k = 3), and the proposed SRU. We vary the number of tokens per sequence (l) and feature dimension (d).",How does the processing time of SRU compare to that of cuDNN LSTM and word-level convolution with filter widths k=2 and k=3?
spiqa_133,1705.09966v2,"How does the identity-preserving face super-resolution method depicted in the top row of Fig. 1 ensure the preservation of Ivanka Trump's identity in the generated high-resolution image, and how does this compare to the results of conventional face super-resolution methods?","The proposed attribute-guided face generation method preserves the identity of the person in the high-resolution result, while conventional face super-resolution methods do not necessarily guarantee this.",1705.09966v2.pdf,"['1705.09966v2.pdf', '1710.01507v4.pdf', '1707.08608v3.pdf', '1805.08751v2.pdf', '1705.10667v4.pdf', '1709.02755v5.pdf', '1705.07164v8.pdf', '1708.05239v3.pdf', '1611.04363v2.pdf', '1709.08294v3.pdf', '1906.06589v3.pdf']","The top row of Figure 1 shows an example of identity-preserving face super-resolution using the proposed method. The identity image of Ivanka Trump (a) is used to guide the generation of a high-resolution image (d) from a low-resolution input image (c). The generated image (d) clearly preserves the identity of Ivanka Trump, while the conventional face super-resolution method may not produce a result that is as faithful to the original identity.",1705.09966v2-Figure1-1.png,Attribute-Guided Face Generation Using Conditional CycleGAN,"We are interested in attribute-guided face generation: given a low-res face
input image, an attribute vector that can be extracted from a high-res image
(attribute image), our new method generates a high-res face image for the
low-res input that satisfies the given attributes. To address this problem, we
condition the CycleGAN and propose conditional CycleGAN, which is designed to
1) handle unpaired training data because the training low/high-res and high-res
attribute images may not necessarily align with each other, and to 2) allow
easy control of the appearance of the generated face via the input attributes.
We demonstrate impressive results on the attribute-guided conditional CycleGAN,
which can synthesize realistic face images with appearance easily controlled by
user-supplied attributes (e.g., gender, makeup, hair color, eyeglasses). Using
the attribute image as identity to produce the corresponding conditional vector
and by incorporating a face verification network, the attribute-guided network
becomes the identity-guided conditional CycleGAN which produces impressive and
interesting results on identity transfer. We demonstrate three applications on
identity-guided conditional CycleGAN: identity-preserving face superresolution,
face swapping, and frontal face generation, which consistently show the
advantage of our new method.","Fig. 1. Identity-guided face generation. Top: identity-preserving face super-resolution where (a) is the identity image; (b) input photo; (c) image crop from (b) in low resolution; (d) our generated high-res result; (e) ground truth image. Bottom: face transfer, where (f) is the identity image; (g) input low-res image of another person provides overall shape constraint; (h) our generated high-res result where the man’s identity is transferred. To produce the low-res input (g) we down-sample from (i), which is a woman’s face.",How does the proposed attribute-guided face generation method compare to conventional face super-resolution methods in terms of identity preservation?
spiqa_134,1811.10673v1,New question:,The proposed method achieves significantly higher MS-SSIM scores than H.264 at bitrates below 10 Kbps.,1811.10673v1.pdf,"['1811.10673v1.pdf', '1802.07459v2.pdf', '1811.02721v3.pdf', '1705.09882v2.pdf', '1707.01922v5.pdf', '1608.02784v2.pdf', '1605.07496v3.pdf', '1803.04572v2.pdf', '1805.08751v2.pdf', '1812.00281v3.pdf', '1706.03847v3.pdf', '1708.06832v3.pdf', '1812.10735v2.pdf', '1803.04383v2.pdf']"," The figure shows the rate-distortion curves for both the proposed method (red) and H.264 (blue). At low bitrates, the red curves are higher than the blue curves, indicating that the proposed method achieves better MS-SSIM scores.",1811.10673v1-Figure8-1.png,Adversarial Video Compression Guided by Soft Edge Detection,"We propose a video compression framework using conditional Generative
Adversarial Networks (GANs). We rely on two encoders: one that deploys a
standard video codec and another which generates low-level maps via a pipeline
of down-sampling, a newly devised soft edge detector, and a novel lossless
compression scheme. For decoding, we use a standard video decoder as well as a
neural network based one, which is trained using a conditional GAN. Recent
""deep"" approaches to video compression require multiple videos to pre-train
generative networks to conduct interpolation. In contrast to this prior work,
our scheme trains a generative decoder on pairs of a very limited number of key
frames taken from a single video and corresponding low-level maps. The trained
decoder produces reconstructed frames relying on a guidance of low-level maps,
without any interpolation. Experiments on a diverse set of 131 videos
demonstrate that our proposed GAN-based compression engine achieves much higher
quality reconstructions at very low bitrates than prevailing standard codecs
such as H.264 or HEVC.","Figure 8: Rate-distortion curves (MS-SSIM) against bitrate for four semantic categories of 100 videos from the KTH dataset [8]. The red curves and dots correspond to our model while the blue curves and dots correspond to H.264. In the very low bitrate region (below 10Kbps), our scheme yielded higher MS-SSIM scores. Similar results were observed on PSNR, SSIM and VMAF (see supplementary material).",How does the proposed method compare to H.264 in terms of MS-SSIM score at low bitrates?
spiqa_135,1705.09966v2,"In Figure 7, how does the proposed method compare to icGAN in terms of accurately generating different hair colors when facial attributes are swapped?",The proposed method is able to generate images with different hair colors more accurately than icGAN.,1705.09966v2.pdf,"['1705.09966v2.pdf', '1805.04609v3.pdf', '1809.02731v3.pdf', '1804.05936v2.pdf', '1701.06171v4.pdf', '1611.07718v2.pdf']","The figure shows that the proposed method is able to generate images with different hair colors that are more faithful to the input image than icGAN. For example, in the example with blonde hair, the proposed method is able to generate an image with blonde hair that is very similar to the input image, while icGAN generates an image with a different hair color.",1705.09966v2-Figure7-1.png,Attribute-Guided Face Generation Using Conditional CycleGAN,"We are interested in attribute-guided face generation: given a low-res face
input image, an attribute vector that can be extracted from a high-res image
(attribute image), our new method generates a high-res face image for the
low-res input that satisfies the given attributes. To address this problem, we
condition the CycleGAN and propose conditional CycleGAN, which is designed to
1) handle unpaired training data because the training low/high-res and high-res
attribute images may not necessarily align with each other, and to 2) allow
easy control of the appearance of the generated face via the input attributes.
We demonstrate impressive results on the attribute-guided conditional CycleGAN,
which can synthesize realistic face images with appearance easily controlled by
user-supplied attributes (e.g., gender, makeup, hair color, eyeglasses). Using
the attribute image as identity to produce the corresponding conditional vector
and by incorporating a face verification network, the attribute-guided network
becomes the identity-guided conditional CycleGAN which produces impressive and
interesting results on identity transfer. We demonstrate three applications on
identity-guided conditional CycleGAN: identity-preserving face superresolution,
face swapping, and frontal face generation, which consistently show the
advantage of our new method.","Fig. 7. Comparison with [13] by swapping facial attributes. Four paired examples are shown. Generally, our method can generate much better images compared to [13].",How does the proposed method compare to icGAN in terms of generating images with different hair colors?
spiqa_136,1705.09966v2,"Referring to the comparison shown in the red and blue bounding boxes in Fig. 8, how does the proposed conditional CycleGAN perform in generating realistic and natural-looking facial features, such as hair color changes and skin tone preservation, compared to the method in~\cite{kim2017learning}?",The proposed method produces more realistic and natural-looking images than the method in~\cite{kim2017learning}.,1705.09966v2.pdf,"['1705.09966v2.pdf', '1708.00160v2.pdf', '1805.02349v2.pdf', '1812.00108v4.pdf', '1809.00458v1.pdf', '1708.03797v1.pdf', '1803.06506v3.pdf', '1704.08615v2.pdf', '1901.00398v2.pdf', '1703.10730v2.pdf', '1906.06589v3.pdf', '1803.03467v4.pdf', '1708.01425v4.pdf', '1704.07854v4.pdf']","The figure shows that the images generated by the proposed method (red bounding boxes) are more realistic and natural-looking than the images generated by the method in~\cite{kim2017learning} (blue bounding boxes). For example, in the first column, the proposed method is able to change the hair color of the woman to blonde while preserving her natural skin tone and facial features. In contrast, the method in~\cite{kim2017learning} produces an image with an unnatural skin tone and distorted facial features.",1705.09966v2-Figure8-1.png,Attribute-Guided Face Generation Using Conditional CycleGAN,"We are interested in attribute-guided face generation: given a low-res face
input image, an attribute vector that can be extracted from a high-res image
(attribute image), our new method generates a high-res face image for the
low-res input that satisfies the given attributes. To address this problem, we
condition the CycleGAN and propose conditional CycleGAN, which is designed to
1) handle unpaired training data because the training low/high-res and high-res
attribute images may not necessarily align with each other, and to 2) allow
easy control of the appearance of the generated face via the input attributes.
We demonstrate impressive results on the attribute-guided conditional CycleGAN,
which can synthesize realistic face images with appearance easily controlled by
user-supplied attributes (e.g., gender, makeup, hair color, eyeglasses). Using
the attribute image as identity to produce the corresponding conditional vector
and by incorporating a face verification network, the attribute-guided network
becomes the identity-guided conditional CycleGAN which produces impressive and
interesting results on identity transfer. We demonstrate three applications on
identity-guided conditional CycleGAN: identity-preserving face superresolution,
face swapping, and frontal face generation, which consistently show the
advantage of our new method.","Fig. 8. Comparison results with [9]. Four source images are shown in top row. Images with blue and red bounding boxes indicates transferred results by [9] and results by our method, respectively.",How does the proposed method compare to the method in~\cite{kim2017learning}?
spiqa_137,1705.09966v2,"What technique does the proposed method, as visualized in Fig. 11, use to swap key identity features like eyes and hair while preserving original facial factors such as head pose, shape, and expression?","The proposed method utilizes Light-CNN as both the source of the identity features and face verification loss. This allows the method to transfer the appearance of eyes, eyebrows, hairs, etc., while keeping other factors intact, e.g., head pose, shape of face, and facial expression.",1705.09966v2.pdf,"['1705.09966v2.pdf', '1703.00060v2.pdf', '1704.05958v2.pdf', '1705.02946v3.pdf', '1703.04887v4.pdf', '1702.03584v3.pdf', '1603.03833v4.pdf', '1702.08694v3.pdf', '1811.02553v4.pdf']","The figure shows that the proposed method can swap the identity of two people while preserving their facial details and expressions. For example, in the first row, the face of the woman in (a) is swapped with the face of the man in (c), but the woman's expression and facial features are still preserved in (b). Similarly, in the second row, the face of the man in (a) is swapped with the face of the woman in (c), but the man's expression and facial features are still preserved in (b).",1705.09966v2-Figure11-1.png,Attribute-Guided Face Generation Using Conditional CycleGAN,"We are interested in attribute-guided face generation: given a low-res face
input image, an attribute vector that can be extracted from a high-res image
(attribute image), our new method generates a high-res face image for the
low-res input that satisfies the given attributes. To address this problem, we
condition the CycleGAN and propose conditional CycleGAN, which is designed to
1) handle unpaired training data because the training low/high-res and high-res
attribute images may not necessarily align with each other, and to 2) allow
easy control of the appearance of the generated face via the input attributes.
We demonstrate impressive results on the attribute-guided conditional CycleGAN,
which can synthesize realistic face images with appearance easily controlled by
user-supplied attributes (e.g., gender, makeup, hair color, eyeglasses). Using
the attribute image as identity to produce the corresponding conditional vector
and by incorporating a face verification network, the attribute-guided network
becomes the identity-guided conditional CycleGAN which produces impressive and
interesting results on identity transfer. We demonstrate three applications on
identity-guided conditional CycleGAN: identity-preserving face superresolution,
face swapping, and frontal face generation, which consistently show the
advantage of our new method.","Fig. 11. Face swapping results within the high-res domain. (a)(c) are inputs of two different persons; (b)(d) their face swapping results. The black arrows indicate the guidance of identity, i.e. (d) is transformed from (c) under the identity constraint of (a). Similarly, (b) is transformed from (a) under the identity of (c). Note how our method transforms the identity by altering the appearance of eyes, eyebrows, hairs etc, while keeping other factors intact, e.g., head pose, shape of face and facial expression.",How does the proposed method preserve facial details and expression during face swapping?
spiqa_138,1803.06506v3,"How does Figure 5 illustrate the differences between the proposed unsupervised visual grounding method's attention maps and VGG16's feature maps, particularly in localizing phrase-relevant regions like the boy, the umbrella drawings, or the surfer's face with greater focus and detail?","The attention map generated by proposed method is able to localize regions that were weak or non-existent in the activations of the input maps, while the VGG16 feature map simply amplifies the activations present in VGG16 channels.",1803.06506v3.pdf,"['1803.06506v3.pdf', '1705.09296v2.pdf', '1705.07384v2.pdf', '1809.00263v5.pdf', '1811.07073v3.pdf', '1708.01425v4.pdf', '1706.08146v3.pdf', '1606.07384v2.pdf', '1809.00458v1.pdf', '1804.07707v2.pdf', '1804.01429v3.pdf']","The figure shows that the model's attention map is more focused on specific regions of the image, such as the boy in purple and white, the drawings on the umbrella, and the surfer's face. This suggests that the model is able to learn a phrase-dependent attention map that is more relevant to the task at hand.",1803.06506v3-Figure5-1.png,Learning Unsupervised Visual Grounding Through Semantic Self-Supervision,"Localizing natural language phrases in images is a challenging problem that
requires joint understanding of both the textual and visual modalities. In the
unsupervised setting, lack of supervisory signals exacerbate this difficulty.
In this paper, we propose a novel framework for unsupervised visual grounding
which uses concept learning as a proxy task to obtain self-supervision. The
simple intuition behind this idea is to encourage the model to localize to
regions which can explain some semantic property in the data, in our case, the
property being the presence of a concept in a set of images. We present
thorough quantitative and qualitative experiments to demonstrate the efficacy
of our approach and show a 5.6% improvement over the current state of the art
on Visual Genome dataset, a 5.8% improvement on the ReferItGame dataset and
comparable to state-of-art performance on the Flickr30k dataset.",Figure 5. Comparison of VGG16 feature maps with our generated attention maps.,How does the proposed method's attention map differ from the VGG16 feature map?
spiqa_139,1706.00633v4,"How does the non-ME metric, as illustrated in Figure 1d, enable the detection of adversarial examples by distinguishing their proximity to the decision boundary and their likelihood of fooling the detector in this study’s defense mechanism?"," The non-ME metric measures the entropy of the normalized non-maximal elements in the final hidden vector of the classifier. Adversarial examples often have low non-ME values, indicating that they are close to the decision boundary and have high confidence in the incorrect class.",1706.00633v4.pdf,"['1706.00633v4.pdf', '1708.02153v2.pdf', '1809.03149v2.pdf', '1812.00281v3.pdf', '1708.00160v2.pdf', '1611.05742v3.pdf', '1811.08257v1.pdf', '1804.07849v4.pdf']"," Figure 1a shows that the isolines of non-ME=t (blue dot-dashed lines) are close to the decision boundary (black solid lines). Adversarial examples that successfully fool the detector (red dots in Figure 1d) tend to fall near the decision boundary, where the non-ME value is low. In contrast, normal examples (blue dots in Figure 1d) and adversarial examples that fail to fool the detector (open circles in Figure 1d) have higher non-ME values.",1706.00633v4-Figure1-1.png,Towards Robust Detection of Adversarial Examples,"Although the recent progress is substantial, deep learning methods can be
vulnerable to the maliciously generated adversarial examples. In this paper, we
present a novel training procedure and a thresholding test strategy, towards
robust detection of adversarial examples. In training, we propose to minimize
the reverse cross-entropy (RCE), which encourages a deep network to learn
latent representations that better distinguish adversarial examples from normal
ones. In testing, we propose to use a thresholding strategy as the detector to
filter out adversarial examples for reliable predictions. Our method is simple
to implement using standard algorithms, with little extra training cost
compared to the common cross-entropy minimization. We apply our method to
defend various attacking methods on the widely used MNIST and CIFAR-10
datasets, and achieve significant improvements on robust predictions under all
the threat models in the adversarial setting.","Figure 1: a, The three black solid lines are the decision boundary of the classifier, and each black line (both solid and dashed parts) is the decision boundary between two classes. The blue dot-dashed lines are the isolines of non-ME = t. b, t-SNE visualization of the final hidden vectors on CIFAR-10. The model is Resnet-32. The training procedure is CE. c, The training procedure is RCE. d, Practical attacks on the trained networks. Blue regions are of the original classes for normal examples, and red regions are of the target classes for adversarial ones.",How does the proposed metric of non-ME help detect adversarial examples?
spiqa_140,1811.10673v1,"Based on the visual comparison in Figure 9, how does the proposed GAN-based compression model outperform H.264 in terms of preserving sharpness and detail at low bitrates, especially in key visual features like facial details and edges?",The proposed model delivers significantly better visual quality at low bitrates than H.264.,1811.10673v1.pdf,"['1811.10673v1.pdf', '1812.00281v3.pdf', '1705.10667v4.pdf', '1811.02721v3.pdf', '1705.02946v3.pdf', '1704.04539v2.pdf', '1603.00286v5.pdf', '1804.07849v4.pdf', '1809.01246v1.pdf', '1906.10843v1.pdf']",The figure shows that the reconstructed video using the proposed model (4th row) has sharper edges and more detail than the reconstructed videos using H.264 at 9 Kbps (2nd row) and 13 Kbps (3rd row). This is especially evident in the details of the person's face and clothing.,1811.10673v1-Figure9-1.png,Adversarial Video Compression Guided by Soft Edge Detection,"We propose a video compression framework using conditional Generative
Adversarial Networks (GANs). We rely on two encoders: one that deploys a
standard video codec and another which generates low-level maps via a pipeline
of down-sampling, a newly devised soft edge detector, and a novel lossless
compression scheme. For decoding, we use a standard video decoder as well as a
neural network based one, which is trained using a conditional GAN. Recent
""deep"" approaches to video compression require multiple videos to pre-train
generative networks to conduct interpolation. In contrast to this prior work,
our scheme trains a generative decoder on pairs of a very limited number of key
frames taken from a single video and corresponding low-level maps. The trained
decoder produces reconstructed frames relying on a guidance of low-level maps,
without any interpolation. Experiments on a diverse set of 131 videos
demonstrate that our proposed GAN-based compression engine achieves much higher
quality reconstructions at very low bitrates than prevailing standard codecs
such as H.264 or HEVC.","Figure 9: Two videos from (a) the KTH and (b) the YouTube dataset. Selected frames from original video and reconstructed videos using H.264 (low bitrate), H.264 (high bitrate), and the proposed model are aligned vertically along time. Our scheme demonstrated significantly better performance than the current standard codecs at low bitrates. The scores produced by several leading perceptual video quality metrics were depicted on the right side. Please refer to the supplementary for reconstructed videos and results on additional 129 videos.",How does the proposed model compare to H.264 in terms of visual quality at low bitrates?
spiqa_141,1611.02654v2,"In the figure comparing the performance of various models on the NIPS Abstracts dataset, as presented in the paper ""Sentence Ordering and Coherence Modeling using Recurrent Neural Networks,"" how does the proposed model's accuracy compare to other models, and what specific accuracy does it achieve?","The proposed model has the highest accuracy on the NIPS Abstracts dataset, with an accuracy of 51.55.",1611.02654v2.pdf,"['1611.02654v2.pdf', '1812.00108v4.pdf', '1707.01922v5.pdf', '1906.06589v3.pdf', '1704.07121v2.pdf', '1705.02798v6.pdf', '1708.06832v3.pdf', '1707.08608v3.pdf', '1704.00774v3.pdf']",The table shows the accuracy of different models on three different datasets. The proposed model has the highest accuracy on the NIPS Abstracts dataset.,1611.02654v2-Table2-1.png,Sentence Ordering and Coherence Modeling using Recurrent Neural Networks,"Modeling the structure of coherent texts is a key NLP problem. The task of
coherently organizing a given set of sentences has been commonly used to build
and evaluate models that understand such structure. We propose an end-to-end
unsupervised deep learning approach based on the set-to-sequence framework to
address this problem. Our model strongly outperforms prior methods in the order
discrimination task and a novel task of ordering abstracts from scientific
articles. Furthermore, our work shows that useful text representations can be
obtained by learning to order sentences. Visualizing the learned sentence
representations shows that the model captures high-level logical structure in
paragraphs. Our representations perform comparably to state-of-the-art
pre-training methods on sentence similarity and paraphrase detection tasks.",Comparison against prior methods on the abstracts data.,How does the proposed model compare to the other models in terms of accuracy on the NIPS Abstracts dataset?
spiqa_142,1705.09882v2,"Based on Fig. 4, how does the proposed split-rate RGB-to-Depth transfer scheme differ from the R3D [90] method in terms of both the relative learning rates and weight initialization for the bottom three layers of the network?","The proposed split-rate RGB-to-Depth transfer scheme differs from the R3D [90] method in two ways. First, the proposed method uses a different learning rate for the bottom three layers of the network. Second, the proposed method uses a different initialization for the weights of the bottom three layers of the network.",1705.09882v2.pdf,"['1705.09882v2.pdf', '1901.00398v2.pdf', '1709.02755v5.pdf', '1906.10843v1.pdf', '1704.08615v2.pdf', '1703.00060v2.pdf', '1811.09393v4.pdf', '1706.04269v2.pdf', '1611.04363v2.pdf']","The figure shows the two methods side-by-side. The R3D [90] method uses the same learning rate for all layers of the network, while the proposed method uses a lower learning rate for the bottom three layers. Additionally, the R3D [90] method initializes the weights of the bottom three layers with random values, while the proposed method initializes the weights of the bottom three layers with the weights from a pre-trained RGB network.",1705.09882v2-Figure4-1.png,Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-Identification,"We address the problem of person re-identification from commodity depth
sensors. One challenge for depth-based recognition is data scarcity. Our first
contribution addresses this problem by introducing split-rate RGB-to-Depth
transfer, which leverages large RGB datasets more effectively than popular
fine-tuning approaches. Our transfer scheme is based on the observation that
the model parameters at the bottom layers of a deep convolutional neural
network can be directly shared between RGB and depth data while the remaining
layers need to be fine-tuned rapidly. Our second contribution enhances
re-identification for video by implementing temporal attention as a
Bernoulli-Sigmoid unit acting upon frame-level features. Since this unit is
stochastic, the temporal attention parameters are trained using reinforcement
learning. Extensive experiments validate the accuracy of our method in person
re-identification from depth sequences. Finally, in a scenario where subjects
wear unseen clothes, we show large performance gains compared to a
state-of-the-art model which relies on RGB data.","Fig. 4. Our split-rate RGB-to-Depth transfer compared with Yosinski et al. [90]. At the top, the two models are trained from scratch with RGB and Depth data. Next we show the “R3D” instances (i.e. the bottom 3 layers’ weights from RGB remain frozen or slowly changing) for both methods, following the notation of [90]. The color of each layer refers to the initialization and the number below is the relative learning rate (the best performing one in bold). The key differences are summarized in the text.",How does the proposed split-rate RGB-to-Depth transfer scheme differ from the R3D [90] method of Yosinski et al.?
spiqa_143,1811.02553v4,"In the figure that depicts the average pairwise cosine similarity between repeated gradient measurements, how does the consistency of gradient estimates improve as the number of state-action pairs used in estimation increases across different policy gradient methods?",The quality of gradient estimation increases as the number of state-action pairs used in estimation increases.,1811.02553v4.pdf,"['1811.02553v4.pdf', '1812.10735v2.pdf', '1805.01216v3.pdf', '1804.07849v4.pdf', '1707.00189v3.pdf', '1704.05426v4.pdf']",The figure shows that the average pairwise cosine similarity between repeated gradient measurements increases as the number of state-action pairs used in estimation increases. This means that the gradient estimates become more consistent and accurate as more data is used.,1811.02553v4-Figure9-1.png,A Closer Look at Deep Policy Gradients,"We study how the behavior of deep policy gradient algorithms reflects the
conceptual framework motivating their development. To this end, we propose a
fine-grained analysis of state-of-the-art methods based on key elements of this
framework: gradient estimation, value prediction, and optimization landscapes.
Our results show that the behavior of deep policy gradient algorithms often
deviates from what their motivating framework would predict: the surrogate
objective does not match the true reward landscape, learned value estimators
fail to fit the true value function, and gradient estimates poorly correlate
with the ""true"" gradient. The mismatch between predicted and empirical behavior
we uncover highlights our poor understanding of current methods, and indicates
the need to move beyond current benchmark-centric evaluation methods.","Empirical variance of the gradient (c.f. (1)) as a function of the number of state-action pairs used in estimation for policy gradient methods. We obtain multiple gradient estimates using a given number of state-action pairs from the policy at a particular iteration. We then measure the average pairwise cosine similarity between these repeated gradient measurements, along with the 95% confidence intervals (shaded). Each of the colored lines (for a specific algorithm) represents a particular trained agent (we perform multiple trials with the same hyperparameter configurations but different random seeds). The dotted vertical black line (at 2K) indicates the sample regime used for gradient estimation in standard practical implementations of policy gradient methods.",How does the quality of gradient estimation change as the number of state-action pairs used in estimation increases?
spiqa_144,1805.06447v3,"Based on Figure 5 of the paper *Resisting Large Data Variations via Introspective Transformation Network*, how does the recognizability and quality of the samples generated by the ITN change as the threshold Tu is progressively increased?",The quality of the generated samples decreases as the update threshold increases.,1805.06447v3.pdf,"['1805.06447v3.pdf', '1809.03550v3.pdf', '1809.02731v3.pdf', '1704.05958v2.pdf']","Figure 5 shows samples generated by ITN with different update thresholds. The number below each sample represents the threshold. As the threshold increases, the samples become increasingly blurry and difficult to recognize.",1805.06447v3-Figure5-1.png,Resisting Large Data Variations via Introspective Transformation Network,"Training deep networks that generalize to a wide range of variations in test
data is essential to building accurate and robust image classifiers. One
standard strategy is to apply data augmentation to synthetically enlarge the
training set. However, data augmentation is essentially a brute-force method
which generates uniform samples from some pre-defined set of transformations.
In this paper, we propose a principled approach to train networks with
significantly improved resistance to large variations between training and
testing data. This is achieved by embedding a learnable transformation module
into the introspective network, which is a convolutional neural network (CNN)
classifier empowered with generative capabilities. Our approach alternates
between synthesizing pseudo-negative samples and transformed positive examples
based on the current model, and optimizing model predictions on these
synthesized samples. Experimental results verify that our approach
significantly improves the ability of deep networks to resist large variations
between training and testing data and achieves classification accuracy
improvements on several benchmark datasets, including MNIST, affNIST, SVHN,
CIFAR-10 and miniImageNet.",Figure 5. Samples generated by ITN with different thresholds Tu. The number below each sample represents the threshold.,How does the quality of the generated samples change as the update threshold increases?
spiqa_145,1803.06506v3,"How does Figure 6 in the paper *Learning Unsupervised Visual Grounding Through Semantic Self-Supervision* demonstrate the effect of alignment between the chosen common concept, predicted common concept, and real entity on the quality of the generated heatmap for accurate visual grounding?","When the selected concept, predicted concept, and the real entity to be grounded are all aligned, the generated heatmap produces a good localization of the phrase.",1803.06506v3.pdf,"['1803.06506v3.pdf', '1811.02553v4.pdf', '1811.06635v1.pdf', '1611.04684v1.pdf', '1704.04539v2.pdf', '1707.00524v2.pdf', '1608.02784v2.pdf']",This is shown in the first row of Figure~\ref{fig:fig2} with the concept \emph{`headlight'} and \emph{`picture'}. The heatmaps for both concepts are concentrated on the correct locations in the image.,1803.06506v3-Figure6-1.png,Learning Unsupervised Visual Grounding Through Semantic Self-Supervision,"Localizing natural language phrases in images is a challenging problem that
requires joint understanding of both the textual and visual modalities. In the
unsupervised setting, lack of supervisory signals exacerbate this difficulty.
In this paper, we propose a novel framework for unsupervised visual grounding
which uses concept learning as a proxy task to obtain self-supervision. The
simple intuition behind this idea is to encourage the model to localize to
regions which can explain some semantic property in the data, in our case, the
property being the presence of a concept in a set of images. We present
thorough quantitative and qualitative experiments to demonstrate the efficacy
of our approach and show a 5.6% improvement over the current state of the art
on Visual Genome dataset, a 5.8% improvement on the ReferItGame dataset and
comparable to state-of-art performance on the Flickr30k dataset.","Figure 6. The figure shows how the quality of output heatmap changes with the alignment of the selected concept, predicted concept and the real entity to be grounded. For some sampled concept batch, the gray box refers to the chosen common concept, the red box refers to the predicted common concept and the blue box refers to the predicted independent concept. See section 10 for details about each row.","How does the quality of the output heatmap change when the selected concept, predicted concept, and the real entity to be grounded are all aligned?"
spiqa_146,1811.10673v1,"Based on the data presented in Figure 6, how do the quality metrics such as PSNR, SSIM, and MS-SSIM reflect improvements in the reconstructed frames when the resolution is increased using the GAN-based video compression method leveraging soft edge detection?",The quality of the reconstructed frames increases monotonically as the resolution increases.,1811.10673v1.pdf,"['1811.10673v1.pdf', '1812.00281v3.pdf', '1809.01989v2.pdf', '1611.04363v2.pdf', '1812.00108v4.pdf', '1901.00398v2.pdf', '1804.05938v2.pdf', '1703.00060v2.pdf', '1710.06177v2.pdf', '1809.04276v2.pdf']","The table in the figure shows the PSNR, SSIM, and MS-SSIM scores for the reconstructed frames at different resolutions. These scores are all higher for higher resolutions, indicating that the quality of the reconstructed frames is better at higher resolutions.",1811.10673v1-Table1-1.png,Adversarial Video Compression Guided by Soft Edge Detection,"We propose a video compression framework using conditional Generative
Adversarial Networks (GANs). We rely on two encoders: one that deploys a
standard video codec and another which generates low-level maps via a pipeline
of down-sampling, a newly devised soft edge detector, and a novel lossless
compression scheme. For decoding, we use a standard video decoder as well as a
neural network based one, which is trained using a conditional GAN. Recent
""deep"" approaches to video compression require multiple videos to pre-train
generative networks to conduct interpolation. In contrast to this prior work,
our scheme trains a generative decoder on pairs of a very limited number of key
frames taken from a single video and corresponding low-level maps. The trained
decoder produces reconstructed frames relying on a guidance of low-level maps,
without any interpolation. Experiments on a diverse set of 131 videos
demonstrate that our proposed GAN-based compression engine achieves much higher
quality reconstructions at very low bitrates than prevailing standard codecs
such as H.264 or HEVC.","Video quality assessment of reconstructed frames in Figure 6. As the resolutions increased, the quality scores of the reconstructed frame increase monotonical.",How does the quality of the reconstructed frames change as the resolution increases?
spiqa_147,1710.05654v2,"Based on Table 1 in the paper, how does the learned graph outperform k-NN and A-NN in assigning more relevant weights to terms like ""insulin"" for ""glucose"" and ""training"" for ""academy""?",The learned graph assigns weights that correspond much better to the relevance of the terms compared to k-NN and A-NN graphs.,1710.05654v2.pdf,"['1710.05654v2.pdf', '1812.06589v2.pdf', '1906.10843v1.pdf', '1805.01216v3.pdf', '1804.07707v2.pdf', '1811.07073v3.pdf', '1611.07718v2.pdf', '1707.01922v5.pdf', '1804.07849v4.pdf']","The caption of Table 1 explicitly states that ""the weights assigned by graph learning correspond much better to the relevance of the terms."" Additionally, when comparing the terms associated with ""glucose"" and ""academy"" across the different methods, the learned graph generally assigns higher weights to terms that are intuitively more relevant to the respective words. For example, ""insulin"" has a much higher weight for ""glucose"" in the learned graph compared to the other methods, and ""training"" has a higher weight for ""academy"" in the learned graph compared to the other methods. This suggests that the learned graph is better at capturing semantic relationships between words.",1710.05654v2-Table1-1.png,Large Scale Graph Learning from Smooth Signals,"Graphs are a prevalent tool in data science, as they model the inherent
structure of the data. They have been used successfully in unsupervised and
semi-supervised learning. Typically they are constructed either by connecting
nearest samples, or by learning them from data, solving an optimization
problem. While graph learning does achieve a better quality, it also comes with
a higher computational cost. In particular, the current state-of-the-art model
cost is $\mathcal{O}(n^2)$ for $n$ samples. In this paper, we show how to scale
it, obtaining an approximation with leading cost of $\mathcal{O}(n\log(n))$,
with quality that approaches the exact graph learning model. Our algorithm uses
known approximate nearest neighbor techniques to reduce the number of
variables, and automatically selects the correct parameters of the model,
requiring a single intuitive input: the desired edge density.","Table 1: Weight comparison between k-NN, A-NN and learned graphs. The weights assigned by graph learning correspond much better to the relevance of the terms.",How does the relevance of terms assigned by the learned graph compare to the relevance assigned by k-NN and A-NN graphs?
spiqa_148,1811.02721v3,"""In Table 7 of the 'Performant TCP for Low-Power Wireless Networks' paper, CoAP shows a slightly higher reliability (99.5%) compared to TCPlp (99.3%). Can the difference in retransmission mechanisms and radio duty cycles between the two protocols under lossy network conditions explain this discrepancy, and what does this suggest for optimizing TCP-based transport layers in LLNs?""","Table 1 shows that CoAP has slightly higher reliability (99.5%) compared to TCPlp (99.3%). While both protocols perform well, this difference could be attributed to several factors, including:

Retransmission mechanisms: CoAP employs a built-in retransmission mechanism for lost packets, while TCPlp relies on the underlying network layer for retransmissions. This could give CoAP an edge in recovering lost packets and achieving higher reliability.
Congestion control: CoAP includes mechanisms to adapt to network congestion, potentially reducing packet loss and improving reliability.
Packet size: CoAP typically uses smaller packets compared to TCPlp. Smaller packets are less prone to loss in wireless networks, potentially contributing to CoAP's slightly higher reliability.",1811.02721v3.pdf,"['1811.02721v3.pdf', '1812.10735v2.pdf', '1809.03550v3.pdf', '1701.03077v10.pdf']","The table directly provides the reliability percentages for both CoAP and \sys{}, allowing for a direct comparison. Additionally, the information about radio duty cycle and CPU duty cycle can be used to infer possible reasons for the difference in reliability. While the table doesn't explicitly state the contributing factors, it provides clues that, when combined with knowledge about the protocols themselves, allow for a reasoned explanation.",1811.02721v3-Table7-1.png,Performant TCP for Low-Power Wireless Networks,"Low-power and lossy networks (LLNs) enable diverse applications integrating
many resource-constrained embedded devices, often requiring interconnectivity
with existing TCP/IP networks as part of the Internet of Things. But TCP has
received little attention in LLNs due to concerns about its overhead and
performance, leading to LLN-specific protocols that require specialized
gateways for interoperability. We present a systematic study of a well-designed
TCP stack in IEEE 802.15.4-based LLNs, based on the TCP protocol logic in
FreeBSD. Through careful implementation and extensive experiments, we show that
modern low-power sensor platforms are capable of running full-scale TCP and
that TCP, counter to common belief, performs well despite the lossy nature of
LLNs. By carefully studying the interaction between the transport and link
layers, we identify subtle but important modifications to both, achieving TCP
goodput within 25% of an upper bound (5-40x higher than prior results) and
low-power operation commensurate to CoAP in a practical LLN application
scenario. This suggests that a TCP-based transport layer, seamlessly
interoperable with existing TCP/IP networks, is viable and performant in LLNs.","Table 7: Performance in the testbed over a full day, averaged over multiple trials. The ideal protocol (§8.2.2) would have a radio DC of≈ 0.63%–0.70% under similarly lossy conditions.",How does the reliability of CoAP compare to TCPlp and what potential factors contribute to this difference?
spiqa_149,1809.00458v1,"In the context of Figure (a) from the GB-KMV paper, how does the running time of the GB-KM algorithm change as the F-1 score increases?",The running time of GB-KM increases as the F-1 score increases.,1809.00458v1.pdf,"['1809.00458v1.pdf', '1706.03847v3.pdf', '1707.00524v2.pdf', '1708.05239v3.pdf', '1612.02803v5.pdf', '1710.05654v2.pdf', '1906.10843v1.pdf', '1805.04609v3.pdf', '1809.03550v3.pdf', '1804.05938v2.pdf', '1804.04786v3.pdf', '1708.01425v4.pdf']","In Figure (a), the GB-KM curve shows an upward trend as the F-1 score increases. This indicates that as the accuracy of the algorithm increases, the running time also increases.",1809.00458v1-Figure19-1.png,GB-KMV: An Augmented KMV Sketch for Approximate Containment Similarity Search,"In this paper, we study the problem of approximate containment similarity
search. Given two records Q and X, the containment similarity between Q and X
with respect to Q is |Q intersect X|/ |Q|. Given a query record Q and a set of
records S, the containment similarity search finds a set of records from S
whose containment similarity regarding Q are not less than the given threshold.
This problem has many important applications in commercial and scientific
fields such as record matching and domain search. Existing solution relies on
the asymmetric LSH method by transforming the containment similarity to
well-studied Jaccard similarity. In this paper, we use a different framework by
transforming the containment similarity to set intersection. We propose a novel
augmented KMV sketch technique, namely GB-KMV, which is data-dependent and can
achieve a good trade-off between the sketch size and the accuracy. We provide a
set of theoretical analysis to underpin the proposed augmented KMV sketch
technique, and show that it outperforms the state-of-the-art technique LSH-E in
terms of estimation accuracy under practical assumption. Our comprehensive
experiments on real-life datasets verify that GB-KMV is superior to LSH-E in
terms of the space-accuracy trade-off, time-accuracy trade-off, and the sketch
construction time. For instance, with similar estimation accuracy (F-1 score),
GB-KMV is over 100 times faster than LSH-E on some real-life dataset.",Supplementary experiments,How does the running time of GB-KM vary with the F-1 score?
spiqa_150,1805.04687v2,"According to the figure showcasing drivable area prediction via segmentation in the BDD100K paper, how does the model predict drivable spaces when lane markings are absent?",The segmentation model learns to interpolate in areas that have no lane markings.,1805.04687v2.pdf,"['1805.04687v2.pdf', '1805.06447v3.pdf', '1811.08481v2.pdf', '1603.03833v4.pdf', '1804.04410v2.pdf', '1710.01507v4.pdf', '1709.02755v5.pdf', '1804.07931v2.pdf', '1803.04383v2.pdf', '1701.03077v10.pdf', '1811.08257v1.pdf']","This can be seen in the bottom row of the figure, where the model correctly predicts the drivable area even though there are no lane markings present.",1805.04687v2-Figure11-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.","Drivable area prediction by segmentation. The segmentation predicts the drivable area with lanes well, as shown in the top row. Also, we find that the segmentation model learns to interpolate in areas that has no lane markings.",How does the segmentation model perform in areas with no lane markings?
spiqa_151,1803.04383v2,"Based on Figure 2 of the paper ""Delayed Impact of Fair Machine Learning,"" how do different decision rules, such as MaxUtil and EqOpt, influence the relationship between selection rates and the maximization of expected outcomes (∆µ) and institution utilities (U)?","The selection rate has a different effect on the expected outcome and institution utilities for different decision rules. For example, the maximum expected outcome is achieved at a higher selection rate for the MaxUtil rule than for the EqOpt rule.",1803.04383v2.pdf,"['1803.04383v2.pdf', '1804.00863v3.pdf', '1901.00056v2.pdf', '1803.05776v2.pdf', '1704.00774v3.pdf', '1611.05742v3.pdf']","The figure shows that the expected outcome and institution utilities are both functions of the selection rate. The different decision rules correspond to different curves in the figure, and the maximum of each curve corresponds to the selection rate that maximizes the corresponding objective function.",1803.04383v2-Figure2-1.png,Delayed Impact of Fair Machine Learning,"Fairness in machine learning has predominantly been studied in static
classification settings without concern for how decisions change the underlying
population over time. Conventional wisdom suggests that fairness criteria
promote the long-term well-being of those groups they aim to protect.
  We study how static fairness criteria interact with temporal indicators of
well-being, such as long-term improvement, stagnation, and decline in a
variable of interest. We demonstrate that even in a one-step feedback model,
common fairness criteria in general do not promote improvement over time, and
may in fact cause harm in cases where an unconstrained objective would not.
  We completely characterize the delayed impact of three standard criteria,
contrasting the regimes in which these exhibit qualitatively different
behavior. In addition, we find that a natural form of measurement error
broadens the regime in which fairness criteria perform favorably.
  Our results highlight the importance of measurement and temporal modeling in
the evaluation of fairness criteria, suggesting a range of new challenges and
trade-offs.",Figure 2: Both outcomes ∆µ and institution utilities U can be plotted as a function of selection rate for one group. The maxima of the utility curves determine the selection rates resulting from various decision rules.,How does the selection rate affect the expected outcome and institution utilities for different decision rules?
spiqa_152,1701.03077v10,"In the left panel of the figure from *A General and Adaptive Robust Loss Function*, how does the shape of the IRLS weight function change as the parameter α increases?",The IRLS weight function becomes more peaked and concentrated around zero as the shape parameter α increases.,1701.03077v10.pdf,"['1701.03077v10.pdf', '1805.04687v2.pdf', '1705.09966v2.pdf', '1710.05654v2.pdf', '1805.06447v3.pdf', '1703.00899v2.pdf', '1708.03797v1.pdf', '1611.04363v2.pdf', '1706.04269v2.pdf', '1709.08294v3.pdf', '1906.06589v3.pdf', '1805.02349v2.pdf', '1809.01989v2.pdf', '1707.01917v2.pdf']","The left panel of the figure shows the IRLS weight function for different values of α. As α increases, the function becomes narrower and taller, with a sharper peak at zero. This indicates that the weight function gives more importance to data points that are close to zero and less importance to data points that are further away.",1701.03077v10-Figure7-1.png,A General and Adaptive Robust Loss Function,"We present a generalization of the Cauchy/Lorentzian, Geman-McClure,
Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2
loss functions. By introducing robustness as a continuous parameter, our loss
function allows algorithms built around robust loss minimization to be
generalized, which improves performance on basic vision tasks such as
registration and clustering. Interpreting our loss as the negative log of a
univariate density yields a general probability distribution that includes
normal and Cauchy distributions as special cases. This probabilistic
interpretation enables the training of neural networks in which the robustness
of the loss automatically adapts itself during training, which improves
performance on learning-based tasks such as generative image synthesis and
unsupervised monocular depth estimation, without requiring any manual parameter
tuning.",Our general loss’s IRLS weight function (left) and Ψfunction (right) for different values of the shape parameter α.,How does the shape of the IRLS weight function change as the shape parameter α increases?
spiqa_153,1701.03077v10,"Referring to the figure depicting the NLL and probability density functions in the context of your adaptive robust loss function, how do increasing values of α affect the peakedness and concentration of these functions?"," As the value of α increases, the NLL functions become more peaked and the probability density functions become more concentrated around the mean.",1701.03077v10.pdf,"['1701.03077v10.pdf', '1710.01507v4.pdf', '1812.00108v4.pdf', '1809.01246v1.pdf', '1802.07222v1.pdf', '1804.07849v4.pdf', '1703.04887v4.pdf', '1706.04284v3.pdf', '1705.10667v4.pdf', '1805.02349v2.pdf', '1606.07384v2.pdf']"," The left panel of the figure shows the NLL functions for different values of α. As α increases, the NLL functions become more peaked, indicating that the loss function is more sensitive to deviations from the mean. The right panel of the figure shows the corresponding probability density functions. As α increases, the probability density functions become more concentrated around the mean, indicating that the distribution is becoming more peaked.",1701.03077v10-Figure2-1.png,A General and Adaptive Robust Loss Function,"We present a generalization of the Cauchy/Lorentzian, Geman-McClure,
Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2
loss functions. By introducing robustness as a continuous parameter, our loss
function allows algorithms built around robust loss minimization to be
generalized, which improves performance on basic vision tasks such as
registration and clustering. Interpreting our loss as the negative log of a
univariate density yields a general probability distribution that includes
normal and Cauchy distributions as special cases. This probabilistic
interpretation enables the training of neural networks in which the robustness
of the loss automatically adapts itself during training, which improves
performance on learning-based tasks such as generative image synthesis and
unsupervised monocular depth estimation, without requiring any manual parameter
tuning.",The negative log-likelihoods (left) and probability densities (right) of the distribution corresponding to our loss function when it is defined (α ≥ 0). NLLs are simply losses (Fig. 1) shifted by a log partition function. Densities are bounded by a scaled Cauchy distribution.,How does the shape of the negative log-likelihood (NLL) and probability density functions change as the value of α increases?
spiqa_154,1809.01246v1,How does the figure illustrating the square hashing process in the proposed Graph Stream Sketch (GSS) explain how two hash functions are used to map a source/destination pair to a specific bucket in the two-dimensional array?,"Square hashing is a process that uses two hash functions to map a source/destination pair to a bucket in a two-dimensional array. The first hash function, h_i(s), maps the source address to a row in the array, and the second hash function, h_i(d), maps the destination address to a column in the array. The intersection of the row and column is the bucket where the fingerprint is stored.",1809.01246v1.pdf,"['1809.01246v1.pdf', '1809.03449v3.pdf', '1708.01425v4.pdf', '1710.06177v2.pdf', '1805.04609v3.pdf', '1804.05995v2.pdf', '1811.08481v2.pdf', '1705.07384v2.pdf', '1702.03584v3.pdf', '1802.07222v1.pdf']","The figure shows a two-dimensional array of buckets. Each bucket can be either empty, occupied, or mapped. An occupied bucket contains a fingerprint, which is a hash of the source and destination addresses. A mapped bucket is a bucket that has been assigned to a source/destination pair but does not yet contain a fingerprint. An empty bucket has not been assigned to a source/destination pair. The figure also shows an example of how a fingerprint is stored in a bucket. The fingerprint <f(s), f(d)> is stored in the bucket at the intersection of row 1 and column 3.",1809.01246v1-Figure5-1.png,Fast and Accurate Graph Stream Summarization,"A graph stream is a continuous sequence of data items, in which each item
indicates an edge, including its two endpoints and edge weight. It forms a
dynamic graph that changes with every item in the stream. Graph streams play
important roles in cyber security, social networks, cloud troubleshooting
systems and other fields. Due to the vast volume and high update speed of graph
streams, traditional data structures for graph storage such as the adjacency
matrix and the adjacency list are no longer sufficient. However, prior art of
graph stream summarization, like CM sketches, gSketches, TCM and gMatrix,
either supports limited kinds of queries or suffers from poor accuracy of query
results. In this paper, we propose a novel Graph Stream Sketch (GSS for short)
to summarize the graph streams, which has the linear space cost (O(|E|), E is
the edge set of the graph) and the constant update time complexity (O(1)) and
supports all kinds of queries over graph streams with the controllable errors.
Both theoretical analysis and experiment results confirm the superiority of our
solution with regard to the time/space complexity and query results' precision
compared with the state-of-the-art.",The square hashing,How does the square hashing process work?
spiqa_155,1705.08016v3,"What trend is observed in the test accuracy of different models on the CUB-200-2011 dataset as λ increases, based on the logarithmic scale in the left plot of the figure from the Pairwise Confusion paper?",The test accuracy of all models decreases as λ increases.,1705.08016v3.pdf,"['1705.08016v3.pdf', '1710.06177v2.pdf', '1705.09882v2.pdf', '1707.08608v3.pdf', '1706.00633v4.pdf', '1803.02750v3.pdf']","The left plot shows the test accuracy of the different models as a function of λ. The x-axis is logarithmic, so the values of λ increase from left to right. The y-axis shows the test accuracy, which decreases for all models as λ increases.",1705.08016v3-Figure2-1.png,Pairwise Confusion for Fine-Grained Visual Classification,"Fine-Grained Visual Classification (FGVC) datasets contain small sample
sizes, along with significant intra-class variation and inter-class similarity.
While prior work has addressed intra-class variation using localization and
segmentation techniques, inter-class similarity may also affect feature
learning and reduce classification performance. In this work, we address this
problem using a novel optimization procedure for the end-to-end neural network
training on FGVC tasks. Our procedure, called Pairwise Confusion (PC) reduces
overfitting by intentionally {introducing confusion} in the activations. With
PC regularization, we obtain state-of-the-art performance on six of the most
widely-used FGVC datasets and demonstrate improved localization ability. {PC}
is easy to implement, does not need excessive hyperparameter tuning during
training, and does not add significant overhead during test time.",(left) Variation of test accuracy on CUB-200-2011 with logarithmic variation in hyperparameter λ. (right) Convergence plot of GoogLeNet on CUB-200-2011.,How does the test accuracy of the different models vary with the hyperparameter λ?
spiqa_156,1611.07718v2,"Based on the results shown in Figure (a) of the *Deep Convolutional Neural Networks with Merge-and-Run Mappings* paper, how does the training loss of DMRNet compare to that of ResNet on the CIFAR-10 dataset for networks with 30 layers?",The training loss of DMRNet is lower than that of ResNet on the CIFAR-10 dataset with L = 30.,1611.07718v2.pdf,"['1611.07718v2.pdf', '1703.04887v4.pdf', '1804.07931v2.pdf', '1706.04269v2.pdf']",The plot in Figure (a) shows that the training loss of DMRNet (dashed red line) is consistently lower than that of ResNet (dashed blue line) throughout the training process.,1611.07718v2-Figure7-1.png,Deep Convolutional Neural Networks with Merge-and-Run Mappings,"A deep residual network, built by stacking a sequence of residual blocks, is
easy to train, because identity mappings skip residual branches and thus
improve information flow. To further reduce the training difficulty, we present
a simple network architecture, deep merge-and-run neural networks. The novelty
lies in a modularized building block, merge-and-run block, which assembles
residual branches in parallel through a merge-and-run mapping: Average the
inputs of these residual branches (Merge), and add the average to the output of
each residual branch as the input of the subsequent residual branch (Run),
respectively. We show that the merge-and-run mapping is a linear idempotent
function in which the transformation matrix is idempotent, and thus improves
information flow, making training easy. In comparison to residual networks, our
networks enjoy compelling advantages: they contain much shorter paths, and the
width, i.e., the number of channels, is increased. We evaluate the performance
on the standard recognition tasks. Our approach demonstrates consistent
improvements over ResNets with the comparable setup, and achieves competitive
results (e.g., $3.57\%$ testing error on CIFAR-$10$, $19.00\%$ on CIFAR-$100$,
$1.51\%$ on SVHN).","Comparing the optimization of ResNets and the DMRNets with the same number of layers/parameters. The vertical axis corresponds to training losses and testing errors, and the horizontal axis corresponds to #epochs.",How does the training loss of DMRNet compare to that of ResNet on the CIFAR-10 dataset with L = 30?
spiqa_157,1706.04269v2,"Referring to Figure (c) of the ""Action Search"" paper, how do the mAP and S score change as the training size increases, and what trend does this reveal about the model's performance in temporal action localization?","As the training size increases, the mAP and S score of the Action Search model also improve.",1706.04269v2.pdf,"['1706.04269v2.pdf', '1606.07384v2.pdf', '1707.00189v3.pdf', '1901.00056v2.pdf', '1803.05776v2.pdf', '1611.03780v2.pdf', '1704.00774v3.pdf', '1705.09882v2.pdf', '1709.08294v3.pdf', '1702.08694v3.pdf']","This can be seen in Figure (c), where the mAP and S score are shown for different training sizes.",1706.04269v2-Table1-1.png,Action Search: Spotting Actions in Videos and Its Application to Temporal Action Localization,"State-of-the-art temporal action detectors inefficiently search the entire
video for specific actions. Despite the encouraging progress these methods
achieve, it is crucial to design automated approaches that only explore parts
of the video which are the most relevant to the actions being searched for. To
address this need, we propose the new problem of action spotting in video,
which we define as finding a specific action in a video while observing a small
portion of that video. Inspired by the observation that humans are extremely
efficient and accurate in spotting and finding action instances in video, we
propose Action Search, a novel Recurrent Neural Network approach that mimics
the way humans spot actions. Moreover, to address the absence of data recording
the behavior of human annotators, we put forward the Human Searches dataset,
which compiles the search sequences employed by human annotators spotting
actions in the AVA and THUMOS14 datasets. We consider temporal action
localization as an application of the action spotting problem. Experiments on
the THUMOS14 dataset reveal that our model is not only able to explore the
video efficiently (observing on average 17.3% of the video) but it also
accurately finds human activities with 30.8% mAP.",Temporal localization results (mAP at tIoU) on the THUMOS14 testing set. We assign ‘–’ to unavailable mAP values. We report the average percentage of observed frames (S) for each approach. (a) Comparison against state-of-theart methods: Our method (Action Search + Priors + Res3D + S-CNN) achieves state-of-the-art results while observing only 17.3% of the video; (b) Video features effect: We compare C3D for Action Search visual encoder + the C3D-based classifier from [35] vs. ResNet for Action Search visual encoder + the Res3Dbased classifier from [41]; (c) The trade-off between Action Search training size and performance: mAP and S score improve as we increase the training size.,How does the training size of the Action Search model affect its performance?
spiqa_158,1706.03847v3,"Based on the figure in *Recurrent Neural Networks with Top-k Gains for Session-based Recommendations*, how does the training time of different ranking loss functions on the CLASS dataset scale as the number of additional samples increases?",The training time of all losses increases as the number of additional samples increases.,1706.03847v3.pdf,"['1706.03847v3.pdf', '1706.08146v3.pdf', '1706.04284v3.pdf', '1805.06447v3.pdf', '1809.02731v3.pdf', '1704.05426v4.pdf']","The figure shows the training time of different losses on the y-axis and the number of additional samples on the x-axis. As the number of additional samples increases, the training time for all losses also increases. This is because the model has to process more data as the number of samples increases, which takes more time.",1706.03847v3-Figure4-1.png,Recurrent Neural Networks with Top-k Gains for Session-based Recommendations,"RNNs have been shown to be excellent models for sequential data and in
particular for data that is generated by users in an session-based manner. The
use of RNNs provides impressive performance benefits over classical methods in
session-based recommendations. In this work we introduce novel ranking loss
functions tailored to RNNs in the recommendation setting. The improved
performance of these losses over alternatives, along with further tricks and
refinements described in this work, allow for an overall improvement of up to
35% in terms of MRR and Recall@20 over previous session-based RNN solutions and
up to 53% over classical collaborative filtering approaches. Unlike data
augmentation-based improvements, our method does not increase training times
significantly. We further demonstrate the performance gain of the RNN over
baselines in an online A/B test.",Training times with different sample sizes on the CLASS dataset.,How does the training time of the different losses change as the number of additional samples increases?
spiqa_159,1709.02755v5,"Based on Figure 3, how does the validation accuracy of the SRU model evolve over time compared to the CNN and cuDNN LSTM models during the first 100 epochs of training, and does it demonstrate any advantages in terms of accuracy trends using the NVIDIA GeForce GTX 1070 GPU and cuDNN 7003 setup?",The SRU model achieves comparable or slightly higher validation accuracy than the cuDNN LSTM and CNN models on all six benchmarks.,1709.02755v5.pdf,"['1709.02755v5.pdf', '1705.10667v4.pdf', '1702.03584v3.pdf', '1803.03467v4.pdf', '1809.03550v3.pdf', '1702.08694v3.pdf', '1710.06177v2.pdf', '1809.01989v2.pdf', '1706.00633v4.pdf', '1706.04284v3.pdf', '1611.07718v2.pdf']","Figure 2 shows the mean validation accuracies of the three models on the six benchmarks. The SRU model's curve is generally above or close to the curves of the other two models, indicating that it achieves comparable or slightly higher accuracy.",1709.02755v5-Figure3-1.png,Simple Recurrent Units for Highly Parallelizable Recurrence,"Common recurrent neural architectures scale poorly due to the intrinsic
difficulty in parallelizing their state computations. In this work, we propose
the Simple Recurrent Unit (SRU), a light recurrent unit that balances model
capacity and scalability. SRU is designed to provide expressive recurrence,
enable highly parallelized implementation, and comes with careful
initialization to facilitate training of deep models. We demonstrate the
effectiveness of SRU on multiple NLP tasks. SRU achieves 5--9x speed-up over
cuDNN-optimized LSTM on classification and question answering datasets, and
delivers stronger results than LSTM and convolutional models. We also obtain an
average of 0.7 BLEU improvement over the Transformer model on translation by
incorporating SRU into the architecture.","Figure 3: Mean validation accuracies (y-axis) and standard deviations of the CNN, 2-layer LSTM and 2-layer SRU models. We plot the curves of the first 100 epochs. X-axis is the training time used (in seconds). Timings are performed on NVIDIA GeForce GTX 1070 GPU, Intel Core i7-7700K Processor and cuDNN 7003.",How does the validation accuracy of the SRU model compare to that of the cuDNN LSTM and CNN models?
spiqa_160,1709.02755v5,"Based on Figure 6 in the SRU model, how does the ratio of the variance of the cell state $c_t$ to the variance of the input $x_t$ evolve across layers, and what does this imply about the hidden state's variance in deeper layers?","According to the passage, the variance of the hidden state is approximately equal to the variance of the input in deep layers.",1709.02755v5.pdf,"['1709.02755v5.pdf', '1803.02750v3.pdf', '1709.00139v4.pdf', '1811.02721v3.pdf', '1809.00263v5.pdf', '1805.04609v3.pdf', '1811.10673v1.pdf', '1708.06832v3.pdf', '1803.04383v2.pdf']",The figure shows that the ratio of the variance of the cell state to the variance of the input increases to 1 in deep layers. This implies that the variance of the hidden state is approximately equal to the variance of the input.,1709.02755v5-Figure6-1.png,Simple Recurrent Units for Highly Parallelizable Recurrence,"Common recurrent neural architectures scale poorly due to the intrinsic
difficulty in parallelizing their state computations. In this work, we propose
the Simple Recurrent Unit (SRU), a light recurrent unit that balances model
capacity and scalability. SRU is designed to provide expressive recurrence,
enable highly parallelized implementation, and comes with careful
initialization to facilitate training of deep models. We demonstrate the
effectiveness of SRU on multiple NLP tasks. SRU achieves 5--9x speed-up over
cuDNN-optimized LSTM on classification and question answering datasets, and
delivers stronger results than LSTM and convolutional models. We also obtain an
average of 0.7 BLEU improvement over the Transformer model on translation by
incorporating SRU into the architecture.","Figure 6: Empirical estimation of the variance ratio Var[ct]/Var[xt] at each layer in a randomly initialized SRU model. We use the pre-trained word2vec embeddings as input, resulting an initial ratio slightly higher than 1/3. As expected, the ratio increases to 1 in deep layers.",How does the variance of the hidden state $h_t$ compare to the variance of the input $x_t$ in deep layers of the SRU model?
spiqa_161,1811.02721v3,"Based on the figure depicting TCP goodput over a single IEEE 802.15.4 hop, how does increasing the buffer size initially improve TCP performance, and at what point does further buffer size expansion become detrimental due to latency?","Increasing the buffer size generally leads to increased TCP goodput, but only up to a certain point.",1811.02721v3.pdf,"['1811.02721v3.pdf', '1606.07384v2.pdf', '1707.00524v2.pdf', '1706.04269v2.pdf', '1603.03833v4.pdf', '1805.06447v3.pdf', '1703.04887v4.pdf', '1802.07351v2.pdf', '1706.04284v3.pdf', '1906.10843v1.pdf', '1804.01429v3.pdf', '1707.08608v3.pdf', '1710.05654v2.pdf', '1812.00108v4.pdf']","The figure shows that TCP goodput increases as the buffer size increases, but then plateaus. This is because a larger buffer allows for more data to be sent before the sender has to wait for an acknowledgement from the receiver. However, if the buffer is too large, it can lead to increased latency and decreased performance.",1811.02721v3-Figure3-1.png,Performant TCP for Low-Power Wireless Networks,"Low-power and lossy networks (LLNs) enable diverse applications integrating
many resource-constrained embedded devices, often requiring interconnectivity
with existing TCP/IP networks as part of the Internet of Things. But TCP has
received little attention in LLNs due to concerns about its overhead and
performance, leading to LLN-specific protocols that require specialized
gateways for interoperability. We present a systematic study of a well-designed
TCP stack in IEEE 802.15.4-based LLNs, based on the TCP protocol logic in
FreeBSD. Through careful implementation and extensive experiments, we show that
modern low-power sensor platforms are capable of running full-scale TCP and
that TCP, counter to common belief, performs well despite the lossy nature of
LLNs. By carefully studying the interaction between the transport and link
layers, we identify subtle but important modifications to both, achieving TCP
goodput within 25% of an upper bound (5-40x higher than prior results) and
low-power operation commensurate to CoAP in a practical LLN application
scenario. This suggests that a TCP-based transport layer, seamlessly
interoperable with existing TCP/IP networks, is viable and performant in LLNs.",TCP goodput over one IEEE 802.15.4 hop,How does varying the buffer size affect TCP goodput?
spiqa_162,1811.08257v1,"In the figure illustrating the convolution operations for multiple channels in plaintext, how does FALCON use FFT and IFFT to represent convolution in the frequency domain, and what operation is performed on the frequency-domain representations of the input and filter before applying IFFT?","In the frequency domain, convolution is represented by element-wise multiplication.",1811.08257v1.pdf,"['1811.08257v1.pdf', '1803.01128v3.pdf', '1812.06589v2.pdf', '1704.04539v2.pdf', '1705.02946v3.pdf', '1803.04383v2.pdf', '1805.04687v2.pdf']","The figure shows how convolution is performed in the frequency domain. First, the FFT (Fast Fourier Transform) is applied to the filter kernel f and the input signal x. Then, the two resulting frequency domain representations F(f) and F(x) are multiplied element-wise. Finally, the IFFT (Inverse Fast Fourier Transform) is applied to the result to obtain the convolution output in the spatial domain.",1811.08257v1-Figure3-1.png,FALCON: A Fourier Transform Based Approach for Fast and Secure Convolutional Neural Network Predictions,"Machine learning as a service has been widely deployed to utilize deep neural
network models to provide prediction services. However, this raises privacy
concerns since clients need to send sensitive information to servers. In this
paper, we focus on the scenario where clients want to classify private images
with a convolutional neural network model hosted in the server, while both
parties keep their data private. We present FALCON, a fast and secure approach
for CNN predictions based on Fourier Transform. Our solution enables linear
layers of a CNN model to be evaluated simply and efficiently with fully
homomorphic encryption. We also introduce the first efficient and
privacy-preserving protocol for softmax function, which is an indispensable
component in CNNs and has not yet been evaluated in previous works due to its
high complexity. We implemented the FALCON and evaluated the performance on
real-world CNN models. The experimental results show that FALCON outperforms
the best known works in both computation and communication cost.",The convolution operations for multiple channels in plaintext.,How is convolution represented in the frequency domain?
spiqa_163,1611.03780v2,"According to the figure in the paper ""Randomized Experimental Design via Geographic Clustering,"" how does the Hilbert space-filling curve recursively subdivide into smaller squares, and how is the curve drawn through each of these subdivisions?","The Hilbert space-filling curve is constructed recursively. The curve starts with a simple square, and then at each subsequent iteration, the curve is subdivided into four smaller squares. The curve is then drawn through each of these squares in a specific order.",1611.03780v2.pdf,"['1611.03780v2.pdf', '1804.07931v2.pdf', '1707.01922v5.pdf', '1809.01989v2.pdf', '1811.06635v1.pdf', '1705.07384v2.pdf', '1707.00189v3.pdf', '1611.05742v3.pdf', '1705.08016v3.pdf', '1811.07073v3.pdf', '1901.00056v2.pdf']","The figure shows the first four iterations of the Hilbert space-filling curve. Each iteration is shown in a different color. The curve starts with a simple square (shown in red), and then at each subsequent iteration, the curve is subdivided into four smaller squares. The curve is then drawn through each of these squares in a specific order.",1611.03780v2-Figure1-1.png,Randomized Experimental Design via Geographic Clustering,"Web-based services often run randomized experiments to improve their
products. A popular way to run these experiments is to use geographical regions
as units of experimentation, since this does not require tracking of individual
users or browser cookies. Since users may issue queries from multiple
geographical locations, geo-regions cannot be considered independent and
interference may be present in the experiment. In this paper, we study this
problem, and first present GeoCUTS, a novel algorithm that forms geographical
clusters to minimize interference while preserving balance in cluster size. We
use a random sample of anonymized traffic from Google Search to form a graph
representing user movements, then construct a geographically coherent
clustering of the graph. Our main technical contribution is a statistical
framework to measure the effectiveness of clusterings. Furthermore, we perform
empirical evaluations showing that the performance of GeoCUTS is comparable to
hand-crafted geo-regions with respect to both novel and existing metrics.",Hilbert space-filling curves are constructed recursively up to any desired resolution.,How is the Hilbert space-filling curve constructed?
spiqa_164,1811.02721v3,"How many hops are shown between Node 1 (Hamilton) and the Internet in the OpenThread topology at a transmission power of -8 dBm, as depicted in the figure?",5 hops,1811.02721v3.pdf,"['1811.02721v3.pdf', '1606.07384v2.pdf', '1809.02731v3.pdf', '1809.03550v3.pdf', '1809.03449v3.pdf', '1809.01246v1.pdf', '1804.07707v2.pdf', '1805.04609v3.pdf', '1809.00263v5.pdf', '1804.05995v2.pdf', '1805.00912v4.pdf']",The caption states that the figure shows a snapshot of uplink routes in an OpenThread topology at a transmission power of -8 dBm (5 hops). This means that there are 5 hops between the Hamilton and the Internet.,1811.02721v3-Figure1-1.png,Performant TCP for Low-Power Wireless Networks,"Low-power and lossy networks (LLNs) enable diverse applications integrating
many resource-constrained embedded devices, often requiring interconnectivity
with existing TCP/IP networks as part of the Internet of Things. But TCP has
received little attention in LLNs due to concerns about its overhead and
performance, leading to LLN-specific protocols that require specialized
gateways for interoperability. We present a systematic study of a well-designed
TCP stack in IEEE 802.15.4-based LLNs, based on the TCP protocol logic in
FreeBSD. Through careful implementation and extensive experiments, we show that
modern low-power sensor platforms are capable of running full-scale TCP and
that TCP, counter to common belief, performs well despite the lossy nature of
LLNs. By carefully studying the interaction between the transport and link
layers, we identify subtle but important modifications to both, achieving TCP
goodput within 25% of an upper bound (5-40x higher than prior results) and
low-power operation commensurate to CoAP in a practical LLN application
scenario. This suggests that a TCP-based transport layer, seamlessly
interoperable with existing TCP/IP networks, is viable and performant in LLNs.",Snapshot of uplink routes in OpenThread topology at transmission power of -8 dBm (5 hops). Node 1 is the border router with Internet connectivity.,How many hops are there between the Hamilton and the Internet?
spiqa_165,1701.06171v4,"How many iterations did the greedy EM-type learning process take to learn the part models for the watch image, as shown in the figure from the paper ""Greedy Structure Learning of Hierarchical Compositional Models""?",22 iterations,1701.06171v4.pdf,"['1701.06171v4.pdf', '1811.08257v1.pdf', '1707.08608v3.pdf', '1701.03077v10.pdf', '1804.07707v2.pdf', '1705.09296v2.pdf', '1805.07567v2.pdf', '1707.00189v3.pdf']","The figure shows the evolution of the part models over time, with each column representing one iteration of the learning process. The last column shows the final learned part models, which were learned after 22 iterations.",1701.06171v4-Figure4-1.png,Greedy Structure Learning of Hierarchical Compositional Models,"In this work, we consider the problem of learning a hierarchical generative
model of an object from a set of images which show examples of the object in
the presence of variable background clutter. Existing approaches to this
problem are limited by making strong a-priori assumptions about the object's
geometric structure and require segmented training data for learning. In this
paper, we propose a novel framework for learning hierarchical compositional
models (HCMs) which do not suffer from the mentioned limitations. We present a
generalized formulation of HCMs and describe a greedy structure learning
framework that consists of two phases: Bottom-up part learning and top-down
model composition. Our framework integrates the foreground-background
segmentation problem into the structure learning task via a background model.
As a result, we can jointly optimize for the number of layers in the hierarchy,
the number of parts per layer and a foreground-background segmentation based on
class labels only. We show that the learned HCMs are semantically meaningful
and achieve competitive results when compared to other generative object models
at object classification on a standard transfer learning dataset.","Illustration of the proposed greedy EM-type learning process. The part models are composed of 5 Gabor filters which are represented as colored ellipses. (a) The first t = 22 iterations of the greedy learning scheme. Each row shows the evolution of a part model over time. Each column shows the learning result at one iteration of the learning process. When a new part is initialized (t = 1, 6, 11, . . . ), also a generic background model is learned from the training image (marked by dashed rectangles). The background model and the learned part models are not adapted in the subsequent iterations (gray background) but serve as competitors for data in the E-step. For more details refer to Section 4.1. (b) An example encoding of a training image with the learned part models.",How many iterations did the greedy EM-type learning process take to learn the part models for the watch image?
spiqa_166,1710.05654v2,"How effective are the approximate $\theta$ bounds from equation (18) in predicting the edge sparsity of the ""spherical"" dataset with 262,000 nodes, specifically in terms of the requested versus obtained degree as shown in Figure 4?","The approximate bounds of $\theta$ are very effective at predicting sparsity in the ""spherical"" dataset.",1710.05654v2.pdf,"['1710.05654v2.pdf', '1701.06171v4.pdf', '1805.00912v4.pdf', '1704.07854v4.pdf', '1805.06447v3.pdf', '1811.07073v3.pdf', '1704.07121v2.pdf', '1704.08615v2.pdf', '1811.02721v3.pdf', '1708.06832v3.pdf', '1803.04383v2.pdf', '1809.02731v3.pdf', '1803.01128v3.pdf', '1706.03847v3.pdf']",The figure shows that the obtained degree closely follows the requested degree for both the large-scale log and A-NN methods. This indicates that the approximate bounds of $\theta$ are accurately predicting the sparsity of the resulting graph.,1710.05654v2-Figure4-1.png,Large Scale Graph Learning from Smooth Signals,"Graphs are a prevalent tool in data science, as they model the inherent
structure of the data. They have been used successfully in unsupervised and
semi-supervised learning. Typically they are constructed either by connecting
nearest samples, or by learning them from data, solving an optimization
problem. While graph learning does achieve a better quality, it also comes with
a higher computational cost. In particular, the current state-of-the-art model
cost is $\mathcal{O}(n^2)$ for $n$ samples. In this paper, we show how to scale
it, obtaining an approximation with leading cost of $\mathcal{O}(n\log(n))$,
with quality that approaches the exact graph learning model. Our algorithm uses
known approximate nearest neighbor techniques to reduce the number of
variables, and automatically selects the correct parameters of the model,
requiring a single intuitive input: the desired edge density.","Figure 4: Effectiveness of θ bounds eq. (18). Requested versus obtained degree, ""spherical"" data (262, 000 nodes).","How well do the approximate bounds of $\theta$ predict sparsity in the ""spherical"" dataset?"
spiqa_167,1803.02750v3,"According to the workload distribution in Table II of the research paper, how many CRDT updates will be triggered when a user with 100 followers performs a ""Post Tweet"" operation, and what percentage of the total system workload does this operation account for?",Posting a tweet will result in 1 + 100 = 101 CRDT updates. This represents 35% of the overall workload.,1803.02750v3.pdf,"['1803.02750v3.pdf', '1611.04363v2.pdf', '1811.02553v4.pdf', '1805.01216v3.pdf', '1705.10667v4.pdf', '1611.02654v2.pdf']","The table shows that a ""Post Tweet"" operation requires 1 + #Followers updates. In this case, the user has 100 followers, so the total updates are 1 + 100. The table also tells us that ""Post Tweet"" operations make up 35% of the total workload.",1803.02750v3-TableII-1.png,Efficient Synchronization of State-based CRDTs,"To ensure high availability in large scale distributed systems, Conflict-free
Replicated Data Types (CRDTs) relax consistency by allowing immediate query and
update operations at the local replica, with no need for remote
synchronization. State-based CRDTs synchronize replicas by periodically sending
their full state to other replicas, which can become extremely costly as the
CRDT state grows. Delta-based CRDTs address this problem by producing small
incremental states (deltas) to be used in synchronization instead of the full
state. However, current synchronisation algorithms for Delta-based CRDTs induce
redundant wasteful delta propagation, performing worse than expected, and
surprisingly, no better than State-based. In this paper we: 1) identify two
sources of inefficiency in current synchronization algorithms for delta-based
CRDTs; 2) bring the concept of join decomposition to state-based CRDTs; 3)
exploit join decompositions to obtain optimal deltas and 4) improve the
efficiency of synchronization algorithms; and finally, 5) evaluate the improved
algorithms.","TABLE II: Retwis workload characterization: for each operation, the number of CRDT updates performed and its workload percentage.","If a user with 100 followers posts a tweet, how many CRDT updates will be performed in total, and what percentage of the overall workload does this represent?"
spiqa_168,1803.02750v3,"According to Table III of the paper, when constructing a CRDT lattice using the lexicographic product, if the first component is a chain and the second component is a distributive lattice, does the resulting lattice guarantee both distributivity and the descending chain condition (DCC)?","Yes, the resulting CRDT lattice will be guaranteed to be both distributive and satisfy the DCC.",1803.02750v3.pdf,"['1803.02750v3.pdf', '1710.06177v2.pdf', '1906.06589v3.pdf', '1811.10673v1.pdf', '1702.08694v3.pdf', '1704.08615v2.pdf']","Table 1 shows that when the first component of the lexicographic product is a chain and the second component is a distributive lattice, the resulting lattice is both distributive and satisfies the DCC. This is indicated by the checkmarks in the corresponding cells of the table. The passage further emphasizes this point by highlighting that this specific use of the lexicographic product is typical in designing CRDTs and that the resulting construct inherits distributivity from the second component.",1803.02750v3-TableIII-1.png,Efficient Synchronization of State-based CRDTs,"To ensure high availability in large scale distributed systems, Conflict-free
Replicated Data Types (CRDTs) relax consistency by allowing immediate query and
update operations at the local replica, with no need for remote
synchronization. State-based CRDTs synchronize replicas by periodically sending
their full state to other replicas, which can become extremely costly as the
CRDT state grows. Delta-based CRDTs address this problem by producing small
incremental states (deltas) to be used in synchronization instead of the full
state. However, current synchronisation algorithms for Delta-based CRDTs induce
redundant wasteful delta propagation, performing worse than expected, and
surprisingly, no better than State-based. In this paper we: 1) identify two
sources of inefficiency in current synchronization algorithms for delta-based
CRDTs; 2) bring the concept of join decomposition to state-based CRDTs; 3)
exploit join decompositions to obtain optimal deltas and 4) improve the
efficiency of synchronization algorithms; and finally, 5) evaluate the improved
algorithms.","TABLE III: Composition techniques that yield lattices satisfying DCC and distributive lattices, given lattices A and B, chain C, partial order P and (unordered) set U .","If we use the lexicographic product with a chain as the first component and a distributive lattice as the second component to design a CRDT, will the resulting CRDT lattice be guaranteed to be distributive and satisfy the descending chain condition (DCC)?"
spiqa_169,1707.01922v5,"According to Table 1 of the ""Zero-Shot Deep Domain Adaptation"" paper, which layer in AlexNet is identified as the dividing point between the source CNN and the source classifier for domain adaptation with $D_F$ as the target domain?","In this scenario, the source CNN would consist of the AlexNet architecture up to and including the ""fc7"" layer. The remaining layers of AlexNet would then be used as the source classifier.",1707.01922v5.pdf,"['1707.01922v5.pdf', '1707.00189v3.pdf', '1803.03467v4.pdf', '1809.03550v3.pdf']","Table 1 provides information about the base network architectures (BNA) used in the experiments and specifies the layer that separates the source/target CNN from the source classifier. Looking at the row for AlexNet, we see that ""fc7"" is listed as the dividing layer. This means that for tasks using AlexNet, the source CNN comprises the network up to and including ""fc7"", while the remaining layers form the source classifier.",1707.01922v5-Table4-1.png,Zero-Shot Deep Domain Adaptation,"Domain adaptation is an important tool to transfer knowledge about a task
(e.g. classification) learned in a source domain to a second, or target domain.
Current approaches assume that task-relevant target-domain data is available
during training. We demonstrate how to perform domain adaptation when no such
task-relevant target-domain data is available. To tackle this issue, we propose
zero-shot deep domain adaptation (ZDDA), which uses privileged information from
task-irrelevant dual-domain pairs. ZDDA learns a source-domain representation
which is not only tailored for the task of interest but also close to the
target-domain representation. Therefore, the source-domain task of interest
solution (e.g. a classifier for classification tasks) which is jointly trained
with the source-domain representation can be applicable to both the source and
target representations. Using the MNIST, Fashion-MNIST, NIST, EMNIST, and SUN
RGB-D datasets, we show that ZDDA can perform domain adaptation in
classification tasks without access to task-relevant target-domain training
data. We also extend ZDDA to perform sensor fusion in the SUN RGB-D scene
classification task by simulating task-relevant target-domain representations
with task-relevant source-domain data. To the best of our knowledge, ZDDA is
the first domain adaptation and sensor fusion method which requires no
task-relevant target-domain data. The underlying principle is not particular to
computer vision data, but should be extensible to other domains.","Table 4. The base network architecture (BNA) we use in our experiments. For each BNA, We specify the layer separating the source/target CNN and the source classifier in Fig. 2. The layer name in the right column is based on the official Caffe [24] and SqueezeNet v1.1 [23] implementation of each BNA","If you are performing domain adaptation with ZDDA using AlexNet as the base network architecture and $D_F$ as the target domain, which layers of the network would be considered part of the source CNN and which would be part of the source classifier?"
spiqa_170,1706.00827v2,"Based on Table 6 of the paper, in which specific test case did Multi-X exhibit a higher misclassification error than PEARL during simultaneous plane and cylinder fitting on LIDAR data?","Multi-X performed worse than PEARL in test case (6), with a misclassification error of 21.72% compared to PEARL's 17.35%. ",1706.00827v2.pdf,"['1706.00827v2.pdf', '1804.04410v2.pdf', '1710.01507v4.pdf', '1804.07707v2.pdf', '1805.02349v2.pdf', '1812.00281v3.pdf', '1804.05938v2.pdf', '1706.00633v4.pdf', '1803.04572v2.pdf']","Table 2 presents the misclassification error percentages for different methods, including Multi-X and PEARL, across seven test cases. By comparing the values in the ""Multi-X"" and ""PEARL"" columns, we can identify instances where Multi-X underperformed. In test case (6), Multi-X has a higher error rate than PEARL, indicating its relatively poorer performance in that specific scenario.",1706.00827v2-Table6-1.png,Multi-Class Model Fitting by Energy Minimization and Mode-Seeking,"We propose a general formulation, called Multi-X, for multi-class
multi-instance model fitting - the problem of interpreting the input data as a
mixture of noisy observations originating from multiple instances of multiple
classes. We extend the commonly used alpha-expansion-based technique with a new
move in the label space. The move replaces a set of labels with the
corresponding density mode in the model parameter domain, thus achieving fast
and robust optimization. Key optimization parameters like the bandwidth of the
mode seeking are set automatically within the algorithm. Considering that a
group of outliers may form spatially coherent structures in the data, we
propose a cross-validation-based technique removing statistically insignificant
instances. Multi-X outperforms significantly the state-of-the-art on publicly
available datasets for diverse problems: multiple plane and rigid motion
detection; motion segmentation; simultaneous plane and cylinder fitting; circle
and line fitting.",Table 6: Misclassification error (%) of simultaneous plane and cylinder fitting to LIDAR data. See Fig. 6 for examples.,In which scenario did Multi-X perform worse than another method in terms of misclassification error for simultaneous plane and cylinder fitting?
spiqa_171,1805.06431v4,"When examining Table 13 of the ""Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks"" paper, does applying Mixup to ConvNet+CN lead to better test accuracy compared to using ConvNet+CN without Mixup, specifically when the corruption probability is 80%?",ConvNet+CN with Mixup achieves a higher accuracy (75.4%) than ConvNet+CN without Mixup (65.2%) when the corruption probability is 80%.,1805.06431v4.pdf,"['1805.06431v4.pdf', '1901.00398v2.pdf', '1805.06447v3.pdf', '1612.02803v5.pdf', '1705.09966v2.pdf']","The table directly compares the test accuracies of different methods under varying corruption probabilities. By looking at the row corresponding to 80% corruption, we can see the performance of each method under that specific condition. The table clearly shows that ConvNet+CN combined with Mixup yields a higher accuracy than ConvNet+CN alone at that corruption level.",1805.06431v4-Table13-1.png,Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks,"In this paper, we focus on weakly supervised learning with noisy training
data for both classification and regression problems.We assume that the
training outputs are collected from a mixture of a target and correlated noise
distributions.Our proposed method simultaneously estimates the target
distribution and the quality of each data which is defined as the correlation
between the target and data generating distributions.The cornerstone of the
proposed method is a Cholesky Block that enables modeling dependencies among
mixture distributions in a differentiable manner where we maintain the
distribution over the network weights.We first provide illustrative examples in
both regression and classification tasks to show the effectiveness of the
proposed method.Then, the proposed method is extensively evaluated in a number
of experiments where we show that it constantly shows comparable or superior
performances compared to existing baseline methods in the handling of noisy
data.",Table 13: Test accuracies on the CIFAR-10 datasets with symmetric noises.,Is it more beneficial to use ConvNet+CN with or without Mixup when the corruption probability is 80%? Explain your reasoning.
spiqa_172,1701.03077v10,"Based on the clustering results in Table 4, which dataset demonstrated the highest relative improvement in AMI when using the gRCC* algorithm compared to the RCC algorithm, and approximately what was the percentage of this improvement?","The gRCC* algorithm achieved the largest relative improvement over the RCC algorithm on the YTF dataset, with a relative improvement of approximately 31.9%.",1701.03077v10.pdf,"['1701.03077v10.pdf', '1707.01917v2.pdf', '1804.07931v2.pdf', '1811.08481v2.pdf', '1804.05995v2.pdf', '1805.02349v2.pdf', '1805.04687v2.pdf', '1612.02803v5.pdf', '1804.04410v2.pdf', '1809.03550v3.pdf']","The table shows the AMI values for both gRCC* and RCC algorithms on various datasets. The ""Rel. Impr."" column shows the relative improvement of gRCC* compared to RCC. By looking at this column, we can identify that the YTF dataset has the highest value (31.9%), indicating the most significant improvement achieved by gRCC*.",1701.03077v10-Table4-1.png,A General and Adaptive Robust Loss Function,"We present a generalization of the Cauchy/Lorentzian, Geman-McClure,
Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2
loss functions. By introducing robustness as a continuous parameter, our loss
function allows algorithms built around robust loss minimization to be
generalized, which improves performance on basic vision tasks such as
registration and clustering. Interpreting our loss as the negative log of a
univariate density yields a general probability distribution that includes
normal and Cauchy distributions as special cases. This probabilistic
interpretation enables the training of neural networks in which the robustness
of the loss automatically adapts itself during training, which improves
performance on learning-based tasks such as generative image synthesis and
unsupervised monocular depth estimation, without requiring any manual parameter
tuning.",Table 4. Results on the clustering task of [32] where we compare their “RCC” algorithm to our “gRCC*” generalization in terms of AMI on several datasets. We also report the AMI increase of “gRCC*” with respect to “RCC”. Baselines are taken from [32].,"On which dataset did the gRCC* algorithm achieve the largest relative improvement over the RCC algorithm, and by approximately how much did it improve?"
spiqa_173,1803.04572v2,"What medications are listed under the Sickle Cell Anemia phenotype in Table 7 of the COPA research paper, where the phenotypes were discovered using the constrained PARAFAC2 method?","According to the table, some common medications used to treat Sickle Cell Anemia include:

Beta-adrenergic agents
Analgesics (narcotics and non-narcotics)
NSAIDs (cyclooxygenase inhibitor - type)
Potassium replacement
Sodium/saline preparations
General inhalation agents
Laxatives and cathartics
IV solutions (dextrose-saline)
Antiemetic/antivertigo agents
Sedative-hypnotics (non-barbiturate)
Glucocorticoids (orally inhaled)
Folic acid preparations
Analgesic narcotic anesthetic adjunct agents",1803.04572v2.pdf,"['1803.04572v2.pdf', '1611.03780v2.pdf', '1705.09882v2.pdf', '1708.03797v1.pdf', '1708.01425v4.pdf', '1612.02803v5.pdf', '1710.01507v4.pdf', '1805.04609v3.pdf', '1705.09966v2.pdf', '1706.00633v4.pdf', '1809.03449v3.pdf', '1805.02349v2.pdf', '1901.00056v2.pdf']","The table presents various phenotypes discovered by the mentioned method, including Sickle Cell Anemia. Under each phenotype, the associated diagnoses and medications are listed. Therefore, by looking at the medications listed under the ""Sickle Cell Anemia"" phenotype, we can identify the common medications used for its treatment.",1803.04572v2-Table7-1.png,COPA: Constrained PARAFAC2 for Sparse & Large Datasets,"PARAFAC2 has demonstrated success in modeling irregular tensors, where the
tensor dimensions vary across one of the modes. An example scenario is modeling
treatments across a set of patients with the varying number of medical
encounters over time. Despite recent improvements on unconstrained PARAFAC2,
its model factors are usually dense and sensitive to noise which limits their
interpretability. As a result, the following open challenges remain: a) various
modeling constraints, such as temporal smoothness, sparsity and non-negativity,
are needed to be imposed for interpretable temporal modeling and b) a scalable
approach is required to support those constraints efficiently for large
datasets. To tackle these challenges, we propose a {\it CO}nstrained {\it
PA}RAFAC2 (COPA) method, which carefully incorporates optimization constraints
such as temporal smoothness, sparsity, and non-negativity in the resulting
factors. To efficiently support all those constraints, COPA adopts a hybrid
optimization framework using alternating optimization and alternating direction
method of multiplier (AO-ADMM). As evaluated on large electronic health record
(EHR) datasets with hundreds of thousands of patients, COPA achieves
significant speedups (up to 36 times faster) over prior PARAFAC2 approaches
that only attempt to handle a subset of the constraints that COPA enables.
Overall, our method outperforms all the baselines attempting to handle a subset
of the constraints in terms of speed, while achieving the same level of
accuracy. Through a case study on temporal phenotyping of medically complex
children, we demonstrate how the constraints imposed by COPA reveal concise
phenotypes and meaningful temporal profiles of patients. The clinical
interpretation of both the phenotypes and the temporal profiles was confirmed
by a medical expert.",Table 7: Phenotypes discovered by COPA . The red color corresponds to diagnosis and blue color corresponds to medication. The meaningfulness of phenotypes endorsed by a medical expert. No additional post-processing was performed on these results.,What are some common medications used to treat Sickle Cell Anemia?
spiqa_174,1802.07459v2,"Referring to the figure outlining the process of building the Concept Interaction Graph (CIG) in this paper, can you provide the stages from the initial KeyGraph construction to the final Aggregation of matching results, including concept detection, vertex similarity weighting, and various matching techniques?","The different stages involved in constructing the Concept Interaction Graph (CIG) from a pair of documents are: (a) Representation, (b) Encoding, (c) Transformation, and (d) Aggregation.",1802.07459v2.pdf,"['1802.07459v2.pdf', '1606.07384v2.pdf', '1704.07854v4.pdf', '1701.06171v4.pdf', '1704.04539v2.pdf', '1706.00827v2.pdf', '1704.00774v3.pdf', '1704.05426v4.pdf']","The figure shows the different stages involved in constructing the CIG. The first stage, Representation, involves constructing a KeyGraph from the document pair by word co-occurrence and detecting concepts by community detection. The second stage, Encoding, involves getting edge weights by vertex similarities and assigning sentences by similarities. The third stage, Transformation, involves using a Siamese encoder to generate vertex features and a term-based feature extractor to generate vertex features. The fourth stage, Aggregation, involves performing Siamese-based matching, term-based matching, and global matching to generate the final CIG.",1802.07459v2-Figure2-1.png,Matching Article Pairs with Graphical Decomposition and Convolutions,"Identifying the relationship between two articles, e.g., whether two articles
published from different sources describe the same breaking news, is critical
to many document understanding tasks. Existing approaches for modeling and
matching sentence pairs do not perform well in matching longer documents, which
embody more complex interactions between the enclosed entities than a sentence
does. To model article pairs, we propose the Concept Interaction Graph to
represent an article as a graph of concepts. We then match a pair of articles
by comparing the sentences that enclose the same concept vertex through a
series of encoding techniques, and aggregate the matching signals through a
graph convolutional network. To facilitate the evaluation of long article
matching, we have created two datasets, each consisting of about 30K pairs of
breaking news articles covering diverse topics in the open domain. Extensive
evaluations of the proposed methods on the two datasets demonstrate significant
improvements over a wide range of state-of-the-art methods for natural language
matching.",An overview of our approach for constructing the Concept Interaction Graph (CIG) from a pair of documents and classifying it by Graph Convolutional Networks.,What are the different stages involved in constructing the Concept Interaction Graph (CIG) from a pair of documents?
spiqa_175,1812.00281v3,"What are the specific stages of HUMBI body and cloth reconstruction, as shown in the figure, starting from the input image (Ibody) to the final cloth model fitting (Mcloth)?","The different stages of HUMBI body and cloth reconstruction are: 
1. Input image of the person (Ibody)
2. Keypoint estimation (Kbody)
3. Occupancy map generation (Obody)
4. Body model fitting (Mbody)
5. Cloth model fitting (Mcloth)",1812.00281v3.pdf,"['1812.00281v3.pdf', '1811.06635v1.pdf', '1803.03467v4.pdf', '1803.06506v3.pdf', '1802.07459v2.pdf', '1805.02349v2.pdf', '1811.10673v1.pdf', '1706.00827v2.pdf', '1701.03077v10.pdf', '1708.00160v2.pdf', '1705.09296v2.pdf']","The figure shows the different stages of HUMBI body and cloth reconstruction. The input image is first used to estimate the keypoints of the body. These keypoints are then used to generate an occupancy map, which is a 3D representation of the body. The body model is then fitted to the occupancy map, and finally, the cloth model is fitted to the body model.",1812.00281v3-Figure11-1.png,HUMBI: A Large Multiview Dataset of Human Body Expressions,"This paper presents a new large multiview dataset called HUMBI for human body
expressions with natural clothing. The goal of HUMBI is to facilitate modeling
view-specific appearance and geometry of gaze, face, hand, body, and garment
from assorted people. 107 synchronized HD cameras are used to capture 772
distinctive subjects across gender, ethnicity, age, and physical condition.
With the multiview image streams, we reconstruct high fidelity body expressions
using 3D mesh models, which allows representing view-specific appearance using
their canonical atlas. We demonstrate that HUMBI is highly effective in
learning and reconstructing a complete human model and is complementary to the
existing datasets of human body expressions with limited views and subjects
such as MPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets.",HUMBI body and cloth reconstruction results.,What are the different stages of HUMBI body and cloth reconstruction?
spiqa_176,1708.01425v4,"Based on the methodology shown in the figure, what are the distinct phases involved in reconstructing implicit warrants for argument reasoning comprehension from news comments, including sampling, annotating, and validating stages?","The different steps involved in reconstructing implicit warrants for argument reasoning comprehension are:
1. Sampling comments
2. Stance annotation
3. Reason span annotations
4. Reason gist summarization
5. Reason disambiguation
6. Alternative warrant
7. Alternative warrant validation
8. Warrant for original claim
9. Warrant validation",1708.01425v4.pdf,"['1708.01425v4.pdf', '1703.10730v2.pdf', '1703.00060v2.pdf', '1802.07459v2.pdf', '1709.08294v3.pdf', '1705.08016v3.pdf', '1703.00899v2.pdf']","The figure shows a schematic of the methodology for reconstructing implicit warrants for argument reasoning comprehension. The figure shows the different steps involved in the process, from sampling comments to validating the warrant for the original claim.",1708.01425v4-Figure2-1.png,The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants,"Reasoning is a crucial part of natural language argumentation. To comprehend
an argument, one must analyze its warrant, which explains why its claim follows
from its premises. As arguments are highly contextualized, warrants are usually
presupposed and left implicit. Thus, the comprehension does not only require
language understanding and logic skills, but also depends on common sense. In
this paper we develop a methodology for reconstructing warrants systematically.
We operationalize it in a scalable crowdsourcing process, resulting in a freely
licensed dataset with warrants for 2k authentic arguments from news comments.
On this basis, we present a new challenging task, the argument reasoning
comprehension task. Given an argument with a claim and a premise, the goal is
to choose the correct implicit warrant from two options. Both warrants are
plausible and lexically close, but lead to contradicting claims. A solution to
this task will define a substantial step towards automatic warrant
reconstruction. However, experiments with several neural attention and language
models reveal that current approaches do not suffice.",Overview of the methodology of reconstructing implicit warrants for argument reasoning comprehension.,What are the different steps involved in reconstructing implicit warrants for argument reasoning comprehension?
spiqa_177,1805.04687v2,"Based on the figure depicting the BDD100K dataset, what are the detailed types of annotations visualized, including object bounding boxes around cars, lane markings, colored areas for drivable regions, and other provided annotations such as scene tagging and segmentation data?","The dataset includes a rich set of annotations: scene tagging, object bounding box, lane marking, drivable area, full-frame semantic and instance segmentation, multiple object tracking, and multiple object tracking with segmentation.",1805.04687v2.pdf,"['1805.04687v2.pdf', '1702.08694v3.pdf', '1704.05426v4.pdf', '1805.00912v4.pdf', '1705.09296v2.pdf', '1811.10673v1.pdf', '1803.03467v4.pdf', '1811.08257v1.pdf']","The figure shows different types of annotations that are included in the dataset. For example, the bounding boxes around the cars indicate the object bounding box annotations, the different colored lines on the road indicate the lane marking annotations, and the different colored areas indicate the drivable area annotations.",1805.04687v2-Figure1-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.","Overview of our dataset. Our dataset includes a diverse set of driving videos under various weather conditions, time, and scene types. The dataset also comes with a rich set of annotations: scene tagging, object bounding box, lane marking, drivable area, full-frame semantic and instance segmentation, multiple object tracking, and multiple object tracking with segmentation.",What are the different types of annotations that are included in the dataset?
spiqa_178,1804.01429v3,"Based on the process shown in Figure 4, what are the three types of features extracted by the Layout-induced Video Representation Network that focus on place-specific activities, agent movement directions, and place connectivity?","The Layout-induced Video Representation Network uses three types of features: place-based features, distance-based features, and topological features.",1804.01429v3.pdf,"['1804.01429v3.pdf', '1710.05654v2.pdf', '1611.07718v2.pdf', '1705.02946v3.pdf', '1901.00056v2.pdf', '1804.05936v2.pdf', '1803.02750v3.pdf', '1705.02798v6.pdf', '1704.07121v2.pdf', '1612.02803v5.pdf', '1809.00263v5.pdf', '1802.07351v2.pdf', '1805.04609v3.pdf', '1803.01128v3.pdf']","The figure shows the different types of features used by the network. Place-based features are extracted by place-based models (NN-sidewalk, NN-street, NN-porch) and describe the activities occurring in different places. Distance-based features are extracted by distance-based place discretization and model the moving directions of agents. Topological features are extracted by topological feature aggregation and capture the connectivity of places.",1804.01429v3-Figure4-1.png,Layout-induced Video Representation for Recognizing Agent-in-Place Actions,"We address the recognition of agent-in-place actions, which are associated
with agents who perform them and places where they occur, in the context of
outdoor home surveillance. We introduce a representation of the geometry and
topology of scene layouts so that a network can generalize from the layouts
observed in the training set to unseen layouts in the test set. This
Layout-Induced Video Representation (LIVR) abstracts away low-level appearance
variance and encodes geometric and topological relationships of places in a
specific scene layout. LIVR partitions the semantic features of a video clip
into different places to force the network to learn place-based feature
descriptions; to predict the confidence of each action, LIVR aggregates
features from the place associated with an action and its adjacent places on
the scene layout. We introduce the Agent-in-Place Action dataset to show that
our method allows neural network models to generalize significantly better to
unseen scenes.","Figure 4. Layout-induced Video Representation Network: The dashed blue box indicates a shared 3D ConvNet to extract lowlevel features. We utilize the segmentation maps to decompose features into different places, and the solid blue boxes indicate that we train place-based models to extract place-based feature descriptions. When relevant to the activities of interest, we conduct distance-based place discretization to model moving directions; finally, we leverage the connectivity of places to aggregate the place-based feature descriptions at inference level.",What are the different types of features used by the Layout-induced Video Representation Network?
spiqa_179,1703.07015v3,"In the LSTNet model architecture shown in Figure 2 of the paper ""Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks,"" how do the convolutional, recurrent, and autoregressive layers collaborate to capture both local short-term and global long-term time series dependencies, and how do their outputs contribute to the final prediction layer?","The LSTNet model has four main types of layers:

1. Convolutional layer: This layer extracts local dependency patterns from the input data. 
2. Recurrent and recurrent-skip layer: These layers capture long-term dependencies in the data. 
3. Fully connected and element-wise sum output layer: This layer combines the outputs from the convolutional and recurrent layers to produce the final prediction.
4. Autoregressive layer: This layer provides a linear bypass to the non-linear neural network part of the model. 

The convolutional layer receives the input data and passes its output to the recurrent and recurrent-skip layers. These layers then pass their output to the fully connected and element-wise sum output layer. The autoregressive layer receives the input data directly and its output is also fed into the fully connected and element-wise sum output layer.",1703.07015v3.pdf,"['1703.07015v3.pdf', '1804.04786v3.pdf', '1804.05995v2.pdf', '1811.08481v2.pdf', '1809.03149v2.pdf', '1707.06320v2.pdf', '1812.00108v4.pdf', '1811.07073v3.pdf']",The figure shows the different layers of the LSTNet model and how they are connected. The arrows indicate the direction of data flow.,1703.07015v3-Figure2-1.png,Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks,"Multivariate time series forecasting is an important machine learning problem
across many domains, including predictions of solar plant energy output,
electricity consumption, and traffic jam situation. Temporal data arise in
these real-world applications often involves a mixture of long-term and
short-term patterns, for which traditional approaches such as Autoregressive
models and Gaussian Process may fail. In this paper, we proposed a novel deep
learning framework, namely Long- and Short-term Time-series network (LSTNet),
to address this open challenge. LSTNet uses the Convolution Neural Network
(CNN) and the Recurrent Neural Network (RNN) to extract short-term local
dependency patterns among variables and to discover long-term patterns for time
series trends. Furthermore, we leverage traditional autoregressive model to
tackle the scale insensitive problem of the neural network model. In our
evaluation on real-world data with complex mixtures of repetitive patterns,
LSTNet achieved significant performance improvements over that of several
state-of-the-art baseline methods. All the data and experiment codes are
available online.",Figure 2: An overview of the Long- and Short-term Time-series network (LSTNet),What are the different types of layers in the LSTNet model and how are they connected?
spiqa_180,1901.00056v2,"According to Figure 2, what are the four sequential steps that SYNONYMNET follows during the inference phase to transform a query entity into its discovered synonyms through context matching and synonym score calculations?"," The four steps involved in the synonym discovery process are: 

1. **Entity representation learning:** Learn entity representations from the corpus using WEMBED.
2. **NN search:** Perform a nearest neighbor search to find candidate entities for the query entity.
3. **Synonym score calculation:** Calculate the synonym score between the query entity and each candidate entity using SYNONYM NET.
4. **Synonym entity discovery:** Select the candidate entities with the highest synonym scores as the discovered synonym entities.",1901.00056v2.pdf,"['1901.00056v2.pdf', '1705.02946v3.pdf', '1804.05995v2.pdf', '1809.04276v2.pdf', '1606.07384v2.pdf', '1708.00160v2.pdf', '1703.04887v4.pdf', '1804.07707v2.pdf', '1704.07854v4.pdf', '1706.03847v3.pdf', '1805.01216v3.pdf', '1706.04269v2.pdf', '1701.03077v10.pdf']"," The figure shows the four steps involved in the synonym discovery process using {\modelname}. The figure shows how the query entity is first embedded into a vector space, and then how this vector is used to find candidate entities. The figure also shows how the SYNONYM NET is used to calculate the synonym score between the query entity and each candidate entity. Finally, the figure shows how the candidate entities with the highest synonym scores are selected as the discovered synonym entities.",1901.00056v2-Figure2-1.png,Entity Synonym Discovery via Multipiece Bilateral Context Matching,"Being able to automatically discover synonymous entities in an open-world
setting benefits various tasks such as entity disambiguation or knowledge graph
canonicalization. Existing works either only utilize entity features, or rely
on structured annotations from a single piece of context where the entity is
mentioned. To leverage diverse contexts where entities are mentioned, in this
paper, we generalize the distributional hypothesis to a multi-context setting
and propose a synonym discovery framework that detects entity synonyms from
free-text corpora with considerations on effectiveness and robustness. As one
of the key components in synonym discovery, we introduce a neural network model
SYNONYMNET to determine whether or not two given entities are synonym with each
other. Instead of using entities features, SYNONYMNET makes use of multiple
pieces of contexts in which the entity is mentioned, and compares the
context-level similarity via a bilateral matching schema. Experimental results
demonstrate that the proposed model is able to detect synonym sets that are not
observed during training on both generic and domain-specific datasets:
Wiki+Freebase, PubMed+UMLS, and MedBook+MKG, with up to 4.16% improvement in
terms of Area Under the Curve and 3.19% in terms of Mean Average Precision
compared to the best baseline method.",Figure 2: Synonym discovery during the inference phase with SYNONYMNET.,What are the four steps involved in the synonym discovery process using {\modelname}?
spiqa_181,1703.10730v2,"According to the network architecture depicted in Figure 2, what inputs are fed into the image generation network in your unsupervised image generation framework?",The inputs to the image generation network are the observed images (x) and a random noise vector (z).,1703.10730v2.pdf,"['1703.10730v2.pdf', '1805.00912v4.pdf', '1603.03833v4.pdf', '1811.06635v1.pdf', '1705.07164v8.pdf', '1709.02418v2.pdf']",The figure shows that the image generation network takes two inputs: one from the part encoding network (which represents the observed images) and one from a random noise vector.,1703.10730v2-Figure2-1.png,Unsupervised Holistic Image Generation from Key Local Patches,"We introduce a new problem of generating an image based on a small number of
key local patches without any geometric prior. In this work, key local patches
are defined as informative regions of the target object or scene. This is a
challenging problem since it requires generating realistic images and
predicting locations of parts at the same time. We construct adversarial
networks to tackle this problem. A generator network generates a fake image as
well as a mask based on the encoder-decoder framework. On the other hand, a
discriminator network aims to detect fake images. The network is trained with
three losses to consider spatial, appearance, and adversarial information. The
spatial loss determines whether the locations of predicted parts are correct.
Input patches are restored in the output image without much modification due to
the appearance loss. The adversarial loss ensures output images are realistic.
The proposed network is trained without supervisory signals since no labels of
key parts are required. Experimental results on six datasets demonstrate that
the proposed algorithm performs favorably on challenging objects and scenes.","Figure 2: Proposed network architecture. A bar represents a layer in the network. Layers of the same size and the same color have the same convolutional feature maps. Dashed lines in the part encoding network represent shared weights. In addition, E denotes an embedded vector and z is a random noise vector.",What are the inputs to the image generation network?
spiqa_182,1803.01128v3,"Based on Table 1, how do Seq2Sick's innovations in search strategy (group lasso and gradient regularization) and targeted keyword attack compare to the other RNN-based attack methods listed?","Seq2Sick differs from existing attack methods in two key aspects:

1. Search Strategy: While previous methods primarily rely on greedy search, which becomes increasingly inefficient for longer sequences, Seq2Sick employs group lasso regularization and projected gradient descent with gradient regularization. This allows for simultaneous searching of all replacement positions, leading to improved efficiency.

2. Targeted Attack Type: Existing methods focus on targeting specific classes or binary classifications, while Seq2Sick introduces a novel ""keyword"" target type, allowing attacks to be directed towards specific keywords within the generated sequence.",1803.01128v3.pdf,"['1803.01128v3.pdf', '1805.07567v2.pdf', '1812.06589v2.pdf', '1811.02553v4.pdf', '1603.03833v4.pdf', '1811.07073v3.pdf', '1611.07718v2.pdf', '1703.10730v2.pdf']","Table 1 provides a comparative overview of different attack methods, highlighting their characteristics in terms of gradient-based approach, word-level RNN usage, sequential output, and targeted attack type. By analyzing this information in conjunction with the passage's emphasis on Seq2Sick's unique search strategy and keyword targeting capability, we can identify the key differences between this method and existing approaches.",1803.01128v3-Table1-1.png,Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples,"Crafting adversarial examples has become an important technique to evaluate
the robustness of deep neural networks (DNNs). However, most existing works
focus on attacking the image classification problem since its input space is
continuous and output space is finite.
  In this paper, we study the much more challenging problem of crafting
adversarial examples for sequence-to-sequence (seq2seq) models, whose inputs
are discrete text strings and outputs have an almost infinite number of
possibilities. To address the challenges caused by the discrete input space, we
propose a projected gradient method combined with group lasso and gradient
regularization. To handle the almost infinite output space, we design some
novel loss functions to conduct non-overlapping attack and targeted keyword
attack. We apply our algorithm to machine translation and text summarization
tasks, and verify the effectiveness of the proposed algorithm: by changing less
than 3 words, we can make seq2seq model to produce desired outputs with high
success rates. On the other hand, we recognize that, compared with the
well-evaluated CNN-based classifiers, seq2seq models are intrinsically more
robust to adversarial attacks.","Table 1: Summary of existing works that are designed to attack RNN models. “BINARY” indicates the attack is for binary classifications, and there is no difference between untargeted and targeted attack in this case. “CLASS” means targeted attack to a specific class. “KEYWORD” means targeted attack to a specific keyword. Here we omit follow-up works based on Seq2Sick.",What are the key differences between Seq2Sick and existing attack methods on RNN-based models?
spiqa_183,1611.04684v1,"What educational approaches of the Bonaparte and Voltaire schools are contrasted in the figure titled ""A difficult example from QA,"" and how are these approaches specifically differentiated in terms of physical versus cognitive development, and their respective goals of leadership versus philosophical thinking?","The Bonaparte school focuses on outdoor physical activities, maneuvers, and strategies, with a specialization in horse riding, lances, and swords. They aim to develop students into good leaders. The Voltaire school, on the other hand, encourages independent thinking and focuses on indoor activities. They aim to instill good moral values and develop students into philosophical thinkers.",1611.04684v1.pdf,"['1611.04684v1.pdf', '1706.08146v3.pdf', '1803.04572v2.pdf', '1805.04609v3.pdf', '1805.02349v2.pdf', '1707.00524v2.pdf', '1804.04410v2.pdf', '1906.10843v1.pdf', '1701.06171v4.pdf', '1803.04383v2.pdf', '1705.09966v2.pdf', '1802.07351v2.pdf', '1805.01216v3.pdf', '1704.05426v4.pdf']","The figure presents a question and answer format, where the answer explicitly describes the contrasting educational approaches of the two schools.",1611.04684v1-Table1-1.png,Knowledge Enhanced Hybrid Neural Network for Text Matching,"Long text brings a big challenge to semantic matching due to their
complicated semantic and syntactic structures. To tackle the challenge, we
consider using prior knowledge to help identify useful information and filter
out noise to matching in long text. To this end, we propose a knowledge
enhanced hybrid neural network (KEHNN). The model fuses prior knowledge into
word representations by knowledge gates and establishes three matching channels
with words, sequential structures of sentences given by Gated Recurrent Units
(GRU), and knowledge enhanced representations. The three channels are processed
by a convolutional neural network to generate high level features for matching,
and the features are synthesized as a matching score by a multilayer
perceptron. The model extends the existing methods by conducting matching on
words, local structures of sentences, and global context of sentences.
Evaluation results from extensive experiments on public data sets for question
answering and conversation show that KEHNN can significantly outperform
the-state-of-the-art matching models and particularly improve the performance
on pairs with long text.",A difficult example from QA,What are the main differences between the educational philosophies of the Bonaparte and Voltaire schools?
spiqa_184,1811.10673v1,"What are the three key steps in the second encoding stage ($E_2$), as depicted in Figure 3, of your GAN-based video compression framework, and how do they contribute to the soft edge detection and compression process?","The second encoding stage involves three steps: down-sampling, soft edge detection, and spatio-temporal edge map compression.",1811.10673v1.pdf,"['1811.10673v1.pdf', '1708.03797v1.pdf', '1704.05958v2.pdf', '1612.02803v5.pdf', '1707.00524v2.pdf', '1705.09296v2.pdf', '1805.01216v3.pdf', '1811.08257v1.pdf']","The figure shows a schematic of the second encoding stage, with the three steps clearly labeled.",1811.10673v1-Figure3-1.png,Adversarial Video Compression Guided by Soft Edge Detection,"We propose a video compression framework using conditional Generative
Adversarial Networks (GANs). We rely on two encoders: one that deploys a
standard video codec and another which generates low-level maps via a pipeline
of down-sampling, a newly devised soft edge detector, and a novel lossless
compression scheme. For decoding, we use a standard video decoder as well as a
neural network based one, which is trained using a conditional GAN. Recent
""deep"" approaches to video compression require multiple videos to pre-train
generative networks to conduct interpolation. In contrast to this prior work,
our scheme trains a generative decoder on pairs of a very limited number of key
frames taken from a single video and corresponding low-level maps. The trained
decoder produces reconstructed frames relying on a guidance of low-level maps,
without any interpolation. Experiments on a diverse set of 131 videos
demonstrate that our proposed GAN-based compression engine achieves much higher
quality reconstructions at very low bitrates than prevailing standard codecs
such as H.264 or HEVC.",Figure 3: Overview of the second encoding stage (E2).,What are the steps involved in the second encoding stage ($E_2$)?
spiqa_185,1803.04572v2,What are the three constraints shown in the figure from the COPA paper that are applied to PARAFAC2 model factors for temporal phenotyping of EHR data?,"Non-negativity, smoothness, and sparsity.",1803.04572v2.pdf,"['1803.04572v2.pdf', '1611.03780v2.pdf', '1812.10735v2.pdf', '1809.03550v3.pdf', '1804.05938v2.pdf', '1705.10667v4.pdf', '1704.05426v4.pdf', '1707.00524v2.pdf', '1611.05742v3.pdf', '1705.09966v2.pdf', '1811.08481v2.pdf', '1906.10843v1.pdf', '1803.02750v3.pdf', '1805.04687v2.pdf']","The figure shows how COPA imposes three constraints on PARAFAC2 model factors: non-negativity, smoothness, and sparsity. The non-negativity constraint ensures that all factor values are non-negative. The smoothness constraint ensures that the factor values change smoothly over time. The sparsity constraint ensures that only a small number of factor values are non-zero.",1803.04572v2-Figure1-1.png,COPA: Constrained PARAFAC2 for Sparse & Large Datasets,"PARAFAC2 has demonstrated success in modeling irregular tensors, where the
tensor dimensions vary across one of the modes. An example scenario is modeling
treatments across a set of patients with the varying number of medical
encounters over time. Despite recent improvements on unconstrained PARAFAC2,
its model factors are usually dense and sensitive to noise which limits their
interpretability. As a result, the following open challenges remain: a) various
modeling constraints, such as temporal smoothness, sparsity and non-negativity,
are needed to be imposed for interpretable temporal modeling and b) a scalable
approach is required to support those constraints efficiently for large
datasets. To tackle these challenges, we propose a {\it CO}nstrained {\it
PA}RAFAC2 (COPA) method, which carefully incorporates optimization constraints
such as temporal smoothness, sparsity, and non-negativity in the resulting
factors. To efficiently support all those constraints, COPA adopts a hybrid
optimization framework using alternating optimization and alternating direction
method of multiplier (AO-ADMM). As evaluated on large electronic health record
(EHR) datasets with hundreds of thousands of patients, COPA achieves
significant speedups (up to 36 times faster) over prior PARAFAC2 approaches
that only attempt to handle a subset of the constraints that COPA enables.
Overall, our method outperforms all the baselines attempting to handle a subset
of the constraints in terms of speed, while achieving the same level of
accuracy. Through a case study on temporal phenotyping of medically complex
children, we demonstrate how the constraints imposed by COPA reveal concise
phenotypes and meaningful temporal profiles of patients. The clinical
interpretation of both the phenotypes and the temporal profiles was confirmed
by a medical expert.","An illustration of the constraints imposed by COPA on PARAFAC2 model factors, targeting temporal phenotyping via EHR data.",What are the three constraints imposed by COPA on PARAFAC2 model factors?
spiqa_186,1603.03833v4,"What are the three network architectures depicted in Figure 5 of the study on transferring virtual demonstrations to real-world robot manipulation, which compare different approaches for performance on manipulation tasks?","Feedforward-MSE, LSTM-MSE, and Feedforward-MDN.",1603.03833v4.pdf,"['1603.03833v4.pdf', '1804.07707v2.pdf', '1809.01989v2.pdf', '1706.00633v4.pdf', '1811.06635v1.pdf', '1708.06832v3.pdf']",The figure shows the three different network architectures used in the comparison study. Each network architecture is shown with its corresponding name.,1603.03833v4-Figure5-1.png,From virtual demonstration to real-world manipulation using LSTM and MDN,"Robots assisting the disabled or elderly must perform complex manipulation
tasks and must adapt to the home environment and preferences of their user.
Learning from demonstration is a promising choice, that would allow the
non-technical user to teach the robot different tasks. However, collecting
demonstrations in the home environment of a disabled user is time consuming,
disruptive to the comfort of the user, and presents safety challenges. It would
be desirable to perform the demonstrations in a virtual environment. In this
paper we describe a solution to the challenging problem of behavior transfer
from virtual demonstration to a physical robot. The virtual demonstrations are
used to train a deep neural network based controller, which is using a Long
Short Term Memory (LSTM) recurrent neural network to generate trajectories. The
training process uses a Mixture Density Network (MDN) to calculate an error
signal suitable for the multimodal nature of demonstrations. The controller
learned in the virtual environment is transferred to a physical robot (a
Rethink Robotics Baxter). An off-the-shelf vision component is used to
substitute for geometric knowledge available in the simulation and an inverse
kinematics module is used to allow the Baxter to enact the trajectory. Our
experimental studies validate the three contributions of the paper: (1) the
controller learned from virtual demonstrations can be used to successfully
perform the manipulation tasks on a physical robot, (2) the LSTM+MDN
architectural choice outperforms other choices, such as the use of feedforward
networks and mean-squared error based training signals and (3) allowing
imperfect demonstrations in the training set also allows the controller to
learn how to correct its manipulation mistakes.","Figure 5: Alternative network architectures used in the comparison study: Feedforward-MSE, LSTM-MSE and Feedforward-MDN",What are the three different network architectures used in the comparison study?
spiqa_187,1706.04269v2,"What are the three key components, as depicted in Fig. 3 of the ""Action Search"" paper, that work together to predict the model’s next search location in the video?","The three main components of the Action Search model architecture are the visual encoder, the LSTM, and the spotting target.",1706.04269v2.pdf,"['1706.04269v2.pdf', '1709.02755v5.pdf', '1811.02721v3.pdf', '1706.00827v2.pdf', '1704.07121v2.pdf', '1805.07567v2.pdf', '1707.08608v3.pdf', '1804.07707v2.pdf', '1706.04284v3.pdf']","The figure shows how these three components are connected and how they work together to predict the next search location in the video. The visual encoder takes the visual observation from the current temporal location and transforms it into a feature vector. The LSTM takes this feature vector, as well as the state and temporal location from the previous step, and outputs its updated state and the next search location. The spotting target is the location in the video where the model is currently searching for the action.",1706.04269v2-Figure3-1.png,Action Search: Spotting Actions in Videos and Its Application to Temporal Action Localization,"State-of-the-art temporal action detectors inefficiently search the entire
video for specific actions. Despite the encouraging progress these methods
achieve, it is crucial to design automated approaches that only explore parts
of the video which are the most relevant to the actions being searched for. To
address this need, we propose the new problem of action spotting in video,
which we define as finding a specific action in a video while observing a small
portion of that video. Inspired by the observation that humans are extremely
efficient and accurate in spotting and finding action instances in video, we
propose Action Search, a novel Recurrent Neural Network approach that mimics
the way humans spot actions. Moreover, to address the absence of data recording
the behavior of human annotators, we put forward the Human Searches dataset,
which compiles the search sequences employed by human annotators spotting
actions in the AVA and THUMOS14 datasets. We consider temporal action
localization as an application of the action spotting problem. Experiments on
the THUMOS14 dataset reveal that our model is not only able to explore the
video efficiently (observing on average 17.3% of the video) but it also
accurately finds human activities with 30.8% mAP.","Fig. 3: Our model harnesses the temporal context from its current location and the history of what it has observed to predict the next search location in the video. At each step, (i) a visual encoder transforms the visual observation extracted from the model’s current temporal location to a representative feature vector; (ii) an LSTM consumes this feature vector plus the state and temporal location produced in the previous step; (iii) the LSTM outputs its updated state and the next search location; (iv) the model moves to the new temporal location.",What are the three main components of the Action Search model architecture?
spiqa_188,1805.04687v2,"According to the geographical distribution depicted in Figure 2 of the BDD100K paper, what are the three primary regions in the US where most of the driving video data was collected, based on the starting locations of the clips?","New York, San Francisco Bay Area, and Berkeley.",1805.04687v2.pdf,"['1805.04687v2.pdf', '1811.10673v1.pdf', '1612.02803v5.pdf', '1705.09882v2.pdf', '1809.02731v3.pdf', '1707.01922v5.pdf', '1709.00139v4.pdf', '1703.00899v2.pdf', '1703.00060v2.pdf']","The figure shows the geographical distribution of the data sources, with each dot representing the starting location of a video clip. The dots are concentrated in three main regions: New York, San Francisco Bay Area, and Berkeley.",1805.04687v2-Figure2-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.",Figure 2: Geographical distribution of our data sources. Each dot represents the starting location of every video clip. Our videos are from many cities and regions in the populous areas in the US.,What are the three main geographical regions where the data for this study was collected?
spiqa_189,1706.08146v3,"In the compressed matrix factorization process shown in Figure 1 of the ""Compressed Factorization: Fast and Accurate Low-Rank Factorization of Compressively-Sensed Data"" paper, what are the three sequential steps involving data compression, factorization, and recovery of the left factor?","The three steps involved in compressed matrix factorization are: 

1. Compress the full data matrix M to obtain a compressed matrix M̃. 
2. Factorize M̃ to obtain matrices W̃ and H̃. 
3. Approximate the left factor of M via sparse recovery on each column of W̃.",1706.08146v3.pdf,"['1706.08146v3.pdf', '1804.05995v2.pdf', '1809.00458v1.pdf', '1812.00108v4.pdf', '1611.05742v3.pdf']",The figure shows the three steps involved in compressed matrix factorization.,1706.08146v3-Figure1-1.png,Compressed Factorization: Fast and Accurate Low-Rank Factorization of Compressively-Sensed Data,"What learning algorithms can be run directly on compressively-sensed data? In
this work, we consider the question of accurately and efficiently computing
low-rank matrix or tensor factorizations given data compressed via random
projections. We examine the approach of first performing factorization in the
compressed domain, and then reconstructing the original high-dimensional
factors from the recovered (compressed) factors. In both the matrix and tensor
settings, we establish conditions under which this natural approach will
provably recover the original factors. While it is well-known that random
projections preserve a number of geometric properties of a dataset, our work
can be viewed as showing that they can also preserve certain solutions of
non-convex, NP-Hard problems like non-negative matrix factorization. We support
these theoretical results with experiments on synthetic data and demonstrate
the practical applicability of compressed factorization on real-world gene
expression and EEG time series datasets.","Figure 1: Schematic illustration of compressed matrix factorization. (i) The matrix M̃ is a compressed version of the full data matrix M . (ii) We directly factorize M̃ to obtain matrices W̃ and H̃. (iii) Finally, we approximate the left factor of M via sparse recovery on each column of W̃ .",What are the three steps involved in compressed matrix factorization?
spiqa_190,1704.05426v4,"What are the three specific types of sentences annotators were instructed to write for each situation or event, as shown in the figure containing the prompt in the MultiNLI corpus paper?","The three types of sentences are: 
1. A sentence that is definitely correct about the situation or event in the line.
2. A sentence that might be correct about the situation or event in the line.
3. A sentence that is definitely incorrect about the situation or event in the line.",1704.05426v4.pdf,"['1704.05426v4.pdf', '1611.03780v2.pdf', '1710.01507v4.pdf', '1606.07384v2.pdf', '1608.02784v2.pdf', '1805.04687v2.pdf', '1811.02721v3.pdf', '1809.00263v5.pdf', '1812.06589v2.pdf', '1805.06431v4.pdf', '1809.01246v1.pdf', '1706.04269v2.pdf']","The figure shows the instructions given to the annotators, which explicitly state the three types of sentences they are asked to write.",1704.05426v4-Figure1-1.png,A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference,"This paper introduces the Multi-Genre Natural Language Inference (MultiNLI)
corpus, a dataset designed for use in the development and evaluation of machine
learning models for sentence understanding. In addition to being one of the
largest corpora available for the task of NLI, at 433k examples, this corpus
improves upon available resources in its coverage: it offers data from ten
distinct genres of written and spoken English--making it possible to evaluate
systems on nearly the full complexity of the language--and it offers an
explicit setting for the evaluation of cross-genre domain adaptation.",The main text of a prompt (truncated) that was presented to our annotators. This version is used for the written non-fiction genres.,What are the three types of sentences that the annotators are asked to write?
spiqa_191,1804.05995v2,"Based on the figure in the paper showing section recommendations for the Wikipedia article on Lausanne, what are the top 5 sections suggested by the category-section counts method?","The top 5 section recommendations for the Wikipedia article on Lausanne according to the category-section counts method are HISTORY, DEMOGRAPHICS, ECONOMY, EDUCATION, and POLITICS.",1804.05995v2.pdf,"['1804.05995v2.pdf', '1811.07073v3.pdf', '1706.08146v3.pdf', '1608.02784v2.pdf', '1809.01989v2.pdf', '1704.04539v2.pdf', '1702.08694v3.pdf', '1707.01922v5.pdf', '1809.04276v2.pdf', '1710.05654v2.pdf', '1704.07121v2.pdf', '1709.02418v2.pdf', '1811.09393v4.pdf']",The table in the figure shows the top 5 section recommendations for the Wikipedia article on Lausanne according to four different methods. The category-section counts method is one of these methods. The top 5 recommendations for this method are listed in the third column of the table.,1804.05995v2-Table1-1.png,Structuring Wikipedia Articles with Section Recommendations,"Sections are the building blocks of Wikipedia articles. They enhance
readability and can be used as a structured entry point for creating and
expanding articles. Structuring a new or already existing Wikipedia article
with sections is a hard task for humans, especially for newcomers or less
experienced editors, as it requires significant knowledge about how a
well-written article looks for each possible topic. Inspired by this need, the
present paper defines the problem of section recommendation for Wikipedia
articles and proposes several approaches for tackling it. Our systems can help
editors by recommending what sections to add to already existing or newly
created Wikipedia articles. Our basic paradigm is to generate recommendations
by sourcing sections from articles that are similar to the input article. We
explore several ways of defining similarity for this purpose (based on topic
modeling, collaborative filtering, and Wikipedia's category system). We use
both automatic and human evaluation approaches for assessing the performance of
our recommendation system, concluding that the category-based approach works
best, achieving precision@10 of about 80% in the human evaluation.","Top 5 section recommendations for the Wikipedia article lausanne, according to various methods.",What are the top 5 section recommendations for the Wikipedia article on Lausanne according to the category-section counts method?
spiqa_192,1804.07931v2,What are the two auxiliary tasks introduced in Figure 2 of the ESMM architecture that contribute to modeling CVR over the entire input space and aid in feature representation transfer learning?,The two auxiliary tasks are CTR and CTCVR.,1804.07931v2.pdf,"['1804.07931v2.pdf', '1812.10735v2.pdf', '1811.02553v4.pdf', '1706.04269v2.pdf']"," The figure shows that the ESMM architecture consists of two sub-networks: a CVR network and a CTR network. The CTR network is used to predict the click-through rate, while the CVR network is used to predict the conversion rate. The CTCVR task takes the product of the outputs from the CTR and CVR networks as its output. ",1804.07931v2-Figure2-1.png,Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate,"Estimating post-click conversion rate (CVR) accurately is crucial for ranking
systems in industrial applications such as recommendation and advertising.
Conventional CVR modeling applies popular deep learning methods and achieves
state-of-the-art performance. However it encounters several task-specific
problems in practice, making CVR modeling challenging. For example,
conventional CVR models are trained with samples of clicked impressions while
utilized to make inference on the entire space with samples of all impressions.
This causes a sample selection bias problem. Besides, there exists an extreme
data sparsity problem, making the model fitting rather difficult. In this
paper, we model CVR in a brand-new perspective by making good use of sequential
pattern of user actions, i.e., impression -> click -> conversion. The proposed
Entire Space Multi-task Model (ESMM) can eliminate the two problems
simultaneously by i) modeling CVR directly over the entire space, ii) employing
a feature representation transfer learning strategy. Experiments on dataset
gathered from Taobao's recommender system demonstrate that ESMM significantly
outperforms competitive methods. We also release a sampling version of this
dataset to enable future research. To the best of our knowledge, this is the
first public dataset which contains samples with sequential dependence of click
and conversion labels for CVR modeling.","Figure 2: Architecture overview of ESMM for CVR modeling. In ESMM, two auxiliary tasks of CTR and CTCVR are introduced which: i) help to model CVR over entire input space, ii) provide feature representation transfer learning. ESMM mainly consists of two sub-networks: CVR network illustrated in the left part of this figure and CTR network in the right part. Embedding parameters of CTR and CVR network are shared. CTCVR takes the product of outputs from CTR and CVR network as the output.",What are the two auxiliary tasks that are used in the ESMM architecture for CVR modeling?
spiqa_193,1812.10735v2,"In the CAN network architecture, as visualized in Figure 2, what are the two main tasks modeled through aspect-specific attention and prediction paths?",Aspect-level sentiment classification (ALSC) and aspect category detection (ACD).,1812.10735v2.pdf,"['1812.10735v2.pdf', '1811.08257v1.pdf', '1804.05995v2.pdf', '1804.05936v2.pdf']","The figure shows two separate paths for ALSC and ACD, each with its own attention layer and prediction layer. The ALSC path predicts the sentiment of each aspect, while the ACD path detects the categories of the aspects.",1812.10735v2-Figure2-1.png,CAN: Constrained Attention Networks for Multi-Aspect Sentiment Analysis,"Aspect level sentiment classification is a fine-grained sentiment analysis
task. To detect the sentiment towards a particular aspect in a sentence,
previous studies have developed various attention-based methods for generating
aspect-specific sentence representations. However, the attention may inherently
introduce noise and downgrade the performance. In this paper, we propose
constrained attention networks (CAN), a simple yet effective solution, to
regularize the attention for multi-aspect sentiment analysis, which alleviates
the drawback of the attention mechanism. Specifically, we introduce orthogonal
regularization on multiple aspects and sparse regularization on each single
aspect. Experimental results on two public datasets demonstrate the
effectiveness of our approach. We further extend our approach to multi-task
settings and outperform the state-of-the-art methods.","Figure 2: Network Architecture. The aspect categories are embedded as vectors. The model encodes the sentence using LSTM. Based on its hidden states, aspect-specific sentence representations for ALSC and ACD tasks are learned via constrained attention. Then aspect level sentiment prediction and aspect category detection are made.",What are the two main tasks that the CAN network is designed to perform?
spiqa_194,1705.02798v6,"In the Reinforced Mnemonic Reader architecture illustrated in Figure 3, what are the two attention mechanisms identified as refining the evidence embedding (Et) and context embedding (Bt)?",The two types of attention mechanisms are reattention and self-attention.,1705.02798v6.pdf,"['1705.02798v6.pdf', '1804.04410v2.pdf', '1906.06589v3.pdf', '1802.07222v1.pdf', '1803.06506v3.pdf', '1705.08016v3.pdf', '1706.00827v2.pdf', '1811.02553v4.pdf', '1704.08615v2.pdf', '1706.04284v3.pdf', '1804.05938v2.pdf', '1809.00263v5.pdf']","The figure shows the architecture of the Reinforced Mnemonic Reader, which includes two types of attention mechanisms: reattention and self-attention. Reattention is used to refine the evidence embedding Et to attend to the query, and self-attention is used to refine the context embedding Bt to attend to the context.",1705.02798v6-Figure3-1.png,Reinforced Mnemonic Reader for Machine Reading Comprehension,"In this paper, we introduce the Reinforced Mnemonic Reader for machine
reading comprehension tasks, which enhances previous attentive readers in two
aspects. First, a reattention mechanism is proposed to refine current
attentions by directly accessing to past attentions that are temporally
memorized in a multi-round alignment architecture, so as to avoid the problems
of attention redundancy and attention deficiency. Second, a new optimization
approach, called dynamic-critical reinforcement learning, is introduced to
extend the standard supervised method. It always encourages to predict a more
acceptable answer so as to address the convergence suppression problem occurred
in traditional reinforcement learning algorithms. Extensive experiments on the
Stanford Question Answering Dataset (SQuAD) show that our model achieves
state-of-the-art results. Meanwhile, our model outperforms previous systems by
over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD
datasets.",Figure 3: The architecture overview of Reinforced Mnemonic Reader. The subfigures to the right show detailed demonstrations of the reattention mechanism: 1) refined Et to attend the query; 2) refined Bt to attend the context.,What are the two types of attention mechanisms used in the Reinforced Mnemonic Reader architecture?
spiqa_195,1705.07164v8,"Based on the figure showing the training curves of the ACGAN for MNIST and Fashion-MNIST from the ""Relaxed Wasserstein with Applications to GANs"" paper, what insights can be drawn from the trends in both the generator (orange) and discriminator (blue) losses over time regarding their learning behavior?",The training curves for the ACGAN show that the generator and discriminator losses both decrease over time. This indicates that the ACGAN is able to learn to generate realistic images.,1705.07164v8.pdf,"['1705.07164v8.pdf', '1804.07931v2.pdf', '1809.02731v3.pdf', '1611.03780v2.pdf', '1805.06431v4.pdf', '1803.03467v4.pdf', '1805.06447v3.pdf', '1703.04887v4.pdf', '1811.02553v4.pdf']","The figure shows the training curves for the ACGAN. The orange line represents the generator loss, and the blue line represents the discriminator loss. The fact that both lines decrease over time indicates that the ACGAN is learning to generate realistic images.",1705.07164v8-Figure1-1.png,Relaxed Wasserstein with Applications to GANs,"Wasserstein Generative Adversarial Networks (WGANs) provide a versatile class
of models, which have attracted great attention in various applications.
However, this framework has two main drawbacks: (i) Wasserstein-1 (or
Earth-Mover) distance is restrictive such that WGANs cannot always fit data
geometry well; (ii) It is difficult to achieve fast training of WGANs. In this
paper, we propose a new class of \textit{Relaxed Wasserstein} (RW) distances by
generalizing Wasserstein-1 distance with Bregman cost functions. We show that
RW distances achieve nice statistical properties while not sacrificing the
computational tractability. Combined with the GANs framework, we develop
Relaxed WGANs (RWGANs) which are not only statistically flexible but can be
approximated efficiently using heuristic approaches. Experiments on real images
demonstrate that the RWGAN with Kullback-Leibler (KL) cost function outperforms
other competing approaches, e.g., WGANs, even with gradient penalty.",Training curves by running all approaches. Generator and discriminator losses are plotted in orange and blue lines. mnist fashion-mnist,What can you infer from the training curves for the ACGAN?
spiqa_196,1611.02654v2,"Based on the t-SNE embeddings figure in the ""Sentence Ordering and Coherence Modeling using Recurrent Neural Networks"" paper, how does the proximity of sentence embeddings, color-coded by their position in the document, reflect their semantic similarity?",Sentences that are closer together in the embedding space are more semantically similar than those that are farther apart.,1611.02654v2.pdf,"['1611.02654v2.pdf', '1811.06635v1.pdf', '1805.00912v4.pdf', '1803.05776v2.pdf', '1703.07015v3.pdf', '1611.03780v2.pdf', '1703.10730v2.pdf']"," The t-SNE embeddings are color-coded by the position of the sentence in the document. We can see that sentences that are close together in the embedding space tend to be from the same part of the document, which suggests that they are semantically similar. ",1611.02654v2-Figure2-1.png,Sentence Ordering and Coherence Modeling using Recurrent Neural Networks,"Modeling the structure of coherent texts is a key NLP problem. The task of
coherently organizing a given set of sentences has been commonly used to build
and evaluate models that understand such structure. We propose an end-to-end
unsupervised deep learning approach based on the set-to-sequence framework to
address this problem. Our model strongly outperforms prior methods in the order
discrimination task and a novel task of ordering abstracts from scientific
articles. Furthermore, our work shows that useful text representations can be
obtained by learning to order sentences. Visualizing the learned sentence
representations shows that the model captures high-level logical structure in
paragraphs. Our representations perform comparably to state-of-the-art
pre-training methods on sentence similarity and paraphrase detection tasks.",t-SNE embeddings of representations learned by the model for sentences from the test set. Embeddings are color coded by the position of the sentence in the document it appears.,What can you say about the relationship between the sentences in a document based on the t-SNE embeddings?
spiqa_197,1703.07015v3,"Based on the figure in the paper, which component's removal leads to the most significant performance drop across most datasets, emphasizing its crucial role in LSTNet's predictive accuracy?",The AR component.,1703.07015v3.pdf,"['1703.07015v3.pdf', '1705.07384v2.pdf', '1804.07707v2.pdf', '1706.00827v2.pdf', '1901.00398v2.pdf', '1709.02755v5.pdf', '1811.07073v3.pdf', '1706.08146v3.pdf', '1706.03847v3.pdf']","Removing the AR component from the model caused the most significant performance drops on most of the datasets, indicating its importance.",1703.07015v3-Figure56-1.png,Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks,"Multivariate time series forecasting is an important machine learning problem
across many domains, including predictions of solar plant energy output,
electricity consumption, and traffic jam situation. Temporal data arise in
these real-world applications often involves a mixture of long-term and
short-term patterns, for which traditional approaches such as Autoregressive
models and Gaussian Process may fail. In this paper, we proposed a novel deep
learning framework, namely Long- and Short-term Time-series network (LSTNet),
to address this open challenge. LSTNet uses the Convolution Neural Network
(CNN) and the Recurrent Neural Network (RNN) to extract short-term local
dependency patterns among variables and to discover long-term patterns for time
series trends. Furthermore, we leverage traditional autoregressive model to
tackle the scale insensitive problem of the neural network model. In our
evaluation on real-world data with complex mixtures of repetitive patterns,
LSTNet achieved significant performance improvements over that of several
state-of-the-art baseline methods. All the data and experiment codes are
available online.",Several observations from these results are worth highlighting:,What component of LSTNet is most important for its performance?
spiqa_198,1803.04383v2,"How does the outcome curve in the figure depict the varying impacts of increasing selection rates on mean score across different population groups with high versus low potential for gain, particularly in the regions of relative harm, no harm, and active harm?","The outcome curve shows that the relationship between selection rate and mean change in score is complex and depends on the specific group being considered. For groups with high potential for gain, increasing the selection rate can lead to large increases in mean score. However, for groups with low potential for gain, increasing the selection rate can actually lead to decreases in mean score.",1803.04383v2.pdf,"['1803.04383v2.pdf', '1603.00286v5.pdf', '1804.04410v2.pdf', '1906.06589v3.pdf', '1811.02721v3.pdf', '1703.00899v2.pdf', '1809.01989v2.pdf', '1611.05742v3.pdf', '1704.07121v2.pdf', '1706.00827v2.pdf', '1611.02654v2.pdf', '1710.05654v2.pdf', '1804.05995v2.pdf']","The figure shows that the outcome curve is divided into three regions: relative improvement, relative harm, and active harm. The relative improvement region is the area where increasing the selection rate leads to increases in mean score. The relative harm region is the area where increasing the selection rate leads to smaller increases in mean score than would be expected if the selection rate were lower. The active harm region is the area where increasing the selection rate leads to decreases in mean score.",1803.04383v2-Figure1-1.png,Delayed Impact of Fair Machine Learning,"Fairness in machine learning has predominantly been studied in static
classification settings without concern for how decisions change the underlying
population over time. Conventional wisdom suggests that fairness criteria
promote the long-term well-being of those groups they aim to protect.
  We study how static fairness criteria interact with temporal indicators of
well-being, such as long-term improvement, stagnation, and decline in a
variable of interest. We demonstrate that even in a one-step feedback model,
common fairness criteria in general do not promote improvement over time, and
may in fact cause harm in cases where an unconstrained objective would not.
  We completely characterize the delayed impact of three standard criteria,
contrasting the regimes in which these exhibit qualitatively different
behavior. In addition, we find that a natural form of measurement error
broadens the regime in which fairness criteria perform favorably.
  Our results highlight the importance of measurement and temporal modeling in
the evaluation of fairness criteria, suggesting a range of new challenges and
trade-offs.","The above figure shows the outcome curve. The horizontal axis represents the selection rate for the population; the vertical axis represents the mean change in score. (a) depicts the full spectrum of outcome regimes, and colors indicate regions of active harm, relative harm, and no harm. In (b): a group that has much potential for gain, in (c): a group that has no potential for gain.",What does the outcome curve tell us about the relationship between selection rate and mean change in score?
spiqa_199,1704.07854v4,"As illustrated in the figure, how does the parameter network modify the initial liquid surface to bring it closer to the reference surface during the deformation process?","The parameter network weights the initial surface, causing it to deform.",1704.07854v4.pdf,"['1704.07854v4.pdf', '1811.08481v2.pdf', '1812.10735v2.pdf', '1805.07567v2.pdf', '1803.01128v3.pdf', '1708.05239v3.pdf', '1809.03149v2.pdf', '1811.09393v4.pdf', '1704.08615v2.pdf', '1611.03780v2.pdf', '1804.07931v2.pdf', '1611.07718v2.pdf', '1703.00060v2.pdf']","The figure shows three different surfaces: the initial surface, the surface deformed by the parameter network, and the reference surface. The deformed surface is closer in shape to the reference surface than the initial surface, which suggests that the parameter network is able to learn the desired shape of the surface.",1704.07854v4-Figure3-1.png,Generating Liquid Simulations with Deformation-aware Neural Networks,"We propose a novel approach for deformation-aware neural networks that learn
the weighting and synthesis of dense volumetric deformation fields. Our method
specifically targets the space-time representation of physical surfaces from
liquid simulations. Liquids exhibit highly complex, non-linear behavior under
changing simulation conditions such as different initial conditions. Our
algorithm captures these complex phenomena in two stages: a first neural
network computes a weighting function for a set of pre-computed deformations,
while a second network directly generates a deformation field for refining the
surface. Key for successful training runs in this setting is a suitable loss
function that encodes the effect of the deformations, and a robust calculation
of the corresponding gradients. To demonstrate the effectiveness of our
approach, we showcase our method with several complex examples of flowing
liquids with topology changes. Our representation makes it possible to rapidly
generate the desired implicit surfaces. We have implemented a mobile
application to demonstrate that real-time interactions with complex liquid
effects are possible with our approach.","An example of our parameter learning approach. F.l.t.r.: the initial undeformed surface, the surface deformed by the weighting from the trained parameter network, and the reference surface only. The reference surface is shown again in the middle in light brown for comparison. The weighted deformations especially match the left liquid arm well, while there are not enough degrees of freedom in the pre-computed deformations to independently raise the surface on the right side.",What does the parameter network do to the initial surface?
spiqa_200,1809.00458v1,"According to the figure in the GB-KMV paper depicting a dataset of four records and a query Q, what is the containment similarity of Q in X1 as presented under the column labeled C'(Q, Xi)?",0.67,1809.00458v1.pdf,"['1809.00458v1.pdf', '1805.04609v3.pdf', '1603.00286v5.pdf', '1704.05426v4.pdf', '1811.08481v2.pdf', '1703.10730v2.pdf']","The containment similarity of Q in X1 is shown in the table under the column C'(Q, Xi).",1809.00458v1-Figure1-1.png,GB-KMV: An Augmented KMV Sketch for Approximate Containment Similarity Search,"In this paper, we study the problem of approximate containment similarity
search. Given two records Q and X, the containment similarity between Q and X
with respect to Q is |Q intersect X|/ |Q|. Given a query record Q and a set of
records S, the containment similarity search finds a set of records from S
whose containment similarity regarding Q are not less than the given threshold.
This problem has many important applications in commercial and scientific
fields such as record matching and domain search. Existing solution relies on
the asymmetric LSH method by transforming the containment similarity to
well-studied Jaccard similarity. In this paper, we use a different framework by
transforming the containment similarity to set intersection. We propose a novel
augmented KMV sketch technique, namely GB-KMV, which is data-dependent and can
achieve a good trade-off between the sketch size and the accuracy. We provide a
set of theoretical analysis to underpin the proposed augmented KMV sketch
technique, and show that it outperforms the state-of-the-art technique LSH-E in
terms of estimation accuracy under practical assumption. Our comprehensive
experiments on real-life datasets verify that GB-KMV is superior to LSH-E in
terms of the space-accuracy trade-off, time-accuracy trade-off, and the sketch
construction time. For instance, with similar estimation accuracy (F-1 score),
GB-KMV is over 100 times faster than LSH-E on some real-life dataset.","A four-record dataset and a query Q; C(Q,Xi) is the containment similarity of Q in Xi",What is the containment similarity of Q in X1?
spiqa_201,1707.01917v2,"Based on the figure titled ""Notations used in the paper"" from the research on Higher-order Relation Schema Induction using Tensor Factorization with Back-off and Aggregation, how is a non-negative tensor formally defined in the context of the proposed framework?",A non-negative tensor is a tensor whose elements are all non-negative real numbers.,1707.01917v2.pdf,"['1707.01917v2.pdf', '1705.09882v2.pdf', '1706.08146v3.pdf', '1809.03550v3.pdf', '1707.00524v2.pdf', '1906.06589v3.pdf', '1706.04284v3.pdf', '1804.05995v2.pdf', '1812.06589v2.pdf', '1707.06320v2.pdf', '1702.03584v3.pdf', '1803.04572v2.pdf', '1606.07384v2.pdf', '1703.04887v4.pdf']",The figure defines a non-negative tensor as an element of the set of non-negative real numbers raised to the power of the tensor's order.,1707.01917v2-Table1-1.png,Higher-order Relation Schema Induction using Tensor Factorization with Back-off and Aggregation,"Relation Schema Induction (RSI) is the problem of identifying type signatures
of arguments of relations from unlabeled text. Most of the previous work in
this area have focused only on binary RSI, i.e., inducing only the subject and
object type signatures per relation. However, in practice, many relations are
high-order, i.e., they have more than two arguments and inducing type
signatures of all arguments is necessary. For example, in the sports domain,
inducing a schema win(WinningPlayer, OpponentPlayer, Tournament, Location) is
more informative than inducing just win(WinningPlayer, OpponentPlayer). We
refer to this problem as Higher-order Relation Schema Induction (HRSI). In this
paper, we propose Tensor Factorization with Back-off and Aggregation (TFBA), a
novel framework for the HRSI problem. To the best of our knowledge, this is the
first attempt at inducing higher-order relation schemata from unlabeled text.
Using the experimental analysis on three real world datasets, we show how TFBA
helps in dealing with sparsity and induce higher order schemata.",Notations used in the paper.,What is the definition of a non-negative tensor?
spiqa_202,1803.02750v3,"In the context of TABLE I from the ""Efficient Synchronization of State-based CRDTs"" paper, how does GCounter differentiate itself from GSet in tracking periodic event occurrences compared to counting unique element additions?","GCounter measures the number of times an event has occurred, while GSet measures the number of unique elements in a set.",1803.02750v3.pdf,"['1803.02750v3.pdf', '1906.10843v1.pdf', '1710.05654v2.pdf', '1705.09882v2.pdf', '1804.07707v2.pdf', '1611.07718v2.pdf', '1809.00458v1.pdf', '1805.04687v2.pdf', '1804.05938v2.pdf', '1709.02418v2.pdf', '1803.03467v4.pdf', '1611.05742v3.pdf']","The table shows that GCounter is incremented each time a periodic event occurs, while GSet is only incremented when a unique element is added. This means that GCounter can be used to count the total number of events, while GSet can be used to count the number of unique events.",1803.02750v3-TableI-1.png,Efficient Synchronization of State-based CRDTs,"To ensure high availability in large scale distributed systems, Conflict-free
Replicated Data Types (CRDTs) relax consistency by allowing immediate query and
update operations at the local replica, with no need for remote
synchronization. State-based CRDTs synchronize replicas by periodically sending
their full state to other replicas, which can become extremely costly as the
CRDT state grows. Delta-based CRDTs address this problem by producing small
incremental states (deltas) to be used in synchronization instead of the full
state. However, current synchronisation algorithms for Delta-based CRDTs induce
redundant wasteful delta propagation, performing worse than expected, and
surprisingly, no better than State-based. In this paper we: 1) identify two
sources of inefficiency in current synchronization algorithms for delta-based
CRDTs; 2) bring the concept of join decomposition to state-based CRDTs; 3)
exploit join decompositions to obtain optimal deltas and 4) improve the
efficiency of synchronization algorithms; and finally, 5) evaluate the improved
algorithms.",TABLE I: Description of micro-benchmarks.,What is the difference between GCounter and GSet?
spiqa_203,1611.07718v2,"Based on the figure comparing (a) deep residual networks and (b) networks built by stacking inception-like blocks, how do the dashed projection shortcuts in residual networks enhance gradient flow compared to the absence of such connections in inception-like networks?","Deep residual networks have skip connections that allow the gradient to flow directly from one layer to another, while networks built by stacking inception-like blocks do not.",1611.07718v2.pdf,"['1611.07718v2.pdf', '1802.07222v1.pdf', '1812.00108v4.pdf', '1805.06447v3.pdf', '1803.03467v4.pdf', '1705.09882v2.pdf', '1709.08294v3.pdf', '1705.08016v3.pdf', '1901.00398v2.pdf']","The figure shows that deep residual networks have dashed lines connecting layers, which represent skip connections. Networks built by stacking inception-like blocks do not have these dashed lines.",1611.07718v2-Figure2-1.png,Deep Convolutional Neural Networks with Merge-and-Run Mappings,"A deep residual network, built by stacking a sequence of residual blocks, is
easy to train, because identity mappings skip residual branches and thus
improve information flow. To further reduce the training difficulty, we present
a simple network architecture, deep merge-and-run neural networks. The novelty
lies in a modularized building block, merge-and-run block, which assembles
residual branches in parallel through a merge-and-run mapping: Average the
inputs of these residual branches (Merge), and add the average to the output of
each residual branch as the input of the subsequent residual branch (Run),
respectively. We show that the merge-and-run mapping is a linear idempotent
function in which the transformation matrix is idempotent, and thus improves
information flow, making training easy. In comparison to residual networks, our
networks enjoy compelling advantages: they contain much shorter paths, and the
width, i.e., the number of channels, is increased. We evaluate the performance
on the standard recognition tasks. Our approach demonstrates consistent
improvements over ResNets with the comparable setup, and achieves competitive
results (e.g., $3.57\%$ testing error on CIFAR-$10$, $19.00\%$ on CIFAR-$100$,
$1.51\%$ on SVHN).","(a) a deep residual network; (b) a network built by stacking inception-like blocks; (c) our deep merge-and-run neural network built by stacking merge-and-run blocks. The trapezoid shape indicates that down-sampling occurs in the corresponding layer, and the dashed line denotes a projection shortcut as in [7].",What is the difference between a deep residual network and a network built by stacking inception-like blocks?
spiqa_204,1804.01429v3,"As depicted in Figure 1 of the ""Layout-Induced Video Representation"" paper, how does the specific place associated with an agent-in-place action, such as ""vehicle, move away, driveway,"" differentiate it from more general action categories that can occur in any location like ""walking"" or ""running""?","An agent-in-place action is an action that is performed by an agent in a specific place, while a generic action category is a more general category of action that does not specify the place where the action is performed.",1804.01429v3.pdf,"['1804.01429v3.pdf', '1901.00398v2.pdf', '1705.02946v3.pdf', '1707.00189v3.pdf', '1803.01128v3.pdf', '1706.00827v2.pdf', '1611.04684v1.pdf', '1708.06832v3.pdf', '1809.04276v2.pdf', '1709.02418v2.pdf', '1809.03550v3.pdf', '1704.07121v2.pdf', '1906.10843v1.pdf', '1811.07073v3.pdf']","The figure shows examples of agent-in-place actions, such as ""vehicle, move away (home), driveway"" and ""person, move along, sidewalk."" These actions are specific to the places where they are performed. In contrast, a generic action category, such as ""walking"" or ""running,"" does not specify the place where the action is performed.",1804.01429v3-Figure1-1.png,Layout-induced Video Representation for Recognizing Agent-in-Place Actions,"We address the recognition of agent-in-place actions, which are associated
with agents who perform them and places where they occur, in the context of
outdoor home surveillance. We introduce a representation of the geometry and
topology of scene layouts so that a network can generalize from the layouts
observed in the training set to unseen layouts in the test set. This
Layout-Induced Video Representation (LIVR) abstracts away low-level appearance
variance and encodes geometric and topological relationships of places in a
specific scene layout. LIVR partitions the semantic features of a video clip
into different places to force the network to learn place-based feature
descriptions; to predict the confidence of each action, LIVR aggregates
features from the place associated with an action and its adjacent places on
the scene layout. We introduce the Agent-in-Place Action dataset to show that
our method allows neural network models to generalize significantly better to
unseen scenes.","Figure 1. Example agent-in-place actions and segmentation maps. Different colors represent different places. We zoom in to the agents performing the actions for clarity. An agent-in-place action is represented as <agent, action, place>. Same colors indicate same place types (e.g., green for lawn, blue for walkway, etc.).","What is the difference between an ""agent-in-place"" action and a generic action category?"
spiqa_205,1812.10735v2,"Referring to the examples in Figure 6 of the CAN paper, how does the model's treatment of overlapping cases, where multiple aspects share an opinion snippet, differ from its handling of error cases, where an aspect or opinion is misidentified in multi-aspect sentiment analysis?","An overlapping case is when multiple aspects share the same opinion snippet, while an error case is when the model incorrectly identifies an aspect or opinion.",1812.10735v2.pdf,"['1812.10735v2.pdf', '1612.02803v5.pdf', '1710.05654v2.pdf', '1611.05742v3.pdf', '1804.01429v3.pdf', '1803.03467v4.pdf', '1605.07496v3.pdf', '1705.02946v3.pdf', '1709.00139v4.pdf', '1709.08294v3.pdf', '1701.03077v10.pdf']","The figure shows two examples of each case. In the overlapping case, the sentence contains two aspects, ""food"" and ""service,"" both described by the opinion snippet ""highly disappointed."" The model correctly identifies both aspects and the shared opinion words. In the error case, the model incorrectly identifies the aspect ""a/m"" and the opinion ""disappointing.""",1812.10735v2-Figure6-1.png,CAN: Constrained Attention Networks for Multi-Aspect Sentiment Analysis,"Aspect level sentiment classification is a fine-grained sentiment analysis
task. To detect the sentiment towards a particular aspect in a sentence,
previous studies have developed various attention-based methods for generating
aspect-specific sentence representations. However, the attention may inherently
introduce noise and downgrade the performance. In this paper, we propose
constrained attention networks (CAN), a simple yet effective solution, to
regularize the attention for multi-aspect sentiment analysis, which alleviates
the drawback of the attention mechanism. Specifically, we introduce orthogonal
regularization on multiple aspects and sparse regularization on each single
aspect. Experimental results on two public datasets demonstrate the
effectiveness of our approach. We further extend our approach to multi-task
settings and outperform the state-of-the-art methods.",Figure 6: Examples of overlapping case and error case. The a/m is short for anecdotes/miscellaneous.,What is the difference between an overlapping case and an error case?
spiqa_206,1605.07496v3,"In the contour plot of F-SRE1 and F-SRE2 presented in the ""Alternating Optimisation and Quadrature for Robust Control"" paper, why does the ""ALOQ"" curve consistently underestimate the ""True max"" curve, and what is the implication of this in terms of policy optimisation?","The ""True max"" curve is the true maximum of the function, while the ""ALOQ"" curve is an approximation of the maximum. The ""ALOQ"" curve is lower than the ""True max"" curve, indicating that it underestimates the maximum value of the function.",1605.07496v3.pdf,"['1605.07496v3.pdf', '1803.06506v3.pdf', '1805.07567v2.pdf', '1704.04539v2.pdf', '1710.06177v2.pdf', '1809.03449v3.pdf', '1805.01216v3.pdf', '1710.01507v4.pdf', '1901.00398v2.pdf', '1811.06635v1.pdf']","The figure shows that the ""True max"" curve is higher than the ""ALOQ"" curve at all points. This indicates that the ""ALOQ"" curve is an underestimate of the true maximum.",1605.07496v3-Figure9-1.png,Alternating Optimisation and Quadrature for Robust Control,"Bayesian optimisation has been successfully applied to a variety of
reinforcement learning problems. However, the traditional approach for learning
optimal policies in simulators does not utilise the opportunity to improve
learning by adjusting certain environment variables: state features that are
unobservable and randomly determined by the environment in a physical setting
but are controllable in a simulator. This paper considers the problem of
finding a robust policy while taking into account the impact of environment
variables. We present Alternating Optimisation and Quadrature (ALOQ), which
uses Bayesian optimisation and Bayesian quadrature to address such settings.
ALOQ is robust to the presence of significant rare events, which may not be
observable under random sampling, but play a substantial role in determining
the optimal policy. Experimental results across different domains show that
ALOQ can learn more efficiently and robustly than existing methods.",Contour plot of F-SRE1 and F-SRE2 (values in SRE region have been reduced by a factor of 10).,"What is the difference between the ""True max"" and the ""ALOQ"" curves?"
spiqa_207,1812.00281v3,"In the figure from the HUMBI paper that presents gaze, face, hand, and body appearance, how does the ""median appearance,"" representing the average of all multiview images, contrast with the ""view-specific appearance,"" which is rendered from a particular viewpoint?","The median appearance is the average of all the multiview images, while the view-specific appearance is a single image that is rendered from a specific viewpoint.",1812.00281v3.pdf,"['1812.00281v3.pdf', '1704.04539v2.pdf', '1809.00458v1.pdf', '1803.04572v2.pdf', '1703.04887v4.pdf', '1705.02946v3.pdf', '1705.02798v6.pdf', '1703.10730v2.pdf', '1906.10843v1.pdf']",The figure shows that the median appearance is a single image that represents the average of all the multiview images. The view-specific appearance is a single image that is rendered from a specific viewpoint. The variance image shows how much the different multiview images vary from the median appearance.,1812.00281v3-Figure4-1.png,HUMBI: A Large Multiview Dataset of Human Body Expressions,"This paper presents a new large multiview dataset called HUMBI for human body
expressions with natural clothing. The goal of HUMBI is to facilitate modeling
view-specific appearance and geometry of gaze, face, hand, body, and garment
from assorted people. 107 synchronized HD cameras are used to capture 772
distinctive subjects across gender, ethnicity, age, and physical condition.
With the multiview image streams, we reconstruct high fidelity body expressions
using 3D mesh models, which allows representing view-specific appearance using
their canonical atlas. We demonstrate that HUMBI is highly effective in
learning and reconstructing a complete human model and is complementary to the
existing datasets of human body expressions with limited views and subjects
such as MPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets.","View-specific appearance rendered from multiview images with median appearance and variance for (a) gaze, (b) face, (c) hand, (d) body.","What is the difference between the ""median appearance"" and the ""view-specific appearance""?"
spiqa_208,1705.07164v8,"Based on the figure illustrating the Bregman cost functions in the Relaxed WGANs paper, how does the Mahalanobis Bregman cost function incorporate the covariance matrix of the data compared to the Euclidean Bregman cost function, and what is the mathematical distinction between the two?","The Euclidean Bregman cost function is simply the squared difference between two points, while the Mahalanobis Bregman cost function takes into account the covariance of the data.",1705.07164v8.pdf,"['1705.07164v8.pdf', '1708.01425v4.pdf', '1812.00281v3.pdf', '1805.07567v2.pdf', '1804.07931v2.pdf', '1708.06832v3.pdf', '1611.02654v2.pdf']","The table shows that the Euclidean Bregman cost function is given by ||x - y||^2, while the Mahalanobis Bregman cost function is given by (x - y)^T A (x - y), where A is a positive semi-definite matrix that represents the covariance of the data. This means that the Mahalanobis Bregman cost function gives more weight to differences in directions where the data is more spread out.",1705.07164v8-Table1-1.png,Relaxed Wasserstein with Applications to GANs,"Wasserstein Generative Adversarial Networks (WGANs) provide a versatile class
of models, which have attracted great attention in various applications.
However, this framework has two main drawbacks: (i) Wasserstein-1 (or
Earth-Mover) distance is restrictive such that WGANs cannot always fit data
geometry well; (ii) It is difficult to achieve fast training of WGANs. In this
paper, we propose a new class of \textit{Relaxed Wasserstein} (RW) distances by
generalizing Wasserstein-1 distance with Bregman cost functions. We show that
RW distances achieve nice statistical properties while not sacrificing the
computational tractability. Combined with the GANs framework, we develop
Relaxed WGANs (RWGANs) which are not only statistically flexible but can be
approximated efficiently using heuristic approaches. Experiments on real images
demonstrate that the RWGAN with Kullback-Leibler (KL) cost function outperforms
other competing approaches, e.g., WGANs, even with gradient penalty.",Examples of the function φ and the resulting Bregman cost functions. Note that A 0 is positive semidefinite.,What is the difference between the Euclidean and Mahalanobis Bregman cost functions?
spiqa_209,1809.00263v5,"In the context of SDVI’s training figure, how do the Inference module’s input of the previous frame (Xt-1) and dynamic constraint (ĥt) compare to the Posterior module’s input of the current frame (Xt) for generating the sequence of plausible frames?","The Inference module takes the previous frame (Xt-1) and the dynamic constraint (ĥt) as input, while the Posterior module takes the current frame (Xt) as input. This means that the Inference module is trying to predict the next frame based on the previous frame and the dynamic constraint, while the Posterior module is trying to reconstruct the current frame.",1809.00263v5.pdf,"['1809.00263v5.pdf', '1805.02349v2.pdf', '1704.07121v2.pdf', '1805.04609v3.pdf', '1706.04269v2.pdf', '1811.08481v2.pdf', '1707.00189v3.pdf', '1804.04786v3.pdf', '1805.07567v2.pdf', '1611.02654v2.pdf']","The figure shows that the Inference module and the Posterior module have different inputs. The Inference module takes Xt-1 and ĥt as input, while the Posterior module takes Xt as input.",1809.00263v5-Figure3-1.png,Stochastic Dynamics for Video Infilling,"In this paper, we introduce a stochastic dynamics video infilling (SDVI)
framework to generate frames between long intervals in a video. Our task
differs from video interpolation which aims to produce transitional frames for
a short interval between every two frames and increase the temporal resolution.
Our task, namely video infilling, however, aims to infill long intervals with
plausible frame sequences. Our framework models the infilling as a constrained
stochastic generation process and sequentially samples dynamics from the
inferred distribution. SDVI consists of two parts: (1) a bi-directional
constraint propagation module to guarantee the spatial-temporal coherence among
frames, (2) a stochastic sampling process to generate dynamics from the
inferred distributions. Experimental results show that SDVI can generate clear
frame sequences with varying contents. Moreover, motions in the generated
sequence are realistic and able to transfer smoothly from the given start frame
to the terminal frame. Our project site is
https://xharlie.github.io/projects/project_sites/SDVI/video_results.html","Training of SDVI: All Encoder (green) share the same weights. The blue and the yellow network are Extractor and Decoder. Reference module creates dynamic constraint ĥt at each step. At step t, Inference module takes Xt−1 and ĥt, while Posterior module takes Xt. Inference module and Posterior module will produce different zt and therefore different output frames X̃infr",What is the difference between the Inference module and the Posterior module?
spiqa_210,1809.00458v1,"Based on the GB-KMV paper's TABLE I, how do the formulas for Jaccard similarity and containment similarity differ in how they measure the overlap between two sets, and what distinct aspects of set relationships do they each focus on?","The Jaccard similarity measures the overlap between two sets, while the containment similarity measures how much one set is contained within another set.",1809.00458v1.pdf,"['1809.00458v1.pdf', '1804.07707v2.pdf', '1804.07849v4.pdf', '1906.10843v1.pdf', '1804.00863v3.pdf', '1708.03797v1.pdf', '1611.02654v2.pdf']","The table defines the Jaccard similarity as J(Q, X) = |Q ∩ X| / |Q ∪ X|, which measures the ratio of the number of elements in the intersection of Q and X to the number of elements in the union of Q and X. The containment similarity is defined as C(Q, X) = |Q ∩ X| / |Q|, which measures the ratio of the number of elements in the intersection of Q and X to the number of elements in Q.",1809.00458v1-TableI-1.png,GB-KMV: An Augmented KMV Sketch for Approximate Containment Similarity Search,"In this paper, we study the problem of approximate containment similarity
search. Given two records Q and X, the containment similarity between Q and X
with respect to Q is |Q intersect X|/ |Q|. Given a query record Q and a set of
records S, the containment similarity search finds a set of records from S
whose containment similarity regarding Q are not less than the given threshold.
This problem has many important applications in commercial and scientific
fields such as record matching and domain search. Existing solution relies on
the asymmetric LSH method by transforming the containment similarity to
well-studied Jaccard similarity. In this paper, we use a different framework by
transforming the containment similarity to set intersection. We propose a novel
augmented KMV sketch technique, namely GB-KMV, which is data-dependent and can
achieve a good trade-off between the sketch size and the accuracy. We provide a
set of theoretical analysis to underpin the proposed augmented KMV sketch
technique, and show that it outperforms the state-of-the-art technique LSH-E in
terms of estimation accuracy under practical assumption. Our comprehensive
experiments on real-life datasets verify that GB-KMV is superior to LSH-E in
terms of the space-accuracy trade-off, time-accuracy trade-off, and the sketch
construction time. For instance, with similar estimation accuracy (F-1 score),
GB-KMV is over 100 times faster than LSH-E on some real-life dataset.",TABLE I. THE SUMMARY OF NOTATIONS,What is the difference between the Jaccard similarity and the containment similarity?
spiqa_211,1803.02750v3,"Referring to the figure specifications of the Grow-only Counter data type in *Efficient Synchronization of State-based CRDTs*, how do the `inc_i(p)` and `inc_i'(p)` operations differ in their treatment of the key `i` when incrementing the counter `p`?","The `inc_i(p)` operation increments the value associated with the key `i` in the counter `p`, while the `inc_i'(p)` operation increments the value associated with the key `i` in the counter `p` only if the key `i` is not already present in the counter.",1803.02750v3.pdf,"['1803.02750v3.pdf', '1709.02418v2.pdf', '1805.04609v3.pdf', '1811.08257v1.pdf', '1703.10730v2.pdf', '1809.00458v1.pdf', '1804.07931v2.pdf', '1906.10843v1.pdf', '1705.09296v2.pdf', '1812.10735v2.pdf']","The figure shows the specifications of the Grow-only Counter data type. The `inc_i(p)` operation is defined as `p[i ↦ p(i) + 1]`, which means that it increments the value associated with the key `i` in the counter `p`. The `inc_i'(p)` operation is defined as `[{i ↦ p(i) + 1}]`, which means that it increments the value associated with the key `i` in the counter `p` only if the key `i` is not already present in the counter.",1803.02750v3-Figure2-1.png,Efficient Synchronization of State-based CRDTs,"To ensure high availability in large scale distributed systems, Conflict-free
Replicated Data Types (CRDTs) relax consistency by allowing immediate query and
update operations at the local replica, with no need for remote
synchronization. State-based CRDTs synchronize replicas by periodically sending
their full state to other replicas, which can become extremely costly as the
CRDT state grows. Delta-based CRDTs address this problem by producing small
incremental states (deltas) to be used in synchronization instead of the full
state. However, current synchronisation algorithms for Delta-based CRDTs induce
redundant wasteful delta propagation, performing worse than expected, and
surprisingly, no better than State-based. In this paper we: 1) identify two
sources of inefficiency in current synchronization algorithms for delta-based
CRDTs; 2) bring the concept of join decomposition to state-based CRDTs; 3)
exploit join decompositions to obtain optimal deltas and 4) improve the
efficiency of synchronization algorithms; and finally, 5) evaluate the improved
algorithms.","Specifications of two data types, replica i ∈ I.",What is the difference between the `inc_i(p)` and `inc_i'(p)` operations in the Grow-only Counter data type?
spiqa_212,1805.01216v3,"In Figure 7 of the ""Disentangling Language and Knowledge in Task-Oriented Dialogs"" paper, how do the attention weights for relevant memory entries, such as ""rest_3_str"" and ""rating 3"", differ between the two-level and one-level attention models in terms of focus and selectivity?"," The two-level attention model has higher attention weights on the relevant information in the memory, while the one-level attention model has more uniform attention weights.",1805.01216v3.pdf,"['1805.01216v3.pdf', '1605.07496v3.pdf', '1804.05938v2.pdf', '1708.05239v3.pdf']"," The figure shows that the two-level attention model has higher attention weights on the entries for ""rest_3_str"" and ""rating 3"", which are relevant to the current decoder prediction. In contrast, the one-level attention model has more uniform attention weights across all the entries in the memory. This suggests that the two-level attention model is able to focus on the most relevant information in the memory, while the one-level attention model is not as selective.",1805.01216v3-Figure7-1.png,Disentangling Language and Knowledge in Task-Oriented Dialogs,"The Knowledge Base (KB) used for real-world applications, such as booking a
movie or restaurant reservation, keeps changing over time. End-to-end neural
networks trained for these task-oriented dialogs are expected to be immune to
any changes in the KB. However, existing approaches breakdown when asked to
handle such changes. We propose an encoder-decoder architecture (BoSsNet) with
a novel Bag-of-Sequences (BoSs) memory, which facilitates the disentangled
learning of the response's language model and its knowledge incorporation.
Consequently, the KB can be modified with new knowledge without a drop in
interpretability. We find that BoSsNet outperforms state-of-the-art models,
with considerable improvements (> 10\%) on bAbI OOV test sets and other
human-human datasets. We also systematically modify existing datasets to
measure disentanglement and show BoSsNet to be robust to KB modifications.",Figure 7: Visualization of attention weights on selected portions of memory in (a) BOSSNET with two-level attention vs (b) BOSSNET with one-level attention,What is the difference between the attention weights in the two-level attention model and the one-level attention model?
spiqa_213,1706.04284v3,"According to Figure (a) in the paper, how does the proposed framework for joint image denoising and semantic segmentation alter the processing of noisy input images compared to the conventional semantic segmentation pipeline?","The conventional semantic segmentation pipeline performs semantic segmentation directly on the noisy input image, while the proposed framework first denoises the image before performing semantic segmentation.",1706.04284v3.pdf,"['1706.04284v3.pdf', '1701.06171v4.pdf', '1803.04572v2.pdf', '1803.05776v2.pdf', '1809.03149v2.pdf']","This can be seen in Figure (a), which shows the two pipelines side-by-side. The conventional pipeline has a single step, where the noisy input image is fed into a semantic segmentation network. The proposed framework has two steps: first, the noisy input image is denoised using a denoising network, and then the denoised image is fed into a semantic segmentation network.",1706.04284v3-Figure1-1.png,When Image Denoising Meets High-Level Vision Tasks: A Deep Learning Approach,"Conventionally, image denoising and high-level vision tasks are handled
separately in computer vision. In this paper, we cope with the two jointly and
explore the mutual influence between them. First we propose a convolutional
neural network for image denoising which achieves the state-of-the-art
performance. Second we propose a deep neural network solution that cascades two
modules for image denoising and various high-level tasks, respectively, and use
the joint loss for updating only the denoising network via back-propagation. We
demonstrate that on one hand, the proposed denoiser has the generality to
overcome the performance degradation of different high-level vision tasks. On
the other hand, with the guidance of high-level vision information, the
denoising network can generate more visually appealing results. To the best of
our knowledge, this is the first work investigating the benefit of exploiting
image semantics simultaneously for image denoising and high-level vision tasks
via deep learning. The code is available online
https://github.com/Ding-Liu/DeepDenoising.","(a) Upper: conventional semantic segmentation pipeline; lower: our proposed framework for joint image denoising and semantic segmentation. (b) Zoom-in regions of a noisy input, its denoised estimates using CBM3D and our proposed method, as well as its ground truth.",What is the difference between the conventional semantic segmentation pipeline and the proposed framework for joint image denoising and semantic segmentation?
spiqa_214,1707.00524v2,"In the action-conditional prediction model shown in the figure, how does the encoder network process the current state and action inputs into a latent representation, and how does this differ from the decoder network’s role in predicting the next state from that latent representation?",The encoder network takes a one-hot action and the current state as input and outputs a latent representation of the state. The decoder network takes the latent representation and outputs a prediction of the next state.,1707.00524v2.pdf,"['1707.00524v2.pdf', '1812.00108v4.pdf', '1705.09882v2.pdf', '1707.08608v3.pdf', '1705.10667v4.pdf', '1707.01922v5.pdf', '1804.07849v4.pdf', '1709.02755v5.pdf', '1805.06447v3.pdf', '1809.01989v2.pdf', '1812.00281v3.pdf']","The figure shows that the encoder network has a smaller input size than the decoder network. This is because the encoder network is only taking in the current state and the action, while the decoder network is taking in the latent representation of the state, which is a more compact representation of the information.",1707.00524v2-Figure2-1.png,Hashing over Predicted Future Frames for Informed Exploration of Deep Reinforcement Learning,"In deep reinforcement learning (RL) tasks, an efficient exploration mechanism
should be able to encourage an agent to take actions that lead to less frequent
states which may yield higher accumulative future return. However, both knowing
about the future and evaluating the frequentness of states are non-trivial
tasks, especially for deep RL domains, where a state is represented by
high-dimensional image frames. In this paper, we propose a novel informed
exploration framework for deep RL, where we build the capability for an RL
agent to predict over the future transitions and evaluate the frequentness for
the predicted future frames in a meaningful manner. To this end, we train a
deep prediction model to predict future frames given a state-action pair, and a
convolutional autoencoder model to hash over the seen frames. In addition, to
utilize the counts derived from the seen frames to evaluate the frequentness
for the predicted frames, we tackle the challenge of matching the predicted
future frames and their corresponding seen frames at the latent feature level.
In this way, we derive a reliable metric for evaluating the novelty of the
future direction pointed by each action, and hence inform the agent to explore
the least frequent one.",Deep neural network architectures adopted for informed exploration. Up: action-conditional prediction model for predicting over future transition frames; down: autoencoder model for conducting hashing over the state space.,What is the difference between the encoder and decoder networks in the action-conditional prediction model?
spiqa_215,1705.09882v2,"What differences in filter responses across the “conv1”, “conv2”, and “conv3” layers for a TUM GAID dataset frame are highlighted in the figure of *Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-Identification*, when comparing an RGB-based re-identification framework [82] to the depth-exclusive fCNN model in Fig. 3?","The filter responses from the “conv1”, “conv2” and “conv3” layers for a given frame from the TUM GAID data using a framework for person re-identification from RGB are more detailed and contain more information than the filter responses from the fCNN of a framework that utilizes depth data. This is because RGB images contain more information than depth images.",1705.09882v2.pdf,"['1705.09882v2.pdf', '1804.05995v2.pdf', '1706.00827v2.pdf', '1606.07384v2.pdf', '1901.00056v2.pdf', '1809.00458v1.pdf', '1706.00633v4.pdf', '1709.08294v3.pdf', '1803.01128v3.pdf', '1611.02654v2.pdf', '1811.10673v1.pdf']","The figure shows the filter responses from the “conv1”, “conv2” and “conv3” layers for a given frame from the TUM GAID data using a framework for person re-identification from RGB and the feature embedding fCNN of a framework that utilizes depth data. The filter responses from the RGB framework are more detailed and contain more information than the filter responses from the depth framework.",1705.09882v2-Figure1-1.png,Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-Identification,"We address the problem of person re-identification from commodity depth
sensors. One challenge for depth-based recognition is data scarcity. Our first
contribution addresses this problem by introducing split-rate RGB-to-Depth
transfer, which leverages large RGB datasets more effectively than popular
fine-tuning approaches. Our transfer scheme is based on the observation that
the model parameters at the bottom layers of a deep convolutional neural
network can be directly shared between RGB and depth data while the remaining
layers need to be fine-tuned rapidly. Our second contribution enhances
re-identification for video by implementing temporal attention as a
Bernoulli-Sigmoid unit acting upon frame-level features. Since this unit is
stochastic, the temporal attention parameters are trained using reinforcement
learning. Extensive experiments validate the accuracy of our method in person
re-identification from depth sequences. Finally, in a scenario where subjects
wear unseen clothes, we show large performance gains compared to a
state-of-the-art model which relies on RGB data.","Filter responses from “conv1” (upper right), “conv2” (bottom left) and “conv3” (bottom right) layers for a given frame from the TUM GAID data using (a) a framework for person re-identification from RGB [82] and (b) the feature embedding fCNN of our framework, which is drawn in Fig. 3 and exclusively utilizes depth data.","What is the difference between the filter responses from the “conv1”, “conv2” and “conv3” layers for a given frame from the TUM GAID data using a framework for person re-identification from RGB and the feature embedding fCNN of a framework that utilizes depth data?"
spiqa_216,1705.09882v2,"Based on the figure showing the grayscale depth map Dg p and the background-subtracted image using body index information Bp from skeleton tracking, what key visual differences highlight how the foreground is isolated from the background after the subtraction process?"," The grayscale depth representation shows the depth of each pixel in the image, with darker pixels representing closer objects and lighter pixels representing further objects. The result after background subtraction shows only the foreground object, with the background removed.",1705.09882v2.pdf,"['1705.09882v2.pdf', '1803.03467v4.pdf', '1703.10730v2.pdf', '1804.00863v3.pdf', '1703.00899v2.pdf', '1708.00160v2.pdf', '1611.05742v3.pdf', '1708.06832v3.pdf', '1811.02721v3.pdf', '1804.04410v2.pdf', '1705.07384v2.pdf', '1811.07073v3.pdf', '1809.00458v1.pdf']"," The figure shows the original image, the grayscale depth representation, and the result after background subtraction. The grayscale depth representation shows that the person in the foreground is closer to the camera than the person in the background. The result after background subtraction shows that the background has been removed, leaving only the person in the foreground.",1705.09882v2-Figure2-1.png,Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-Identification,"We address the problem of person re-identification from commodity depth
sensors. One challenge for depth-based recognition is data scarcity. Our first
contribution addresses this problem by introducing split-rate RGB-to-Depth
transfer, which leverages large RGB datasets more effectively than popular
fine-tuning approaches. Our transfer scheme is based on the observation that
the model parameters at the bottom layers of a deep convolutional neural
network can be directly shared between RGB and depth data while the remaining
layers need to be fine-tuned rapidly. Our second contribution enhances
re-identification for video by implementing temporal attention as a
Bernoulli-Sigmoid unit acting upon frame-level features. Since this unit is
stochastic, the temporal attention parameters are trained using reinforcement
learning. Extensive experiments validate the accuracy of our method in person
re-identification from depth sequences. Finally, in a scenario where subjects
wear unseen clothes, we show large performance gains compared to a
state-of-the-art model which relies on RGB data.","The cropped color image (left), the grayscale depth representation Dg p (center) and the result after background subtraction (right) using the body index information Bp from skeleton tracking.",What is the difference between the grayscale depth representation and the result after background subtraction?
spiqa_217,1701.06171v4,"How does the hierarchical part dictionary shown in Figure (b) of your framework, which is learned in the bottom-up process, differ from the holistic object model depicted in Figure (c) after the top-down process, in terms of how they represent the object structure in the ""Greedy Structure Learning of Hierarchical Compositional Models"" paper?",The hierarchical part dictionary learned with the bottom-up process is a set of parts that can be combined to create objects. The holistic object model learned with the top-down process is a single model that represents the entire object.,1701.06171v4.pdf,"['1701.06171v4.pdf', '1706.08146v3.pdf', '1705.08016v3.pdf', '1809.01246v1.pdf', '1611.05742v3.pdf', '1804.05938v2.pdf', '1703.07015v3.pdf', '1811.08257v1.pdf', '1805.04687v2.pdf', '1710.06177v2.pdf', '1805.07567v2.pdf', '1707.01922v5.pdf', '1708.02153v2.pdf', '1705.09882v2.pdf', '1706.04284v3.pdf']",The figure shows that the hierarchical part dictionary is a set of parts that are arranged in a hierarchy. The holistic object model is a single model that represents the entire object.,1701.06171v4-Figure5-1.png,Greedy Structure Learning of Hierarchical Compositional Models,"In this work, we consider the problem of learning a hierarchical generative
model of an object from a set of images which show examples of the object in
the presence of variable background clutter. Existing approaches to this
problem are limited by making strong a-priori assumptions about the object's
geometric structure and require segmented training data for learning. In this
paper, we propose a novel framework for learning hierarchical compositional
models (HCMs) which do not suffer from the mentioned limitations. We present a
generalized formulation of HCMs and describe a greedy structure learning
framework that consists of two phases: Bottom-up part learning and top-down
model composition. Our framework integrates the foreground-background
segmentation problem into the structure learning task via a background model.
As a result, we can jointly optimize for the number of layers in the hierarchy,
the number of parts per layer and a foreground-background segmentation based on
class labels only. We show that the learned HCMs are semantically meaningful
and achieve competitive results when compared to other generative object models
at object classification on a standard transfer learning dataset.","Learned hierarchical compositional models. (a) Samples from the training data. (b) The hierarchical part dictionary learned with our the bottom-up process. (c) The holistic object model after the top-down process. (d) The HCM learned with the HABM approach [5]. The gray squares indicate the parts of their HCM. Compared to the HABM, our method is able to learn the number of parts and layers of the hierarchy. Both approaches are not able to learn the holistic structure of the windmill due to the strong relative rotation between its parts.",What is the difference between the hierarchical part dictionary learned with the bottom-up process and the holistic object model learned with the top-down process?
spiqa_218,1705.09966v2,"In the context of Fig. 13 from ""Attribute-Guided Face Generation Using Conditional CycleGAN,"" what are the specific roles and characteristics of the low-resolution frontal face image and high-resolution side-face image used as inputs, and how are they transformed into the final high-resolution frontal face output?",The input is a low-resolution frontal face image and a high-resolution side-face image. The output is a high-resolution frontal face image.,1705.09966v2.pdf,"['1705.09966v2.pdf', '1709.00139v4.pdf', '1708.02153v2.pdf', '1707.00189v3.pdf', '1803.02750v3.pdf', '1811.02553v4.pdf', '1703.07015v3.pdf', '1805.04687v2.pdf', '1608.02784v2.pdf', '1805.07567v2.pdf', '1703.04887v4.pdf', '1706.08146v3.pdf', '1811.10673v1.pdf']",The figure shows that the input to the frontal face generation process is a low-resolution frontal face image and a high-resolution side-face image. The output is a high-resolution frontal face image that is consistent with the input side-face image.,1705.09966v2-Figure13-1.png,Attribute-Guided Face Generation Using Conditional CycleGAN,"We are interested in attribute-guided face generation: given a low-res face
input image, an attribute vector that can be extracted from a high-res image
(attribute image), our new method generates a high-res face image for the
low-res input that satisfies the given attributes. To address this problem, we
condition the CycleGAN and propose conditional CycleGAN, which is designed to
1) handle unpaired training data because the training low/high-res and high-res
attribute images may not necessarily align with each other, and to 2) allow
easy control of the appearance of the generated face via the input attributes.
We demonstrate impressive results on the attribute-guided conditional CycleGAN,
which can synthesize realistic face images with appearance easily controlled by
user-supplied attributes (e.g., gender, makeup, hair color, eyeglasses). Using
the attribute image as identity to produce the corresponding conditional vector
and by incorporating a face verification network, the attribute-guided network
becomes the identity-guided conditional CycleGAN which produces impressive and
interesting results on identity transfer. We demonstrate three applications on
identity-guided conditional CycleGAN: identity-preserving face superresolution,
face swapping, and frontal face generation, which consistently show the
advantage of our new method.","Fig. 13. Frontal face generation. Given a low-res template (a), our method can generate corresponding frontal faces from different side faces, e.g., (b) to (c), (d) to (e).",What is the difference between the input and output of the frontal face generation process?
spiqa_219,1608.02784v2,"Based on the figure comparing the SMT and CCA inference outputs, how do the CCA-generated descriptions better capture the contextual nuances of the abstract scenes compared to the more literal and potentially awkward translations from the SMT system?","The SMT outputs are literal translations of the images, while the CCA outputs take into account the context of the images and generate more natural-sounding descriptions.",1608.02784v2.pdf,"['1608.02784v2.pdf', '1708.06832v3.pdf', '1611.05742v3.pdf', '1809.00458v1.pdf', '1705.07164v8.pdf', '1706.00827v2.pdf']","The figure shows examples of outputs from both systems, side-by-side. The SMT outputs are often grammatically incorrect or awkward, while the CCA outputs are more fluent and natural. For example, in the first image, the SMT output is ""jenny is waving at mike,"" while the CCA output is ""mike and jenny are camping."" The CCA output is more informative and natural-sounding because it takes into account the context of the image, which shows two people camping.",1608.02784v2-Figure6-1.png,Canonical Correlation Inference for Mapping Abstract Scenes to Text,"We describe a technique for structured prediction, based on canonical
correlation analysis. Our learning algorithm finds two projections for the
input and the output spaces that aim at projecting a given input and its
correct output into points close to each other. We demonstrate our technique on
a language-vision problem, namely the problem of giving a textual description
to an ""abstract scene"".","Examples of outputs from the machine translation system and from CCA inference. The top three images give examples where the CCA inference outputs were rated highly by human evaluations (4 or 5), and the SMT ones were rated poorly (1 or 2). The bottom three pictures give the reverse case.",What is the difference between the outputs of the machine translation system (SMT) and the CCA inference?
spiqa_220,1704.04539v2,"In Figure 1(f) of the paper, which illustrates thematic divergence, how does the argument structure of the AMR parsing tree for ""I like eating"" differ from that of ""I like grapes""?","The parsing tree for ""I like eating"" has only one argument, while the parsing tree for ""I like grapes"" has two arguments.",1704.04539v2.pdf,"['1704.04539v2.pdf', '1706.03847v3.pdf', '1804.07849v4.pdf', '1805.06431v4.pdf', '1701.03077v10.pdf', '1611.07718v2.pdf', '1705.02798v6.pdf', '1710.05654v2.pdf', '1704.08615v2.pdf', '1611.05742v3.pdf', '1611.04684v1.pdf', '1811.08257v1.pdf', '1704.07854v4.pdf', '1705.02946v3.pdf']","The parsing tree for ""I like eating"" shows that the verb ""like"" has only one argument, which is the pronoun ""I"". The parsing tree for ""I like grapes"" shows that the verb ""like"" has two arguments: the pronoun ""I"" and the noun ""grapes"". This difference in the number of arguments is an example of a thematic divergence.",1704.04539v2-Figure4-1.png,Cross-lingual Abstract Meaning Representation Parsing,"Abstract Meaning Representation (AMR) annotation efforts have mostly focused
on English. In order to train parsers on other languages, we propose a method
based on annotation projection, which involves exploiting annotations in a
source language and a parallel corpus of the source language and a target
language. Using English as the source language, we show promising results for
Italian, Spanish, German and Chinese as target languages. Besides evaluating
the target parsers on non-gold datasets, we further propose an evaluation
method that exploits the English gold annotations and does not require access
to gold annotations for the target languages. This is achieved by inverting the
projection process: a new English parser is learned from the target language
parser and evaluated on the existing English gold standard.","Parsing examples in several languages involving common translational divergence phenomena: (a) contains a categorical divergence, (b) and (e) conflational divergences, (c) a structural divergence, (d) an head swapping and (f) a thematic divergence.","What is the difference between the parsing trees for ""I like eating"" and ""I like grapes""?"
spiqa_221,1603.03833v4,"How does the Baxter robot's performance in the pick and place task differ between simulation (first row) and real-world execution (second row), specifically in terms of precision and errors in object handling, as depicted in the figure?","In the simulation, the robot is able to pick up the object and place it in the desired location without any errors. However, in the real world, the robot makes some errors, such as dropping the object or placing it in the wrong location.",1603.03833v4.pdf,"['1603.03833v4.pdf', '1707.01917v2.pdf', '1805.08751v2.pdf', '1708.00160v2.pdf', '1804.04786v3.pdf', '1605.07496v3.pdf', '1901.00056v2.pdf', '1704.05426v4.pdf', '1804.04410v2.pdf', '1802.07459v2.pdf']","The first row of images shows the pick and place task in simulation, while the second row shows the same task in the real world. The images in the real world show that the robot is not as precise as it is in the simulation.",1603.03833v4-Figure4-1.png,From virtual demonstration to real-world manipulation using LSTM and MDN,"Robots assisting the disabled or elderly must perform complex manipulation
tasks and must adapt to the home environment and preferences of their user.
Learning from demonstration is a promising choice, that would allow the
non-technical user to teach the robot different tasks. However, collecting
demonstrations in the home environment of a disabled user is time consuming,
disruptive to the comfort of the user, and presents safety challenges. It would
be desirable to perform the demonstrations in a virtual environment. In this
paper we describe a solution to the challenging problem of behavior transfer
from virtual demonstration to a physical robot. The virtual demonstrations are
used to train a deep neural network based controller, which is using a Long
Short Term Memory (LSTM) recurrent neural network to generate trajectories. The
training process uses a Mixture Density Network (MDN) to calculate an error
signal suitable for the multimodal nature of demonstrations. The controller
learned in the virtual environment is transferred to a physical robot (a
Rethink Robotics Baxter). An off-the-shelf vision component is used to
substitute for geometric knowledge available in the simulation and an inverse
kinematics module is used to allow the Baxter to enact the trajectory. Our
experimental studies validate the three contributions of the paper: (1) the
controller learned from virtual demonstrations can be used to successfully
perform the manipulation tasks on a physical robot, (2) the LSTM+MDN
architectural choice outperforms other choices, such as the use of feedforward
networks and mean-squared error based training signals and (3) allowing
imperfect demonstrations in the training set also allows the controller to
learn how to correct its manipulation mistakes.","A sequence of images showing the autonomous execution of pick and place in simulation (first row), pick and place in real world (second row), pushing in simulation (third row), and pushing in real world (fourth row). The robot is controlled by a mixture density network with 3 layers of LSTM.",What is the difference between the pick and place task in simulation and the real world?
spiqa_222,1707.00524v2,"In the figure comparing the ground-truth, predicted, and reconstructed frames for each task domain, how does the accuracy of the predicted frames generated by the prediction model compare to the reconstructed frames produced by the autoencoder, especially after the second training phase where both reconstruction loss and code matching loss are applied?","The predicted frame is generated by the prediction model, while the reconstructed frame is generated by the autoencoder. The predicted frame is typically more accurate than the reconstructed frame, as the prediction model is trained to predict the future state of the environment, while the autoencoder is only trained to reconstruct the input image.",1707.00524v2.pdf,"['1707.00524v2.pdf', '1707.06320v2.pdf', '1708.05239v3.pdf', '1706.03847v3.pdf']","The figure shows the ground-truth frame, the predicted frame, and the reconstructed frame for each task domain. The predicted frame is typically very similar to the ground-truth frame, while the reconstructed frame is often slightly blurred.",1707.00524v2-Figure3-1.png,Hashing over Predicted Future Frames for Informed Exploration of Deep Reinforcement Learning,"In deep reinforcement learning (RL) tasks, an efficient exploration mechanism
should be able to encourage an agent to take actions that lead to less frequent
states which may yield higher accumulative future return. However, both knowing
about the future and evaluating the frequentness of states are non-trivial
tasks, especially for deep RL domains, where a state is represented by
high-dimensional image frames. In this paper, we propose a novel informed
exploration framework for deep RL, where we build the capability for an RL
agent to predict over the future transitions and evaluate the frequentness for
the predicted future frames in a meaningful manner. To this end, we train a
deep prediction model to predict future frames given a state-action pair, and a
convolutional autoencoder model to hash over the seen frames. In addition, to
utilize the counts derived from the seen frames to evaluate the frequentness
for the predicted frames, we tackle the challenge of matching the predicted
future frames and their corresponding seen frames at the latent feature level.
In this way, we derive a reliable metric for evaluating the novelty of the
future direction pointed by each action, and hence inform the agent to explore
the least frequent one.","The prediction and reconstruction result for each task domain. For each task, we present 1 set of frames, where the four frames are organized as follows: (1) the ground-truth frame seen by the agent; (2) the predicted frame by the prediction model; (3) the reconstruction of autoencoder trained only with reconstruction loss; (4) the reconstruction of autoencoder trained after the second phase (i.e., trained with both reconstruction loss and code matching loss). Overall, the prediction model could perfectly produce frame output, while the fully trained autoencoder generates slightly blurred frames.",What is the difference between the predicted frame and the reconstructed frame for each task domain?
spiqa_223,1804.00863v3,"Based on the architectures depicted in Figure [number] of the ""Deep Appearance Maps"" paper, how does the fixed-channel representation module contrast with the adaptive-channel learning-to-learn module in terms of their respective roles in the appearance estimation and segmentation task?",The representation module takes an input image and outputs a feature representation. The learning-to-learn module takes a set of features and learns how to segment the image.,1804.00863v3.pdf,"['1804.00863v3.pdf', '1811.07073v3.pdf', '1803.04383v2.pdf', '1804.01429v3.pdf', '1704.05426v4.pdf']","The figure shows that the representation module has a fixed number of channels, while the learning-to-learn module has a variable number of channels. This suggests that the learning-to-learn module is more flexible and can adapt to different types of images.",1804.00863v3-Figure4-1.png,Deep Appearance Maps,"We propose a deep representation of appearance, i. e., the relation of color,
surface orientation, viewer position, material and illumination. Previous
approaches have useddeep learning to extract classic appearance
representationsrelating to reflectance model parameters (e. g., Phong)
orillumination (e. g., HDR environment maps). We suggest todirectly represent
appearance itself as a network we call aDeep Appearance Map (DAM). This is a 4D
generalizationover 2D reflectance maps, which held the view direction fixed.
First, we show how a DAM can be learned from images or video frames and later
be used to synthesize appearance, given new surface orientations and viewer
positions. Second, we demonstrate how another network can be used to map from
an image or video frames to a DAM network to reproduce this appearance, without
using a lengthy optimization such as stochastic gradient descent
(learning-to-learn). Finally, we show the example of an appearance
estimation-and-segmentation task, mapping from an image showingmultiple
materials to multiple deep appearance maps.",The four architectures used.,What is the difference between the representation module and the learning-to-learn module?
spiqa_224,1804.00863v3,"In the ""Deep Appearance Maps"" paper, as depicted in the figure showing different appearance processing tasks, how does the input-output relationship of the task mapping normal and view directions to RGB values differ from the task that uses an image to predict a DAM representation?","The representation task takes an appearance as input and outputs an RGB value, while the learning-to-learn task takes an image as input and outputs a DAM representation.",1804.00863v3.pdf,"['1804.00863v3.pdf', '1710.05654v2.pdf', '1701.03077v10.pdf', '1611.04684v1.pdf', '1708.03797v1.pdf', '1709.02755v5.pdf', '1809.03449v3.pdf', '1804.05936v2.pdf', '1809.03550v3.pdf', '1805.06447v3.pdf', '1805.07567v2.pdf', '1811.08481v2.pdf', '1811.10673v1.pdf', '1803.04572v2.pdf', '1709.00139v4.pdf']","The figure shows that the representation task takes a normal and view direction as input, while the learning-to-learn task takes an image as input.",1804.00863v3-Figure3-1.png,Deep Appearance Maps,"We propose a deep representation of appearance, i. e., the relation of color,
surface orientation, viewer position, material and illumination. Previous
approaches have useddeep learning to extract classic appearance
representationsrelating to reflectance model parameters (e. g., Phong)
orillumination (e. g., HDR environment maps). We suggest todirectly represent
appearance itself as a network we call aDeep Appearance Map (DAM). This is a 4D
generalizationover 2D reflectance maps, which held the view direction fixed.
First, we show how a DAM can be learned from images or video frames and later
be used to synthesize appearance, given new surface orientations and viewer
positions. Second, we demonstrate how another network can be used to map from
an image or video frames to a DAM network to reproduce this appearance, without
using a lengthy optimization such as stochastic gradient descent
(learning-to-learn). Finally, we show the example of an appearance
estimation-and-segmentation task, mapping from an image showingmultiple
materials to multiple deep appearance maps.","Different appearance processing tasks that we address using our deep appearance maps. a) The first task simply reproduces a given appearance, i. e., it maps from normal and view directions to RGB values using a NN. b) In a learning-to-learn task a network maps an image to a DAM representation. c) Finally, in the segmentation-and-estimation task, a network maps an image to multiple DAMs and multiple segmentation networks.",What is the difference between the representation task and the learning-to-learn task?
spiqa_225,1611.07718v2,"Based on the figure showing the building blocks of the network in the Deep Convolutional Networks paper, what is the main structural difference in how the residual branches are connected in the residual block compared to the merge-and-run block?","The residual block assembles two residual branches sequentially, while the merge-and-run block assembles the same two residual branches in parallel.",1611.07718v2.pdf,"['1611.07718v2.pdf', '1704.07121v2.pdf', '1705.10667v4.pdf', '1701.06171v4.pdf', '1805.07567v2.pdf', '1804.05995v2.pdf', '1803.04572v2.pdf', '1611.04363v2.pdf', '1704.05958v2.pdf']","The figure shows the different building blocks of a convolutional neural network. The residual block (a) has two residual branches that are connected sequentially, while the merge-and-run block (c) has two residual branches that are connected in parallel.",1611.07718v2-Figure1-1.png,Deep Convolutional Neural Networks with Merge-and-Run Mappings,"A deep residual network, built by stacking a sequence of residual blocks, is
easy to train, because identity mappings skip residual branches and thus
improve information flow. To further reduce the training difficulty, we present
a simple network architecture, deep merge-and-run neural networks. The novelty
lies in a modularized building block, merge-and-run block, which assembles
residual branches in parallel through a merge-and-run mapping: Average the
inputs of these residual branches (Merge), and add the average to the output of
each residual branch as the input of the subsequent residual branch (Run),
respectively. We show that the merge-and-run mapping is a linear idempotent
function in which the transformation matrix is idempotent, and thus improves
information flow, making training easy. In comparison to residual networks, our
networks enjoy compelling advantages: they contain much shorter paths, and the
width, i.e., the number of channels, is increased. We evaluate the performance
on the standard recognition tasks. Our approach demonstrates consistent
improvements over ResNets with the comparable setup, and achieves competitive
results (e.g., $3.57\%$ testing error on CIFAR-$10$, $19.00\%$ on CIFAR-$100$,
$1.51\%$ on SVHN).","Illustrating the building blocks: (a) Two residual blocks; (b) An inception-like block; (c) A merge-and-run block. (a) corresponds to two blocks in ResNets and assembles two residual branches sequentially while (b) and (c) both assemble the same two residual branches in parallel. (b) and (c) adopt two different skip connections: identity mappings and our proposed merge-andrun mappings. The dot circle denotes the average operation, and the solid circle denotes the sum operation.",What is the difference between the residual block and the merge-and-run block?
spiqa_226,1804.04786v3,"How does the figure comparing the ""sequential,"" ""frame-to-frame,"" and ""recurrent"" generation schemes in the TCD-TIMIT dataset illustrate the smoother pixel movement and frame continuity achieved by the recurrent generation scheme, as depicted by the optical flow images?","The sequential generation scheme generates each frame of the video independently, while the recurrent generation scheme uses the previous frame to generate the next frame. This can be seen in the optical flow images, which show the motion of pixels between frames. The recurrent scheme has a smoother flow of motion, while the sequential scheme has more abrupt changes.",1804.04786v3.pdf,"['1804.04786v3.pdf', '1804.04410v2.pdf', '1901.00056v2.pdf', '1805.08751v2.pdf', '1702.03584v3.pdf', '1708.02153v2.pdf', '1707.00524v2.pdf', '1605.07496v3.pdf', '1906.06589v3.pdf', '1603.03833v4.pdf', '1708.03797v1.pdf', '1704.05426v4.pdf', '1704.07854v4.pdf', '1812.06589v2.pdf', '1705.09882v2.pdf']",The figure shows examples of videos generated using the different schemes. The optical flow images show how the pixels in the video are moving.,1804.04786v3-Figure5-1.png,Talking Face Generation by Conditional Recurrent Adversarial Network,"Given an arbitrary face image and an arbitrary speech clip, the proposed work
attempts to generating the talking face video with accurate lip synchronization
while maintaining smooth transition of both lip and facial movement over the
entire video clip. Existing works either do not consider temporal dependency on
face images across different video frames thus easily yielding
noticeable/abrupt facial and lip movement or are only limited to the generation
of talking face video for a specific person thus lacking generalization
capacity. We propose a novel conditional video generation network where the
audio input is treated as a condition for the recurrent adversarial network
such that temporal dependency is incorporated to realize smooth transition for
the lip and facial movement. In addition, we deploy a multi-task adversarial
training scheme in the context of video generation to improve both
photo-realism and the accuracy for lip synchronization. Finally, based on the
phoneme distribution information extracted from the audio clip, we develop a
sample selection method that effectively reduces the size of the training
dataset without sacrificing the quality of the generated video. Extensive
experiments on both controlled and uncontrolled datasets demonstrate the
superiority of the proposed approach in terms of visual quality, lip sync
accuracy, and smooth transition of lip and facial movement, as compared to the
state-of-the-art.","Effect of different generation schemes. The sample is randomly selected from the TCD-TIMIT dataset. From top to bottom: sequential generation, frame-to-frame generation, and our recurrent generation schemes. Optical flow is calculated on the frameto-frame and recurrent schemes as shown under the face images.",What is the difference between the sequential and recurrent generation schemes?
spiqa_227,1701.06171v4,"In the figure, how do the bottom-up process (blue box) and the top-down process (green box) differ in terms of how parts are composed into a holistic object model versus how a holistic object is decomposed into smaller parts within the hierarchical compositional model learning framework?","The top-down compositional learning scheme starts with a holistic object model and decomposes it into smaller parts, while the bottom-up compositional learning scheme starts with basic parts and composes them into a holistic object model.",1701.06171v4.pdf,"['1701.06171v4.pdf', '1710.06177v2.pdf', '1804.04786v3.pdf', '1707.08608v3.pdf', '1707.00524v2.pdf', '1809.03449v3.pdf', '1809.02731v3.pdf', '1709.08294v3.pdf']","The figure shows the two learning schemes side-by-side. The top-down scheme is shown in the green box, where the clock image is decomposed into smaller parts. The bottom-up scheme is shown in the blue box, where basic parts are composed into a clock image.",1701.06171v4-Figure3-1.png,Greedy Structure Learning of Hierarchical Compositional Models,"In this work, we consider the problem of learning a hierarchical generative
model of an object from a set of images which show examples of the object in
the presence of variable background clutter. Existing approaches to this
problem are limited by making strong a-priori assumptions about the object's
geometric structure and require segmented training data for learning. In this
paper, we propose a novel framework for learning hierarchical compositional
models (HCMs) which do not suffer from the mentioned limitations. We present a
generalized formulation of HCMs and describe a greedy structure learning
framework that consists of two phases: Bottom-up part learning and top-down
model composition. Our framework integrates the foreground-background
segmentation problem into the structure learning task via a background model.
As a result, we can jointly optimize for the number of layers in the hierarchy,
the number of parts per layer and a foreground-background segmentation based on
class labels only. We show that the learned HCMs are semantically meaningful
and achieve competitive results when compared to other generative object models
at object classification on a standard transfer learning dataset.",Illustration of the joint bottom-up and top-down compositional learning scheme. During the bottom-up process (blue box) basis filters (black strokes) are grouped into higher-order parts until no further compositions are found. The subsequent top-down process (green box) composes the learned hierarchical part dictionary into a holistic object model (orange box).,What is the difference between the top-down and bottom-up compositional learning schemes?
spiqa_228,1811.02721v3,"What does the figure comparing CoAP vs. HTTP/TCP goodput in the ""Performant TCP for Low-Power Wireless Networks"" paper indicate as the difference in median response time between CoAP and HTTP for a 50 KiB response size in an IEEE 802.15.4-based LLN?",The difference in response time between CoAP and HTTP for a response size of 50 KiB is approximately 20 seconds.,1811.02721v3.pdf,"['1811.02721v3.pdf', '1611.04363v2.pdf', '1611.07718v2.pdf', '1702.03584v3.pdf', '1708.00160v2.pdf']","The figure shows the response time for CoAP and HTTP for different response sizes. For a response size of 50 KiB, the boxplot for CoAP shows a median response time of approximately 20 milliseconds, while the boxplot for HTTP shows a median response time of approximately 40 milliseconds.",1811.02721v3-Figure8-1.png,Performant TCP for Low-Power Wireless Networks,"Low-power and lossy networks (LLNs) enable diverse applications integrating
many resource-constrained embedded devices, often requiring interconnectivity
with existing TCP/IP networks as part of the Internet of Things. But TCP has
received little attention in LLNs due to concerns about its overhead and
performance, leading to LLN-specific protocols that require specialized
gateways for interoperability. We present a systematic study of a well-designed
TCP stack in IEEE 802.15.4-based LLNs, based on the TCP protocol logic in
FreeBSD. Through careful implementation and extensive experiments, we show that
modern low-power sensor platforms are capable of running full-scale TCP and
that TCP, counter to common belief, performs well despite the lossy nature of
LLNs. By carefully studying the interaction between the transport and link
layers, we identify subtle but important modifications to both, achieving TCP
goodput within 25% of an upper bound (5-40x higher than prior results) and
low-power operation commensurate to CoAP in a practical LLN application
scenario. This suggests that a TCP-based transport layer, seamlessly
interoperable with existing TCP/IP networks, is viable and performant in LLNs.",Goodput: CoAP vs. HTTP/TCP,What is the difference in response time between CoAP and HTTP for a response size of 50 KiB?
spiqa_229,1705.02946v3,"According to the construction illustrated in Figure 2 of the cake cutting paper, what is the distance from equitability when Player 1 receives $0.5 + a$ and Player 2 receives $0.5 + b$?",The distance from equitability is $b-a$.,1705.02946v3.pdf,"['1705.02946v3.pdf', '1805.00912v4.pdf', '1803.06506v3.pdf', '1703.00060v2.pdf', '1901.00056v2.pdf', '1701.06171v4.pdf', '1805.04687v2.pdf', '1702.03584v3.pdf', '1805.02349v2.pdf']","The figure shows that for the allocation that can be obtained by cutting at $x$ with the player order $(1,2)$, player 1 gets $V_1(0,x) = 0.5+a$ and player 2 gets $V_2(x,1) = 0.5+b$. The distance from equitability is the absolute value of the difference between these two values, which is $|V_1(0,x) - V_2(x,1)| = |(0.5+a) - (0.5+b)| = b-a$.",1705.02946v3-Figure2-1.png,The Query Complexity of Cake Cutting,"We study the query complexity of cake cutting and give lower and upper bounds
for computing approximately envy-free, perfect, and equitable allocations with
the minimum number of cuts. The lower bounds are tight for computing connected
envy-free allocations among n=3 players and for computing perfect and equitable
allocations with minimum number of cuts between n=2 players.
  We also formalize moving knife procedures and show that a large subclass of
this family, which captures all the known moving knife procedures, can be
simulated efficiently with arbitrarily small error in the Robertson-Webb query
model.","Figure 2: Construction for equitable lower bound. The distance from a connected equitable and proportional allocation is b− a, where 0 < a < b < 0.5 and 0 < x < y < 1.","What is the distance from equitability for the allocation that can be obtained by cutting at $x$ with the player order $(1,2)$?"
spiqa_230,1812.06589v2,"Referring to the ablation study figure in this paper, how does the addition of Dynamic Attention (DA) to the baseline model affect the PSNR, SSIM, and LMD metrics?","Adding DA to the baseline method improves the PSNR and SSIM values, while slightly decreasing the LMD value.",1812.06589v2.pdf,"['1812.06589v2.pdf', '1704.05426v4.pdf', '1901.00398v2.pdf', '1809.03149v2.pdf', '1709.08294v3.pdf', '1704.08615v2.pdf', '1809.03449v3.pdf', '1608.02784v2.pdf', '1804.05995v2.pdf']","The table shows that the baseline method has a PSNR of 28.88, an SSIM of 0.89, and an LMD of 1.36. When DA is added to the baseline method (b), the PSNR increases to 29.19, the SSIM increases to 0.90, and the LMD decreases to 1.37. This indicates that adding DA improves the image quality and reduces the distortion.",1812.06589v2-Table3-1.png,Arbitrary Talking Face Generation via Attentional Audio-Visual Coherence Learning,"Talking face generation aims to synthesize a face video with precise lip
synchronization as well as a smooth transition of facial motion over the entire
video via the given speech clip and facial image. Most existing methods mainly
focus on either disentangling the information in a single image or learning
temporal information between frames. However, cross-modality coherence between
audio and video information has not been well addressed during synthesis. In
this paper, we propose a novel arbitrary talking face generation framework by
discovering the audio-visual coherence via the proposed Asymmetric Mutual
Information Estimator (AMIE). In addition, we propose a Dynamic Attention (DA)
block by selectively focusing the lip area of the input image during the
training stage, to further enhance lip synchronization. Experimental results on
benchmark LRW dataset and GRID dataset transcend the state-of-the-art methods
on prevalent metrics with robust high-resolution synthesizing on gender and
pose variations.","Ablation study of the key components AMIE and DA in our method as well as two strategies applied in AMIE: Asymmetric Training (Asy.) and JS represented estimator (JS). Ours = Baseline + AMIE + DA, and AMIE = MINE + Asy. + JS.",What is the effect of adding DA to the baseline method?
spiqa_231,1710.05654v2,"Based on the results shown in the left panel of the figure comparing Gaussian noise addition to image duplication, how does adding N(0,1) Gaussian noise to 10% of the images affect the measured sparsity of the data matrix?",Adding Gaussian noise to the images increases the measured sparsity.,1710.05654v2.pdf,"['1710.05654v2.pdf', '1710.06177v2.pdf', '1805.06431v4.pdf', '1804.05938v2.pdf', '1804.04410v2.pdf', '1803.04572v2.pdf', '1605.07496v3.pdf', '1906.10843v1.pdf']","The left panel of the figure shows that the measured sparsity (solid line) is higher when Gaussian noise is added to the images (orange line) than when no noise is added (blue line). This is because the noise adds additional non-zero entries to the data matrix, which increases the sparsity.",1710.05654v2-Figure12-1.png,Large Scale Graph Learning from Smooth Signals,"Graphs are a prevalent tool in data science, as they model the inherent
structure of the data. They have been used successfully in unsupervised and
semi-supervised learning. Typically they are constructed either by connecting
nearest samples, or by learning them from data, solving an optimization
problem. While graph learning does achieve a better quality, it also comes with
a higher computational cost. In particular, the current state-of-the-art model
cost is $\mathcal{O}(n^2)$ for $n$ samples. In this paper, we show how to scale
it, obtaining an approximation with leading cost of $\mathcal{O}(n\log(n))$,
with quality that approaches the exact graph learning model. Our algorithm uses
known approximate nearest neighbor techniques to reduce the number of
variables, and automatically selects the correct parameters of the model,
requiring a single intuitive input: the desired edge density.","Robustness of the theoretical bounds of θ in the existence of outliers or duplicate nodes. Same dataset as the one used for Figure 2. Even for extreme cases in terms of distance distribution, the bounds give a good approximation. Left: Results when we add Gaussian noise from N (0, 1) to 10% of the images before calculating Z. Note that the noise added is significant given that the initial pixel values are in [0, 1]. Right: We replaced 10% of the images with duplicates of other images already in the dataset.",What is the effect of adding Gaussian noise to the images on the measured sparsity?
spiqa_232,1811.10673v1,"Referring to Figure 6 of the ""Adversarial Video Compression Guided by Soft Edge Detection"" paper, how does the quality and recognizability of the reconstructed frames change as downsampling intensity increases from 256 × 256 to 32 × 32 in the proposed GAN-based compression framework?",Downsampling reduces the quality of reconstructed frames.,1811.10673v1.pdf,"['1811.10673v1.pdf', '1805.04687v2.pdf', '1901.00398v2.pdf', '1708.03797v1.pdf', '1603.03833v4.pdf', '1805.08751v2.pdf', '1809.01246v1.pdf', '1701.06171v4.pdf', '1803.03467v4.pdf', '1704.05426v4.pdf', '1707.00524v2.pdf']","Figure 1 shows the original frame and the reconstructed frames at different downsampling levels. As the downsampling level increases, the reconstructed frames become less recognizable. This is because downsampling reduces the amount of information in the image, which makes it more difficult to reconstruct the original image.",1811.10673v1-Figure6-1.png,Adversarial Video Compression Guided by Soft Edge Detection,"We propose a video compression framework using conditional Generative
Adversarial Networks (GANs). We rely on two encoders: one that deploys a
standard video codec and another which generates low-level maps via a pipeline
of down-sampling, a newly devised soft edge detector, and a novel lossless
compression scheme. For decoding, we use a standard video decoder as well as a
neural network based one, which is trained using a conditional GAN. Recent
""deep"" approaches to video compression require multiple videos to pre-train
generative networks to conduct interpolation. In contrast to this prior work,
our scheme trains a generative decoder on pairs of a very limited number of key
frames taken from a single video and corresponding low-level maps. The trained
decoder produces reconstructed frames relying on a guidance of low-level maps,
without any interpolation. Experiments on a diverse set of 131 videos
demonstrate that our proposed GAN-based compression engine achieves much higher
quality reconstructions at very low bitrates than prevailing standard codecs
such as H.264 or HEVC.","Figure 6: Performance of proposed framework against different downsampling levels: (a) original 256 × 256 frame, XG. Reconstructions at scales (b) 32 × 32, (c) 64 × 64, (d) and 256 × 256. As the resolution increases, the reconstructed frames become more recognizable.",What is the effect of downsampling on the quality of reconstructed frames?
spiqa_233,1707.08608v3,"Based on the example in the figure, how does enforcing syntactic constraints impact both the consistency of spans and the assignment of semantic roles, specifically for the token sequence ""really like this""?","Enforcing syntactic constraints can correct the number of agreeing spans, and also change the semantic roles assigned to tokens.",1707.08608v3.pdf,"['1707.08608v3.pdf', '1605.07496v3.pdf', '1703.04887v4.pdf', '1704.05426v4.pdf', '1708.01425v4.pdf', '1705.07384v2.pdf', '1703.00899v2.pdf', '1706.03847v3.pdf', '1805.01216v3.pdf', '1705.08016v3.pdf', '1811.02721v3.pdf', '1608.02784v2.pdf', '1804.05995v2.pdf']","The figure shows that the initial output has an inconsistent span for token ”really like this”. Enforcing the constraint not only corrects the number of agreeing spans, but also changes the semantic role ”B-ARG2” to ”B-ARGM-ADV” and ”I-ARG2” to ”B-ARG2”.",1707.08608v3-Table7-1.png,Gradient-based Inference for Networks with Output Constraints,"Practitioners apply neural networks to increasingly complex problems in
natural language processing, such as syntactic parsing and semantic role
labeling that have rich output structures. Many such structured-prediction
problems require deterministic constraints on the output values; for example,
in sequence-to-sequence syntactic parsing, we require that the sequential
outputs encode valid trees. While hidden units might capture such properties,
the network is not always able to learn such constraints from the training data
alone, and practitioners must then resort to post-processing. In this paper, we
present an inference method for neural networks that enforces deterministic
constraints on outputs without performing rule-based post-processing or
expensive discrete search. Instead, in the spirit of gradient-based training,
we enforce constraints with gradient-based inference (GBI): for each input at
test-time, we nudge continuous model weights until the network's unconstrained
inference procedure generates an output that satisfies the constraints. We
study the efficacy of GBI on three tasks with hard constraints: semantic role
labeling, syntactic parsing, and sequence transduction. In each case, the
algorithm not only satisfies constraints but improves accuracy, even when the
underlying network is state-of-the-art.","A semantic role labeling example for which the method successfully enforces syntactic constraints. The initial output has an inconsistent span for token ”really like this”. Enforcing the constraint not only corrects the number of agreeing spans, but also changes the semantic role ”B-ARG2” to ”B-ARGM-ADV” and ”I-ARG2” to ”B-ARG2”..",What is the effect of enforcing syntactic constraints on the semantic role labeling output?
spiqa_234,1704.00774v3,"Based on the PTB test perplexity figure, how does increasing the number of distinct hidden layer matrices (K) from 1 to 10000 affect the test PPL for both r-RNTNs and RNTNs?",The test PPL of all the models decreases as K increases.,1704.00774v3.pdf,"['1704.00774v3.pdf', '1809.01246v1.pdf', '1809.02731v3.pdf', '1809.03550v3.pdf', '1811.02721v3.pdf', '1809.03449v3.pdf', '1809.01989v2.pdf', '1704.05958v2.pdf', '1803.02750v3.pdf', '1703.04887v4.pdf']","The figure shows that the test PPL of all the models decreases as K increases. This is because increasing K allows the models to store more information about the input sequence, which leads to better predictions.",1704.00774v3-Figure1-1.png,Restricted Recurrent Neural Tensor Networks: Exploiting Word Frequency and Compositionality,"Increasing the capacity of recurrent neural networks (RNN) usually involves
augmenting the size of the hidden layer, with significant increase of
computational cost. Recurrent neural tensor networks (RNTN) increase capacity
using distinct hidden layer weights for each word, but with greater costs in
memory usage. In this paper, we introduce restricted recurrent neural tensor
networks (r-RNTN) which reserve distinct hidden layer weights for frequent
vocabulary words while sharing a single set of weights for infrequent words.
Perplexity evaluations show that for fixed hidden layer sizes, r-RNTNs improve
language model performance over RNNs using only a small fraction of the
parameters of unrestricted RNTNs. These results hold for r-RNTNs using Gated
Recurrent Units and Long Short-Term Memory.","PTB test PPL as K varies from 1 to 10000 (100 for gated networks). At K = 100, the r-RNTN with f mapping already closely approximates the much bigger RNTN, with little gain for bigger K , showing that dedicated matrices should be reserved for frequent words as hypothesized.",What is the effect of increasing K on the test PPL of the different models?
spiqa_235,1708.06832v3,"In Figure (a), how does increasing the budget in FLOPS affect the test Top-1 error rates across the different training strategies, particularly between a small ANN with adaptive loss weighting and a large ANN with non-adaptive weights?",The test Top-1 error rate decreases as the budget in FLOPS increases for all three training strategies.,1708.06832v3.pdf,"['1708.06832v3.pdf', '1812.06589v2.pdf', '1709.02418v2.pdf', '1703.02507v3.pdf', '1707.00189v3.pdf', '1704.07121v2.pdf', '1802.07459v2.pdf', '1803.06506v3.pdf', '1901.00398v2.pdf']",The figure shows that the test Top-1 error rate decreases as the budget in FLOPS increases for all three training strategies. This is because increasing the budget in FLOPS allows the models to train for longer and learn more complex features.,1708.06832v3-Figure1-1.png,Learning Anytime Predictions in Neural Networks via Adaptive Loss Balancing,"This work considers the trade-off between accuracy and test-time
computational cost of deep neural networks (DNNs) via \emph{anytime}
predictions from auxiliary predictions. Specifically, we optimize auxiliary
losses jointly in an \emph{adaptive} weighted sum, where the weights are
inversely proportional to average of each loss. Intuitively, this balances the
losses to have the same scale. We demonstrate theoretical considerations that
motivate this approach from multiple viewpoints, including connecting it to
optimizing the geometric mean of the expectation of each loss, an objective
that ignores the scale of losses. Experimentally, the adaptive weights induce
more competitive anytime predictions on multiple recognition data-sets and
models than non-adaptive approaches including weighing all losses equally. In
particular, anytime neural networks (ANNs) can achieve the same accuracy faster
using adaptive weights on a small network than using static constant weights on
a large one. For problems with high performance saturation, we also show a
sequence of exponentially deepening ANNscan achieve near-optimal anytime
results at any budget, at the cost of a const fraction of extra computation.","(a) The common ANN training strategy increases final errors from the optimal (green vs. blue), which decreases exponentially slowly. By learning to focus more on the final auxiliary losses, the proposed adaptive loss weights make a small ANN (orange) to outperform a large one (green) that has non-adaptive weights. (b) Anytime neural networks contain auxiliary predictions and losses, ŷi and `i, for intermediate feature unit fi.",What is the effect of increasing the budget in FLOPS on the test Top-1 error rate for the three different training strategies?
spiqa_236,1901.00056v2,"Based on the sensitivity analysis figure in *Entity Synonym Discovery via Multipiece Bilateral Context Matching*, how does varying the margin affect the AUC and MAP performance metrics of the SYNONYMNET model?","The AUC and MAP values initially increase with increasing margin, but then decrease after a certain point.",1901.00056v2.pdf,"['1901.00056v2.pdf', '1802.07351v2.pdf', '1704.07854v4.pdf', '1705.09966v2.pdf', '1703.00899v2.pdf', '1706.00633v4.pdf']","The figure shows that the AUC and MAP values for both metrics generally increase with increasing margin, but then decrease after a certain point. This suggests that there is an optimal margin value for maximizing the performance of the model.",1901.00056v2-Figure3-1.png,Entity Synonym Discovery via Multipiece Bilateral Context Matching,"Being able to automatically discover synonymous entities in an open-world
setting benefits various tasks such as entity disambiguation or knowledge graph
canonicalization. Existing works either only utilize entity features, or rely
on structured annotations from a single piece of context where the entity is
mentioned. To leverage diverse contexts where entities are mentioned, in this
paper, we generalize the distributional hypothesis to a multi-context setting
and propose a synonym discovery framework that detects entity synonyms from
free-text corpora with considerations on effectiveness and robustness. As one
of the key components in synonym discovery, we introduce a neural network model
SYNONYMNET to determine whether or not two given entities are synonym with each
other. Instead of using entities features, SYNONYMNET makes use of multiple
pieces of contexts in which the entity is mentioned, and compares the
context-level similarity via a bilateral matching schema. Experimental results
demonstrate that the proposed model is able to detect synonym sets that are not
observed during training on both generic and domain-specific datasets:
Wiki+Freebase, PubMed+UMLS, and MedBook+MKG, with up to 4.16% improvement in
terms of Area Under the Curve and 3.19% in terms of Mean Average Precision
compared to the best baseline method.",Sensitivity analysis.,What is the effect of increasing the margin on the AUC and MAP values?
spiqa_237,1706.08146v3,"How does increasing the projection dimension \(d\), as depicted in the figure showing approximation errors for sparse PCA and NMF on synthetic data, impact the error rates under different levels of compression?",Increasing the projection dimension d decreases the approximation error for both sparse PCA and NMF.,1706.08146v3.pdf,"['1706.08146v3.pdf', '1706.04284v3.pdf', '1906.10843v1.pdf', '1803.04572v2.pdf', '1811.02553v4.pdf', '1707.00524v2.pdf', '1708.03797v1.pdf', '1802.07222v1.pdf', '1710.01507v4.pdf']",The figure shows that the approximation error decreases as the projection dimension d increases. This is because a higher projection dimension allows for a more accurate representation of the original data.,1706.08146v3-Figure2-1.png,Compressed Factorization: Fast and Accurate Low-Rank Factorization of Compressively-Sensed Data,"What learning algorithms can be run directly on compressively-sensed data? In
this work, we consider the question of accurately and efficiently computing
low-rank matrix or tensor factorizations given data compressed via random
projections. We examine the approach of first performing factorization in the
compressed domain, and then reconstructing the original high-dimensional
factors from the recovered (compressed) factors. In both the matrix and tensor
settings, we establish conditions under which this natural approach will
provably recover the original factors. While it is well-known that random
projections preserve a number of geometric properties of a dataset, our work
can be viewed as showing that they can also preserve certain solutions of
non-convex, NP-Hard problems like non-negative matrix factorization. We support
these theoretical results with experiments on synthetic data and demonstrate
the practical applicability of compressed factorization on real-world gene
expression and EEG time series datasets.","Approximation errors Err(X,X∗) := ‖X −X∗‖F /‖X∗‖F for sparse PCA and NMF on synthetic data with varying column sparsity k of W and projection dimension d. The values of d correspond to 10×, 5×, and 2.5× compression respectively. Err(W̃ , PW ) measures the distance between factors in the compressed domain: low error here is necessary for accurate sparse recovery. Err(Ŵ ,W ) measures the error after sparse recovery: the recovered factors Ŵ typically incur only slightly higher error than the oracle lower bound (dotted lines) where PW is known exactly.",What is the effect of increasing the projection dimension d on the approximation error for sparse PCA and NMF?
spiqa_238,1709.02418v2,"How does the figure illustrating the left-swap operation on binary vector \( y \) at index \( j' \) demonstrate the change in the number of misclassified pairs \( h(y, \hat{y}) \) compared to \( z \), and what is the implication for understanding AUC constraints?",The left-swap increases the number of misclassified pairs by one.,1709.02418v2.pdf,"['1709.02418v2.pdf', '1811.02721v3.pdf', '1804.00863v3.pdf', '1704.07121v2.pdf']","The figure shows that the number of misclassified pairs for y is 4, while the number of misclassified pairs for z is 5. This means that the left-swap operation increased the number of misclassified pairs by one.",1709.02418v2-Figure1-1.png,How Does Knowledge of the AUC Constrain the Set of Possible Ground-truth Labelings?,"Recent work on privacy-preserving machine learning has considered how
data-mining competitions such as Kaggle could potentially be ""hacked"", either
intentionally or inadvertently, by using information from an oracle that
reports a classifier's accuracy on the test set. For binary classification
tasks in particular, one of the most common accuracy metrics is the Area Under
the ROC Curve (AUC), and in this paper we explore the mathematical structure of
how the AUC is computed from an n-vector of real-valued ""guesses"" with respect
to the ground-truth labels. We show how knowledge of a classifier's AUC on the
test set can constrain the set of possible ground-truth labelings, and we
derive an algorithm both to compute the exact number of such labelings and to
enumerate efficiently over them. Finally, we provide empirical evidence that,
surprisingly, the number of compatible labelings can actually decrease as n
grows, until a test set-dependent threshold is reached.","Illustration of how performing a left-swap on binary vector y at index j′ yields a new vector z such that the number of misclassified pairs h(z, ŷ) is one more than h(y, ŷ). Specifically, ŷ misclassifies pairs (3, 4), (3, 5), (3, 7), and (6, 7) w.r.t. to y, since for each such pair (i, j), ŷi < ŷj but yi > yj . In contrast, ŷ misclassifies (3, 4), (3, 6), (3, 7), (5, 6), and (5, 7) w.r.t. to z.",What is the effect of performing a left-swap on a binary vector y at index j′?
spiqa_239,1611.02654v2,"In the figure comparing extractive summarization models trained with and without pre-training on the sentence ordering task, how does pre-training affect the ROUGE-L score?",Pre-training with the ordering task increases the ROUGE-L score for extractive summarization.,1611.02654v2.pdf,"['1611.02654v2.pdf', '1802.07351v2.pdf', '1906.10843v1.pdf', '1804.01429v3.pdf', '1804.05938v2.pdf', '1702.08694v3.pdf', '1706.08146v3.pdf', '1805.08751v2.pdf', '1803.03467v4.pdf', '1811.08257v1.pdf', '1708.03797v1.pdf', '1805.04687v2.pdf', '1611.05742v3.pdf']","The table shows that the ROUGE-L score for models pre-trained with the ordering task is higher than the ROUGE-L score for models trained from scratch, for both summary lengths.",1611.02654v2-Table3-1.png,Sentence Ordering and Coherence Modeling using Recurrent Neural Networks,"Modeling the structure of coherent texts is a key NLP problem. The task of
coherently organizing a given set of sentences has been commonly used to build
and evaluate models that understand such structure. We propose an end-to-end
unsupervised deep learning approach based on the set-to-sequence framework to
address this problem. Our model strongly outperforms prior methods in the order
discrimination task and a novel task of ordering abstracts from scientific
articles. Furthermore, our work shows that useful text representations can be
obtained by learning to order sentences. Visualizing the learned sentence
representations shows that the model captures high-level logical structure in
paragraphs. Our representations perform comparably to state-of-the-art
pre-training methods on sentence similarity and paraphrase detection tasks.",Comparison on extractive summarization between models trained from scratch and models pre-trained with the ordering task.,What is the effect of pre-training with the ordering task on the ROUGE-L score for extractive summarization?
spiqa_240,1701.03077v10,"Referring to the paper's figure comparing depth estimation results on the KITTI benchmark, what is the impact of using the ""adaptive"" loss over wavelet coefficients instead of the ""Baseline"" loss function on the accuracy of depth estimates?","Replacing the loss function in the ""Baseline"" network with the ""adaptive"" loss over wavelet coefficients results in significantly improved depth estimates.",1701.03077v10.pdf,"['1701.03077v10.pdf', '1805.07567v2.pdf', '1804.00863v3.pdf', '1708.03797v1.pdf', '1705.09966v2.pdf', '1804.04786v3.pdf', '1804.07707v2.pdf', '1705.02946v3.pdf']","The figure shows the input image, the depth estimates from the ""Baseline"" network, the depth estimates from the network with the ""adaptive"" loss, and the ground truth depth. The depth estimates from the network with the ""adaptive"" loss are much closer to the ground truth than the depth estimates from the ""Baseline"" network.",1701.03077v10-Figure4-1.png,A General and Adaptive Robust Loss Function,"We present a generalization of the Cauchy/Lorentzian, Geman-McClure,
Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2
loss functions. By introducing robustness as a continuous parameter, our loss
function allows algorithms built around robust loss minimization to be
generalized, which improves performance on basic vision tasks such as
registration and clustering. Interpreting our loss as the negative log of a
univariate density yields a general probability distribution that includes
normal and Cauchy distributions as special cases. This probabilistic
interpretation enables the training of neural networks in which the robustness
of the loss automatically adapts itself during training, which improves
performance on learning-based tasks such as generative image synthesis and
unsupervised monocular depth estimation, without requiring any manual parameter
tuning.",Monocular depth estimation results on the KITTI benchmark using the “Baseline” network of [42]. Replacing only the network’s loss function with our “adaptive” loss over wavelet coefficients results in significantly improved depth estimates.,"What is the effect of replacing the loss function in the ""Baseline"" network with the ""adaptive"" loss over wavelet coefficients?"
spiqa_241,1809.00263v5,"Based on the figure illustrating the performance of different video infilling methods in the SDVI paper, how does the sliding tendency of SepConv impact the generated motion accuracy, and how is this reflected in the LMS values?",The sliding tendency of SepConv will cause motion errors and high LMS.,1809.00263v5.pdf,"['1809.00263v5.pdf', '1812.10735v2.pdf', '1805.04609v3.pdf', '1710.01507v4.pdf']","The figure shows the ground truth of the missing sequence and the generated images by different methods. The generated images by SepConv have a sliding tendency, which causes the person in the images to appear to be moving faster than they actually are. This is evident in the high LMS values for SepConv.",1809.00263v5-Figure13-1.png,Stochastic Dynamics for Video Infilling,"In this paper, we introduce a stochastic dynamics video infilling (SDVI)
framework to generate frames between long intervals in a video. Our task
differs from video interpolation which aims to produce transitional frames for
a short interval between every two frames and increase the temporal resolution.
Our task, namely video infilling, however, aims to infill long intervals with
plausible frame sequences. Our framework models the infilling as a constrained
stochastic generation process and sequentially samples dynamics from the
inferred distribution. SDVI consists of two parts: (1) a bi-directional
constraint propagation module to guarantee the spatial-temporal coherence among
frames, (2) a stochastic sampling process to generate dynamics from the
inferred distributions. Experimental results show that SDVI can generate clear
frame sequences with varying contents. Moreover, motions in the generated
sequence are realistic and able to transfer smoothly from the given start frame
to the terminal frame. Our project site is
https://xharlie.github.io/projects/project_sites/SDVI/video_results.html",The sliding tendency of SepConv will cause motion errors and high LMS.,What is the effect of the sliding tendency of SepConv on the generated images?
spiqa_242,1811.08481v2,"Referring to the figure on the accuracy of graph representation across different UnCoRd mappers, how does training on increasingly diverse datasets affect the graph's generalization and accuracy in visual question answering tasks?",Training on more diverse data improves the accuracy of graph representation for VQA.,1811.08481v2.pdf,"['1811.08481v2.pdf', '1809.01246v1.pdf', '1705.08016v3.pdf', '1703.02507v3.pdf', '1603.00286v5.pdf', '1803.03467v4.pdf', '1704.04539v2.pdf', '1603.03833v4.pdf', '1611.03780v2.pdf', '1705.09296v2.pdf', '1705.07164v8.pdf', '1804.04410v2.pdf', '1611.04363v2.pdf', '1812.10735v2.pdf']","The table shows that the accuracy of the graph representation increases as the amount of training data increases. The accuracy is highest for the VG mapper, which was trained on the most diverse data.",1811.08481v2-Table4-1.png,VQA with no questions-answers training,"Methods for teaching machines to answer visual questions have made
significant progress in recent years, but current methods still lack important
human capabilities, including integrating new visual classes and concepts in a
modular manner, providing explanations for the answers and handling new domains
without explicit examples. We propose a novel method that consists of two main
parts: generating a question graph representation, and an answering procedure,
guided by the abstract structure of the question graph to invoke an extendable
set of visual estimators. Training is performed for the language part and the
visual part on their own, but unlike existing schemes, the method does not
require any training using images with associated questions and answers. This
approach is able to handle novel domains (extended question types and new
object classes, properties and relations) as long as corresponding visual
estimators are available. In addition, it can provide explanations to its
answers and suggest alternatives when questions are not grounded in the image.
We demonstrate that this approach achieves both high performance and domain
extensibility without any questions-answers training.","Accuracy of graph representation for VQA [8] sample, given for the different UnCoRd mappers. As expected, training on more diverse data allows better generalization across domains.",What is the effect of training on more diverse data on the accuracy of graph representation for VQA?
spiqa_243,1809.03149v2,"Based on the figure comparing DDPG with CHER and without CHER in the ""Learning Adaptive Display Exposure for Real-Time Advertising"" paper, how does CHER impact the percentage of ads displayed per user as reflected in the learning curves?",The percentage of ads displayed for each user is higher when CHER is used.,1809.03149v2.pdf,"['1809.03149v2.pdf', '1811.02553v4.pdf', '1705.09296v2.pdf', '1804.05938v2.pdf', '1811.09393v4.pdf', '1705.08016v3.pdf', '1703.00060v2.pdf', '1704.07854v4.pdf', '1811.02721v3.pdf', '1804.05995v2.pdf']",The figure shows that the blue line (CHER) is consistently higher than the orange line (without CHER) across all user numbers. This indicates that CHER leads to a higher percentage of ads being displayed.,1809.03149v2-Figure3-1.png,Learning Adaptive Display Exposure for Real-Time Advertising,"In E-commerce advertising, where product recommendations and product ads are
presented to users simultaneously, the traditional setting is to display ads at
fixed positions. However, under such a setting, the advertising system loses
the flexibility to control the number and positions of ads, resulting in
sub-optimal platform revenue and user experience. Consequently, major
e-commerce platforms (e.g., Taobao.com) have begun to consider more flexible
ways to display ads. In this paper, we investigate the problem of advertising
with adaptive exposure: can we dynamically determine the number and positions
of ads for each user visit under certain business constraints so that the
platform revenue can be increased? More specifically, we consider two types of
constraints: request-level constraint ensures user experience for each user
visit, and platform-level constraint controls the overall platform monetization
rate. We model this problem as a Constrained Markov Decision Process with
per-state constraint (psCMDP) and propose a constrained two-level reinforcement
learning approach to decompose the original problem into two relatively
independent sub-problems. To accelerate policy learning, we also devise a
constrained hindsight experience replay mechanism. Experimental evaluations on
industry-scale real-world datasets demonstrate the merits of our approach in
both obtaining higher revenue under the constraints and the effectiveness of
the constrained hindsight experience replay mechanism.",Learning Curves: DDPG with CHER Compared with DDPG without CHER.,What is the effect of using CHER on the percentage of ads displayed for each user?
spiqa_244,1612.02803v5,"In the figure illustrating the undamped harmonic oscillator in *""The Physical Systems Behind Optimization Algorithms,""* what is the differential equation that describes the motion of the mass attached to the spring?","The equation that describes the motion of a mass attached to a spring is:
```
m d^2 X / dt^2 + kX = 0
```
where:
* m is the mass of the object
* X is the displacement of the object from its equilibrium position
* k is the spring constant
* t is time",1612.02803v5.pdf,"['1612.02803v5.pdf', '1805.06431v4.pdf', '1703.02507v3.pdf', '1706.00827v2.pdf', '1811.09393v4.pdf', '1804.07849v4.pdf', '1703.00899v2.pdf', '1608.02784v2.pdf', '1901.00056v2.pdf']","This equation is derived from Newton's second law, which states that the force on an object is equal to its mass times its acceleration. In this case, the force on the object is the force exerted by the spring, which is proportional to the displacement of the object from its equilibrium position.",1612.02803v5-Figure1-1.png,The Physical Systems Behind Optimization Algorithms,"We use differential equations based approaches to provide some {\it
\textbf{physics}} insights into analyzing the dynamics of popular optimization
algorithms in machine learning. In particular, we study gradient descent,
proximal gradient descent, coordinate gradient descent, proximal coordinate
gradient, and Newton's methods as well as their Nesterov's accelerated variants
in a unified framework motivated by a natural connection of optimization
algorithms to physical systems. Our analysis is applicable to more general
algorithms and optimization problems {\it \textbf{beyond}} convexity and strong
convexity, e.g. Polyak-\L ojasiewicz and error bound conditions (possibly
nonconvex).",An illustration of the harmonic oscillators: A massive particle connects to a massless spring. (Top) Undamped harmonic oscillator; (Bottom) Damped harmonic oscillator.,What is the equation that describes the motion of a mass attached to a spring?
spiqa_245,1811.02721v3,"In the figure presented in the ""Performant TCP for Low-Power Wireless Networks"" paper, what role does the Hamilton-based PCB play in the operation of the ultrasonic anemometer?","The Hamilton-based PCB is the electronic control board of the anemometer. It houses the microcontroller, sensors, and other electronic components that are necessary for the anemometer to function.",1811.02721v3.pdf,"['1811.02721v3.pdf', '1803.01128v3.pdf', '1708.02153v2.pdf', '1611.05742v3.pdf', '1608.02784v2.pdf', '1804.00863v3.pdf', '1811.08257v1.pdf', '1606.07384v2.pdf', '1809.04276v2.pdf', '1707.01922v5.pdf', '1804.04786v3.pdf', '1706.08146v3.pdf', '1812.00281v3.pdf', '1804.07931v2.pdf']",The photograph shows the Hamilton-based PCB with its various electronic components.,1811.02721v3-Figure13-1.png,Performant TCP for Low-Power Wireless Networks,"Low-power and lossy networks (LLNs) enable diverse applications integrating
many resource-constrained embedded devices, often requiring interconnectivity
with existing TCP/IP networks as part of the Internet of Things. But TCP has
received little attention in LLNs due to concerns about its overhead and
performance, leading to LLN-specific protocols that require specialized
gateways for interoperability. We present a systematic study of a well-designed
TCP stack in IEEE 802.15.4-based LLNs, based on the TCP protocol logic in
FreeBSD. Through careful implementation and extensive experiments, we show that
modern low-power sensor platforms are capable of running full-scale TCP and
that TCP, counter to common belief, performs well despite the lossy nature of
LLNs. By carefully studying the interaction between the transport and link
layers, we identify subtle but important modifications to both, achieving TCP
goodput within 25% of an upper bound (5-40x higher than prior results) and
low-power operation commensurate to CoAP in a practical LLN application
scenario. This suggests that a TCP-based transport layer, seamlessly
interoperable with existing TCP/IP networks, is viable and performant in LLNs.",Hamilton-based ultrasonic anemometer,What is the function of the Hamilton-based PCB in the ultrasonic anemometer?
spiqa_246,1611.05742v3,"In the Projection Block of the Grassmann Network architecture, as illustrated in the figure, what is the role of the ReOrth Layer in re-orthogonalizing the output from the FRMap Layer?",The ReOrth Layer re-orthogonalizes the output of the FRMap Layer.,1611.05742v3.pdf,"['1611.05742v3.pdf', '1906.06589v3.pdf', '1708.05239v3.pdf', '1603.03833v4.pdf', '1708.03797v1.pdf', '1812.10735v2.pdf', '1805.06431v4.pdf', '1804.07849v4.pdf', '1809.02731v3.pdf', '1809.01246v1.pdf', '1611.07718v2.pdf', '1706.04269v2.pdf']",The figure shows that the ReOrth Layer takes the output of the FRMap Layer as its input and outputs a re-orthogonalized version of that input.,1611.05742v3-Figure1-1.png,Building Deep Networks on Grassmann Manifolds,"Learning representations on Grassmann manifolds is popular in quite a few
visual recognition tasks. In order to enable deep learning on Grassmann
manifolds, this paper proposes a deep network architecture by generalizing the
Euclidean network paradigm to Grassmann manifolds. In particular, we design
full rank mapping layers to transform input Grassmannian data to more desirable
ones, exploit re-orthonormalization layers to normalize the resulting matrices,
study projection pooling layers to reduce the model complexity in the
Grassmannian context, and devise projection mapping layers to respect
Grassmannian geometry and meanwhile achieve Euclidean forms for regular output
layers. To train the Grassmann networks, we exploit a stochastic gradient
descent setting on manifolds of the connection weights, and study a matrix
generalization of backpropagation to update the structured data. The
evaluations on three visual recognition tasks show that our Grassmann networks
have clear advantages over existing Grassmann learning methods, and achieve
results comparable with state-of-the-art approaches.","Conceptual illustration of the proposed Grassmann Network (GrNet) architecture. The rectangles in blue represent three basic blocks, i.e., Projection, Pooling and Output blocks, respectively.",What is the function of the ReOrth Layer in the Projection Block of the Grassmann Network architecture?
spiqa_247,1705.02946v3,"In the table presented in the figure captioned ""Initial configuration for envy-free lower bound"" in the context of cake cutting, what is the value of V2 in the interval [0.35, 0.67]?",0.35,1705.02946v3.pdf,"['1705.02946v3.pdf', '1805.06431v4.pdf', '1704.07854v4.pdf', '1804.07849v4.pdf', '1710.01507v4.pdf', '1701.06171v4.pdf', '1611.04684v1.pdf', '1812.10735v2.pdf', '1804.04786v3.pdf', '1901.00056v2.pdf', '1809.02731v3.pdf', '1707.01922v5.pdf', '1812.00108v4.pdf', '1809.04276v2.pdf', '1706.00827v2.pdf']","The table shows the initial values of V1, V2, and V3 in different intervals. The value of V2 in the interval [0.35, 0.67] is shown in the second row and fourth column of the table.",1705.02946v3-Table2-1.png,The Query Complexity of Cake Cutting,"We study the query complexity of cake cutting and give lower and upper bounds
for computing approximately envy-free, perfect, and equitable allocations with
the minimum number of cuts. The lower bounds are tight for computing connected
envy-free allocations among n=3 players and for computing perfect and equitable
allocations with minimum number of cuts between n=2 players.
  We also formalize moving knife procedures and show that a large subclass of
this family, which captures all the known moving knife procedures, can be
simulated efficiently with arbitrarily small error in the Robertson-Webb query
model.",Initial configuration for envy-free lower bound.,"What is the initial value of V2 in the interval [0.35, 0.67]?"
spiqa_248,1811.07073v3,"In Figure 3, what outputs generated by the primary and ancillary models are concatenated and used as input to the convolutional self-correction model?",The input to the convolutional self-correction model is the logits generated by the primary and ancillary models.,1811.07073v3.pdf,"['1811.07073v3.pdf', '1705.09296v2.pdf', '1708.03797v1.pdf', '1705.07164v8.pdf', '1811.02721v3.pdf', '1611.02654v2.pdf', '1812.10735v2.pdf', '1706.08146v3.pdf', '1611.07718v2.pdf', '1702.03584v3.pdf', '1704.00774v3.pdf', '1708.02153v2.pdf', '1706.00827v2.pdf']",The figure shows that the primary and ancillary logits are fed into the convolutional self-correction model.,1811.07073v3-Figure3-1.png,Semi-Supervised Semantic Image Segmentation with Self-correcting Networks,"Building a large image dataset with high-quality object masks for semantic
segmentation is costly and time consuming. In this paper, we introduce a
principled semi-supervised framework that only uses a small set of fully
supervised images (having semantic segmentation labels and box labels) and a
set of images with only object bounding box labels (we call it the weak set).
Our framework trains the primary segmentation model with the aid of an
ancillary model that generates initial segmentation labels for the weak set and
a self-correction module that improves the generated labels during training
using the increasingly accurate primary model. We introduce two variants of the
self-correction module using either linear or convolutional functions.
Experiments on the PASCAL VOC 2012 and Cityscape datasets show that our models
trained with a small fully supervised set perform similar to, or better than,
models trained with a large fully supervised set while requiring ~7x less
annotation effort.","Figure 3: Convolutional self-correction model learns refining the input label distributions. The subnetwork receives logits from the primary and ancillary models, then concatenates and feeds the output to a two-layer CNN.",What is the input to the convolutional self-correction model?
spiqa_249,1707.01922v5,"Based on Table 1 in the *Zero-Shot Deep Domain Adaptation* paper, how does the availability of task-relevant target-domain (T-R) training data differ between ZDDA, UDA, and MVL?","The key difference lies in the availability of target-domain training data. While UDA and MVL methods require T-R training data from the target domain, ZDDA does not. ZDDA only requires T-R training data from a single source domain.",1707.01922v5.pdf,"['1707.01922v5.pdf', '1703.10730v2.pdf', '1703.07015v3.pdf', '1603.00286v5.pdf', '1811.09393v4.pdf', '1805.04687v2.pdf', '1703.00060v2.pdf', '1706.00827v2.pdf', '1707.00189v3.pdf', '1705.07164v8.pdf', '1709.02418v2.pdf']","Table 1 clearly shows this distinction in the first row. Both UDA and MVL have a green ""Y"" under the column ""given T-R target-domain training data?"", indicating that they require such data. In contrast, ZDDA has a red ""N"" under the same column, signifying that it does not need target-domain training data. The passage further emphasizes this difference by stating that ""in ZDDA, T-R target-domain training data is unavailable and the only available T-R training data is in one source domain.""",1707.01922v5-Table1-1.png,Zero-Shot Deep Domain Adaptation,"Domain adaptation is an important tool to transfer knowledge about a task
(e.g. classification) learned in a source domain to a second, or target domain.
Current approaches assume that task-relevant target-domain data is available
during training. We demonstrate how to perform domain adaptation when no such
task-relevant target-domain data is available. To tackle this issue, we propose
zero-shot deep domain adaptation (ZDDA), which uses privileged information from
task-irrelevant dual-domain pairs. ZDDA learns a source-domain representation
which is not only tailored for the task of interest but also close to the
target-domain representation. Therefore, the source-domain task of interest
solution (e.g. a classifier for classification tasks) which is jointly trained
with the source-domain representation can be applicable to both the source and
target representations. Using the MNIST, Fashion-MNIST, NIST, EMNIST, and SUN
RGB-D datasets, we show that ZDDA can perform domain adaptation in
classification tasks without access to task-relevant target-domain training
data. We also extend ZDDA to perform sensor fusion in the SUN RGB-D scene
classification task by simulating task-relevant target-domain representations
with task-relevant source-domain data. To the best of our knowledge, ZDDA is
the first domain adaptation and sensor fusion method which requires no
task-relevant target-domain data. The underlying principle is not particular to
computer vision data, but should be extensible to other domains.","Table 1. Problem setting comparison between ZDDA, unsupervised domain adaptation (UDA), multi-view learning (MVL), and domain generalization (DG)",What is the key difference between ZDDA and UDA/MVL in terms of the available training data?
spiqa_250,1811.09393v4,"Referring to the ""Training parameters"" figure in the paper, what is the generator learning rate specified in the DsOnly column under the VSR Param section?",5.00E-05,1811.09393v4.pdf,"['1811.09393v4.pdf', '1708.00160v2.pdf', '1704.04539v2.pdf', '1603.00286v5.pdf', '1803.03467v4.pdf', '1706.04269v2.pdf', '1708.02153v2.pdf', '1812.10735v2.pdf', '1802.07459v2.pdf', '1809.03149v2.pdf', '1708.03797v1.pdf']",The learning rate for the generator is listed in the table under the VSR Param section for the DsOnly model.,1811.09393v4-Table6-1.png,Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation,"Our work explores temporal self-supervision for GAN-based video generation
tasks. While adversarial training successfully yields generative models for a
variety of areas, temporal relationships in the generated data are much less
explored. Natural temporal changes are crucial for sequential generation tasks,
e.g. video super-resolution and unpaired video translation. For the former,
state-of-the-art methods often favor simpler norm losses such as $L^2$ over
adversarial training. However, their averaging nature easily leads to
temporally smooth results with an undesirable lack of spatial detail. For
unpaired video translation, existing approaches modify the generator networks
to form spatio-temporal cycle consistencies. In contrast, we focus on improving
learning objectives and propose a temporally self-supervised algorithm. For
both tasks, we show that temporal adversarial learning is key to achieving
temporally coherent solutions without sacrificing spatial detail. We also
propose a novel Ping-Pong loss to improve the long-term temporal consistency.
It effectively prevents recurrent networks from accumulating artifacts
temporally without depressing detailed features. Additionally, we propose a
first set of metrics to quantitatively evaluate the accuracy as well as the
perceptual quality of the temporal evolution. A series of user studies confirm
the rankings computed with these metrics. Code, data, models, and results are
provided at https://github.com/thunil/TecoGAN. The project page
https://ge.in.tum.de/publications/2019-tecogan-chu/ contains supplemental
materials.",Training parameters,What is the learning rate for the generator in the DsOnly model?
spiqa_251,1705.10667v4,"Based on the figure in the ""Conditional Adversarial Domain Adaptation"" paper, how does the conditioning mechanism used by the domain discriminator differ between the Multilinear Conditioning and Randomized Multilinear Conditioning architectures when combining feature representations and classifier predictions?","The main difference is that the Multilinear Conditioning architecture uses a multilinear map to condition the domain discriminator on the classifier prediction, while the Randomized Multilinear Conditioning architecture uses a randomized multilinear map.",1705.10667v4.pdf,"['1705.10667v4.pdf', '1704.05958v2.pdf', '1703.04887v4.pdf', '1811.08257v1.pdf', '1704.07121v2.pdf', '1703.07015v3.pdf', '1710.01507v4.pdf']","The figure shows that the Multilinear Conditioning architecture uses a multilinear map to combine the feature representation and the classifier prediction, while the Randomized Multilinear Conditioning architecture uses a randomized multilinear map to do the same thing. The randomized multilinear map is more efficient than the multilinear map, especially for high-dimensional data.",1705.10667v4-Figure1-1.png,Conditional Adversarial Domain Adaptation,"Adversarial learning has been embedded into deep networks to learn
disentangled and transferable representations for domain adaptation. Existing
adversarial domain adaptation methods may not effectively align different
domains of multimodal distributions native in classification problems. In this
paper, we present conditional adversarial domain adaptation, a principled
framework that conditions the adversarial adaptation models on discriminative
information conveyed in the classifier predictions. Conditional domain
adversarial networks (CDANs) are designed with two novel conditioning
strategies: multilinear conditioning that captures the cross-covariance between
feature representations and classifier predictions to improve the
discriminability, and entropy conditioning that controls the uncertainty of
classifier predictions to guarantee the transferability. With theoretical
guarantees and a few lines of codes, the approach has exceeded state-of-the-art
results on five datasets.","Architectures of Conditional Domain Adversarial Networks (CDAN) for domain adaptation, where domain-specific feature representation f and classifier prediction g embody the cross-domain gap to be reduced jointly by the conditional domain discriminatorD. (a) Multilinear (M) Conditioning, applicable to lower-dimensional scenario, where D is conditioned on classifier prediction g via multilinear map f ⊗ g; (b) Randomized Multilinear (RM) Conditioning, fit to higher-dimensional scenario, where D is conditioned on classifier prediction g via randomized multilinear map 1√ d (Rf f) (Rgg). Entropy Conditioning (dashed line) leads to CDAN+E that prioritizesD on easy-to-transfer examples.",What is the main difference between the Multilinear Conditioning architecture and the Randomized Multilinear Conditioning architecture?
spiqa_252,1603.00286v5,"Based on Figure 2 in the ""Redividing the Cake"" paper, what is the minimum number of sides required for a rectilinear polygon that has exactly four circled reflex vertices?",Six.,1603.00286v5.pdf,"['1603.00286v5.pdf', '1809.00263v5.pdf', '1805.04609v3.pdf', '1809.04276v2.pdf', '1707.08608v3.pdf']","A reflex vertex is a vertex with an interior angle greater than 180 degrees. In a rectilinear polygon, each vertex is either a right angle or a reflex angle. Therefore, the minimum number of sides that a rectilinear polygon with four reflex vertices must have is six, as shown in the figure.",1603.00286v5-Figure2-1.png,Redividing the Cake,"The paper considers fair allocation of resources that are already allocated
in an unfair way. This setting requires a careful balance between the fairness
considerations and the rights of the present owners.
  The paper presents re-division algorithms that attain various trade-off
points between fairness and ownership rights, in various settings differing in
the geometric constraints on the allotments: (a) no geometric constraints; (b)
connectivity -- the cake is a one-dimensional interval and each piece must be a
contiguous interval; (c) rectangularity -- the cake is a two-dimensional
rectangle or rectilinear polygon and the pieces should be rectangles; (d)
convexity -- the cake is a two-dimensional convex polygon and the pieces should
be convex.
  These re-division algorithms have implications on another problem: the
price-of-fairness -- the loss of social welfare caused by fairness
requirements. Each algorithm implies an upper bound on the price-of-fairness
with the respective geometric constraints.",Fig. 2 A rectilinear polygon with T = 4 reflex vertices (circled).,What is the minimum number of sides that a rectilinear polygon with four reflex vertices must have?
spiqa_253,1805.04687v2,"Based on the figure depicting the distribution of scene categories in the BDD100K dataset, which specific scene type has the highest number of instances?",City Street,1805.04687v2.pdf,"['1805.04687v2.pdf', '1805.06431v4.pdf', '1703.10730v2.pdf', '1805.02349v2.pdf', '1611.02654v2.pdf', '1811.08257v1.pdf']","The figure shows that City Street has the highest number of instances, with approximately 61,960 instances.",1805.04687v2-Figure10-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.","Distribution of images in weather, scene, and day hours categories.",What is the most common type of scene in the dataset?
spiqa_254,1804.05995v2,"Referring to the figure illustrating the section-count–based recommendation method in the paper on structuring Wikipedia articles, what is the percentage of categories that can generate at least 10 section recommendations?",Around 68%.,1804.05995v2.pdf,"['1804.05995v2.pdf', '1809.02731v3.pdf', '1708.00160v2.pdf', '1704.04539v2.pdf', '1708.06832v3.pdf', '1705.10667v4.pdf', '1710.05654v2.pdf', '1805.04609v3.pdf', '1708.01425v4.pdf', '1811.07073v3.pdf', '1803.05776v2.pdf', '1809.00458v1.pdf']",The figure shows that the percentage of categories that can generate at least 10 recommendations is 60%. This can be seen by looking at the bar for length 10 on the x-axis and following it up to the y-axis.,1804.05995v2-Figure4-1.png,Structuring Wikipedia Articles with Section Recommendations,"Sections are the building blocks of Wikipedia articles. They enhance
readability and can be used as a structured entry point for creating and
expanding articles. Structuring a new or already existing Wikipedia article
with sections is a hard task for humans, especially for newcomers or less
experienced editors, as it requires significant knowledge about how a
well-written article looks for each possible topic. Inspired by this need, the
present paper defines the problem of section recommendation for Wikipedia
articles and proposes several approaches for tackling it. Our systems can help
editors by recommending what sections to add to already existing or newly
created Wikipedia articles. Our basic paradigm is to generate recommendations
by sourcing sections from articles that are similar to the input article. We
explore several ways of defining similarity for this purpose (based on topic
modeling, collaborative filtering, and Wikipedia's category system). We use
both automatic and human evaluation approaches for assessing the performance of
our recommendation system, concluding that the category-based approach works
best, achieving precision@10 of about 80% in the human evaluation.",Percentage of categories (y-axis) that can generate at least x recommendations using the section-count–based,What is the percentage of categories that can generate at least 10 recommendations using the section-count-based method?
spiqa_255,1706.03847v3,"According to the figure captioned ""Performance of GRU4Rec relative to the baseline in the online A/B test,"" how does GRU4Rec perform compared to the baseline in terms of watch time?",GRU4Rec has a slightly higher performance than the baseline in terms of watch time.,1706.03847v3.pdf,"['1706.03847v3.pdf', '1811.08257v1.pdf', '1705.09296v2.pdf', '1804.07849v4.pdf', '1611.03780v2.pdf', '1704.08615v2.pdf', '1710.05654v2.pdf', '1704.07854v4.pdf', '1906.06589v3.pdf', '1709.08294v3.pdf', '1707.01922v5.pdf', '1803.01128v3.pdf', '1702.03584v3.pdf', '1611.02654v2.pdf']","The bar for GRU4Rec in the ""Watch time"" category is slightly higher than the bar for the baseline.",1706.03847v3-Figure6-1.png,Recurrent Neural Networks with Top-k Gains for Session-based Recommendations,"RNNs have been shown to be excellent models for sequential data and in
particular for data that is generated by users in an session-based manner. The
use of RNNs provides impressive performance benefits over classical methods in
session-based recommendations. In this work we introduce novel ranking loss
functions tailored to RNNs in the recommendation setting. The improved
performance of these losses over alternatives, along with further tricks and
refinements described in this work, allow for an overall improvement of up to
35% in terms of MRR and Recall@20 over previous session-based RNN solutions and
up to 53% over classical collaborative filtering approaches. Unlike data
augmentation-based improvements, our method does not increase training times
significantly. We further demonstrate the performance gain of the RNN over
baselines in an online A/B test.",Performance of GRU4Rec relative to the baseline in the online A/B test.,What is the performance of GRU4Rec relative to the baseline in terms of watch time?
spiqa_256,1708.00160v2,"According to the figure showing the Conditional FP-Tree for the pattern {head=F, ant=NAM}, what is the support value of this pattern in the dataset?",1,1708.00160v2.pdf,"['1708.00160v2.pdf', '1611.04684v1.pdf', '1705.07164v8.pdf', '1705.09882v2.pdf', '1805.00912v4.pdf', '1612.02803v5.pdf']","The figure shows the conditional FP-Tree for the pattern {head=F, ant=NAM}. The numbers in parentheses next to each node represent the support of the itemset represented by the path from the root to that node. The support of the pattern {head=F, ant=NAM} is 1.0, which means that the pattern appears in all of the transactions in the data.",1708.00160v2-Figure4-1.png,Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers,"Coreference resolution is an intermediate step for text understanding. It is
used in tasks and domains for which we do not necessarily have coreference
annotated corpora. Therefore, generalization is of special importance for
coreference resolution. However, while recent coreference resolvers have
notable improvements on the CoNLL dataset, they struggle to generalize properly
to new domains or datasets. In this paper, we investigate the role of
linguistic features in building more generalizable coreference resolvers. We
show that generalization improves only slightly by merely using a set of
additional linguistic features. However, employing features and subsets of
their values that are informative for coreference resolution, considerably
improves generalization. Thanks to better generalization, our system achieves
state-of-the-art results in out-of-domain evaluations, e.g., on WikiCoref, our
system, which is trained on CoNLL, achieves on-par performance with a system
designed for this dataset.","Conditional FP-Tree for the p = {head=F, ant=NAM} pattern.","What is the probability of finding the pattern {head=F, ant=NAM} in the data?"
spiqa_257,1706.03847v3,"According to the figure illustrating mini-batch based negative sampling in the ""Recurrent Neural Networks with Top-k Gains for Session-based Recommendations"" paper, how does the sampling of a small subset of negative items contribute to reducing computational cost while maintaining training accuracy?",Negative sampling is a technique used to reduce the number of negative examples that need to be considered during training.,1706.03847v3.pdf,"['1706.03847v3.pdf', '1705.02946v3.pdf', '1809.02731v3.pdf', '1812.06589v2.pdf', '1611.04363v2.pdf', '1811.07073v3.pdf', '1811.06635v1.pdf', '1901.00398v2.pdf', '1708.06832v3.pdf', '1805.04687v2.pdf', '1811.09393v4.pdf', '1804.00863v3.pdf', '1803.06506v3.pdf', '1608.02784v2.pdf', '1811.02553v4.pdf']","The figure shows how negative sampling works. A mini-batch of desired items is fed into the network, and the network outputs a score for each item. The target scores are then computed, with a score of 1 for the positive item and a score of 0 for all other items. However, instead of considering all negative items, only a small number of negative items are sampled. This reduces the computational cost of training while still providing a good approximation of the true loss function.",1706.03847v3-Figure1-1.png,Recurrent Neural Networks with Top-k Gains for Session-based Recommendations,"RNNs have been shown to be excellent models for sequential data and in
particular for data that is generated by users in an session-based manner. The
use of RNNs provides impressive performance benefits over classical methods in
session-based recommendations. In this work we introduce novel ranking loss
functions tailored to RNNs in the recommendation setting. The improved
performance of these losses over alternatives, along with further tricks and
refinements described in this work, allow for an overall improvement of up to
35% in terms of MRR and Recall@20 over previous session-based RNN solutions and
up to 53% over classical collaborative filtering approaches. Unlike data
augmentation-based improvements, our method does not increase training times
significantly. We further demonstrate the performance gain of the RNN over
baselines in an online A/B test.",Mini-batch based negative sampling.,What is the purpose of negative sampling?
spiqa_258,1805.06431v4,"In the figure illustrating the Cholesky Block in the paper on Task Agnostic Robust Learning with noisy outputs, how does the Cholesky Block process the target and auxiliary matrices to distinguish between abnormal and normal patterns?",The Cholesky Block is used to distinguish abnormal patterns from normal patterns.,1805.06431v4.pdf,"['1805.06431v4.pdf', '1802.07222v1.pdf', '1603.03833v4.pdf', '1805.06447v3.pdf', '1803.04383v2.pdf', '1804.00863v3.pdf', '1705.09296v2.pdf', '1709.02755v5.pdf', '1707.06320v2.pdf', '1701.03077v10.pdf', '1703.00899v2.pdf']",The Cholesky Block takes the target weight matrix W∗ and auxiliary matrix Z as input and outputs a correlated weight matrix W̃k. This correlated weight matrix is then used to learn the abnormal patterns.,1805.06431v4-Figure2-1.png,Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks,"In this paper, we focus on weakly supervised learning with noisy training
data for both classification and regression problems.We assume that the
training outputs are collected from a mixture of a target and correlated noise
distributions.Our proposed method simultaneously estimates the target
distribution and the quality of each data which is defined as the correlation
between the target and data generating distributions.The cornerstone of the
proposed method is a Cholesky Block that enables modeling dependencies among
mixture distributions in a differentiable manner where we maintain the
distribution over the network weights.We first provide illustrative examples in
both regression and classification tasks to show the effectiveness of the
proposed method.Then, the proposed method is extensively evaluated in a number
of experiments where we show that it constantly shows comparable or superior
performances compared to existing baseline methods in the handling of noisy
data.","Illustration of a Cholesky Block. Every block shares target weight matrix W∗ and auxiliary matrix Z, and outputs correlated weight matrix W̃k through CholeskyTransform (see (5)) to distinguish the abnormal pattern from normal one which will be learned by W∗.",What is the purpose of the Cholesky Block in this figure?
spiqa_259,1811.09393v4,"How does the UVT cycle link, as shown in the figure of two recurrent generators, facilitate temporal coherence and knowledge transfer between domain A and domain B in the context of the proposed GAN-based video generation method?",The UVT cycle link is used to transfer knowledge between two recurrent generators.,1811.09393v4.pdf,"['1811.09393v4.pdf', '1804.04410v2.pdf', '1611.04684v1.pdf', '1809.04276v2.pdf', '1803.05776v2.pdf', '1703.02507v3.pdf', '1603.00286v5.pdf']","The figure shows that the UVT cycle link connects two recurrent generators, one for domain A and one for domain B. The generators are connected in a cycle, so that each generator can learn from the other. This allows the generators to share knowledge and improve their performance.",1811.09393v4-Figure5-1.png,Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation,"Our work explores temporal self-supervision for GAN-based video generation
tasks. While adversarial training successfully yields generative models for a
variety of areas, temporal relationships in the generated data are much less
explored. Natural temporal changes are crucial for sequential generation tasks,
e.g. video super-resolution and unpaired video translation. For the former,
state-of-the-art methods often favor simpler norm losses such as $L^2$ over
adversarial training. However, their averaging nature easily leads to
temporally smooth results with an undesirable lack of spatial detail. For
unpaired video translation, existing approaches modify the generator networks
to form spatio-temporal cycle consistencies. In contrast, we focus on improving
learning objectives and propose a temporally self-supervised algorithm. For
both tasks, we show that temporal adversarial learning is key to achieving
temporally coherent solutions without sacrificing spatial detail. We also
propose a novel Ping-Pong loss to improve the long-term temporal consistency.
It effectively prevents recurrent networks from accumulating artifacts
temporally without depressing detailed features. Additionally, we propose a
first set of metrics to quantitatively evaluate the accuracy as well as the
perceptual quality of the temporal evolution. A series of user studies confirm
the rankings computed with these metrics. Code, data, models, and results are
provided at https://github.com/thunil/TecoGAN. The project page
https://ge.in.tum.de/publications/2019-tecogan-chu/ contains supplemental
materials.","a) The UVT cycle link formed by two recurrent generators. b) Unconditional UVT Ds,t .",What is the purpose of the UVT cycle link?
spiqa_260,1811.08257v1,"Based on the figure comparing Original ReLU and Max Pooling with your proposed Max Pooling and ReLU in FALCON, what is the role of the `SubsetGate` function in splitting the input `x` into `k` pieces during secure Max Pooling?",The `SubsetGate` function is used to split the input `x` into `k` pieces.,1811.08257v1.pdf,"['1811.08257v1.pdf', '1611.02654v2.pdf', '1703.00060v2.pdf']",The code snippet in the figure shows that the `SubsetGate` function is called with the input `x` as an argument. The comment above the function call states that this is done to split `x` into `k` pieces.,1811.08257v1-Figure5-1.png,FALCON: A Fourier Transform Based Approach for Fast and Secure Convolutional Neural Network Predictions,"Machine learning as a service has been widely deployed to utilize deep neural
network models to provide prediction services. However, this raises privacy
concerns since clients need to send sensitive information to servers. In this
paper, we focus on the scenario where clients want to classify private images
with a convolutional neural network model hosted in the server, while both
parties keep their data private. We present FALCON, a fast and secure approach
for CNN predictions based on Fourier Transform. Our solution enables linear
layers of a CNN model to be evaluated simply and efficiently with fully
homomorphic encryption. We also introduce the first efficient and
privacy-preserving protocol for softmax function, which is an indispensable
component in CNNs and has not yet been evaluated in previous works due to its
high complexity. We implemented the FALCON and evaluated the performance on
real-world CNN models. The experimental results show that FALCON outperforms
the best known works in both computation and communication cost.",Original ReLU and Max Pooling v.s. Our Max Pooling and ReLU.,What is the purpose of the `SubsetGate` function in the MaxPooling function?
spiqa_261,1811.08257v1,"In the figure from the FALCON paper illustrating a CNN architecture, what functional advantage does the ReLU activation layer provide in enhancing the model's ability to learn complex patterns after the convolution operations?",The activation layer applies a non-linear function to the output of the convolution layer. This allows the network to learn more complex features from the data.,1811.08257v1.pdf,"['1811.08257v1.pdf', '1707.08608v3.pdf', '1611.07718v2.pdf', '1611.05742v3.pdf']","The activation layer is shown in the figure as a box with the label ""ReLU"". The ReLU function is a common activation function that sets all negative values to zero and keeps all positive values the same. This non-linearity allows the network to learn more complex features than if it only used linear functions.",1811.08257v1-Figure1-1.png,FALCON: A Fourier Transform Based Approach for Fast and Secure Convolutional Neural Network Predictions,"Machine learning as a service has been widely deployed to utilize deep neural
network models to provide prediction services. However, this raises privacy
concerns since clients need to send sensitive information to servers. In this
paper, we focus on the scenario where clients want to classify private images
with a convolutional neural network model hosted in the server, while both
parties keep their data private. We present FALCON, a fast and secure approach
for CNN predictions based on Fourier Transform. Our solution enables linear
layers of a CNN model to be evaluated simply and efficiently with fully
homomorphic encryption. We also introduce the first efficient and
privacy-preserving protocol for softmax function, which is an indispensable
component in CNNs and has not yet been evaluated in previous works due to its
high complexity. We implemented the FALCON and evaluated the performance on
real-world CNN models. The experimental results show that FALCON outperforms
the best known works in both computation and communication cost.",An example of convolutional neural networks.,What is the purpose of the activation layer in a convolutional neural network?
spiqa_262,1811.07073v3,"How does the ancillary heatmap in the figure depicting results on the PASCAL VOC 2012 weak set help refine object labels, specifically for missing or oversegmented objects?",The ancillary heatmap is used to correct the labels for missing or oversegmented objects in the images.,1811.07073v3.pdf,"['1811.07073v3.pdf', '1802.07222v1.pdf', '1804.07849v4.pdf', '1803.01128v3.pdf', '1809.00458v1.pdf', '1705.07384v2.pdf', '1605.07496v3.pdf', '1706.00633v4.pdf', '1812.06589v2.pdf', '1804.01429v3.pdf']",The heatmap shows the areas of the image that the ancillary model predicts to belong to a particular class. The areas marked by ellipses are examples of where the ancillary model is able to correct the labels for missing or oversegmented objects.,1811.07073v3-Figure5-1.png,Semi-Supervised Semantic Image Segmentation with Self-correcting Networks,"Building a large image dataset with high-quality object masks for semantic
segmentation is costly and time consuming. In this paper, we introduce a
principled semi-supervised framework that only uses a small set of fully
supervised images (having semantic segmentation labels and box labels) and a
set of images with only object bounding box labels (we call it the weak set).
Our framework trains the primary segmentation model with the aid of an
ancillary model that generates initial segmentation labels for the weak set and
a self-correction module that improves the generated labels during training
using the increasingly accurate primary model. We introduce two variants of the
self-correction module using either linear or convolutional functions.
Experiments on the PASCAL VOC 2012 and Cityscape datasets show that our models
trained with a small fully supervised set perform similar to, or better than,
models trained with a large fully supervised set while requiring ~7x less
annotation effort.",Qualitative results on the PASCAL VOC 2012 auxiliary (the weak set). The heatmap of a single class for the ancillary model is shown for several examples. The ancillary model can successfully correct the labels for missing or oversegmented objects in these images (marked by ellipses).,What is the purpose of the ancillary heatmap shown in this paper?
spiqa_263,1804.04786v3,"Referring to Figure 2, what role does the Audio Encoder play in the conditional recurrent adversarial video generation network when transforming MFCC features of audio segments into audio features?",The audio encoder extracts audio features from the MFCC features of each audio segment.,1804.04786v3.pdf,"['1804.04786v3.pdf', '1705.07164v8.pdf', '1805.04687v2.pdf', '1809.00458v1.pdf', '1706.00827v2.pdf', '1605.07496v3.pdf']"," In Fig.~\ref{fig:flow}, the audio encoder is represented by the block labeled ""Audio Encoder."" It takes as input the MFCC features of each audio segment, denoted by $A_t$, and outputs the corresponding audio feature, denoted by $z^A_t$.",1804.04786v3-Figure2-1.png,Talking Face Generation by Conditional Recurrent Adversarial Network,"Given an arbitrary face image and an arbitrary speech clip, the proposed work
attempts to generating the talking face video with accurate lip synchronization
while maintaining smooth transition of both lip and facial movement over the
entire video clip. Existing works either do not consider temporal dependency on
face images across different video frames thus easily yielding
noticeable/abrupt facial and lip movement or are only limited to the generation
of talking face video for a specific person thus lacking generalization
capacity. We propose a novel conditional video generation network where the
audio input is treated as a condition for the recurrent adversarial network
such that temporal dependency is incorporated to realize smooth transition for
the lip and facial movement. In addition, we deploy a multi-task adversarial
training scheme in the context of video generation to improve both
photo-realism and the accuracy for lip synchronization. Finally, based on the
phoneme distribution information extracted from the audio clip, we develop a
sample selection method that effectively reduces the size of the training
dataset without sacrificing the quality of the generated video. Extensive
experiments on both controlled and uncontrolled datasets demonstrate the
superiority of the proposed approach in terms of visual quality, lip sync
accuracy, and smooth transition of lip and facial movement, as compared to the
state-of-the-art.",Figure 2: The proposed conditional recurrent adversarial video generation network structure.,What is the purpose of the audio encoder in the proposed conditional recurrent adversarial video generation network structure?
spiqa_264,1603.00286v5,"In Fig. 4 of the ""Redividing the Cake"" paper, how is the blank space labeled Z'5 utilized to complete the allocation of the original pieces Z1-Z4 in the allocation-completion process?",The blank space labeled Z'5 is used to complete the allocation of the original pieces.,1603.00286v5.pdf,"['1603.00286v5.pdf', '1611.07718v2.pdf', '1811.07073v3.pdf', '1707.00524v2.pdf', '1809.02731v3.pdf', '1707.01917v2.pdf', '1703.07015v3.pdf', '1706.03847v3.pdf', '1805.01216v3.pdf', '1809.03550v3.pdf']",The figure shows how the original pieces (labeled Z1-Z4) can be allocated to the blank space (labeled Z'5) in order to complete the puzzle.,1603.00286v5-Figure4-1.png,Redividing the Cake,"The paper considers fair allocation of resources that are already allocated
in an unfair way. This setting requires a careful balance between the fairness
considerations and the rights of the present owners.
  The paper presents re-division algorithms that attain various trade-off
points between fairness and ownership rights, in various settings differing in
the geometric constraints on the allotments: (a) no geometric constraints; (b)
connectivity -- the cake is a one-dimensional interval and each piece must be a
contiguous interval; (c) rectangularity -- the cake is a two-dimensional
rectangle or rectilinear polygon and the pieces should be rectangles; (d)
convexity -- the cake is a two-dimensional convex polygon and the pieces should
be convex.
  These re-division algorithms have implications on another problem: the
price-of-fairness -- the loss of social welfare caused by fairness
requirements. Each algorithm implies an upper bound on the price-of-fairness
with the respective geometric constraints.","Fig. 4 Allocation-completion with n = 4 original pieces and b = 1 blank, denoted Z′5.",What is the purpose of the blank space labeled Z'5?
spiqa_265,1812.00281v3,"Based on the figure illustrating the training setup for 3D mesh prediction in the HUMBI dataset, how does the decoder refine or process the intermediate outputs from the regression network to generate the final 3D mesh model of human body expressions?",The decoder is responsible for generating the final 3D mesh from the intermediate representations produced by the regression network.,1812.00281v3.pdf,"['1812.00281v3.pdf', '1809.00263v5.pdf', '1705.07164v8.pdf', '1605.07496v3.pdf', '1710.05654v2.pdf', '1811.06635v1.pdf', '1706.04284v3.pdf', '1702.03584v3.pdf', '1709.02418v2.pdf', '1707.01922v5.pdf', '1812.00108v4.pdf', '1612.02803v5.pdf', '1805.04609v3.pdf', '1901.00398v2.pdf', '1611.04684v1.pdf']",The figure shows that the decoder takes the outputs of the regression network (Output 1 and Output 2) as input and produces the final 3D mesh. This suggests that the decoder performs some sort of post-processing or refinement on the intermediate representations to generate the final output.,1812.00281v3-Figure12-1.png,HUMBI: A Large Multiview Dataset of Human Body Expressions,"This paper presents a new large multiview dataset called HUMBI for human body
expressions with natural clothing. The goal of HUMBI is to facilitate modeling
view-specific appearance and geometry of gaze, face, hand, body, and garment
from assorted people. 107 synchronized HD cameras are used to capture 772
distinctive subjects across gender, ethnicity, age, and physical condition.
With the multiview image streams, we reconstruct high fidelity body expressions
using 3D mesh models, which allows representing view-specific appearance using
their canonical atlas. We demonstrate that HUMBI is highly effective in
learning and reconstructing a complete human model and is complementary to the
existing datasets of human body expressions with limited views and subjects
such as MPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets.",The training setup for 3D mesh prediction from a single image.,What is the purpose of the decoder in the 3D mesh prediction pipeline?
spiqa_266,1805.00912v4,"How does the positional mask, as depicted in the figure of the ""Tensorized Self-Attention"" (TSA) mechanism, contribute to encoding the relative positions of tokens and influence the computation of attention weights between tokens in the input sequence?","The positional mask is used to provide information about the relative position of tokens in the input sequence. This information is used to compute the attention weights, which determine how much each token attends to each other token.",1805.00912v4.pdf,"['1805.00912v4.pdf', '1704.07854v4.pdf', '1605.07496v3.pdf', '1702.08694v3.pdf']","The figure shows that the positional mask is used as input to the Softmax function, which computes the attention weights. The positional mask is a matrix of values that indicate the relative position of each token in the input sequence. This information is used to compute the attention weights, which determine how much each token attends to each other token.",1805.00912v4-Figure2-1.png,Tensorized Self-Attention: Efficiently Modeling Pairwise and Global Dependencies Together,"Neural networks equipped with self-attention have parallelizable computation,
light-weight structure, and the ability to capture both long-range and local
dependencies. Further, their expressive power and performance can be boosted by
using a vector to measure pairwise dependency, but this requires to expand the
alignment matrix to a tensor, which results in memory and computation
bottlenecks. In this paper, we propose a novel attention mechanism called
""Multi-mask Tensorized Self-Attention"" (MTSA), which is as fast and as
memory-efficient as a CNN, but significantly outperforms previous
CNN-/RNN-/attention-based models. MTSA 1) captures both pairwise (token2token)
and global (source2token) dependencies by a novel compatibility function
composed of dot-product and additive attentions, 2) uses a tensor to represent
the feature-wise alignment scores for better expressive power but only requires
parallelizable matrix multiplications, and 3) combines multi-head with
multi-dimensional attentions, and applies a distinct positional mask to each
head (subspace), so the memory and computation can be distributed to multiple
heads, each with sequential information encoded independently. The experiments
show that a CNN/RNN-free model based on MTSA achieves state-of-the-art or
competitive performance on nine NLP benchmarks with compelling memory- and
time-efficiency.",Tensorized self-attention (TSA) Mechanism.,What is the purpose of the positional mask in the TSA mechanism?
spiqa_267,1802.07351v2,"In the decoding module figure of the ""Deformable Volume Network"" (Devon), specifically labeled in Figure g, what is the role of the residual connection in improving the flow of information by adding outputs of different layers?","The residual connection allows the output of a layer to be added to the output of another layer, which helps to improve the flow of information through the network.",1802.07351v2.pdf,"['1802.07351v2.pdf', '1710.05654v2.pdf', '1809.03550v3.pdf', '1612.02803v5.pdf']","The figure shows a schematic of the decoding module, and the residual connection is labeled as such.",1802.07351v2-Figure7-1.png,Devon: Deformable Volume Network for Learning Optical Flow,"State-of-the-art neural network models estimate large displacement optical
flow in multi-resolution and use warping to propagate the estimation between
two resolutions. Despite their impressive results, it is known that there are
two problems with the approach. First, the multi-resolution estimation of
optical flow fails in situations where small objects move fast. Second, warping
creates artifacts when occlusion or dis-occlusion happens. In this paper, we
propose a new neural network module, Deformable Cost Volume, which alleviates
the two problems. Based on this module, we designed the Deformable Volume
Network (Devon) which can estimate multi-scale optical flow in a single high
resolution. Experiments show Devon is more suitable in handling small objects
moving fast and achieves comparable results to the state-of-the-art methods in
public benchmarks.",Decoding module g. The residual connection denotes the output of a layer is added to the output of another layer.,What is the purpose of the residual connection in the decoding module?
spiqa_268,1802.07351v2,"In the encoding module depicted in Figure f of the Devon paper, how does the residual connection between the Conv 512 × 3 × 3, stride 2 layer and the Conv 512 × 3 × 3, stride 1 layer contribute to improving gradient flow during training and addressing the vanishing gradient problem?","The residual connection adds the output of a layer to the output of another layer, which helps to prevent the vanishing gradient problem.",1802.07351v2.pdf,"['1802.07351v2.pdf', '1701.06171v4.pdf', '1809.00263v5.pdf', '1703.00060v2.pdf', '1805.07567v2.pdf', '1710.06177v2.pdf', '1707.06320v2.pdf', '1704.00774v3.pdf', '1705.02946v3.pdf', '1703.02507v3.pdf', '1708.00160v2.pdf', '1809.03550v3.pdf', '1811.06635v1.pdf']","The figure shows that the residual connection connects the output of the Conv 512 × 3 × 3, stride 2 layer to the output of the Conv 512 × 3 × 3, stride 1 layer. This allows the gradient to flow more easily through the network, which can help to improve training performance.",1802.07351v2-Figure4-1.png,Devon: Deformable Volume Network for Learning Optical Flow,"State-of-the-art neural network models estimate large displacement optical
flow in multi-resolution and use warping to propagate the estimation between
two resolutions. Despite their impressive results, it is known that there are
two problems with the approach. First, the multi-resolution estimation of
optical flow fails in situations where small objects move fast. Second, warping
creates artifacts when occlusion or dis-occlusion happens. In this paper, we
propose a new neural network module, Deformable Cost Volume, which alleviates
the two problems. Based on this module, we designed the Deformable Volume
Network (Devon) which can estimate multi-scale optical flow in a single high
resolution. Experiments show Devon is more suitable in handling small objects
moving fast and achieves comparable results to the state-of-the-art methods in
public benchmarks.",Encoding module f . The residual connection denotes the output of a layer is added to the output of another layer.,What is the purpose of the residual connection in the encoding module?
spiqa_269,1811.10673v1,"Referring to Figure 2 of the paper, how does the second-stage decoder $D_2$ reconstruct frames using the soft edge maps $x_G$, and what purpose does it serve in the adversarial video compression pipeline?",The second-stage decoder $D_2$ takes soft edges $x_G$ as input and produces reconstructed frames.,1811.10673v1.pdf,"['1811.10673v1.pdf', '1611.05742v3.pdf', '1709.02755v5.pdf', '1901.00056v2.pdf', '1603.00286v5.pdf', '1804.01429v3.pdf', '1804.07849v4.pdf', '1809.03149v2.pdf']","The figure shows that the second-stage decoder $D_2$ is trained on the reconstructed frames $X'_I$ and the soft edges $x_G$. After training, $D_2$ is able to generate reconstructed frames from the soft edges $x_G$.",1811.10673v1-Figure2-1.png,Adversarial Video Compression Guided by Soft Edge Detection,"We propose a video compression framework using conditional Generative
Adversarial Networks (GANs). We rely on two encoders: one that deploys a
standard video codec and another which generates low-level maps via a pipeline
of down-sampling, a newly devised soft edge detector, and a novel lossless
compression scheme. For decoding, we use a standard video decoder as well as a
neural network based one, which is trained using a conditional GAN. Recent
""deep"" approaches to video compression require multiple videos to pre-train
generative networks to conduct interpolation. In contrast to this prior work,
our scheme trains a generative decoder on pairs of a very limited number of key
frames taken from a single video and corresponding low-level maps. The trained
decoder produces reconstructed frames relying on a guidance of low-level maps,
without any interpolation. Experiments on a diverse set of 131 videos
demonstrate that our proposed GAN-based compression engine achieves much higher
quality reconstructions at very low bitrates than prevailing standard codecs
such as H.264 or HEVC.","Figure 2: Proposed framework for adversarial video compression. Note that X consists of only one video to be compressed. The video X is partitioned into two sets containing different types of frames: XI and XG. XI is lightly compressed into xI using the standard H.264 encoder, while XG is highly compressed into xG, that contains only soft edge information at low resolution. The XI are used to train a generative model that we call the second-stage decoder D2. This generative model is trained at the receiver using x′I and X ′I using a discriminator DD. After training, D2 takes soft edges xG as input and produces reconstructed frames (see also Figure 6). Only xI and xG are required to reconstruct the decompressed video.",What is the purpose of the second-stage decoder $D_2$?
spiqa_270,1608.02784v2,"What is the significance of the singular value decomposition step on \( D^{-1/2} \Omega D^{-1/2} \) in deriving the projection matrices \( U \) and \( V \), as shown in the figure explaining the CCA algorithm applied to the task of mapping abstract scenes to text?",The singular value decomposition step is used to find the projection matrices U and V.,1608.02784v2.pdf,"['1608.02784v2.pdf', '1704.05958v2.pdf', '1802.07222v1.pdf', '1804.07931v2.pdf', '1707.00524v2.pdf', '1804.07849v4.pdf', '1804.04410v2.pdf']","The figure shows that the singular value decomposition step is performed on the matrix D^(-1/2) * Ω * D^(-1/2). This step results in two projection matrices, U and V, which are used to project the data into a lower-dimensional space.",1608.02784v2-Figure1-1.png,Canonical Correlation Inference for Mapping Abstract Scenes to Text,"We describe a technique for structured prediction, based on canonical
correlation analysis. Our learning algorithm finds two projections for the
input and the output spaces that aim at projecting a given input and its
correct output into points close to each other. We demonstrate our technique on
a language-vision problem, namely the problem of giving a textual description
to an ""abstract scene"".",The CCA learning algorithm.,What is the purpose of the singular value decomposition step in the CCA algorithm?
spiqa_271,1706.04284v3,"How do the skip connections between the feature encoding and decoding modules, as illustrated in Figure (a) of the proposed denoising network, contribute to preserving spatial information and improving image reconstruction during the denoising process?",The skip connections are used to combine the features from the encoding and decoding modules at each scale. This helps to preserve the spatial information that is lost during the downsampling and upsampling operations.,1706.04284v3.pdf,"['1706.04284v3.pdf', '1704.00774v3.pdf', '1707.06320v2.pdf', '1901.00056v2.pdf', '1707.08608v3.pdf', '1704.05426v4.pdf', '1809.02731v3.pdf']","The figure shows that the skip connections connect the output of the feature encoding module at each scale to the input of the feature decoding module at the same scale. This allows the decoding module to access the features from the encoding module, which helps to improve the reconstruction of the image.",1706.04284v3-Figure2-1.png,When Image Denoising Meets High-Level Vision Tasks: A Deep Learning Approach,"Conventionally, image denoising and high-level vision tasks are handled
separately in computer vision. In this paper, we cope with the two jointly and
explore the mutual influence between them. First we propose a convolutional
neural network for image denoising which achieves the state-of-the-art
performance. Second we propose a deep neural network solution that cascades two
modules for image denoising and various high-level tasks, respectively, and use
the joint loss for updating only the denoising network via back-propagation. We
demonstrate that on one hand, the proposed denoiser has the generality to
overcome the performance degradation of different high-level vision tasks. On
the other hand, with the guidance of high-level vision information, the
denoising network can generate more visually appealing results. To the best of
our knowledge, this is the first work investigating the benefit of exploiting
image semantics simultaneously for image denoising and high-level vision tasks
via deep learning. The code is available online
https://github.com/Ding-Liu/DeepDenoising.",(a) Overview of our proposed denoising network. (b) Architecture of the feature encoding module. (c) Architecture of the feature decoding module.,What is the purpose of the skip connections in the proposed denoising network?
spiqa_272,1811.09393v4,"In the user study depicted in the figure, what specific task are participants asked to perform when comparing the perceptual closeness between images ""A"" and ""B"" to the reference video?",The user study is designed to test which of two images is closer to a reference video.,1811.09393v4.pdf,"['1811.09393v4.pdf', '1804.01429v3.pdf', '1703.07015v3.pdf', '1812.06589v2.pdf', '1803.02750v3.pdf', '1702.03584v3.pdf', '1809.01989v2.pdf', '1703.00060v2.pdf', '1804.07931v2.pdf', '1709.08294v3.pdf', '1906.10843v1.pdf', '1708.00160v2.pdf', '1809.03149v2.pdf', '1804.07849v4.pdf']","The image shows three images, one labeled ""Reference"" and two labeled ""A"" and ""B."" The question asks which of the two images is closer to the reference video, suggesting that the user study is designed to test the participants' ability to identify the image that is most similar to the reference video.",1811.09393v4-Figure19-1.png,Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation,"Our work explores temporal self-supervision for GAN-based video generation
tasks. While adversarial training successfully yields generative models for a
variety of areas, temporal relationships in the generated data are much less
explored. Natural temporal changes are crucial for sequential generation tasks,
e.g. video super-resolution and unpaired video translation. For the former,
state-of-the-art methods often favor simpler norm losses such as $L^2$ over
adversarial training. However, their averaging nature easily leads to
temporally smooth results with an undesirable lack of spatial detail. For
unpaired video translation, existing approaches modify the generator networks
to form spatio-temporal cycle consistencies. In contrast, we focus on improving
learning objectives and propose a temporally self-supervised algorithm. For
both tasks, we show that temporal adversarial learning is key to achieving
temporally coherent solutions without sacrificing spatial detail. We also
propose a novel Ping-Pong loss to improve the long-term temporal consistency.
It effectively prevents recurrent networks from accumulating artifacts
temporally without depressing detailed features. Additionally, we propose a
first set of metrics to quantitatively evaluate the accuracy as well as the
perceptual quality of the temporal evolution. A series of user studies confirm
the rankings computed with these metrics. Code, data, models, and results are
provided at https://github.com/thunil/TecoGAN. The project page
https://ge.in.tum.de/publications/2019-tecogan-chu/ contains supplemental
materials.",A sample setup of user study.,What is the purpose of the user study?
spiqa_273,1901.00056v2,"Referring to the figure illustrating ""Hyperparameter settings"" in the Entity Synonym Discovery via Multipiece Bilateral Context Matching paper, what is the specified range of values for the context number hyperparameter in SYNONYMNET?",The range of values for the context number hyperparameter is from 1 to 20.,1901.00056v2.pdf,"['1901.00056v2.pdf', '1804.05995v2.pdf', '1802.07222v1.pdf', '1804.07849v4.pdf', '1811.02553v4.pdf', '1709.02418v2.pdf', '1803.05776v2.pdf', '1708.01425v4.pdf']","The table shows the hyperparameter settings, and the context number hyperparameter is listed with a range of values from 1 to 20.",1901.00056v2-Table5-1.png,Entity Synonym Discovery via Multipiece Bilateral Context Matching,"Being able to automatically discover synonymous entities in an open-world
setting benefits various tasks such as entity disambiguation or knowledge graph
canonicalization. Existing works either only utilize entity features, or rely
on structured annotations from a single piece of context where the entity is
mentioned. To leverage diverse contexts where entities are mentioned, in this
paper, we generalize the distributional hypothesis to a multi-context setting
and propose a synonym discovery framework that detects entity synonyms from
free-text corpora with considerations on effectiveness and robustness. As one
of the key components in synonym discovery, we introduce a neural network model
SYNONYMNET to determine whether or not two given entities are synonym with each
other. Instead of using entities features, SYNONYMNET makes use of multiple
pieces of contexts in which the entity is mentioned, and compares the
context-level similarity via a bilateral matching schema. Experimental results
demonstrate that the proposed model is able to detect synonym sets that are not
observed during training on both generic and domain-specific datasets:
Wiki+Freebase, PubMed+UMLS, and MedBook+MKG, with up to 4.16% improvement in
terms of Area Under the Curve and 3.19% in terms of Mean Average Precision
compared to the best baseline method.",Hyperparameter settings.,What is the range of values for the context number hyperparameter?
spiqa_274,1701.03077v10,"According to the figure caption for the unsupervised monocular depth estimation model trained on the KITTI dataset, visualized in the ""YUV + Wavelet"" output space, what is the range of the shape parameter α, where black corresponds to α = 0 and white corresponds to α = 2?",The range of values for the shape parameter α is from 0 to 2.,1701.03077v10.pdf,"['1701.03077v10.pdf', '1707.00189v3.pdf', '1809.03550v3.pdf', '1906.06589v3.pdf', '1705.02946v3.pdf', '1710.05654v2.pdf', '1805.04609v3.pdf', '1812.06589v2.pdf', '1812.00108v4.pdf', '1805.00912v4.pdf', '1803.02750v3.pdf', '1809.03149v2.pdf']","The caption states that ""black is α = 0 and white is α = 2.""",1701.03077v10-Figure13-1.png,A General and Adaptive Robust Loss Function,"We present a generalization of the Cauchy/Lorentzian, Geman-McClure,
Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2
loss functions. By introducing robustness as a continuous parameter, our loss
function allows algorithms built around robust loss minimization to be
generalized, which improves performance on basic vision tasks such as
registration and clustering. Interpreting our loss as the negative log of a
univariate density yields a general probability distribution that includes
normal and Cauchy distributions as special cases. This probabilistic
interpretation enables the training of neural networks in which the robustness
of the loss automatically adapts itself during training, which improves
performance on learning-based tasks such as generative image synthesis and
unsupervised monocular depth estimation, without requiring any manual parameter
tuning.","The final shape parameters α for our unsupervised monocular depth estimation model trained on KITTI data. The parameters are visualized in the same “YUV + Wavelet” output space as was used during training, where black is α = 0 and white is α = 2.",What is the range of values for the shape parameter α?
spiqa_275,1608.02784v2,"Based on Figure 4 of the Canonical Correlation Analysis method for generating text descriptions of abstract scenes, what does the weak positive correlation between BLEU scores (0.3 for CCA, 0.31 for SMT) and human rankings reveal about the relationship between machine translation metrics and human evaluation in this task?",The correlation between BLEU scores and human ranking is not high for either CCA or SMT systems.,1608.02784v2.pdf,"['1608.02784v2.pdf', '1805.08751v2.pdf', '1705.09966v2.pdf', '1710.01507v4.pdf']","The passage states that the correlation between the $x$-axis (ranking) and $y$-axis (BLEU scores) for CCA is $0.3$ and for the SMT system $0.31$. This indicates a weak positive correlation, meaning that higher BLEU scores are not necessarily associated with higher human rankings.",1608.02784v2-Figure4-1.png,Canonical Correlation Inference for Mapping Abstract Scenes to Text,"We describe a technique for structured prediction, based on canonical
correlation analysis. Our learning algorithm finds two projections for the
input and the output spaces that aim at projecting a given input and its
correct output into points close to each other. We demonstrate our technique on
a language-vision problem, namely the problem of giving a textual description
to an ""abstract scene"".",Figure 4: Scatter plot of SMT (statistical machine translation) and CCA BLEU scores versus human ratings.,What is the relationship between BLEU score and human ranking for CCA and SMT systems?
spiqa_276,1804.07931v2,"Referring to the figure that demonstrates the sample selection bias in conventional CVR modeling, what is the set-theoretical relationship between clicks and the entire set of impressions?",Clicks are a subset of impressions.,1804.07931v2.pdf,"['1804.07931v2.pdf', '1906.06589v3.pdf', '1803.04572v2.pdf', '1804.07849v4.pdf']","The figure shows that the set of clicks is completely contained within the set of impressions. This means that every click is also an impression, but not every impression is a click.",1804.07931v2-Figure1-1.png,Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate,"Estimating post-click conversion rate (CVR) accurately is crucial for ranking
systems in industrial applications such as recommendation and advertising.
Conventional CVR modeling applies popular deep learning methods and achieves
state-of-the-art performance. However it encounters several task-specific
problems in practice, making CVR modeling challenging. For example,
conventional CVR models are trained with samples of clicked impressions while
utilized to make inference on the entire space with samples of all impressions.
This causes a sample selection bias problem. Besides, there exists an extreme
data sparsity problem, making the model fitting rather difficult. In this
paper, we model CVR in a brand-new perspective by making good use of sequential
pattern of user actions, i.e., impression -> click -> conversion. The proposed
Entire Space Multi-task Model (ESMM) can eliminate the two problems
simultaneously by i) modeling CVR directly over the entire space, ii) employing
a feature representation transfer learning strategy. Experiments on dataset
gathered from Taobao's recommender system demonstrate that ESMM significantly
outperforms competitive methods. We also release a sampling version of this
dataset to enable future research. To the best of our knowledge, this is the
first public dataset which contains samples with sequential dependence of click
and conversion labels for CVR modeling.",Illustration of sample selection bias problem in conventional CVRmodeling. Training space is composed of samples with clicked impressions. It is only part of the inference space which is composed of all impressions.,What is the relationship between clicks and impressions?
spiqa_277,1706.08146v3,"Based on the results shown in Figure 6 of the Compressed Factorization paper, how does increasing the compression factor impact the normalized reconstruction error in the tensor decomposition of compressed EEG data?",The reconstruction error increases as the compression factor increases.,1706.08146v3.pdf,"['1706.08146v3.pdf', '1707.08608v3.pdf', '1703.07015v3.pdf', '1708.02153v2.pdf', '1805.00912v4.pdf', '1703.00899v2.pdf', '1611.02654v2.pdf', '1704.07854v4.pdf']","The left panel of Figure 1 shows that the normalized reconstruction error increases as the compression factor increases. This is because compressing the data reduces the amount of information that is available, which makes it more difficult to reconstruct the original data accurately.",1706.08146v3-Figure6-1.png,Compressed Factorization: Fast and Accurate Low-Rank Factorization of Compressively-Sensed Data,"What learning algorithms can be run directly on compressively-sensed data? In
this work, we consider the question of accurately and efficiently computing
low-rank matrix or tensor factorizations given data compressed via random
projections. We examine the approach of first performing factorization in the
compressed domain, and then reconstructing the original high-dimensional
factors from the recovered (compressed) factors. In both the matrix and tensor
settings, we establish conditions under which this natural approach will
provably recover the original factors. While it is well-known that random
projections preserve a number of geometric properties of a dataset, our work
can be viewed as showing that they can also preserve certain solutions of
non-convex, NP-Hard problems like non-negative matrix factorization. We support
these theoretical results with experiments on synthetic data and demonstrate
the practical applicability of compressed factorization on real-world gene
expression and EEG time series datasets.",Figure 6: Accuracy of tensor decomposition on compressed EEG data. Left: Normalized reconstruction error; dashed line indicates baseline reconstruction error on original data. Right: Median Pearson correlations between recovered factors and factors computed from original data.,What is the relationship between compression factor and reconstruction error?
spiqa_278,1804.00863v3,"In the figure illustrating the relationship between gloss and representation error in the Deep Appearance Maps paper, how does the DSSIM error change as the gloss of the sphere decreases?",The representation error decreases as the gloss decreases.,1804.00863v3.pdf,"['1804.00863v3.pdf', '1812.06589v2.pdf', '1708.05239v3.pdf', '1611.05742v3.pdf', '1707.01922v5.pdf']","The figure shows that the DSSIM (Deep Structural Similarity Index Measure) for the test, total, and train sets decreases as the gloss of the sphere decreases. This means that the representation error is lower for spheres with lower gloss.",1804.00863v3-Figure10-1.png,Deep Appearance Maps,"We propose a deep representation of appearance, i. e., the relation of color,
surface orientation, viewer position, material and illumination. Previous
approaches have useddeep learning to extract classic appearance
representationsrelating to reflectance model parameters (e. g., Phong)
orillumination (e. g., HDR environment maps). We suggest todirectly represent
appearance itself as a network we call aDeep Appearance Map (DAM). This is a 4D
generalizationover 2D reflectance maps, which held the view direction fixed.
First, we show how a DAM can be learned from images or video frames and later
be used to synthesize appearance, given new surface orientations and viewer
positions. Second, we demonstrate how another network can be used to map from
an image or video frames to a DAM network to reproduce this appearance, without
using a lengthy optimization such as stochastic gradient descent
(learning-to-learn). Finally, we show the example of an appearance
estimation-and-segmentation task, mapping from an image showingmultiple
materials to multiple deep appearance maps.",Relation of gloss and representation error.,What is the relationship between gloss and representation error?
spiqa_279,1809.03149v2,"Based on the figure showing the impact of different ad positions on CTR in the *Learning Adaptive Display Exposure for Real-Time Advertising* paper, how does the non-linear relationship between position and CTR, including the local peaks and valleys, inform potential strategies for optimizing ad placement in the context of dynamic ad exposure?","The relationship between position and CTR is complex and non-linear. In general, CTR decreases as position increases, but there are also local peaks and valleys in the CTR curve. This suggests that there are other factors besides position that affect CTR.",1809.03149v2.pdf,"['1809.03149v2.pdf', '1811.08257v1.pdf', '1603.03833v4.pdf', '1803.03467v4.pdf', '1701.03077v10.pdf', '1705.08016v3.pdf', '1703.00060v2.pdf', '1803.02750v3.pdf']","The figure shows the real data and a curve fitting for CTR as a function of position. The real data shows a general downward trend, but there are also local peaks and valleys. The curve fitting captures the general trend of the data, but it does not capture all of the local variation. This suggests that there are other factors besides position that affect CTR.",1809.03149v2-Figure9-1.png,Learning Adaptive Display Exposure for Real-Time Advertising,"In E-commerce advertising, where product recommendations and product ads are
presented to users simultaneously, the traditional setting is to display ads at
fixed positions. However, under such a setting, the advertising system loses
the flexibility to control the number and positions of ads, resulting in
sub-optimal platform revenue and user experience. Consequently, major
e-commerce platforms (e.g., Taobao.com) have begun to consider more flexible
ways to display ads. In this paper, we investigate the problem of advertising
with adaptive exposure: can we dynamically determine the number and positions
of ads for each user visit under certain business constraints so that the
platform revenue can be increased? More specifically, we consider two types of
constraints: request-level constraint ensures user experience for each user
visit, and platform-level constraint controls the overall platform monetization
rate. We model this problem as a Constrained Markov Decision Process with
per-state constraint (psCMDP) and propose a constrained two-level reinforcement
learning approach to decompose the original problem into two relatively
independent sub-problems. To accelerate policy learning, we also devise a
constrained hindsight experience replay mechanism. Experimental evaluations on
industry-scale real-world datasets demonstrate the merits of our approach in
both obtaining higher revenue under the constraints and the effectiveness of
the constrained hindsight experience replay mechanism.",The impact of different positions on CTR.,What is the relationship between position and click-through rate (CTR)?
spiqa_280,1706.08146v3,"In Figure 5 of the ""Compressed Factorization: Fast and Accurate Low-Rank Factorization of Compressively-Sensed Data"" paper, how does the normalized reconstruction error vary with changes in the projection matrix column sparsity during non-negative matrix factorization, and what does this suggest about the optimal sparsity level?","The normalized reconstruction error decreases as the projection sparsity increases, up to a certain point. After that, the error starts to increase again.",1706.08146v3.pdf,"['1706.08146v3.pdf', '1709.02755v5.pdf', '1804.05995v2.pdf', '1809.03550v3.pdf', '1707.00524v2.pdf', '1802.07222v1.pdf', '1803.03467v4.pdf', '1804.07707v2.pdf', '1603.03833v4.pdf', '1703.02507v3.pdf', '1603.00286v5.pdf']","The figure shows a plot of the normalized reconstruction error versus the projection sparsity. The error decreases rapidly at first, then levels off and starts to increase again. This suggests that there is an optimal value of projection sparsity that minimizes the reconstruction error.",1706.08146v3-Figure5-1.png,Compressed Factorization: Fast and Accurate Low-Rank Factorization of Compressively-Sensed Data,"What learning algorithms can be run directly on compressively-sensed data? In
this work, we consider the question of accurately and efficiently computing
low-rank matrix or tensor factorizations given data compressed via random
projections. We examine the approach of first performing factorization in the
compressed domain, and then reconstructing the original high-dimensional
factors from the recovered (compressed) factors. In both the matrix and tensor
settings, we establish conditions under which this natural approach will
provably recover the original factors. While it is well-known that random
projections preserve a number of geometric properties of a dataset, our work
can be viewed as showing that they can also preserve certain solutions of
non-convex, NP-Hard problems like non-negative matrix factorization. We support
these theoretical results with experiments on synthetic data and demonstrate
the practical applicability of compressed factorization on real-world gene
expression and EEG time series datasets.",Figure 5: NMF reconstruction error vs. projection matrix column sparsity.,What is the relationship between projection sparsity and normalized reconstruction error?
spiqa_281,1703.02507v3,"Based on the left figure in Figure 1 of the ""Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features"" paper, what is the specific relationship between the $L_2$-norm of word vectors and the log of word frequency as observed in the unigram model trained on the Toronto books corpus?",The $L_2$-norm of a word vector is inversely proportional to its frequency.,1703.02507v3.pdf,"['1703.02507v3.pdf', '1708.02153v2.pdf', '1804.04786v3.pdf', '1811.10673v1.pdf', '1804.05995v2.pdf', '1805.01216v3.pdf']","The left figure in Figure 1 shows that the $L_2$-norm of a word vector decreases as the frequency of the word increases. This is consistent with Luhn's hypothesis, which states that mid-rank terms are the most significant to discriminate content.",1703.02507v3-Figure1-1.png,Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features,"The recent tremendous success of unsupervised word embeddings in a multitude
of applications raises the obvious question if similar methods could be derived
to improve embeddings (i.e. semantic representations) of word sequences as
well. We present a simple but efficient unsupervised objective to train
distributed representations of sentences. Our method outperforms the
state-of-the-art unsupervised models on most benchmark tasks, highlighting the
robustness of the produced general-purpose sentence embeddings.","Figure 1: Left figure: the profile of the word vector L2norms as a function of log(fw) for each vocabulary word w, as learnt by our unigram model trained on Toronto books. Right figure: down-weighting scheme proposed by Arora et al. (2017): weight(w) = a a+fw .",What is the relationship between the $L_2$-norm of a word vector and its frequency?
spiqa_282,1705.09882v2,How does the figure illustrate the role of the Bernoulli parameter predicted by the temporal attention unit in identifying the person’s silhouette within depth-based person re-identification?,"The Bernoulli parameter is a measure of the probability of a pixel being foreground or background. The higher the Bernoulli parameter, the more likely the pixel is to be foreground. This is reflected in the images, where the pixels with higher Bernoulli parameters are more likely to be part of the person's silhouette.",1705.09882v2.pdf,"['1705.09882v2.pdf', '1611.07718v2.pdf', '1603.03833v4.pdf', '1707.08608v3.pdf']","The Bernoulli parameter is a key component of the GrabCut algorithm, which is used to segment images into foreground and background regions. The algorithm works by iteratively assigning pixels to either the foreground or background, based on their similarity to neighboring pixels and the Bernoulli parameter.",1705.09882v2-Figure6-1.png,Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-Identification,"We address the problem of person re-identification from commodity depth
sensors. One challenge for depth-based recognition is data scarcity. Our first
contribution addresses this problem by introducing split-rate RGB-to-Depth
transfer, which leverages large RGB datasets more effectively than popular
fine-tuning approaches. Our transfer scheme is based on the observation that
the model parameters at the bottom layers of a deep convolutional neural
network can be directly shared between RGB and depth data while the remaining
layers need to be fine-tuned rapidly. Our second contribution enhances
re-identification for video by implementing temporal attention as a
Bernoulli-Sigmoid unit acting upon frame-level features. Since this unit is
stochastic, the temporal attention parameters are trained using reinforcement
learning. Extensive experiments validate the accuracy of our method in person
re-identification from depth sequences. Finally, in a scenario where subjects
wear unseen clothes, we show large performance gains compared to a
state-of-the-art model which relies on RGB data.",Example sequence with the predicted Bernoulli parameter printed.,What is the relationship between the Bernoulli parameter and the image?
spiqa_283,1704.04539v2,"Based on the scatter plot in the ""Cross-lingual Abstract Meaning Representation Parsing"" paper, what does the linear regression line suggest about the statistical relationship between the Silver Snatch and the Gold Snatch?","The Silver Snatch and the Gold Snatch are positively correlated. As the Gold Snatch increases, the Silver Snatch also increases.",1704.04539v2.pdf,"['1704.04539v2.pdf', '1812.00281v3.pdf', '1803.03467v4.pdf', '1611.02654v2.pdf']","The figure shows two scatter plots, one for the Silver Snatch vs. the Gold Snatch and one for the Full-cycle Snatch vs. the Gold Snatch. The linear regression lines for both plots have a positive slope, indicating a positive correlation between the variables.",1704.04539v2-Figure5-1.png,Cross-lingual Abstract Meaning Representation Parsing,"Abstract Meaning Representation (AMR) annotation efforts have mostly focused
on English. In order to train parsers on other languages, we propose a method
based on annotation projection, which involves exploiting annotations in a
source language and a parallel corpus of the source language and a target
language. Using English as the source language, we show promising results for
Italian, Spanish, German and Chinese as target languages. Besides evaluating
the target parsers on non-gold datasets, we further propose an evaluation
method that exploits the English gold annotations and does not require access
to gold annotations for the target languages. This is achieved by inverting the
projection process: a new English parser is learned from the target language
parser and evaluated on the existing English gold standard.",Linear regression lines for silver and fullcycle.,What is the relationship between the Silver Snatch and the Gold Snatch?
spiqa_284,1710.06177v2,"Based on the figure illustrating the linear regression of AUC improvement, how does the Similarity Ratio affect the AUC gains for novel classes in the proposed Visual Analogy Graph Embedded Regression (VAGER) model?",There is a positive linear relationship between the Similarity Ratio and AUC Increasing.,1710.06177v2.pdf,"['1710.06177v2.pdf', '1804.04410v2.pdf', '1805.06447v3.pdf', '1809.01989v2.pdf', '1704.04539v2.pdf', '1811.08481v2.pdf', '1809.03550v3.pdf', '1705.10667v4.pdf', '1805.08751v2.pdf', '1611.03780v2.pdf', '1804.07849v4.pdf', '1804.07931v2.pdf', '1705.07164v8.pdf', '1809.03449v3.pdf']","The plot shows that as the Similarity Ratio increases, the AUC Increasing also increases. This is evident from the upward trend of the data points and the positive slope of the linear regression line.",1710.06177v2-Figure3-1.png,Learning to Learn Image Classifiers with Visual Analogy,"Humans are far better learners who can learn a new concept very fast with
only a few samples compared with machines. The plausible mystery making the
difference is two fundamental learning mechanisms: learning to learn and
learning by analogy. In this paper, we attempt to investigate a new human-like
learning method by organically combining these two mechanisms. In particular,
we study how to generalize the classification parameters from previously
learned concepts to a new concept. we first propose a novel Visual Analogy
Graph Embedded Regression (VAGER) model to jointly learn a low-dimensional
embedding space and a linear mapping function from the embedding space to
classification parameters for base classes. We then propose an out-of-sample
embedding method to learn the embedding of a new class represented by a few
samples through its visual analogy with base classes and derive the
classification parameters for the new class. We conduct extensive experiments
on ImageNet dataset and the results show that our method could consistently and
significantly outperform state-of-the-art baselines.",Linear regression of AUC improvement on Similarity Ratio for all novel classes,What is the relationship between the Similarity Ratio and AUC Increasing?
spiqa_285,1906.06589v3,"As depicted in the right panel of the figure in the ""Distillation for Membership Privacy"" (DMP) paper, how does the generalization gap change with increasing average X_ref entropy?",The generalization gap increases as the average X_ref entropy increases.,1906.06589v3.pdf,"['1906.06589v3.pdf', '1611.05742v3.pdf', '1706.03847v3.pdf', '1804.07707v2.pdf', '1709.02755v5.pdf']",The right panel of the figure shows that the generalization gap (red dashed line) increases as the average X_ref entropy increases.,1906.06589v3-Figure2-1.png,Membership Privacy for Machine Learning Models Through Knowledge Transfer,"Large capacity machine learning (ML) models are prone to membership inference
attacks (MIAs), which aim to infer whether the target sample is a member of the
target model's training dataset. The serious privacy concerns due to the
membership inference have motivated multiple defenses against MIAs, e.g.,
differential privacy and adversarial regularization. Unfortunately, these
defenses produce ML models with unacceptably low classification performances.
Our work proposes a new defense, called distillation for membership privacy
(DMP), against MIAs that preserves the utility of the resulting models
significantly better than prior defenses. DMP leverages knowledge distillation
to train ML models with membership privacy. We provide a novel criterion to
tune the data used for knowledge transfer in order to amplify the membership
privacy of DMP. Our extensive evaluation shows that DMP provides significantly
better tradeoffs between membership privacy and classification accuracies
compared to state-of-the-art MIA defenses. For instance, DMP achieves ~100%
accuracy improvement over adversarial regularization for DenseNet trained on
CIFAR100, for similar membership privacy (measured using MIA risk): when the
MIA risk is 53.7%, adversarially regularized DenseNet is 33.6% accurate, while
DMP-trained DenseNet is 65.3% accurate.","The lower the entropy of predictions of unprotected model on Xref , the higher the membership privacy.",What is the relationship between the average X_ref entropy and the generalization gap?
spiqa_286,1809.01246v1,"Based on the figure showing ""Buffer Percentage"" in *Fast and Accurate Graph Stream Summarization*, how does the buffer percentage trend change as the width of the room increases across all three data sets, and why?",The buffer percentage decreases as the width of the room increases.,1809.01246v1.pdf,"['1809.01246v1.pdf', '1707.06320v2.pdf', '1707.01917v2.pdf', '1707.00189v3.pdf', '1811.02553v4.pdf', '1706.00633v4.pdf', '1811.09393v4.pdf', '1812.06589v2.pdf', '1704.07854v4.pdf', '1611.03780v2.pdf', '1803.02750v3.pdf', '1811.10673v1.pdf', '1705.02946v3.pdf']","The figure shows that the buffer percentage decreases as the width of the room increases for all three data sets. This is because as the width of the room increases, there is more space for the data to be stored, and therefore less need for buffering.",1809.01246v1-Figure13-1.png,Fast and Accurate Graph Stream Summarization,"A graph stream is a continuous sequence of data items, in which each item
indicates an edge, including its two endpoints and edge weight. It forms a
dynamic graph that changes with every item in the stream. Graph streams play
important roles in cyber security, social networks, cloud troubleshooting
systems and other fields. Due to the vast volume and high update speed of graph
streams, traditional data structures for graph storage such as the adjacency
matrix and the adjacency list are no longer sufficient. However, prior art of
graph stream summarization, like CM sketches, gSketches, TCM and gMatrix,
either supports limited kinds of queries or suffers from poor accuracy of query
results. In this paper, we propose a novel Graph Stream Sketch (GSS for short)
to summarize the graph streams, which has the linear space cost (O(|E|), E is
the edge set of the graph) and the constant update time complexity (O(1)) and
supports all kinds of queries over graph streams with the controllable errors.
Both theoretical analysis and experiment results confirm the superiority of our
solution with regard to the time/space complexity and query results' precision
compared with the state-of-the-art.",Buffer Percentage,What is the relationship between the buffer percentage and the width of the room?
spiqa_287,1812.00281v3,"In the figure depicting garment silhouette error in the HUMBI dataset, what effect does increasing camera yaw angle have on the silhouette distance?",The silhouette distance generally increases as the camera yaw angle increases.,1812.00281v3.pdf,"['1812.00281v3.pdf', '1707.06320v2.pdf', '1809.03149v2.pdf', '1901.00056v2.pdf', '1809.03449v3.pdf', '1705.09296v2.pdf', '1802.07222v1.pdf', '1804.04410v2.pdf', '1812.00108v4.pdf', '1811.10673v1.pdf', '1710.05654v2.pdf']","The figure shows that the silhouette distance is higher for larger camera yaw angles, indicating that the silhouette is more distorted when the camera is at a greater angle to the person.",1812.00281v3-Figure16-1.png,HUMBI: A Large Multiview Dataset of Human Body Expressions,"This paper presents a new large multiview dataset called HUMBI for human body
expressions with natural clothing. The goal of HUMBI is to facilitate modeling
view-specific appearance and geometry of gaze, face, hand, body, and garment
from assorted people. 107 synchronized HD cameras are used to capture 772
distinctive subjects across gender, ethnicity, age, and physical condition.
With the multiview image streams, we reconstruct high fidelity body expressions
using 3D mesh models, which allows representing view-specific appearance using
their canonical atlas. We demonstrate that HUMBI is highly effective in
learning and reconstructing a complete human model and is complementary to the
existing datasets of human body expressions with limited views and subjects
such as MPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets.",Garment silhouette error.,What is the relationship between the camera yaw angle and the silhouette distance?
spiqa_288,1706.00827v2,"In the figure depicting examples from the AdelaideRMF and Hopkins datasets, how does the color of the points correspond to the different motions assigned to those points by the Multi-X algorithm in the context of motion segmentation?",The color of the points indicates the motion that the Multi-X algorithm assigned to each point.,1706.00827v2.pdf,"['1706.00827v2.pdf', '1706.08146v3.pdf', '1710.06177v2.pdf', '1805.04687v2.pdf', '1705.02946v3.pdf']","The caption states that the color of the points indicates the motion that Multi-X assigned to a point. For example, in the top left image, the blue points on the Rubik's Cube are likely assigned to a different motion than the green points on the poker chips.",1706.00827v2-Figure5-1.png,Multi-Class Model Fitting by Energy Minimization and Mode-Seeking,"We propose a general formulation, called Multi-X, for multi-class
multi-instance model fitting - the problem of interpreting the input data as a
mixture of noisy observations originating from multiple instances of multiple
classes. We extend the commonly used alpha-expansion-based technique with a new
move in the label space. The move replaces a set of labels with the
corresponding density mode in the model parameter domain, thus achieving fast
and robust optimization. Key optimization parameters like the bandwidth of the
mode seeking are set automatically within the algorithm. Considering that a
group of outliers may form spatially coherent structures in the data, we
propose a cross-validation-based technique removing statistically insignificant
instances. Multi-X outperforms significantly the state-of-the-art on publicly
available datasets for diverse problems: multiple plane and rigid motion
detection; motion segmentation; simultaneous plane and cylinder fitting; circle
and line fitting.",AdelaideRMF (top) and Hopkins (bot.) examples. Color indicates the motion Multi-X assigned a point to.,What is the relationship between the color of the points and the motion of the object?
spiqa_289,1611.04363v2,"Based on the results presented in Figure 1 of the ""Weakly Learning to Match Experts in Online Community"" paper, how is an expert's probability of declining influenced by the prior decision of a correlated ""friend"" across the QA-Expert and Paper-Reviewer datasets?","The decline probability of an expert is higher if they have a ""friend"" who has already declined.",1611.04363v2.pdf,"['1611.04363v2.pdf', '1901.00056v2.pdf', '1812.06589v2.pdf', '1708.02153v2.pdf', '1605.07496v3.pdf', '1706.00633v4.pdf']","The figure shows that the decline probability for experts with a ""friend"" who has declined is consistently higher than the decline probability for experts without such a friend, across all three datasets. This suggests that there is a correlation between the decline decisions of experts and their friends.",1611.04363v2-Figure1-1.png,Weakly Learning to Match Experts in Online Community,"In online question-and-answer (QA) websites like Quora, one central issue is
to find (invite) users who are able to provide answers to a given question and
at the same time would be unlikely to say ""no"" to the invitation. The challenge
is how to trade off the matching degree between users' expertise and the
question topic, and the likelihood of positive response from the invited users.
In this paper, we formally formulate the problem and develop a weakly
supervised factor graph (WeakFG) model to address the problem. The model
explicitly captures expertise matching degree between questions and users. To
model the likelihood that an invited user is willing to answer a specific
question, we incorporate a set of correlations based on social identity theory
into the WeakFG model. We use two different genres of datasets: QA-Expert and
Paper-Reviewer, to validate the proposed model. Our experimental results show
that the proposed model can significantly outperform (+1.5-10.7% by MAP) the
state-of-the-art algorithms for matching users (experts) with community
questions. We have also developed an online system to further demonstrate the
advantages of the proposed method.","Figure 1: Decline probability of an expert conditioned on whether or not the expert has a correlated “friend” who has already declined on two datasets: QA-Expert and Paper-Reviewer, and our online evaluation of journal reviewer recommendation.","What is the relationship between the decline probability of an expert and whether or not they have a ""friend"" who has already declined?"
spiqa_290,1809.00458v1,How does Figure 1 in Example 1 of the GB-KMV paper illustrate the relationship between the number of element-hash value pairs and the signature size for each record?,"The element-hash value pairs are the elements of the signature, and the signature size is the number of element-hash value pairs in the signature.",1809.00458v1.pdf,"['1809.00458v1.pdf', '1709.00139v4.pdf', '1705.02798v6.pdf', '1805.02349v2.pdf', '1804.04786v3.pdf', '1612.02803v5.pdf', '1804.05995v2.pdf', '1901.00398v2.pdf', '1708.02153v2.pdf', '1804.04410v2.pdf', '1606.07384v2.pdf', '1704.08615v2.pdf', '1702.03584v3.pdf']",The table shows that each signature consists of a set of element-hash value pairs. The number of element-hash value pairs in each signature is given by the corresponding value in the  column.,1809.00458v1-Figure2-1.png,GB-KMV: An Augmented KMV Sketch for Approximate Containment Similarity Search,"In this paper, we study the problem of approximate containment similarity
search. Given two records Q and X, the containment similarity between Q and X
with respect to Q is |Q intersect X|/ |Q|. Given a query record Q and a set of
records S, the containment similarity search finds a set of records from S
whose containment similarity regarding Q are not less than the given threshold.
This problem has many important applications in commercial and scientific
fields such as record matching and domain search. Existing solution relies on
the asymmetric LSH method by transforming the containment similarity to
well-studied Jaccard similarity. In this paper, we use a different framework by
transforming the containment similarity to set intersection. We propose a novel
augmented KMV sketch technique, namely GB-KMV, which is data-dependent and can
achieve a good trade-off between the sketch size and the accuracy. We provide a
set of theoretical analysis to underpin the proposed augmented KMV sketch
technique, and show that it outperforms the state-of-the-art technique LSH-E in
terms of estimation accuracy under practical assumption. Our comprehensive
experiments on real-life datasets verify that GB-KMV is superior to LSH-E in
terms of the space-accuracy trade-off, time-accuracy trade-off, and the sketch
construction time. For instance, with similar estimation accuracy (F-1 score),
GB-KMV is over 100 times faster than LSH-E on some real-life dataset.","The KMV sketch of the dataset in Example 1, each signature consists of element-hash value pairs. ki is the signature size of Xi",What is the relationship between the element-hash value pairs and the signature size?
spiqa_291,1809.00263v5,"Based on the figure illustrating the operations applied in the SDVI framework, how is the sampled vector combined with the feature maps of $\sigma$ and $\mu$, and what do the ""$\times$"" and ""$+$"" symbols shown in the figure denote?",The sampled vector is element-wise multiplied by the feature map of $\sigma$ and added to the feature map of $\mu$.,1809.00263v5.pdf,"['1809.00263v5.pdf', '1705.10667v4.pdf', '1803.06506v3.pdf', '1812.00108v4.pdf', '1804.01429v3.pdf', '1706.08146v3.pdf']","The figure shows the feature maps of $\sigma$ and $\mu$ on the left and right, respectively. The sampled vector is shown in the middle. The ""$\times$"" and ""$+$"" symbols indicate that the sampled vector is multiplied and added to the feature maps, respectively.",1809.00263v5-Figure6-1.png,Stochastic Dynamics for Video Infilling,"In this paper, we introduce a stochastic dynamics video infilling (SDVI)
framework to generate frames between long intervals in a video. Our task
differs from video interpolation which aims to produce transitional frames for
a short interval between every two frames and increase the temporal resolution.
Our task, namely video infilling, however, aims to infill long intervals with
plausible frame sequences. Our framework models the infilling as a constrained
stochastic generation process and sequentially samples dynamics from the
inferred distribution. SDVI consists of two parts: (1) a bi-directional
constraint propagation module to guarantee the spatial-temporal coherence among
frames, (2) a stochastic sampling process to generate dynamics from the
inferred distributions. Experimental results show that SDVI can generate clear
frame sequences with varying contents. Moreover, motions in the generated
sequence are realistic and able to transfer smoothly from the given start frame
to the terminal frame. Our project site is
https://xharlie.github.io/projects/project_sites/SDVI/video_results.html",The sampled vector (in the middle) is applied on all locations.,What is the relationship between the feature maps of $\sigma$ and $\mu$ and the sampled vector?
spiqa_292,1704.08615v2,"What does the figure depicting fixation densities (panels a–c) reveal about the relationship between the predicted fixation density map (panel b) by DeepGaze II and the actual recorded fixations (panel a), specifically in terms of how accurately the model distributes probability mass across the four contour-separated areas relative to the ground truth fixation distribution shown in panel c?",The fixation density map predicts the probability of a person fixating on a particular location in the image. The ground truth fixations are the actual locations where people fixated on the image.,1704.08615v2.pdf,"['1704.08615v2.pdf', '1706.04269v2.pdf', '1710.01507v4.pdf', '1703.10730v2.pdf', '1811.10673v1.pdf']",The fixation density map is shown in panel b) and the ground truth fixations are shown in panel a). The contour lines in the fixation density map separate the image into four areas of decreasing probability density. The number of ground truth fixations in each of these areas is shown in panel c).,1704.08615v2-Figure5-1.png,"Saliency Benchmarking Made Easy: Separating Models, Maps and Metrics","Dozens of new models on fixation prediction are published every year and
compared on open benchmarks such as MIT300 and LSUN. However, progress in the
field can be difficult to judge because models are compared using a variety of
inconsistent metrics. Here we show that no single saliency map can perform well
under all metrics. Instead, we propose a principled approach to solve the
benchmarking problem by separating the notions of saliency models, maps and
metrics. Inspired by Bayesian decision theory, we define a saliency model to be
a probabilistic model of fixation density prediction and a saliency map to be a
metric-specific prediction derived from the model density which maximizes the
expected performance on that metric given the model density. We derive these
optimal saliency maps for the most commonly used saliency metrics (AUC, sAUC,
NSS, CC, SIM, KL-Div) and show that they can be computed analytically or
approximated with high precision. We show that this leads to consistent
rankings in all metrics and avoids the penalties of using one saliency map for
all metrics. Our method allows researchers to have their model compete on many
different metrics with state-of-the-art in those metrics: ""good"" models will
perform well in all metrics.","Visualizing fixation densities: a) an example stimulus with N = 97 ground truth fixations. b) DeepGaze II predicts a fixation density for this stimulus. The contour lines separate the image into four areas of decreasing probability density such that each area has the same total probability mass. c) The number of ground truth fixations in each of the four areas. The model expects the same number of fixations for each area (horizontal line: 24.25 fixations for N fixations total). The gray area shows the expected standard deviation from this number. DeepGaze II overestimates the how peaked the density is: there are too few fixations in darkest area. Vice versa, it misses some probability mass in the second to last area. However, the large error margin (gray area) indicates that substantial deviations from the expected number of fixations are to be expected.",What is the relationship between the fixation density map and the ground truth fixations?
spiqa_293,1704.08615v2,"How does the ground truth fixation density in panel (a) of the figure lead to the creation of different saliency maps in panel (b) for metrics such as AUC, NSS, and KL-Div, and why do these maps differ despite originating from the same model?",The ground truth fixation density predicts different saliency maps depending on the intended metric.,1704.08615v2.pdf,"['1704.08615v2.pdf', '1703.00899v2.pdf', '1804.05938v2.pdf', '1805.06447v3.pdf', '1704.05426v4.pdf', '1606.07384v2.pdf', '1811.06635v1.pdf', '1707.06320v2.pdf', '1706.00633v4.pdf']","The figure shows that the ground truth fixation density (a) is used to generate different saliency maps (b) for different metrics. The saliency maps differ dramatically due to the different properties of the metrics, but they all reflect the same underlying model.",1704.08615v2-Figure1-1.png,"Saliency Benchmarking Made Easy: Separating Models, Maps and Metrics","Dozens of new models on fixation prediction are published every year and
compared on open benchmarks such as MIT300 and LSUN. However, progress in the
field can be difficult to judge because models are compared using a variety of
inconsistent metrics. Here we show that no single saliency map can perform well
under all metrics. Instead, we propose a principled approach to solve the
benchmarking problem by separating the notions of saliency models, maps and
metrics. Inspired by Bayesian decision theory, we define a saliency model to be
a probabilistic model of fixation density prediction and a saliency map to be a
metric-specific prediction derived from the model density which maximizes the
expected performance on that metric given the model density. We derive these
optimal saliency maps for the most commonly used saliency metrics (AUC, sAUC,
NSS, CC, SIM, KL-Div) and show that they can be computed analytically or
approximated with high precision. We show that this leads to consistent
rankings in all metrics and avoids the penalties of using one saliency map for
all metrics. Our method allows researchers to have their model compete on many
different metrics with state-of-the-art in those metrics: ""good"" models will
perform well in all metrics.","No single saliency map can perform best in all metrics even when the true fixation distribution is known. This problem can be solved by separating saliency models from saliency maps. a) Fixations are distributed according to a ground truth fixation density p(x, y | I) for some stimulus I (see supplementary material for details on the visualization). b) This ground truth density predicts different saliency maps depending on the intended metric. The saliency maps differ dramatically due to the different properties of the metrics but always reflect the same underlying model. Note that the maps for the NSS and IG metrics are the same, as are those for CC and KL-Div. c) Performances of the saliency maps from b) under seven saliency metrics on a large number of fixations sampled from the model distribution in a). Colors of the bars correspond to the frame colors in b). The predicted saliency map for the specific metric (framed bar) yields best performance in all cases.",What is the relationship between the ground truth fixation density and the saliency maps?
spiqa_294,1703.10730v2,"How does the generator network, as depicted in the figure with two local input patches, incorporate these patches to generate new images, and what is the relationship between the input patches and the generated output images?",The input patches are used to generate the images. The generator network takes the input patches as input and generates new images that are similar to the input patches.,1703.10730v2.pdf,"['1703.10730v2.pdf', '1705.10667v4.pdf', '1707.08608v3.pdf', '1709.02418v2.pdf', '1802.07222v1.pdf', '1809.03449v3.pdf', '1809.00263v5.pdf', '1804.01429v3.pdf', '1804.07707v2.pdf', '1611.04363v2.pdf', '1803.05776v2.pdf', '1704.05426v4.pdf', '1809.03550v3.pdf', '1704.08615v2.pdf', '1811.07073v3.pdf']","The figure shows two input patches (Input 1 and Input 2) and the corresponding generated images (Gen). The generated images are similar to the input patches, but they are not identical. This suggests that the generator network is able to learn the features of the input patches and generate new images that are similar to the input patches.",1703.10730v2-Figure13-1.png,Unsupervised Holistic Image Generation from Key Local Patches,"We introduce a new problem of generating an image based on a small number of
key local patches without any geometric prior. In this work, key local patches
are defined as informative regions of the target object or scene. This is a
challenging problem since it requires generating realistic images and
predicting locations of parts at the same time. We construct adversarial
networks to tackle this problem. A generator network generates a fake image as
well as a mask based on the encoder-decoder framework. On the other hand, a
discriminator network aims to detect fake images. The network is trained with
three losses to consider spatial, appearance, and adversarial information. The
spatial loss determines whether the locations of predicted parts are correct.
Input patches are restored in the output image without much modification due to
the appearance loss. The adversarial loss ensures output images are realistic.
The proposed network is trained without supervisory signals since no labels of
key parts are required. Experimental results on six datasets demonstrate that
the proposed algorithm performs favorably on challenging objects and scenes.",Image generation results with two input patches. Input 1 and 2 are local patches from the image Real.,What is the relationship between the input patches and the generated images?
spiqa_295,1608.02784v2,"According to Figure 2, how does cosine similarity facilitate the mapping between the vector representation of an object from the input space and its corresponding textual description in the output space?",The input space and the output space are related by a cosine similarity measure.,1608.02784v2.pdf,"['1608.02784v2.pdf', '1702.03584v3.pdf', '1804.05938v2.pdf', '1703.02507v3.pdf', '1803.04572v2.pdf', '1709.02755v5.pdf', '1703.00060v2.pdf', '1702.08694v3.pdf', '1706.00633v4.pdf', '1901.00398v2.pdf', '1802.07222v1.pdf']","The figure shows that an object from the input space (the image on the left) is mapped to a unit vector. Then, the closest unit vector which has an embodiment in the output space is found. The cosine similarity between the two unit vectors is used to measure the relationship between the input and output spaces.",1608.02784v2-Figure2-1.png,Canonical Correlation Inference for Mapping Abstract Scenes to Text,"We describe a technique for structured prediction, based on canonical
correlation analysis. Our learning algorithm finds two projections for the
input and the output spaces that aim at projecting a given input and its
correct output into points close to each other. We demonstrate our technique on
a language-vision problem, namely the problem of giving a textual description
to an ""abstract scene"".","Figure 2: Demonstration of CCA inference. An object from the input space X (the image on the left x) is mapped to a unit vector. Then, we find the closest unit vector which has an embodiment in the output space, Y . That embodiment is the text on the right, y. It also also holds that ρ(u(x), v(y)) = cos θ.",What is the relationship between the input space and the output space in CCA inference?
spiqa_296,1809.00263v5,"In the context of Figure 2 from the ""Stochastic Dynamics for Video Infilling"" paper, how does increasing the interval between captured frames (from every other frame in scenario 1 to every 4 frames in scenario 2) affect the uncertainty in the generated frame sequences?",The uncertainty in the generated frames increases with the length of the interval.,1809.00263v5.pdf,"['1809.00263v5.pdf', '1812.00108v4.pdf', '1710.01507v4.pdf', '1603.00286v5.pdf', '1605.07496v3.pdf', '1705.09296v2.pdf', '1708.03797v1.pdf', '1802.07222v1.pdf', '1707.08608v3.pdf', '1703.07015v3.pdf', '1706.04269v2.pdf', '1812.00281v3.pdf', '1703.00060v2.pdf', '1612.02803v5.pdf', '1704.08615v2.pdf']","The figure shows two scenarios: one with a short-term interval and one with a long-term interval. In the short-term interval scenario, the camera captures every other frame, resulting in less uncertainty in the generated frames. In the long-term interval scenario, the camera captures 1 frame for every 4 frames, resulting in more uncertainty in the generated frames.",1809.00263v5-Figure2-1.png,Stochastic Dynamics for Video Infilling,"In this paper, we introduce a stochastic dynamics video infilling (SDVI)
framework to generate frames between long intervals in a video. Our task
differs from video interpolation which aims to produce transitional frames for
a short interval between every two frames and increase the temporal resolution.
Our task, namely video infilling, however, aims to infill long intervals with
plausible frame sequences. Our framework models the infilling as a constrained
stochastic generation process and sequentially samples dynamics from the
inferred distribution. SDVI consists of two parts: (1) a bi-directional
constraint propagation module to guarantee the spatial-temporal coherence among
frames, (2) a stochastic sampling process to generate dynamics from the
inferred distributions. Experimental results show that SDVI can generate clear
frame sequences with varying contents. Moreover, motions in the generated
sequence are realistic and able to transfer smoothly from the given start frame
to the terminal frame. Our project site is
https://xharlie.github.io/projects/project_sites/SDVI/video_results.html",Figure 2: The difference of the randomness between shortterm and long-term intervals: The camera in scenario 1 can capture every other frame and the camera in scenario 2 captures 1 frame for every 4 frames. The red and the green trajectories indicate two possible motions in each scenario.,What is the relationship between the length of the interval and the uncertainty in the generated frames?
spiqa_297,1803.03467v4,"Based on the figure illustrating the ripple sets in the movie knowledge graph in the RippleNet paper, how are ""Forrest Gump"" and ""Cast Away"" connected?","The movies ""Forrest Gump"" and ""Cast Away"" are connected by the actor Tom Hanks.",1803.03467v4.pdf,"['1803.03467v4.pdf', '1811.02553v4.pdf', '1906.06589v3.pdf', '1703.00899v2.pdf', '1805.04687v2.pdf', '1708.03797v1.pdf', '1704.08615v2.pdf', '1709.00139v4.pdf', '1809.01246v1.pdf', '1803.05776v2.pdf', '1705.08016v3.pdf']","The figure shows that Tom Hanks is connected to both ""Forrest Gump"" and ""Cast Away"" by the ""actor.film"" relationship. This indicates that Tom Hanks starred in both movies.",1803.03467v4-Figure3-1.png,RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems,"To address the sparsity and cold start problem of collaborative filtering,
researchers usually make use of side information, such as social networks or
item attributes, to improve recommendation performance. This paper considers
the knowledge graph as the source of side information. To address the
limitations of existing embedding-based and path-based methods for
knowledge-graph-aware recommendation, we propose Ripple Network, an end-to-end
framework that naturally incorporates the knowledge graph into recommender
systems. Similar to actual ripples propagating on the surface of water, Ripple
Network stimulates the propagation of user preferences over the set of
knowledge entities by automatically and iteratively extending a user's
potential interests along links in the knowledge graph. The multiple ""ripples""
activated by a user's historically clicked items are thus superposed to form
the preference distribution of the user with respect to a candidate item, which
could be used for predicting the final clicking probability. Through extensive
experiments on real-world datasets, we demonstrate that Ripple Network achieves
substantial gains in a variety of scenarios, including movie, book and news
recommendation, over several state-of-the-art baselines.","Illustration of ripple sets of ""Forrest Gump"" in KG ofmovies. The concentric circles denotes the ripple setswith different hops. The fading blue indicates decreasing relatedness between the center and surrounding entities.Note that the ripple sets of different hops are not necessarily disjoint in practice.","What is the relationship between the movies ""Forrest Gump"" and ""Cast Away""?"
spiqa_298,1710.06177v2,"In the context of the Visual Analogy Graph Embedded Regression (VAGER) model, how does the figure depicting the top-3 most visually similar base classes to the novel class on the embedding layer in the 5-shot setting illustrate the relationship between the novel class and its base classes?","The top-3 most similar base classes are the three classes that are most similar to the novel class, based on the embedding layer in a 5-shot setting.",1710.06177v2.pdf,"['1710.06177v2.pdf', '1708.02153v2.pdf', '1802.07351v2.pdf', '1805.06431v4.pdf']","The figure shows a table of images, with each row representing a different class. The first row shows the novel class, and the second row shows the top-3 most similar base classes. The images in the second row are similar to the images in the first row, which suggests that the top-3 most similar base classes are indeed similar to the novel class.",1710.06177v2-Figure4-1.png,Learning to Learn Image Classifiers with Visual Analogy,"Humans are far better learners who can learn a new concept very fast with
only a few samples compared with machines. The plausible mystery making the
difference is two fundamental learning mechanisms: learning to learn and
learning by analogy. In this paper, we attempt to investigate a new human-like
learning method by organically combining these two mechanisms. In particular,
we study how to generalize the classification parameters from previously
learned concepts to a new concept. we first propose a novel Visual Analogy
Graph Embedded Regression (VAGER) model to jointly learn a low-dimensional
embedding space and a linear mapping function from the embedding space to
classification parameters for base classes. We then propose an out-of-sample
embedding method to learn the embedding of a new class represented by a few
samples through its visual analogy with base classes and derive the
classification parameters for the new class. We conduct extensive experiments
on ImageNet dataset and the results show that our method could consistently and
significantly outperform state-of-the-art baselines.",Top-3 most similar base classes to novel class on embedding layer in 5-shot setting.,What is the relationship between the novel class and the top-3 most similar base classes?
spiqa_299,1703.04887v4,"How does Table 3 in the BR-CSGAN study illustrate the impact of increasing Monte Carlo samples (N) on translation performance, and what trade-offs between BLEU score improvements and computational complexity arise as N increases?","The table and passage show that the translation performance of the BR-CSGAN model generally improves as the number of Monte Carlo samples (N) increases. However, this improvement plateaus after N reaches a certain point (around 20 in this case).

There is a trade-off when choosing the value of N because increasing N also increases the computational complexity and training time. While a higher N leads to more accurate reward estimations and better performance, it also requires more computational resources and longer training times. Therefore, choosing the optimal N involves balancing the desired performance with the available computational resources and time constraints.",1703.04887v4.pdf,"['1703.04887v4.pdf', '1811.10673v1.pdf', '1710.01507v4.pdf', '1704.07121v2.pdf', '1703.00899v2.pdf', '1703.07015v3.pdf', '1703.10730v2.pdf']","Table 2 presents the BLEU scores of the BR-CSGAN model with different N values. As N increases from 15 to 20, the BLEU scores consistently improve across all datasets. However, further increasing N to 25 and 30 shows minimal improvement, indicating a plateau in performance. The passage explains that this is because a higher N leads to more accurate reward estimations, guiding the model towards better performance. However, it also clarifies that higher N values require more sampling, significantly increasing computation time. Therefore, the choice of N involves a trade-off between accuracy and computational efficiency.",1703.04887v4-Table3-1.png,Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets,"This paper proposes an approach for applying GANs to NMT. We build a
conditional sequence generative adversarial net which comprises of two
adversarial sub models, a generator and a discriminator. The generator aims to
generate sentences which are hard to be discriminated from human-translated
sentences (i.e., the golden target sentences), And the discriminator makes
efforts to discriminate the machine-generated sentences from human-translated
ones. The two sub models play a mini-max game and achieve the win-win situation
when they reach a Nash Equilibrium. Additionally, the static sentence-level
BLEU is utilized as the reinforced objective for the generator, which biases
the generation towards high BLEU points. During training, both the dynamic
discriminator and the static BLEU objective are employed to evaluate the
generated sentences and feedback the evaluations to guide the learning of the
generator. Experimental results show that the proposed model consistently
outperforms the traditional RNNSearch and the newly emerged state-of-the-art
Transformer on English-German and Chinese-English translation tasks.","Table 3: The translation performance of the BRCSGAN with different N for Monte Carlo search. ”-” means that the proposed model shows no improvement than the pre-trained generator or it can not be trained stably. With N set as 0, it is referred to as the pretrained generator. Similarly, we only report results on the RNNSearch and λ is set as 0.7.",What is the relationship between the number of Monte Carlo samples (N) and the translation performance of the BR-CSGAN model? Why is there a trade-off when choosing the value of N?
spiqa_300,1704.08615v2,"Based on the figure where 100,000 empirical and normalized saliency maps are sampled, how does the CC score change as the number of fixations increases from 1 to 200?",The CC score increases as the number of fixations increases.,1704.08615v2.pdf,"['1704.08615v2.pdf', '1705.09882v2.pdf', '1705.02946v3.pdf', '1708.02153v2.pdf', '1705.07164v8.pdf', '1804.05938v2.pdf', '1611.04684v1.pdf', '1803.05776v2.pdf', '1805.06431v4.pdf', '1708.01425v4.pdf', '1809.02731v3.pdf', '1608.02784v2.pdf', '1805.00912v4.pdf', '1804.01429v3.pdf']","The figure shows that the CC score for the mean empirical and mean normalized empirical saliency maps increases as the number of fixations increases from 1 to 200. This is because the more fixations there are, the more information is available to calculate the CC score.",1704.08615v2-Figure6-1.png,"Saliency Benchmarking Made Easy: Separating Models, Maps and Metrics","Dozens of new models on fixation prediction are published every year and
compared on open benchmarks such as MIT300 and LSUN. However, progress in the
field can be difficult to judge because models are compared using a variety of
inconsistent metrics. Here we show that no single saliency map can perform well
under all metrics. Instead, we propose a principled approach to solve the
benchmarking problem by separating the notions of saliency models, maps and
metrics. Inspired by Bayesian decision theory, we define a saliency model to be
a probabilistic model of fixation density prediction and a saliency map to be a
metric-specific prediction derived from the model density which maximizes the
expected performance on that metric given the model density. We derive these
optimal saliency maps for the most commonly used saliency metrics (AUC, sAUC,
NSS, CC, SIM, KL-Div) and show that they can be computed analytically or
approximated with high precision. We show that this leads to consistent
rankings in all metrics and avoids the penalties of using one saliency map for
all metrics. Our method allows researchers to have their model compete on many
different metrics with state-of-the-art in those metrics: ""good"" models will
perform well in all metrics.","Predicting optimal saliency maps for the CC metric: Starting from a density (a) we sampled 100000 sets of either 1, 10 or 100 fixations and used them to create empirical saliency maps. Using these empirical saliency maps, we calculated the mean empirical saliency map (shown for 10 fixations per empirical saliency map in (b)). Additionally, we normalized the empirical saliency maps to have zero mean and unit variance to compute the mean normalized empirical saliency map (c) which is optimal with respect to the CC metric. Then we sampled another 100000 empirical saliency maps from the original density and evaluated CC scores of the mean empirical and mean normalized empirical saliency maps (d). The mean normalized saliency map yields slighly higher scores in all cases but the difference to the mean empirical saliency map is tiny, indicating that the expected empirical saliency map is a very good approximation of the optimal saliency map for the CC metric.",What is the relationship between the number of fixations and the CC score?
spiqa_301,1707.08608v3,"Referring to the figure illustrating the shift-reduce process, how does correcting the number of shifts from 9 to 10 using gradient-based inference impact the completeness and accuracy of the syntactic tree structure?",The accuracy of the output increases as the number of shifts increases.,1707.08608v3.pdf,"['1707.08608v3.pdf', '1804.05938v2.pdf', '1803.05776v2.pdf', '1811.08257v1.pdf', '1906.10843v1.pdf', '1701.03077v10.pdf', '1710.01507v4.pdf', '1704.07854v4.pdf', '1706.08146v3.pdf']","The figure shows that the accuracy of the output is 33.3% when there are 9 shifts, and 100% when there are 10 shifts.",1707.08608v3-Table8-1.png,Gradient-based Inference for Networks with Output Constraints,"Practitioners apply neural networks to increasingly complex problems in
natural language processing, such as syntactic parsing and semantic role
labeling that have rich output structures. Many such structured-prediction
problems require deterministic constraints on the output values; for example,
in sequence-to-sequence syntactic parsing, we require that the sequential
outputs encode valid trees. While hidden units might capture such properties,
the network is not always able to learn such constraints from the training data
alone, and practitioners must then resort to post-processing. In this paper, we
present an inference method for neural networks that enforces deterministic
constraints on outputs without performing rule-based post-processing or
expensive discrete search. Instead, in the spirit of gradient-based training,
we enforce constraints with gradient-based inference (GBI): for each input at
test-time, we nudge continuous model weights until the network's unconstrained
inference procedure generates an output that satisfies the constraints. We
study the efficacy of GBI on three tasks with hard constraints: semantic role
labeling, syntactic parsing, and sequence transduction. In each case, the
algorithm not only satisfies constraints but improves accuracy, even when the
underlying network is state-of-the-art.","A shift-reduce example for which the method successfully enforces constraints. The initial output has only nine shifts, but there are ten tokens in the input. Enforcing the constraint not only corrects the number of shifts to ten, but changes the implied tree structure to the correct tree.",What is the relationship between the number of shifts and the accuracy of the output?
spiqa_302,1708.01425v4,"According to Figure 3 in this paper, how does increasing the number of workers per ""expert"" affect Cohen’s kappa agreement for stance annotation in the context of the crowdsourcing process for argument reasoning comprehension?","The Cohen's kappa agreement for stance annotation increases as the number of workers per ""expert"" increases.",1708.01425v4.pdf,"['1708.01425v4.pdf', '1701.06171v4.pdf', '1708.05239v3.pdf', '1709.02755v5.pdf', '1906.10843v1.pdf', '1812.10735v2.pdf', '1703.00899v2.pdf', '1804.00863v3.pdf', '1811.09393v4.pdf']","The figure shows that the Cohen's kappa agreement increases as the number of workers per ""expert"" increases, regardless of the MACE threshold. This suggests that having more workers per ""expert"" leads to more reliable annotations.",1708.01425v4-Figure3-1.png,The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants,"Reasoning is a crucial part of natural language argumentation. To comprehend
an argument, one must analyze its warrant, which explains why its claim follows
from its premises. As arguments are highly contextualized, warrants are usually
presupposed and left implicit. Thus, the comprehension does not only require
language understanding and logic skills, but also depends on common sense. In
this paper we develop a methodology for reconstructing warrants systematically.
We operationalize it in a scalable crowdsourcing process, resulting in a freely
licensed dataset with warrants for 2k authentic arguments from news comments.
On this basis, we present a new challenging task, the argument reasoning
comprehension task. Given an argument with a claim and a premise, the goal is
to choose the correct implicit warrant from two options. Both warrants are
plausible and lexically close, but lead to contradicting claims. A solution to
this task will define a substantial step towards automatic warrant
reconstruction. However, experiments with several neural attention and language
models reveal that current approaches do not suffice.","Figure 3: Cohen’s κ agreement for stance annotation on 98 comments. As a trade-off between reducing costs (i.e., discarding fewer instances) and increasing reliability, we chose 5 annotators and a threshold of 0.95 for this task, which resulted in κ = 0.58 (moderate to substantial agreement).","What is the relationship between the number of workers per ""expert"" and Cohen's kappa agreement for stance annotation?"
spiqa_303,1611.03780v2,"How does the figure illustrate the process of folding the bipartite user-region graph into the interference graph, and how are the geographic clusters and their corresponding interferences represented through edge weights?"," The interference graph is a folded version of the query graph. The nodes in the interference graph represent regions, and the edges represent the interference between regions. The edge weights in the interference graph are calculated from the edge weights in the query graph.",1611.03780v2.pdf,"['1611.03780v2.pdf', '1705.07384v2.pdf', '1703.02507v3.pdf', '1708.05239v3.pdf', '1611.04363v2.pdf', '1611.04684v1.pdf']"," The figure shows how the query graph is folded to create the interference graph. The nodes in the query graph represent users and regions, and the edges represent the relationships between them. The folding process involves combining the user nodes into a single node for each region. The edge weights in the interference graph are then calculated by summing the weights of the edges between the corresponding user nodes in the query graph.",1611.03780v2-Figure2-1.png,Randomized Experimental Design via Geographic Clustering,"Web-based services often run randomized experiments to improve their
products. A popular way to run these experiments is to use geographical regions
as units of experimentation, since this does not require tracking of individual
users or browser cookies. Since users may issue queries from multiple
geographical locations, geo-regions cannot be considered independent and
interference may be present in the experiment. In this paper, we study this
problem, and first present GeoCUTS, a novel algorithm that forms geographical
clusters to minimize interference while preserving balance in cluster size. We
use a random sample of anonymized traffic from Google Search to form a graph
representing user movements, then construct a geographically coherent
clustering of the graph. Our main technical contribution is a statistical
framework to measure the effectiveness of clusterings. Furthermore, we perform
empirical evaluations showing that the performance of GeoCUTS is comparable to
hand-crafted geo-regions with respect to both novel and existing metrics.",Diagram of the bipartite user-region graph and the resulting “folded” interference graph between regions. The edge weights of the folded graph correspond to the unnormalized weights qkk (cf. Equation 1).,What is the relationship between the query graph and the interference graph?
spiqa_304,1809.03550v3,"In the ""Pursuit of Low-Rank Models of Time-Varying Matrices Robust to Sparse and Measurement Noise"" paper, as illustrated in Figure 1, how are the residuals prior to thresholding transformed into the Boolean map through a thresholding process in the context of detecting moving objects?",The Boolean map is obtained by thresholding the residuals prior to thresholding.,1809.03550v3.pdf,"['1809.03550v3.pdf', '1705.09966v2.pdf', '1805.08751v2.pdf', '1705.09882v2.pdf', '1811.08481v2.pdf', '1811.06635v1.pdf', '1703.00060v2.pdf', '1709.02755v5.pdf', '1703.10730v2.pdf', '1603.00286v5.pdf', '1804.07707v2.pdf', '1809.00458v1.pdf', '1812.10735v2.pdf', '1803.06506v3.pdf']","The residuals prior to thresholding show the difference between the current frame and the estimated background. The Boolean map is a binary image where pixels are set to 1 if the corresponding residual value is above a certain threshold and 0 otherwise. This means that the Boolean map highlights the areas where the current frame differs significantly from the estimated background, which is likely due to the presence of moving objects.",1809.03550v3-Figure4-1.png,Pursuit of Low-Rank Models of Time-Varying Matrices Robust to Sparse and Measurement Noise,"In tracking of time-varying low-rank models of time-varying matrices, we
present a method robust to both uniformly-distributed measurement noise and
arbitrarily-distributed ``sparse'' noise. In theory, we bound the tracking
error. In practice, our use of randomised coordinate descent is scalable and
allows for encouraging results on changedetection net, a benchmark.","One snapshot from the video baseline/highway (from the top left, clock-wise): one frame of the original video, our estimate of the background, our residuals prior to thresholding, the ground truth, an exponential smoothing of all frames prior to the current one with smoothing factor of 1/35, and finally, our Boolean map obtained by thresholding residuals.",What is the relationship between the residuals prior to thresholding and the Boolean map?
spiqa_305,1704.07854v4,"Referring to the figure showcasing timings on the Xeon E5-1630, how does the training time scale with increasing simulation resolution for both the 2D (100^2) liquid simulation and the 4D (110^3 * 110) setup, and what does this reveal about the relationship between resolution and training duration?"," The higher the resolution of the simulation, the longer the training time. ",1704.07854v4.pdf,"['1704.07854v4.pdf', '1802.07351v2.pdf', '1611.02654v2.pdf', '1809.02731v3.pdf']"," The table in the figure shows that the training time increases as the resolution of the simulation increases. For example, the 2D setup with a resolution of 100^2 has a training time of 186 seconds, while the 4D setup with a resolution of 110^3 * 110 has a training time of 186 minutes. ",1704.07854v4-Table2-1.png,Generating Liquid Simulations with Deformation-aware Neural Networks,"We propose a novel approach for deformation-aware neural networks that learn
the weighting and synthesis of dense volumetric deformation fields. Our method
specifically targets the space-time representation of physical surfaces from
liquid simulations. Liquids exhibit highly complex, non-linear behavior under
changing simulation conditions such as different initial conditions. Our
algorithm captures these complex phenomena in two stages: a first neural
network computes a weighting function for a set of pre-computed deformations,
while a second network directly generates a deformation field for refining the
surface. Key for successful training runs in this setting is a suitable loss
function that encodes the effect of the deformations, and a robust calculation
of the corresponding gradients. To demonstrate the effectiveness of our
approach, we showcase our method with several complex examples of flowing
liquids with topology changes. Our representation makes it possible to rapidly
generate the desired implicit surfaces. We have implemented a mobile
application to demonstrate that real-time interactions with complex liquid
effects are possible with our approach.","Overview of our 2D and 4D simulation and machine learning setups. Timings were measured on a Xeon E5-1630 with 3.7GHz. Res, SDF and Defo denote resolutions for simulation, training, and the NN deformation, respectively; Sim and Train denote simulation and training runtimes. sp, sd, γ1, γ2 denote training steps for parameters, training steps for deformation, and regularization parameters, respectively.",What is the relationship between the resolution of the simulation and the training time?
spiqa_306,1701.03077v10,"Referring to the figure illustrating the general loss function in ""A General and Adaptive Robust Loss Function,"" how does the shape parameter α control the sharpness and behavior of the loss function around zero, and what impact does it have on penalizing small errors as α is varied from high to low values?","The shape parameter α controls the shape of the loss function. As α increases, the loss function becomes more peaked, and as α decreases, the loss function becomes more flat.",1701.03077v10.pdf,"['1701.03077v10.pdf', '1701.06171v4.pdf', '1809.00458v1.pdf', '1811.02721v3.pdf', '1611.04363v2.pdf', '1706.08146v3.pdf', '1811.02553v4.pdf', '1805.00912v4.pdf', '1710.06177v2.pdf', '1708.05239v3.pdf', '1703.04887v4.pdf']","The left plot shows the loss function for different values of α. For large values of α, the loss function is very peaked around zero, which means that small errors are penalized more heavily. For small values of α, the loss function is flatter, which means that small errors are penalized less heavily.",1701.03077v10-Figure1-1.png,A General and Adaptive Robust Loss Function,"We present a generalization of the Cauchy/Lorentzian, Geman-McClure,
Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2
loss functions. By introducing robustness as a continuous parameter, our loss
function allows algorithms built around robust loss minimization to be
generalized, which improves performance on basic vision tasks such as
registration and clustering. Interpreting our loss as the negative log of a
univariate density yields a general probability distribution that includes
normal and Cauchy distributions as special cases. This probabilistic
interpretation enables the training of neural networks in which the robustness
of the loss automatically adapts itself during training, which improves
performance on learning-based tasks such as generative image synthesis and
unsupervised monocular depth estimation, without requiring any manual parameter
tuning.","Our general loss function (left) and its gradient (right) for different values of its shape parameter α. Several values of α reproduce existing loss functions: L2 loss (α = 2), Charbonnier loss (α = 1), Cauchy loss (α = 0), Geman-McClure loss (α = −2), and Welsch loss (α = −∞).",What is the relationship between the shape parameter α and the shape of the loss function?
spiqa_307,1803.01128v3,"Based on the experimental results presented in Table 3, how does the success rate of non-overlapping attacks in text summarization tasks vary as the number of changed words in the input sentences decreases, particularly in datasets like Gigaword and DUC2004?","There is a negative correlation between the success rate of the non-overlapping attack and the number of words changed in the input sentence. In other words, the fewer words that are changed, the higher the success rate of the attack.",1803.01128v3.pdf,"['1803.01128v3.pdf', '1704.04539v2.pdf', '1809.03550v3.pdf', '1906.10843v1.pdf', '1802.07459v2.pdf', '1611.02654v2.pdf', '1611.05742v3.pdf', '1804.04786v3.pdf', '1811.02553v4.pdf']","This can be observed by comparing the ""Success%"" and ""# changed"" columns in the table. For example, the Gigaword dataset has the highest success rate (86.0%) and the lowest average number of changed words (2.17). Conversely, the DUC2004 dataset has the lowest success rate (84.2%) and a higher average number of changed words (2.50). This suggests that the attack is more effective when it can achieve its goal (generating a completely different summarization) with minimal changes to the original sentence.",1803.01128v3-Table3-1.png,Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples,"Crafting adversarial examples has become an important technique to evaluate
the robustness of deep neural networks (DNNs). However, most existing works
focus on attacking the image classification problem since its input space is
continuous and output space is finite.
  In this paper, we study the much more challenging problem of crafting
adversarial examples for sequence-to-sequence (seq2seq) models, whose inputs
are discrete text strings and outputs have an almost infinite number of
possibilities. To address the challenges caused by the discrete input space, we
propose a projected gradient method combined with group lasso and gradient
regularization. To handle the almost infinite output space, we design some
novel loss functions to conduct non-overlapping attack and targeted keyword
attack. We apply our algorithm to machine translation and text summarization
tasks, and verify the effectiveness of the proposed algorithm: by changing less
than 3 words, we can make seq2seq model to produce desired outputs with high
success rates. On the other hand, we recognize that, compared with the
well-evaluated CNN-based classifiers, seq2seq models are intrinsically more
robust to adversarial attacks.","Table 3: Results of non-overlapping attack in text summarization. # changed is how many words are changed in the input sentence. The high BLEU scores and low average number of changed words indicate that the crafted adversarial inputs are very similar to their originals, and we achieve high success rates to generate a summarization that differs with the original at every position for all three datasets.",What is the relationship between the success rate of the non-overlapping attack and the number of words changed in the input sentence?
spiqa_308,1809.01246v1,"In the Fast and Accurate Graph Stream Summarization paper, how does the table of hash values (H(v)) in Fig. 2 influence the process of combining nodes like a and d in the graph sketch?","The table provides the mapping between the nodes in the original graph and their corresponding hash values, which are used to create the graph sketch.",1809.01246v1.pdf,"['1809.01246v1.pdf', '1708.05239v3.pdf', '1703.00899v2.pdf', '1611.02654v2.pdf']"," The table shows the hash values (H(v)) for each node (a-g) in the original graph. These hash values are used to map the nodes to their corresponding nodes in the graph sketch. For example, nodes a and d both have a hash value of 2, so they are combined into a single node in the graph sketch.

**Figure type:** Schematic",1809.01246v1-Figure2-1.png,Fast and Accurate Graph Stream Summarization,"A graph stream is a continuous sequence of data items, in which each item
indicates an edge, including its two endpoints and edge weight. It forms a
dynamic graph that changes with every item in the stream. Graph streams play
important roles in cyber security, social networks, cloud troubleshooting
systems and other fields. Due to the vast volume and high update speed of graph
streams, traditional data structures for graph storage such as the adjacency
matrix and the adjacency list are no longer sufficient. However, prior art of
graph stream summarization, like CM sketches, gSketches, TCM and gMatrix,
either supports limited kinds of queries or suffers from poor accuracy of query
results. In this paper, we propose a novel Graph Stream Sketch (GSS for short)
to summarize the graph streams, which has the linear space cost (O(|E|), E is
the edge set of the graph) and the constant update time complexity (O(1)) and
supports all kinds of queries over graph streams with the controllable errors.
Both theoretical analysis and experiment results confirm the superiority of our
solution with regard to the time/space complexity and query results' precision
compared with the state-of-the-art.",Fig. 2. A sample map function,What is the relationship between the table and the graph sketch in the figure?
spiqa_309,1703.00899v2,"Based on the noise-adding mechanism depicted in the figure, how does the true market state qt at time t relate to the noisy version q̂t, and how is the sum of Laplace noise vectors accumulated by following the arrows backward from t to 0?",The noisy version q̂t at time t is equal to the true market state qt plus a sum of Laplace noise vectors obtained by following the arrows all the way back to 0.,1703.00899v2.pdf,"['1703.00899v2.pdf', '1811.08481v2.pdf', '1702.03584v3.pdf', '1809.03449v3.pdf']","The figure shows a directed graph where each node represents a trade (dqt) at a given time t. The arrows in the graph represent the relationship between the true market state and the noisy version. Each arrow originates at t, points backwards to s(t), and is labeled with an independent Laplace noise vector zt. The noisy version q̂t at time t is then calculated by adding the true market state qt to the sum of all the noise vectors encountered by following the arrows backwards to 0.",1703.00899v2-Figure1-1.png,Bounded-Loss Private Prediction Markets,"Prior work has investigated variations of prediction markets that preserve
participants' (differential) privacy, which formed the basis of useful
mechanisms for purchasing data for machine learning objectives. Such markets
required potentially unlimited financial subsidy, however, making them
impractical. In this work, we design an adaptively-growing prediction market
with a bounded financial subsidy, while achieving privacy, incentives to
produce accurate predictions, and precision in the sense that market prices are
not heavily impacted by the added privacy-preserving noise. We briefly discuss
how our mechanism can extend to the data-purchasing setting, and its
relationship to traditional learning algorithms.","Picturing the continual observation technique for preserving privacy [7, 10]. Each dqt is a trade. The true market state at t is qt = ∑t j=1 dq j and the goal is to release a noisy version q̂t Each arrow originates at t, points backwards to s(t), and is labeled with independent Laplace noise vector zt. Now q̂t = qt + zt + zs(t) + zs(s(t)) + · · · . In other words, the noise added at t is a sum of noises obtained by following the arrows all the way back to 0. There are two key properties: Each t has only log T arrows passing above it, and each path backwards takes only log T jumps.",What is the relationship between the true market state qt and the noisy version q̂t at time t?
spiqa_310,1701.06171v4,"Based on the graphical model illustrated in Figure (a) and (b) of the ""Greedy Structure Learning of Hierarchical Compositional Models"" paper, how do the variables at different layers of the Compositional Active Basis Model exhibit hierarchical dependence?",The variables in the Compositional Active Basis Model are hierarchically dependent. The variables at each layer are dependent on the variables at the layer above it.,1701.06171v4.pdf,"['1701.06171v4.pdf', '1805.08751v2.pdf', '1901.00056v2.pdf', '1709.08294v3.pdf', '1803.02750v3.pdf', '1611.07718v2.pdf', '1708.02153v2.pdf']"," This is shown in Figure (a) and (b), where the variables at each layer are connected to the variables at the layer above it by arrows. ",1701.06171v4-Figure2-1.png,Greedy Structure Learning of Hierarchical Compositional Models,"In this work, we consider the problem of learning a hierarchical generative
model of an object from a set of images which show examples of the object in
the presence of variable background clutter. Existing approaches to this
problem are limited by making strong a-priori assumptions about the object's
geometric structure and require segmented training data for learning. In this
paper, we propose a novel framework for learning hierarchical compositional
models (HCMs) which do not suffer from the mentioned limitations. We present a
generalized formulation of HCMs and describe a greedy structure learning
framework that consists of two phases: Bottom-up part learning and top-down
model composition. Our framework integrates the foreground-background
segmentation problem into the structure learning task via a background model.
As a result, we can jointly optimize for the number of layers in the hierarchy,
the number of parts per layer and a foreground-background segmentation based on
class labels only. We show that the learned HCMs are semantically meaningful
and achieve competitive results when compared to other generative object models
at object classification on a standard transfer learning dataset.","The dependence structure between random variables in a Compositional Active Basis Model. (a) The simplest possible CABM, a binary-tree structured Markov random field. (b) The graphical model of a generalized multi-layer CABM (Section 3.3). We learn the full multi-layer structure of a CABM including the number of layers L, the number of parts per layer NL, . . . , N0 as well as their hierarchical dependence structure.",What is the relationship between the variables in the Compositional Active Basis Model?
spiqa_311,1707.01917v2,"What specific role does OpenIE play in extracting tuples from the unlabeled text corpus that are subsequently transformed into the 3-mode tensors X1, X2, and X3 used in the joint Tucker decomposition in Step 1 of TFBA?","OpenIE is used to extract tuples from the unlabeled text corpus. These tuples are then used to create the 3-mode tensors X1, X2, and X3.",1707.01917v2.pdf,"['1707.01917v2.pdf', '1706.04269v2.pdf', '1705.02946v3.pdf', '1707.01922v5.pdf']","The figure shows that OpenIE is used to process the unlabeled text corpus and produce tuples. These tuples are then used as input to the joint Tucker decomposition process, which produces the binary schemata.",1707.01917v2-Figure1-1.png,Higher-order Relation Schema Induction using Tensor Factorization with Back-off and Aggregation,"Relation Schema Induction (RSI) is the problem of identifying type signatures
of arguments of relations from unlabeled text. Most of the previous work in
this area have focused only on binary RSI, i.e., inducing only the subject and
object type signatures per relation. However, in practice, many relations are
high-order, i.e., they have more than two arguments and inducing type
signatures of all arguments is necessary. For example, in the sports domain,
inducing a schema win(WinningPlayer, OpponentPlayer, Tournament, Location) is
more informative than inducing just win(WinningPlayer, OpponentPlayer). We
refer to this problem as Higher-order Relation Schema Induction (HRSI). In this
paper, we propose Tensor Factorization with Back-off and Aggregation (TFBA), a
novel framework for the HRSI problem. To the best of our knowledge, this is the
first attempt at inducing higher-order relation schemata from unlabeled text.
Using the experimental analysis on three real world datasets, we show how TFBA
helps in dealing with sparsity and induce higher order schemata.","Overview of Step 1 of TFBA. Rather than factorizing the higher-order tensor X , TFBA performs joint Tucker decomposition of multiple 3-mode tensors, X 1, X 2, and X 3, derived out of X . This joint factorization is performed using shared latent factors A, B, and C. This results in binary schemata, each of which is stored as a cell in one of the core tensors G1, G2, and G3. Please see Section 3.2.1 for details.",What is the role of OpenIE in Step 1 of TFBA?
spiqa_312,1707.06320v2,"In the model architecture shown in the figure, how is the ""max"" function applied to the output of the LSTM during the decoding process for tasks like Cap2Img and Cap2Both, and how does it influence word selection?","The ""max"" function is used to select the most probable word at each time step in the decoding process.",1707.06320v2.pdf,"['1707.06320v2.pdf', '1811.07073v3.pdf', '1706.00633v4.pdf', '1804.01429v3.pdf']","The figure shows that the ""max"" function is applied to the output of the LSTM at each time step. This selects the word with the highest probability, which is then used as input to the next time step.",1707.06320v2-Figure1-1.png,Learning Visually Grounded Sentence Representations,"We introduce a variety of models, trained on a supervised image captioning
corpus to predict the image features for a given caption, to perform sentence
representation grounding. We train a grounded sentence encoder that achieves
good performance on COCO caption and image retrieval and subsequently show that
this encoder can successfully be transferred to various NLP tasks, with
improved performance over text-only models. Lastly, we analyze the contribution
of grounding, and show that word embeddings learned by this system outperform
non-grounded ones.","Model architecture: predicting either an image (Cap2Img), an alternative caption (Cap2Cap), or both at the same time (Cap2Both).","What is the role of the ""max"" function in the model architecture?"
spiqa_313,1804.01429v3,"According to the figure illustrating distance-based place discretization, how does the 3D ConvNet contribute to extracting features from input images and generating place-based feature descriptions in the network model?",The 3D ConvNet is used to extract features from the input images. These features are then used to generate place-based feature descriptions.,1804.01429v3.pdf,"['1804.01429v3.pdf', '1705.02946v3.pdf', '1708.02153v2.pdf', '1704.04539v2.pdf', '1709.00139v4.pdf', '1811.08481v2.pdf', '1705.02798v6.pdf', '1803.03467v4.pdf', '1708.01425v4.pdf', '1705.08016v3.pdf', '1703.04887v4.pdf', '1805.04687v2.pdf', '1703.02507v3.pdf']",The figure shows that the 3D ConvNet is applied to the input images to extract features. These features are then concatenated and fed into another 3D ConvNet to generate place-based feature descriptions.,1804.01429v3-Figure5-1.png,Layout-induced Video Representation for Recognizing Agent-in-Place Actions,"We address the recognition of agent-in-place actions, which are associated
with agents who perform them and places where they occur, in the context of
outdoor home surveillance. We introduce a representation of the geometry and
topology of scene layouts so that a network can generalize from the layouts
observed in the training set to unseen layouts in the test set. This
Layout-Induced Video Representation (LIVR) abstracts away low-level appearance
variance and encodes geometric and topological relationships of places in a
specific scene layout. LIVR partitions the semantic features of a video clip
into different places to force the network to learn place-based feature
descriptions; to predict the confidence of each action, LIVR aggregates
features from the place associated with an action and its adjacent places on
the scene layout. We introduce the Agent-in-Place Action dataset to show that
our method allows neural network models to generalize significantly better to
unseen scenes.",The process of distance-based place discretization.,What is the role of the 3D ConvNet in the distance-based place discretization process?
spiqa_314,1804.07849v4,"In the figure illustrating the example sentence ""had these keys in my"" with the target word ""keys,"" how does the BiLSTM transform the character-level input into word-level representations during the part-of-speech induction process?",The BiLSTM takes as input the character-level representations of the words and outputs a word-level representation for each word.,1804.07849v4.pdf,"['1804.07849v4.pdf', '1811.08481v2.pdf', '1704.04539v2.pdf', '1804.01429v3.pdf', '1708.02153v2.pdf', '1809.03550v3.pdf']","The BiLSTM is shown at the bottom of the figure, taking as input the character-level representations of the words and outputting a word-level representation for each word.",1804.07849v4-Figure1-1.png,Mutual Information Maximization for Simple and Accurate Part-Of-Speech Induction,"We address part-of-speech (POS) induction by maximizing the mutual
information between the induced label and its context. We focus on two training
objectives that are amenable to stochastic gradient descent (SGD): a novel
generalization of the classical Brown clustering objective and a recently
proposed variational lower bound. While both objectives are subject to noise in
gradient updates, we show through analysis and experiments that the variational
lower bound is robust whereas the generalized Brown objective is vulnerable. We
obtain competitive performance on a multitude of datasets and languages with a
simple architecture that encodes morphology and context.",Architecture illustrated on the example text “had these keys in my” with target Y = “keys”.,What is the role of the BiLSTM in the architecture?
spiqa_315,1805.06431v4,"In the ChoiceNet architecture, as depicted in the figure of the paper, how does the Cholesky block decompose the covariance matrix Σk to ensure that it is positive definite and suitable for generating a valid Gaussian distribution?","The Cholesky block is used to decompose the covariance matrix Σk into a lower triangular matrix and its transpose. This decomposition is used to ensure that the covariance matrix is positive definite, which is a requirement for the Gaussian distribution.",1805.06431v4.pdf,"['1805.06431v4.pdf', '1804.05938v2.pdf', '1803.04383v2.pdf', '1703.02507v3.pdf', '1809.04276v2.pdf', '1709.02418v2.pdf', '1702.03584v3.pdf', '1809.00458v1.pdf', '1703.10730v2.pdf', '1803.03467v4.pdf', '1611.05742v3.pdf', '1805.00912v4.pdf']",The Cholesky block is shown in the figure as a blue box. It takes the covariance matrix Σk as input and outputs a lower triangular matrix. This matrix is then used to generate the variance of the Gaussian distribution.,1805.06431v4-Figure3-1.png,Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks,"In this paper, we focus on weakly supervised learning with noisy training
data for both classification and regression problems.We assume that the
training outputs are collected from a mixture of a target and correlated noise
distributions.Our proposed method simultaneously estimates the target
distribution and the quality of each data which is defined as the correlation
between the target and data generating distributions.The cornerstone of the
proposed method is a Cholesky Block that enables modeling dependencies among
mixture distributions in a differentiable manner where we maintain the
distribution over the network weights.We first provide illustrative examples in
both regression and classification tasks to show the effectiveness of the
proposed method.Then, the proposed method is extensively evaluated in a number
of experiments where we show that it constantly shows comparable or superior
performances compared to existing baseline methods in the handling of noisy
data.","Overall mechanism of ChoiceNet. It consists of K mixtures and each mixture outputs triplet (πk, µk,Σk) via Algorithm 1. ρ1 = 1 is reserved to model the target distribution.",What is the role of the Cholesky block in the ChoiceNet architecture?
spiqa_316,1805.08751v2,"Based on the architecture diagram presented in the FAKEDETECTOR paper, how do the GDU and HFLU modules function in both the encoder and decoder parts to handle feature extraction and fusion for fake news detection?","The GDU (Gated Dilated Unit) modules are responsible for extracting features from the input data, while the HFLU (Hybrid Feature Learning Unit) modules are responsible for fusing the features extracted by the GDU modules.",1805.08751v2.pdf,"['1805.08751v2.pdf', '1906.10843v1.pdf', '1811.02721v3.pdf', '1809.03550v3.pdf', '1704.00774v3.pdf', '1708.06832v3.pdf', '1803.05776v2.pdf', '1705.02946v3.pdf', '1704.04539v2.pdf']","The figure shows that the GDU and HFLU modules are used in both the encoder and decoder parts of the FAKEDETECTOR framework. The GDU modules take the input data and extract features, which are then passed to the HFLU modules. The HFLU modules then fuse the features from the different GDU modules and produce the final output.",1805.08751v2-Figure5-1.png,FAKEDETECTOR: Effective Fake News Detection with Deep Diffusive Neural Network,"In recent years, due to the booming development of online social networks,
fake news for various commercial and political purposes has been appearing in
large numbers and widespread in the online world. With deceptive words, online
social network users can get infected by these online fake news easily, which
has brought about tremendous effects on the offline society already. An
important goal in improving the trustworthiness of information in online social
networks is to identify the fake news timely. This paper aims at investigating
the principles, methodologies and algorithms for detecting fake news articles,
creators and subjects from online social networks and evaluating the
corresponding performance. This paper addresses the challenges introduced by
the unknown characteristics of fake news and diverse connections among news
articles, creators and subjects. This paper introduces a novel automatic fake
news credibility inference model, namely FAKEDETECTOR. Based on a set of
explicit and latent features extracted from the textual information,
FAKEDETECTOR builds a deep diffusive network model to learn the representations
of news articles, creators and subjects simultaneously. Extensive experiments
have been done on a real-world fake news dataset to compare FAKEDETECTOR with
several state-of-the-art models, and the experimental results have demonstrated
the effectiveness of the proposed model.",The Architecture of Framework FAKEDETECTOR.,What is the role of the GDU and HFLU modules in the FAKEDETECTOR framework?
spiqa_317,1704.05958v2,"In the figure depicting the embedding model, what is the role of the separate GRU cell in mapping the textual relation embedding to a probability distribution over knowledge base (KB) relations?",The GRU cell is used to map a textual relation embedding to a probability distribution over KB relations.,1704.05958v2.pdf,"['1704.05958v2.pdf', '1603.00286v5.pdf', '1804.07931v2.pdf', '1804.05938v2.pdf', '1603.03833v4.pdf', '1703.00060v2.pdf', '1706.04269v2.pdf', '1704.05426v4.pdf', '1709.02418v2.pdf', '1803.03467v4.pdf', '1611.04684v1.pdf', '1811.09393v4.pdf', '1809.04276v2.pdf']","The figure shows a RNN with GRU for embedding, which is used to generate a textual relation embedding. This embedding is then fed into a separate GRU cell, which outputs a probability distribution over KB relations.",1704.05958v2-Figure3-1.png,Global Relation Embedding for Relation Extraction,"We study the problem of textual relation embedding with distant supervision.
To combat the wrong labeling problem of distant supervision, we propose to
embed textual relations with global statistics of relations, i.e., the
co-occurrence statistics of textual and knowledge base relations collected from
the entire corpus. This approach turns out to be more robust to the training
noise introduced by distant supervision. On a popular relation extraction
dataset, we show that the learned textual relation embedding can be used to
augment existing relation extraction models and significantly improve their
performance. Most remarkably, for the top 1,000 relational facts discovered by
the best existing model, the precision can be improved from 83.9% to 89.3%.",Embedding model. Left: A RNN with GRU for embedding. Middle: embedding of textual relation. Right: a separate GRU cell to map a textual relation embedding to a probability distribution over KB relations.,What is the role of the GRU cell in the embedding model?
spiqa_318,1804.05936v2,"How does the GRU in the Deep Listwise Context Model, as shown in Figure 1, process the ranked list of documents and compute the final ranking score using feature vectors and hidden outputs?",The GRU is used to process the ranked list of documents provided by a global ranking function.,1804.05936v2.pdf,"['1804.05936v2.pdf', '1704.08615v2.pdf', '1803.04383v2.pdf', '1704.04539v2.pdf', '1805.02349v2.pdf', '1703.00899v2.pdf', '1705.10667v4.pdf', '1803.04572v2.pdf', '1707.08608v3.pdf', '1707.06320v2.pdf', '1704.07121v2.pdf', '1708.05239v3.pdf', '1809.00263v5.pdf']",The GRU takes the feature vector of each document in the ranked list as input and outputs a final network state and hidden outputs. These outputs are then used to compute the final ranking score of each document.,1804.05936v2-Figure1-1.png,Learning a Deep Listwise Context Model for Ranking Refinement,"Learning to rank has been intensively studied and widely applied in
information retrieval. Typically, a global ranking function is learned from a
set of labeled data, which can achieve good performance on average but may be
suboptimal for individual queries by ignoring the fact that relevant documents
for different queries may have different distributions in the feature space.
Inspired by the idea of pseudo relevance feedback where top ranked documents,
which we refer as the \textit{local ranking context}, can provide important
information about the query's characteristics, we propose to use the inherent
feature distributions of the top results to learn a Deep Listwise Context Model
that helps us fine tune the initial ranked list. Specifically, we employ a
recurrent neural network to sequentially encode the top results using their
feature vectors, learn a local context model and use it to re-rank the top
results. There are three merits with our model: (1) Our model can capture the
local ranking context based on the complex interactions between top results
using a deep neural network; (2) Our model can be built upon existing
learning-to-rank methods by directly using their extracted feature vectors; (3)
Our model is trained with an attention-based loss function, which is more
effective and efficient than many existing listwise methods. Experimental
results show that the proposed model can significantly improve the
state-of-the-art learning to rank methods on benchmark retrieval corpora.","The overall structure of the Deep Listwise Context Model (DLCM). Rnq is a ranked list provided by a global ranking function f for query q; x(q,di ) is the feature vector for document di ; sn and oi is the final network state and hidden outputs of the RNN with GRU in I (Rnq ,Xn q ); and Score(di ) is the final ranking score of di computed with ϕ(on+1−i , sn )",What is the role of the GRU in the Deep Listwise Context Model (DLCM)?
spiqa_319,1809.03149v2,"Based on the figure illustrating the framework for adaptive display exposure in the paper, how does the Higher Level Policy guide the decisions and set constraints for the Lower Level Policy when determining the next sub-trajectory?",The Higher Level Policy sets constraints for the next sub-trajectory and provides information about the previous stage to the Lower Level Policy.,1809.03149v2.pdf,"['1809.03149v2.pdf', '1703.07015v3.pdf', '1805.06447v3.pdf', '1706.03847v3.pdf', '1706.00633v4.pdf', '1809.03550v3.pdf', '1805.06431v4.pdf', '1811.08481v2.pdf', '1708.00160v2.pdf', '1709.08294v3.pdf', '1809.00263v5.pdf', '1803.06506v3.pdf', '1708.02153v2.pdf', '1701.06171v4.pdf', '1708.01425v4.pdf']",The figure shows that the Higher Level Policy is located above the Lower Level Policy and provides input to it. The text in the figure also states that the Higher Level Policy sets constraints for the next sub-trajectory.,1809.03149v2-Figure2-1.png,Learning Adaptive Display Exposure for Real-Time Advertising,"In E-commerce advertising, where product recommendations and product ads are
presented to users simultaneously, the traditional setting is to display ads at
fixed positions. However, under such a setting, the advertising system loses
the flexibility to control the number and positions of ads, resulting in
sub-optimal platform revenue and user experience. Consequently, major
e-commerce platforms (e.g., Taobao.com) have begun to consider more flexible
ways to display ads. In this paper, we investigate the problem of advertising
with adaptive exposure: can we dynamically determine the number and positions
of ads for each user visit under certain business constraints so that the
platform revenue can be increased? More specifically, we consider two types of
constraints: request-level constraint ensures user experience for each user
visit, and platform-level constraint controls the overall platform monetization
rate. We model this problem as a Constrained Markov Decision Process with
per-state constraint (psCMDP) and propose a constrained two-level reinforcement
learning approach to decompose the original problem into two relatively
independent sub-problems. To accelerate policy learning, we also devise a
constrained hindsight experience replay mechanism. Experimental evaluations on
industry-scale real-world datasets demonstrate the merits of our approach in
both obtaining higher revenue under the constraints and the effectiveness of
the constrained hindsight experience replay mechanism.",The Framework Structure.,What is the role of the Higher Level Policy in the framework?
spiqa_320,1803.06506v3,"In the context of Figure 2, how does the Joint Attention Module process the embedded features from images (V^i) and phrases (t^i) to create a spatial attention map that aids the decoder in predicting the common concept?",The Joint Attention Module takes the embedded image and phrase features as input and uses them to induce a parameterization for spatial attention. This spatial attention map is then used by the decoder to predict the common concept.,1803.06506v3.pdf,"['1803.06506v3.pdf', '1809.03149v2.pdf', '1705.02946v3.pdf', '1706.00633v4.pdf', '1705.08016v3.pdf', '1805.07567v2.pdf', '1803.04383v2.pdf']","The figure shows that the Joint Attention Module takes the embedded image features (V^i) and the embedded phrase features (t^i) as input. The output of the Joint Attention Module is a spatial attention map, which is used by the decoder to predict the common concept.

Figure type: schematic",1803.06506v3-Figure2-1.png,Learning Unsupervised Visual Grounding Through Semantic Self-Supervision,"Localizing natural language phrases in images is a challenging problem that
requires joint understanding of both the textual and visual modalities. In the
unsupervised setting, lack of supervisory signals exacerbate this difficulty.
In this paper, we propose a novel framework for unsupervised visual grounding
which uses concept learning as a proxy task to obtain self-supervision. The
simple intuition behind this idea is to encourage the model to localize to
regions which can explain some semantic property in the data, in our case, the
property being the presence of a concept in a set of images. We present
thorough quantitative and qualitative experiments to demonstrate the efficacy
of our approach and show a 5.6% improvement over the current state of the art
on Visual Genome dataset, a 5.8% improvement on the ReferItGame dataset and
comparable to state-of-art performance on the Flickr30k dataset.","Figure 2. An overview of our model for unsupervised visual grounding of phrases. The encoder takes in a set of image-phrase pairs, indexed by i, all sharing a common concept. The encoder embeds the image and the phrase to Vi and ti respectively. These features are used to induce a parametrization for spatial attention. Next, the decoder uses the visual attention map to predict the common concept. In addition, the decoder also predicts the common concept independently for each pair (i). For details, see Section 3.2.",What is the role of the Joint Attention Module in the model?
spiqa_321,1809.03449v3,"In the figure illustrating the Knowledge Aided Reader (KAR) model, how does the Knowledge Aided Similarity Matrix function in computing the relevance between the enhanced question and passage context embeddings to weight the passage embeddings?","The Knowledge Aided Similarity Matrix is used to compute the similarity between the question and passage context embeddings. This similarity score is then used to weight the passage context embeddings, giving more weight to those parts of the passage that are most relevant to the question.",1809.03449v3.pdf,"['1809.03449v3.pdf', '1706.03847v3.pdf', '1703.07015v3.pdf', '1805.07567v2.pdf', '1804.05995v2.pdf']","The figure shows that the Knowledge Aided Similarity Matrix is used to compute the similarity between the enhanced question context embeddings and the enhanced passage context embeddings. This similarity score is then used to weight the passage context embeddings, giving more weight to those parts of the passage that are most relevant to the question.",1809.03449v3-Figure1-1.png,Explicit Utilization of General Knowledge in Machine Reading Comprehension,"To bridge the gap between Machine Reading Comprehension (MRC) models and
human beings, which is mainly reflected in the hunger for data and the
robustness to noise, in this paper, we explore how to integrate the neural
networks of MRC models with the general knowledge of human beings. On the one
hand, we propose a data enrichment method, which uses WordNet to extract
inter-word semantic connections as general knowledge from each given
passage-question pair. On the other hand, we propose an end-to-end MRC model
named as Knowledge Aided Reader (KAR), which explicitly uses the above
extracted general knowledge to assist its attention mechanisms. Based on the
data enrichment method, KAR is comparable in performance with the
state-of-the-art MRC models, and significantly more robust to noise than them.
When only a subset (20%-80%) of the training examples are available, KAR
outperforms the state-of-the-art MRC models by a large margin, and is still
reasonably robust to noise.",An end-to-end MRC model: Knowledge Aided Reader (KAR),What is the role of the Knowledge Aided Similarity Matrix in the KAR model?
spiqa_322,1710.01507v4,"In the figure depicting the multi-strategy architecture for clickbait detection, what is the specific role of the LSTM network in processing the post text, and how does its output contribute to the final prediction?",The LSTM network is used to process the post text and generate a post text embedding.,1710.01507v4.pdf,"['1710.01507v4.pdf', '1704.07854v4.pdf', '1702.03584v3.pdf', '1705.09296v2.pdf', '1709.00139v4.pdf', '1705.02946v3.pdf', '1804.04786v3.pdf', '1906.10843v1.pdf', '1812.10735v2.pdf', '1804.07849v4.pdf', '1705.09882v2.pdf', '1805.04687v2.pdf', '1705.10667v4.pdf', '1703.02507v3.pdf', '1701.03077v10.pdf']",The LSTM network is shown in the left-hand side of the figure. It takes the post text as input and outputs a post text embedding. This embedding is then used as input to the fully connected layers that predict the probability of clickbait.,1710.01507v4-Figure1-1.png,Identifying Clickbait: A Multi-Strategy Approach Using Neural Networks,"Online media outlets, in a bid to expand their reach and subsequently
increase revenue through ad monetisation, have begun adopting clickbait
techniques to lure readers to click on articles. The article fails to fulfill
the promise made by the headline. Traditional methods for clickbait detection
have relied heavily on feature engineering which, in turn, is dependent on the
dataset it is built for. The application of neural networks for this task has
only been explored partially. We propose a novel approach considering all
information found in a social media post. We train a bidirectional LSTM with an
attention mechanism to learn the extent to which a word contributes to the
post's clickbait score in a differential manner. We also employ a Siamese net
to capture the similarity between source and target information. Information
gleaned from images has not been considered in previous approaches. We learn
image embeddings from large amounts of data using Convolutional Neural Networks
to add another layer of complexity to our model. Finally, we concatenate the
outputs from the three separate components, serving it as input to a fully
connected layer. We conduct experiments over a test corpus of 19538 social
media posts, attaining an F1 score of 65.37% on the dataset bettering the
previous state-of-the-art, as well as other proposed approaches, feature
engineering or otherwise.",Model Architecture,What is the role of the LSTM network in the model architecture?
spiqa_323,1603.03833v4,"How does the LSTM-MDN network in Figure 3 utilize the gripper pose, status, and object pose inputs over the 50-step unrolling during the training phase to learn the trajectory mapping and control the robot arm through backpropagation of errors?","The LSTM-MDN network is used to learn the relationship between the gripper pose and status, the pose of relevant objects, and the joint angles of the robot arm.",1603.03833v4.pdf,"['1603.03833v4.pdf', '1612.02803v5.pdf', '1705.02798v6.pdf', '1804.04786v3.pdf', '1702.03584v3.pdf', '1804.05938v2.pdf', '1611.03780v2.pdf', '1710.05654v2.pdf', '1704.05958v2.pdf', '1811.06635v1.pdf']",The figure shows that the LSTM-MDN network is unrolled for 50 time-steps during the training phase. The gripper pose and status (open/close)  𝑒𝑡  and the pose of relevant objects  𝑞𝑡  at time-step  𝑡  is used as input and output of the network to calculate and backpropagate the error to update the weights. This process allows the network to learn the relationship between the inputs and outputs.,1603.03833v4-Figure3-1.png,From virtual demonstration to real-world manipulation using LSTM and MDN,"Robots assisting the disabled or elderly must perform complex manipulation
tasks and must adapt to the home environment and preferences of their user.
Learning from demonstration is a promising choice, that would allow the
non-technical user to teach the robot different tasks. However, collecting
demonstrations in the home environment of a disabled user is time consuming,
disruptive to the comfort of the user, and presents safety challenges. It would
be desirable to perform the demonstrations in a virtual environment. In this
paper we describe a solution to the challenging problem of behavior transfer
from virtual demonstration to a physical robot. The virtual demonstrations are
used to train a deep neural network based controller, which is using a Long
Short Term Memory (LSTM) recurrent neural network to generate trajectories. The
training process uses a Mixture Density Network (MDN) to calculate an error
signal suitable for the multimodal nature of demonstrations. The controller
learned in the virtual environment is transferred to a physical robot (a
Rethink Robotics Baxter). An off-the-shelf vision component is used to
substitute for geometric knowledge available in the simulation and an inverse
kinematics module is used to allow the Baxter to enact the trajectory. Our
experimental studies validate the three contributions of the paper: (1) the
controller learned from virtual demonstrations can be used to successfully
perform the manipulation tasks on a physical robot, (2) the LSTM+MDN
architectural choice outperforms other choices, such as the use of feedforward
networks and mean-squared error based training signals and (3) allowing
imperfect demonstrations in the training set also allows the controller to
learn how to correct its manipulation mistakes.","Figure 3: The training and evaluation phase. During the training the LSTM network is unrolled for 50 time-steps. The gripper pose and status (open/close) et and the pose of relevant objects qt at time-step t is used as input and output of the network to calculate and backpropagate the error to update the weights. During the evaluation phase, the mixture density parameters are used to form a mixture of Gaussians and draw a sample from it. The sample is used to control the robot arm.",What is the role of the LSTM-MDN network in the training phase?
spiqa_324,1901.00056v2,"In the figure of the SYNONYMNET model, how does the Leaky Unit aggregate context information from the Context Encoder and Retriever, and what is its contribution to the subsequent bilateral matching process?",The Leaky Unit helps to aggregate the context information from different sources and allows the model to learn the relationships between entities and their contexts.,1901.00056v2.pdf,"['1901.00056v2.pdf', '1709.00139v4.pdf', '1804.07849v4.pdf', '1804.05938v2.pdf', '1710.05654v2.pdf', '1708.06832v3.pdf', '1705.10667v4.pdf', '1701.03077v10.pdf', '1809.03149v2.pdf']","The Leaky Unit is shown in the figure as a white circle with arrows pointing to it from the Context Encoder and the Context Retriever. The arrows indicate that the Leaky Unit receives information from both of these sources. The Leaky Unit then outputs a single representation of the context information, which is used by the Bilateral Matching unit to learn the relationships between entities and their contexts.",1901.00056v2-Figure1-1.png,Entity Synonym Discovery via Multipiece Bilateral Context Matching,"Being able to automatically discover synonymous entities in an open-world
setting benefits various tasks such as entity disambiguation or knowledge graph
canonicalization. Existing works either only utilize entity features, or rely
on structured annotations from a single piece of context where the entity is
mentioned. To leverage diverse contexts where entities are mentioned, in this
paper, we generalize the distributional hypothesis to a multi-context setting
and propose a synonym discovery framework that detects entity synonyms from
free-text corpora with considerations on effectiveness and robustness. As one
of the key components in synonym discovery, we introduce a neural network model
SYNONYMNET to determine whether or not two given entities are synonym with each
other. Instead of using entities features, SYNONYMNET makes use of multiple
pieces of contexts in which the entity is mentioned, and compares the
context-level similarity via a bilateral matching schema. Experimental results
demonstrate that the proposed model is able to detect synonym sets that are not
observed during training on both generic and domain-specific datasets:
Wiki+Freebase, PubMed+UMLS, and MedBook+MKG, with up to 4.16% improvement in
terms of Area Under the Curve and 3.19% in terms of Mean Average Precision
compared to the best baseline method.",Overview of the proposed model SYNONYMNET. The diamonds are entities. Each circle is associated with a piece of context in which an entity appears. SYNONYMNET learns to minimize the loss calculated using multiple pieces of contexts via bilateral matching with leaky units.,What is the role of the Leaky Unit in the SYNONYMNET model?
spiqa_325,1811.09393v4,"In the Frame-Recurrent Generator depicted in Figure b of the paper, how does the Motion Compensation block utilize the warping operation to align the previous frame \( g_{t-1} \) with the current frame \( g_t \) and contribute to temporally coherent video generation?","The Motion Compensation block estimates the motion between the previous frame and the current frame, and uses this information to warp the previous frame to the current frame. This helps the generator to produce more realistic images by taking into account the temporal information in the video sequence.",1811.09393v4.pdf,"['1811.09393v4.pdf', '1809.01246v1.pdf', '1708.06832v3.pdf', '1812.10735v2.pdf', '1702.08694v3.pdf']","The Motion Compensation block is shown in Figure b, and it is connected to the input gt−1 and the output gt. The block W represents the warping operation.",1811.09393v4-Figure2-1.png,Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation,"Our work explores temporal self-supervision for GAN-based video generation
tasks. While adversarial training successfully yields generative models for a
variety of areas, temporal relationships in the generated data are much less
explored. Natural temporal changes are crucial for sequential generation tasks,
e.g. video super-resolution and unpaired video translation. For the former,
state-of-the-art methods often favor simpler norm losses such as $L^2$ over
adversarial training. However, their averaging nature easily leads to
temporally smooth results with an undesirable lack of spatial detail. For
unpaired video translation, existing approaches modify the generator networks
to form spatio-temporal cycle consistencies. In contrast, we focus on improving
learning objectives and propose a temporally self-supervised algorithm. For
both tasks, we show that temporal adversarial learning is key to achieving
temporally coherent solutions without sacrificing spatial detail. We also
propose a novel Ping-Pong loss to improve the long-term temporal consistency.
It effectively prevents recurrent networks from accumulating artifacts
temporally without depressing detailed features. Additionally, we propose a
first set of metrics to quantitatively evaluate the accuracy as well as the
perceptual quality of the temporal evolution. A series of user studies confirm
the rankings computed with these metrics. Code, data, models, and results are
provided at https://github.com/thunil/TecoGAN. The project page
https://ge.in.tum.de/publications/2019-tecogan-chu/ contains supplemental
materials.","a) A spatial GAN for image generation. b) A frame recurrent Generator. c) A spatio-temporal Discriminator. In these figures, letter a, b , and д, stand for the input domain, the output domain and the generated results respectively. G and D stand for the generator and the discriminator.",What is the role of the Motion Compensation block in the Frame-Recurrent Generator?
spiqa_326,1803.02750v3,"How does the RR optimization in the figure illustrating delta-based synchronization of a GSet with four replicas (A, B, C, and D) reduce redundant message exchanges, particularly between replicas C and D?",The RR optimization helps to reduce the number of messages that need to be exchanged between replicas.,1803.02750v3.pdf,"['1803.02750v3.pdf', '1803.01128v3.pdf', '1705.09296v2.pdf', '1809.04276v2.pdf', '1901.00056v2.pdf', '1805.07567v2.pdf', '1706.00633v4.pdf', '1706.04269v2.pdf', '1702.08694v3.pdf', '1612.02803v5.pdf', '1803.04572v2.pdf']","The figure shows that the RR optimization allows replica C to send a single message to replica D, which contains the updates from both replicas A and B. This is because replica C knows that replica D has already received the updates from replica B. Without the RR optimization, replica C would have to send two messages, one to replica D and one to replica B.",1803.02750v3-Figure5-1.png,Efficient Synchronization of State-based CRDTs,"To ensure high availability in large scale distributed systems, Conflict-free
Replicated Data Types (CRDTs) relax consistency by allowing immediate query and
update operations at the local replica, with no need for remote
synchronization. State-based CRDTs synchronize replicas by periodically sending
their full state to other replicas, which can become extremely costly as the
CRDT state grows. Delta-based CRDTs address this problem by producing small
incremental states (deltas) to be used in synchronization instead of the full
state. However, current synchronisation algorithms for Delta-based CRDTs induce
redundant wasteful delta propagation, performing worse than expected, and
surprisingly, no better than State-based. In this paper we: 1) identify two
sources of inefficiency in current synchronization algorithms for delta-based
CRDTs; 2) bring the concept of join decomposition to state-based CRDTs; 3)
exploit join decompositions to obtain optimal deltas and 4) improve the
efficiency of synchronization algorithms; and finally, 5) evaluate the improved
algorithms.","Delta-based synchronization of a GSet with 4 replicas A,B,C,D ∈ I. The overlined element represents the RR optimization.",What is the role of the RR optimization in the delta-based synchronization of a GSet?
spiqa_327,1708.03797v1,"In the figure labeled ""Overview of HDMF"" in the Hybrid Deep-Semantic Matrix Factorization paper, what role does the code layer play in compressing the output from the encoder and passing it to the decoder for reconstruction?",The code layer is responsible for generating a compressed representation of the input data. This compressed representation is then used by the decoder to reconstruct the original data.,1708.03797v1.pdf,"['1708.03797v1.pdf', '1811.08481v2.pdf', '1809.00458v1.pdf', '1805.02349v2.pdf', '1812.10735v2.pdf', '1704.08615v2.pdf', '1708.00160v2.pdf']","The figure shows that the code layer is located in the middle of the HDMF architecture, between the encoder and decoder. The encoder takes the input data and transforms it into a compressed representation, which is then fed into the code layer. The code layer further compresses this representation before passing it on to the decoder. The decoder then uses this compressed representation to reconstruct the original data.",1708.03797v1-Figure1-1.png,Hybrid Deep-Semantic Matrix Factorization for Tag-Aware Personalized Recommendation,"Matrix factorization has now become a dominant solution for personalized
recommendation on the Social Web. To alleviate the cold start problem, previous
approaches have incorporated various additional sources of information into
traditional matrix factorization models. These upgraded models, however,
achieve only ""marginal"" enhancements on the performance of personalized
recommendation. Therefore, inspired by the recent development of deep-semantic
modeling, we propose a hybrid deep-semantic matrix factorization (HDMF) model
to further improve the performance of tag-aware personalized recommendation by
integrating the techniques of deep-semantic modeling, hybrid learning, and
matrix factorization. Experimental results show that HDMF significantly
outperforms the state-of-the-art baselines in tag-aware personalized
recommendation, in terms of all evaluation metrics, e.g., its mean reciprocal
rank (resp., mean average precision) is 1.52 (resp., 1.66) times as high as
that of the best baseline.",Overview of HDMF,What is the role of the code layer in the HDMF architecture?
spiqa_328,1811.08257v1,"How does the filter, labeled ""fh * fw"" in the convolution operation figure, interact with the large orange input image to produce the smaller orange output image, and what is its role in feature extraction within the context of the FALCON method described in this paper?","The filter is used to extract features from the input image. It is a small matrix that is applied to each pixel in the image, and the result is a new pixel value.",1811.08257v1.pdf,"['1811.08257v1.pdf', '1704.08615v2.pdf', '1901.00056v2.pdf', '1804.04410v2.pdf', '1702.08694v3.pdf', '1811.09393v4.pdf', '1708.06832v3.pdf', '1710.06177v2.pdf', '1611.05742v3.pdf', '1705.08016v3.pdf', '1805.01216v3.pdf', '1812.10735v2.pdf', '1611.03780v2.pdf']","The figure shows the filter as a small blue box with the label ""fh * fw"". The filter is applied to the input image (the large orange box) and the result is a new image (the smaller orange box). The red lines show the path of the filter as it is applied to the image.",1811.08257v1-Figure2-1.png,FALCON: A Fourier Transform Based Approach for Fast and Secure Convolutional Neural Network Predictions,"Machine learning as a service has been widely deployed to utilize deep neural
network models to provide prediction services. However, this raises privacy
concerns since clients need to send sensitive information to servers. In this
paper, we focus on the scenario where clients want to classify private images
with a convolutional neural network model hosted in the server, while both
parties keep their data private. We present FALCON, a fast and secure approach
for CNN predictions based on Fourier Transform. Our solution enables linear
layers of a CNN model to be evaluated simply and efficiently with fully
homomorphic encryption. We also introduce the first efficient and
privacy-preserving protocol for softmax function, which is an indispensable
component in CNNs and has not yet been evaluated in previous works due to its
high complexity. We implemented the FALCON and evaluated the performance on
real-world CNN models. The experimental results show that FALCON outperforms
the best known works in both computation and communication cost.",Convolution operation.,What is the role of the filter in the convolution operation?
spiqa_329,1812.06589v2,"In the proposed method's pipeline illustrated in Figure 2, how does the frame discriminator help ensure the generated face is aligned with the input audio for better audio-visual synchronization?",The frame discriminator is used to detect whether the generated frame and audio are matched or not.,1812.06589v2.pdf,"['1812.06589v2.pdf', '1705.02798v6.pdf', '1701.03077v10.pdf', '1811.09393v4.pdf']",The figure shows that the frame discriminator takes the generated frame and audio as input and outputs a decision of whether they are matched or not. This information is used to train the talking face generator to produce frames that are more consistent with the input audio.,1812.06589v2-Figure2-1.png,Arbitrary Talking Face Generation via Attentional Audio-Visual Coherence Learning,"Talking face generation aims to synthesize a face video with precise lip
synchronization as well as a smooth transition of facial motion over the entire
video via the given speech clip and facial image. Most existing methods mainly
focus on either disentangling the information in a single image or learning
temporal information between frames. However, cross-modality coherence between
audio and video information has not been well addressed during synthesis. In
this paper, we propose a novel arbitrary talking face generation framework by
discovering the audio-visual coherence via the proposed Asymmetric Mutual
Information Estimator (AMIE). In addition, we propose a Dynamic Attention (DA)
block by selectively focusing the lip area of the input image during the
training stage, to further enhance lip synchronization. Experimental results on
benchmark LRW dataset and GRID dataset transcend the state-of-the-art methods
on prevalent metrics with robust high-resolution synthesizing on gender and
pose variations.",Figure 2: Pipeline of our proposed method.,What is the role of the frame discriminator in the proposed method?
spiqa_330,1611.04684v1,"In the KEHNN model architecture, as depicted in the figure, how do the knowledge gates interact with both the hidden state of the BiGRU (h_t) and the knowledge base (K) to selectively incorporate external knowledge into the hidden state?",The knowledge gates are responsible for selecting relevant information from the knowledge base and incorporating it into the model's hidden state.,1611.04684v1.pdf,"['1611.04684v1.pdf', '1710.05654v2.pdf', '1707.01922v5.pdf', '1703.04887v4.pdf', '1707.08608v3.pdf', '1906.06589v3.pdf', '1707.00524v2.pdf', '1804.04410v2.pdf', '1812.00108v4.pdf', '1705.07164v8.pdf', '1802.07459v2.pdf', '1811.08481v2.pdf']",The figure shows that the knowledge gates take as input the knowledge base (K) and the current hidden state (h_t) of the BiGRU. The output of the knowledge gates is then used to update the hidden state of the BiGRU. This suggests that the knowledge gates are used to selectively incorporate information from the knowledge base into the model's hidden state.,1611.04684v1-Figure1-1.png,Knowledge Enhanced Hybrid Neural Network for Text Matching,"Long text brings a big challenge to semantic matching due to their
complicated semantic and syntactic structures. To tackle the challenge, we
consider using prior knowledge to help identify useful information and filter
out noise to matching in long text. To this end, we propose a knowledge
enhanced hybrid neural network (KEHNN). The model fuses prior knowledge into
word representations by knowledge gates and establishes three matching channels
with words, sequential structures of sentences given by Gated Recurrent Units
(GRU), and knowledge enhanced representations. The three channels are processed
by a convolutional neural network to generate high level features for matching,
and the features are synthesized as a matching score by a multilayer
perceptron. The model extends the existing methods by conducting matching on
words, local structures of sentences, and global context of sentences.
Evaluation results from extensive experiments on public data sets for question
answering and conversation show that KEHNN can significantly outperform
the-state-of-the-art matching models and particularly improve the performance
on pairs with long text.",Architecture of KEHNN,What is the role of the knowledge gates in the KEHNN architecture?
spiqa_331,1704.07854v4,"How does the parameter network calculate the weighting function that is applied to the pre-computed deformations in the first stage of the algorithm, as illustrated in the figure of the paper?",The parameter network is used to infer a weighting function.,1704.07854v4.pdf,"['1704.07854v4.pdf', '1706.04269v2.pdf', '1707.06320v2.pdf', '1709.00139v4.pdf', '1802.07459v2.pdf', '1705.02798v6.pdf', '1709.08294v3.pdf', '1704.08615v2.pdf', '1705.09966v2.pdf', '1804.07707v2.pdf', '1611.03780v2.pdf']","The parameter network takes as input the chosen point and the initial surface, and outputs a weighting function. This weighting function is then used to weight the pre-computed deformations, which are then applied to the initial surface. The result is a deformed surface that is then fed into the deformation network.",1704.07854v4-Figure2-1.png,Generating Liquid Simulations with Deformation-aware Neural Networks,"We propose a novel approach for deformation-aware neural networks that learn
the weighting and synthesis of dense volumetric deformation fields. Our method
specifically targets the space-time representation of physical surfaces from
liquid simulations. Liquids exhibit highly complex, non-linear behavior under
changing simulation conditions such as different initial conditions. Our
algorithm captures these complex phenomena in two stages: a first neural
network computes a weighting function for a set of pre-computed deformations,
while a second network directly generates a deformation field for refining the
surface. Key for successful training runs in this setting is a suitable loss
function that encodes the effect of the deformations, and a robust calculation
of the corresponding gradients. To demonstrate the effectiveness of our
approach, we showcase our method with several complex examples of flowing
liquids with topology changes. Our representation makes it possible to rapidly
generate the desired implicit surfaces. We have implemented a mobile
application to demonstrate that real-time interactions with complex liquid
effects are possible with our approach.","This illustration gives an overview of our algorithm. It works in two stages, a weighting and refinement stage, each of which employs a neural network to infer a weighting function and a dense deformation field, respectively.",What is the role of the parameter network in the weighting and refinement stage?
spiqa_332,1802.07351v2,"In the Deformable Volume Network (Devon) architecture, as represented in the figure, how does the relation module (Rt) compute spatial correspondences between features from the first and second images before passing them to the decoding module (gt) for optical flow estimation?",The relation module (Rt) is responsible for capturing the spatial relationships between the features extracted from the first and second images.,1802.07351v2.pdf,"['1802.07351v2.pdf', '1708.00160v2.pdf', '1704.07121v2.pdf', '1811.06635v1.pdf', '1809.03550v3.pdf', '1708.01425v4.pdf', '1603.00286v5.pdf', '1705.07384v2.pdf', '1804.04786v3.pdf', '1704.00774v3.pdf', '1804.07931v2.pdf']",The figure shows that the relation module (Rt) takes as input the features extracted from the encoding module (f) for both the first and second images. The output of the relation module is then fed into the decoding module (gt). This suggests that the relation module is used to compute some kind of similarity or correspondence between the features of the two images.,1802.07351v2-Figure3-1.png,Devon: Deformable Volume Network for Learning Optical Flow,"State-of-the-art neural network models estimate large displacement optical
flow in multi-resolution and use warping to propagate the estimation between
two resolutions. Despite their impressive results, it is known that there are
two problems with the approach. First, the multi-resolution estimation of
optical flow fails in situations where small objects move fast. Second, warping
creates artifacts when occlusion or dis-occlusion happens. In this paper, we
propose a new neural network module, Deformable Cost Volume, which alleviates
the two problems. Based on this module, we designed the Deformable Volume
Network (Devon) which can estimate multi-scale optical flow in a single high
resolution. Experiments show Devon is more suitable in handling small objects
moving fast and achieves comparable results to the state-of-the-art methods in
public benchmarks.","Deformable Volume Network (Devon) with three stages. I denotes the first image, J denotes the second image, f denotes the encoding module (§4.1), Rt denotes the relation module (§4.2), gt denotes the decoding module (§4.3) and Ft denotes the estimated optical flow for stage t.",What is the role of the relation module (Rt) in the Deformable Volume Network (Devon)?
spiqa_333,1809.00263v5,"In Figure 4 of the SDVI framework, how do the residual connections (red arrows) between the outputs (yellow arrows) and inputs (green arrows) of the RBConvLSTM layers contribute to maintaining information flow and preventing issues such as vanishing gradients?",The residual connections add the output of the previous layer to the input of the next layer. This helps to improve the flow of information through the network and can help to prevent vanishing gradients.,1809.00263v5.pdf,"['1809.00263v5.pdf', '1705.09296v2.pdf', '1809.01989v2.pdf', '1704.05958v2.pdf', '1708.03797v1.pdf', '1708.00160v2.pdf', '1811.02553v4.pdf', '1708.05239v3.pdf', '1707.01922v5.pdf', '1805.04687v2.pdf']","The figure shows that the outputs of each layer (yellow arrows) are added to the inputs of the next layer (green arrows). This is indicated by the red arrows, which represent the residual connections.",1809.00263v5-Figure4-1.png,Stochastic Dynamics for Video Infilling,"In this paper, we introduce a stochastic dynamics video infilling (SDVI)
framework to generate frames between long intervals in a video. Our task
differs from video interpolation which aims to produce transitional frames for
a short interval between every two frames and increase the temporal resolution.
Our task, namely video infilling, however, aims to infill long intervals with
plausible frame sequences. Our framework models the infilling as a constrained
stochastic generation process and sequentially samples dynamics from the
inferred distribution. SDVI consists of two parts: (1) a bi-directional
constraint propagation module to guarantee the spatial-temporal coherence among
frames, (2) a stochastic sampling process to generate dynamics from the
inferred distributions. Experimental results show that SDVI can generate clear
frame sequences with varying contents. Moreover, motions in the generated
sequence are realistic and able to transfer smoothly from the given start frame
to the terminal frame. Our project site is
https://xharlie.github.io/projects/project_sites/SDVI/video_results.html","Figure 4: A two layers RBConvLSTM: The initial cell states of the first layer are assigned as Cstart and Cend. hS and hT are taken as inputs. Combined with the residuals (red arrows), each layer’s outputs (yellow arrows) would go through a convolution module and become the inputs (green arrows) to the next layer.",What is the role of the residual connections in the RBConvLSTM network?
spiqa_334,1803.03467v4,"""In the context of RippleNet, as illustrated in Figure 2, how do ripple sets propagate a user's preferences from their click history through the knowledge graph to predict the probability of clicking on a particular item?""",The ripple sets are used to propagate a user's preferences from his or her click history to his or her relevant entities.,1803.03467v4.pdf,"['1803.03467v4.pdf', '1710.01507v4.pdf', '1906.10843v1.pdf', '1802.07459v2.pdf', '1809.02731v3.pdf', '1707.01922v5.pdf', '1811.08481v2.pdf', '1906.06589v3.pdf', '1809.04276v2.pdf', '1811.06635v1.pdf', '1812.00108v4.pdf', '1805.06431v4.pdf', '1706.08146v3.pdf', '1709.02755v5.pdf']","The figure shows how the ripple sets are used to propagate a user's preferences. The user's click history is used to generate the first-order ripple set, which is then used to generate the second-order ripple set, and so on. Each ripple set contains entities that are relevant to the user's interests, and the relevance probabilities are used to weight the entities in each ripple set. The weighted average of the entities in each ripple set is then used to generate the user's embedding, which is used to predict the probability that the user will click on a particular item.",1803.03467v4-Figure2-1.png,RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems,"To address the sparsity and cold start problem of collaborative filtering,
researchers usually make use of side information, such as social networks or
item attributes, to improve recommendation performance. This paper considers
the knowledge graph as the source of side information. To address the
limitations of existing embedding-based and path-based methods for
knowledge-graph-aware recommendation, we propose Ripple Network, an end-to-end
framework that naturally incorporates the knowledge graph into recommender
systems. Similar to actual ripples propagating on the surface of water, Ripple
Network stimulates the propagation of user preferences over the set of
knowledge entities by automatically and iteratively extending a user's
potential interests along links in the knowledge graph. The multiple ""ripples""
activated by a user's historically clicked items are thus superposed to form
the preference distribution of the user with respect to a candidate item, which
could be used for predicting the final clicking probability. Through extensive
experiments on real-world datasets, we demonstrate that Ripple Network achieves
substantial gains in a variety of scenarios, including movie, book and news
recommendation, over several state-of-the-art baselines.","Figure 2: The overall framework of theRippleNet. It takes one user and one item as input, and outputs the predicted probability that the user will click the item. The KGs in the upper part illustrate the corresponding ripple sets activated by the user’s click history.",What is the role of the ripple sets in the RippleNet framework?
spiqa_335,1811.07073v3,"According to Figure 1 of the ""Semi-Supervised Semantic Image Segmentation with Self-Correcting Networks"" paper, how does the self-correction module improve the segmentations of the weak set by refining the outputs of the ancillary and primary models?",The self-correction module refines the segmentations generated by the ancillary and current primary model for the weak set.,1811.07073v3.pdf,"['1811.07073v3.pdf', '1809.02731v3.pdf', '1706.08146v3.pdf', '1701.06171v4.pdf', '1809.04276v2.pdf', '1803.02750v3.pdf', '1803.04572v2.pdf', '1612.02803v5.pdf', '1608.02784v2.pdf', '1701.03077v10.pdf']",The figure shows that the self-correction module takes as input the segmentations from the ancillary and primary models for the weak set and outputs refined soft labels. These refined labels are then used to train the primary model.,1811.07073v3-Figure1-1.png,Semi-Supervised Semantic Image Segmentation with Self-correcting Networks,"Building a large image dataset with high-quality object masks for semantic
segmentation is costly and time consuming. In this paper, we introduce a
principled semi-supervised framework that only uses a small set of fully
supervised images (having semantic segmentation labels and box labels) and a
set of images with only object bounding box labels (we call it the weak set).
Our framework trains the primary segmentation model with the aid of an
ancillary model that generates initial segmentation labels for the weak set and
a self-correction module that improves the generated labels during training
using the increasingly accurate primary model. We introduce two variants of the
self-correction module using either linear or convolutional functions.
Experiments on the PASCAL VOC 2012 and Cityscape datasets show that our models
trained with a small fully supervised set perform similar to, or better than,
models trained with a large fully supervised set while requiring ~7x less
annotation effort.","Figure 1: An overview of our segmentation framework consisting of three models: i) Primary segmentation model generates a semantic segmentation of objects given an image. This is the main model that is subject to the training and is used at test time. ii) Ancillary segmentation model outputs a segmentation given an image and bounding box. This model generates an initial segmentation for the weak set, which will aid training the primary model. iii) Self-correction module refines segmentations generated by the ancillary model and the current primary model for the weak set. The primary model is trained using the cross-entropy loss that matches its output to either ground-truth segmentation labels for the fully supervised examples or soft refined labels generated by the self-correction module for the weak set.",What is the role of the self-correction module in the segmentation framework?
spiqa_336,1705.08016v3,"What is the role of the shared weights in ensuring that the two branches of the Siamese-like architecture depicted in Fig. 1 learn similar feature representations of the input, and how does this similarity enhance the effectiveness of the Pairwise Confusion (PC) method's Euclidean Confusion loss?","The shared weights allow the two branches of the network to learn similar representations of the input images. This helps to improve the performance of the Euclidean Confusion loss, which measures the distance between the conditional probability distributions of the two branches.",1705.08016v3.pdf,"['1705.08016v3.pdf', '1611.03780v2.pdf', '1805.07567v2.pdf', '1812.00108v4.pdf', '1703.10730v2.pdf', '1708.05239v3.pdf']"," The figure shows that the two branches of the network share the same weights, which means that they are learning the same features from the input images. This is important for the Euclidean Confusion loss because it needs to compare the representations of the two images in order to determine how similar they are. If the two branches of the network were not learning similar representations, then the Euclidean Confusion loss would be less effective.",1705.08016v3-Figure1-1.png,Pairwise Confusion for Fine-Grained Visual Classification,"Fine-Grained Visual Classification (FGVC) datasets contain small sample
sizes, along with significant intra-class variation and inter-class similarity.
While prior work has addressed intra-class variation using localization and
segmentation techniques, inter-class similarity may also affect feature
learning and reduce classification performance. In this work, we address this
problem using a novel optimization procedure for the end-to-end neural network
training on FGVC tasks. Our procedure, called Pairwise Confusion (PC) reduces
overfitting by intentionally {introducing confusion} in the activations. With
PC regularization, we obtain state-of-the-art performance on six of the most
widely-used FGVC datasets and demonstrate improved localization ability. {PC}
is easy to implement, does not need excessive hyperparameter tuning during
training, and does not add significant overhead during test time.","Fig. 1. CNN training pipeline for Pairwise Confusion (PC). We employ a Siamese-like architecture, with individual cross entropy calculations for each branch, followed by a joint energy-distance minimization loss. We split each incoming batch of samples into two mini-batches, and feed the network pairwise samples.",What is the role of the shared weights in the Siamese-like architecture shown in the first figure?
spiqa_337,1707.01922v5,"How does ZDDA leverage the task-irrelevant gray-RGB pairs from the Fashion-MNIST dataset, as shown in Fig. 1, to simulate target-domain (RGB) representations that enable digit classification in the RGB domain without access to task-relevant data?",The task-irrelevant data is used to simulate the RGB representation using the gray scale image. This allows ZDDA to learn a joint network that can be used to classify digits in both the gray scale and RGB domains.,1707.01922v5.pdf,"['1707.01922v5.pdf', '1803.04572v2.pdf', '1809.03449v3.pdf', '1705.09966v2.pdf', '1811.07073v3.pdf', '1710.06177v2.pdf', '1809.00458v1.pdf', '1803.06506v3.pdf']","The figure shows that the task-irrelevant data consists of gray-scale and RGB image pairs. These pairs are used to train a network that can convert gray-scale images to RGB images. This network is then used to classify digits in the RGB domain, even though no RGB training data is available.",1707.01922v5-Figure1-1.png,Zero-Shot Deep Domain Adaptation,"Domain adaptation is an important tool to transfer knowledge about a task
(e.g. classification) learned in a source domain to a second, or target domain.
Current approaches assume that task-relevant target-domain data is available
during training. We demonstrate how to perform domain adaptation when no such
task-relevant target-domain data is available. To tackle this issue, we propose
zero-shot deep domain adaptation (ZDDA), which uses privileged information from
task-irrelevant dual-domain pairs. ZDDA learns a source-domain representation
which is not only tailored for the task of interest but also close to the
target-domain representation. Therefore, the source-domain task of interest
solution (e.g. a classifier for classification tasks) which is jointly trained
with the source-domain representation can be applicable to both the source and
target representations. Using the MNIST, Fashion-MNIST, NIST, EMNIST, and SUN
RGB-D datasets, we show that ZDDA can perform domain adaptation in
classification tasks without access to task-relevant target-domain training
data. We also extend ZDDA to perform sensor fusion in the SUN RGB-D scene
classification task by simulating task-relevant target-domain representations
with task-relevant source-domain data. To the best of our knowledge, ZDDA is
the first domain adaptation and sensor fusion method which requires no
task-relevant target-domain data. The underlying principle is not particular to
computer vision data, but should be extensible to other domains.","Fig. 1. We propose zero-shot deep domain adaptation (ZDDA) for domain adaptation and sensor fusion. ZDDA learns from the task-irrelevant dual-domain pairs when the task-relevant target-domain training data is unavailable. In this example domain adaptation task (MNIST [27]→MNIST-M [13]), the task-irrelevant gray-RGB pairs are from the Fashion-MNIST [46] dataset and the Fashion-MNIST-M dataset (the colored version of the Fashion-MNIST [46] dataset with the details in Sec. 4.1)",What is the role of the task-irrelevant data in ZDDA?
spiqa_338,1608.02784v2,"In the ""Canonical Correlation Inference for Mapping Abstract Scenes to Text"" paper, as depicted in the CCA decoding algorithm figure, how does the temperature parameter t regulate the acceptance probability of suboptimal candidate solutions during the simulated annealing process for generating textual descriptions of abstract scenes?","The temperature variable t controls the probability of accepting a new candidate solution y. As t decreases, the probability of accepting a worse solution decreases.",1608.02784v2.pdf,"['1608.02784v2.pdf', '1704.07121v2.pdf', '1803.03467v4.pdf', '1710.01507v4.pdf', '1705.10667v4.pdf', '1809.01246v1.pdf', '1704.07854v4.pdf', '1706.04269v2.pdf', '1612.02803v5.pdf', '1707.01917v2.pdf', '1805.06431v4.pdf']","The algorithm uses a simulated annealing approach, where the temperature variable t gradually decreases. The acceptance probability of a new solution y is determined by comparing the similarity scores of y and the current best solution y*. If the score of y is higher, it is always accepted. Otherwise, it is accepted with a probability that depends on the difference in scores and the current temperature t. As t decreases, the probability of accepting a worse solution decreases, ensuring that the algorithm converges to a good solution.",1608.02784v2-Figure3-1.png,Canonical Correlation Inference for Mapping Abstract Scenes to Text,"We describe a technique for structured prediction, based on canonical
correlation analysis. Our learning algorithm finds two projections for the
input and the output spaces that aim at projecting a given input and its
correct output into points close to each other. We demonstrate our technique on
a language-vision problem, namely the problem of giving a textual description
to an ""abstract scene"".",The CCA decoding algorithm.,What is the role of the temperature variable t in the CCA decoding algorithm?
spiqa_339,1603.03833v4,"According to Figure 1, what role does the virtual environment play in facilitating the collection of task demonstrations and training the neural network controller for manipulation tasks?",The virtual environment is used to collect demonstrations of the task from the user. This allows for safe and efficient data collection.,1603.03833v4.pdf,"['1603.03833v4.pdf', '1811.07073v3.pdf', '1707.01922v5.pdf', '1811.10673v1.pdf', '1706.04269v2.pdf', '1803.05776v2.pdf', '1702.08694v3.pdf', '1803.04572v2.pdf', '1809.04276v2.pdf', '1706.00633v4.pdf', '1811.02553v4.pdf', '1704.08615v2.pdf', '1805.06431v4.pdf', '1803.06506v3.pdf']","The left side of Figure 1 shows the virtual environment, which includes a simulation of the task and a user interface (Xbox controller) for providing demonstrations. The figure also shows that the collected trajectories are used to train the neural network controller.",1603.03833v4-Figure1-1.png,From virtual demonstration to real-world manipulation using LSTM and MDN,"Robots assisting the disabled or elderly must perform complex manipulation
tasks and must adapt to the home environment and preferences of their user.
Learning from demonstration is a promising choice, that would allow the
non-technical user to teach the robot different tasks. However, collecting
demonstrations in the home environment of a disabled user is time consuming,
disruptive to the comfort of the user, and presents safety challenges. It would
be desirable to perform the demonstrations in a virtual environment. In this
paper we describe a solution to the challenging problem of behavior transfer
from virtual demonstration to a physical robot. The virtual demonstrations are
used to train a deep neural network based controller, which is using a Long
Short Term Memory (LSTM) recurrent neural network to generate trajectories. The
training process uses a Mixture Density Network (MDN) to calculate an error
signal suitable for the multimodal nature of demonstrations. The controller
learned in the virtual environment is transferred to a physical robot (a
Rethink Robotics Baxter). An off-the-shelf vision component is used to
substitute for geometric knowledge available in the simulation and an inverse
kinematics module is used to allow the Baxter to enact the trajectory. Our
experimental studies validate the three contributions of the paper: (1) the
controller learned from virtual demonstrations can be used to successfully
perform the manipulation tasks on a physical robot, (2) the LSTM+MDN
architectural choice outperforms other choices, such as the use of feedforward
networks and mean-squared error based training signals and (3) allowing
imperfect demonstrations in the training set also allows the controller to
learn how to correct its manipulation mistakes.",Figure 1: The general flow of our approach. The demonstrations of the ADL manipulation tasks are collected in a virtual environment. The collected trajectories are used to train the neural network controller.,What is the role of the virtual environment in the proposed approach?
spiqa_340,1811.06635v1,"According to the figure depicting sample complexity for structured sparsity models in the paper, what is the lower bound for recovering a tree-structured sparse signal using standard compressed sensing, where s represents the signal sparsity?",Ω(s),1811.06635v1.pdf,"['1811.06635v1.pdf', '1805.00912v4.pdf', '1611.02654v2.pdf', '1611.03780v2.pdf', '1706.00633v4.pdf', '1705.02798v6.pdf', '1811.02721v3.pdf', '1906.06589v3.pdf', '1704.05958v2.pdf', '1805.08751v2.pdf', '1811.09393v4.pdf', '1804.05938v2.pdf', '1706.00827v2.pdf']","The table in the figure shows the sample complexity lower and upper bounds for different sparsity structures and compressed sensing methods. For standard compressed sensing and a tree-structured sparsity model, the lower bound is Ω(s), where s is the signal sparsity.",1811.06635v1-Table1-1.png,Information Theoretic Limits for Standard and One-Bit Compressed Sensing with Graph-Structured Sparsity,"In this paper, we analyze the information theoretic lower bound on the
necessary number of samples needed for recovering a sparse signal under
different compressed sensing settings. We focus on the weighted graph model, a
model-based framework proposed by Hegde et al. (2015), for standard compressed
sensing as well as for one-bit compressed sensing. We study both the noisy and
noiseless regimes. Our analysis is general in the sense that it applies to any
algorithm used to recover the signal. We carefully construct restricted
ensembles for different settings and then apply Fano's inequality to establish
the lower bound on the necessary number of samples. Furthermore, we show that
our bound is tight for one-bit compressed sensing, while for standard
compressed sensing, our bound is tight up to a logarithmic factor of the number
of non-zero entries in the signal.","Sample Complexity Results for Structured Sparsity Models (d is the dimension of the true signal, s is the signal sparsity, i.e., the number of non-zero entries, g is the number of connected components, ρ(G) is the maximum weight degree of graph G, B is the weight budget in the weighted graph model, K is the block sparsity, J is the number of entries in a block and N is the total number of blocks in the block structured sparsity model – detailed explanation of notations are provided in Sections 3 and 5)",What is the sample complexity lower bound for recovering a tree-structured sparse signal using standard compressed sensing?
spiqa_341,1707.01917v2,"Based on the figure displaying the dimensions of tensors used in the TFBA experiments, what are the exact dimensions of the tensor $x^1$ for the Shootings dataset?",The shape of the tensor $x^1$ for the Shootings dataset is 3365 x 1295 x 50.,1707.01917v2.pdf,"['1707.01917v2.pdf', '1804.07931v2.pdf', '1804.00863v3.pdf', '1906.10843v1.pdf', '1603.03833v4.pdf', '1708.03797v1.pdf', '1710.06177v2.pdf', '1705.10667v4.pdf', '1705.07384v2.pdf', '1703.02507v3.pdf']","The table in the image shows the dimensions of the tensors constructed for each dataset used in the experiments. The first column of the table lists the datasets, and the second column lists the shapes of the corresponding $x^1$ tensors. The shape of the tensor $x^1$ for the Shootings dataset is given in the second row of the table as 3365 x 1295 x 50.",1707.01917v2-Table2-1.png,Higher-order Relation Schema Induction using Tensor Factorization with Back-off and Aggregation,"Relation Schema Induction (RSI) is the problem of identifying type signatures
of arguments of relations from unlabeled text. Most of the previous work in
this area have focused only on binary RSI, i.e., inducing only the subject and
object type signatures per relation. However, in practice, many relations are
high-order, i.e., they have more than two arguments and inducing type
signatures of all arguments is necessary. For example, in the sports domain,
inducing a schema win(WinningPlayer, OpponentPlayer, Tournament, Location) is
more informative than inducing just win(WinningPlayer, OpponentPlayer). We
refer to this problem as Higher-order Relation Schema Induction (HRSI). In this
paper, we propose Tensor Factorization with Back-off and Aggregation (TFBA), a
novel framework for the HRSI problem. To the best of our knowledge, this is the
first attempt at inducing higher-order relation schemata from unlabeled text.
Using the experimental analysis on three real world datasets, we show how TFBA
helps in dealing with sparsity and induce higher order schemata.",Details of dimensions of tensors constructed for each dataset used in the experiments.,What is the shape of the tensor $x^1$ for the Shootings dataset?
spiqa_342,1708.00160v2,"Referring to the FP-Tree illustration in the paper on enhancing neural coreference resolvers with linguistic features, what is the support value assigned to the node labeled ""ana=NAM""?",2,1708.00160v2.pdf,"['1708.00160v2.pdf', '1705.02946v3.pdf', '1708.01425v4.pdf', '1703.00060v2.pdf', '1701.06171v4.pdf', '1709.02755v5.pdf', '1606.07384v2.pdf']","The support value of a node is the number of transactions that contain the itemset represented by the node. In this case, the node ""ana=NAM"" has a support value of 2, which means that there are 2 transactions that contain the itemset ""ana=NAM"".",1708.00160v2-Figure2-1.png,Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers,"Coreference resolution is an intermediate step for text understanding. It is
used in tasks and domains for which we do not necessarily have coreference
annotated corpora. Therefore, generalization is of special importance for
coreference resolution. However, while recent coreference resolvers have
notable improvements on the CoNLL dataset, they struggle to generalize properly
to new domains or datasets. In this paper, we investigate the role of
linguistic features in building more generalizable coreference resolvers. We
show that generalization improves only slightly by merely using a set of
additional linguistic features. However, employing features and subsets of
their values that are informative for coreference resolution, considerably
improves generalization. Thanks to better generalization, our system achieves
state-of-the-art results in out-of-domain evaluations, e.g., on WikiCoref, our
system, which is trained on CoNLL, achieves on-par performance with a system
designed for this dataset.",FP-Tree with corresponding support values of the nodes.,"What is the support value of the node ""ana=NAM""?"
spiqa_343,1901.00398v2,"What specific decision are the Amazon Mechanical Turk workers instructed to make concerning the authenticity of the twenty one product review paragraphs, according to the instructions shown in Figure 8 of the paper?",The AMT workers are being asked to decide whether each of twenty one paragraphs extracted from product reviews is real (written by a person) or fake (written by a computer algorithm).,1901.00398v2.pdf,"['1901.00398v2.pdf', '1804.04786v3.pdf', '1709.02418v2.pdf', '1804.01429v3.pdf', '1707.06320v2.pdf', '1811.02721v3.pdf', '1803.05776v2.pdf', '1811.06635v1.pdf']","The instructions in Figure \ref{user_instructions} state that the workers will be presented with twenty one paragraphs extracted from product reviews, and that they should try to decide whether each review is real or fake.",1901.00398v2-Figure8-1.png,Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation,"We conduct a large-scale, systematic study to evaluate the existing
evaluation methods for natural language generation in the context of generating
online product reviews. We compare human-based evaluators with a variety of
automated evaluation procedures, including discriminative evaluators that
measure how well machine-generated text can be distinguished from human-written
text, as well as word overlap metrics that assess how similar the generated
text compares to human-written references. We determine to what extent these
different evaluators agree on the ranking of a dozen of state-of-the-art
generators for online product reviews. We find that human evaluators do not
correlate well with discriminative evaluators, leaving a bigger question of
whether adversarial accuracy is the correct objective for natural language
generation. In general, distinguishing machine-generated text is challenging
even for human evaluators, and human decisions correlate better with lexical
overlaps. We find lexical diversity an intriguing metric that is indicative of
the assessments of different evaluators. A post-experiment survey of
participants provides insights into how to evaluate and improve the quality of
natural language generation systems.",Figure 8: Screenshot of the instructions presented to Amazon Mechanical Turk workers.,What is the task that the AMT workers are being asked to do?
spiqa_344,1804.05995v2,"Based on the figure illustrating precision and recall as functions of the number of recommended sections \( k \) for all evaluated methods, how do precision and recall trends change as \( k \) increases, and what specific patterns can be observed in the subplot (a) for both metrics?",Precision generally decreases and recall generally increases as k increases.,1804.05995v2.pdf,"['1804.05995v2.pdf', '1805.06447v3.pdf', '1707.00524v2.pdf', '1706.04269v2.pdf', '1804.07849v4.pdf', '1703.10730v2.pdf', '1709.08294v3.pdf', '1705.08016v3.pdf', '1809.02731v3.pdf', '1804.07707v2.pdf', '1709.02418v2.pdf', '1705.07164v8.pdf', '1611.05742v3.pdf', '1809.00263v5.pdf']","This can be seen in all four subplots of the figure. For example, in (a), the blue line representing precision starts at a high value for small k and then decreases as k increases, while the red line representing recall starts at a low value for small k and then increases as k increases.",1804.05995v2-Figure5-1.png,Structuring Wikipedia Articles with Section Recommendations,"Sections are the building blocks of Wikipedia articles. They enhance
readability and can be used as a structured entry point for creating and
expanding articles. Structuring a new or already existing Wikipedia article
with sections is a hard task for humans, especially for newcomers or less
experienced editors, as it requires significant knowledge about how a
well-written article looks for each possible topic. Inspired by this need, the
present paper defines the problem of section recommendation for Wikipedia
articles and proposes several approaches for tackling it. Our systems can help
editors by recommending what sections to add to already existing or newly
created Wikipedia articles. Our basic paradigm is to generate recommendations
by sourcing sections from articles that are similar to the input article. We
explore several ways of defining similarity for this purpose (based on topic
modeling, collaborative filtering, and Wikipedia's category system). We use
both automatic and human evaluation approaches for assessing the performance of
our recommendation system, concluding that the category-based approach works
best, achieving precision@10 of about 80% in the human evaluation.","Precision and recall as a function of the number of recommended sections k , for all methods we evaluate.",What is the trend in precision and recall as the number of recommended sections k increases?
spiqa_345,1705.02946v3,"What is the upper bound on query complexity for finding an ε-perfect allocation with minimum cuts for n ≥ 3 players, as shown in the figure summarizing the paper's results?",O(n^3 / ε),1705.02946v3.pdf,"['1705.02946v3.pdf', '1706.08146v3.pdf', '1811.09393v4.pdf', '1804.04786v3.pdf', '1809.03149v2.pdf', '1809.01246v1.pdf', '1802.07222v1.pdf', '1708.00160v2.pdf', '1611.03780v2.pdf', '1811.08481v2.pdf', '1603.00286v5.pdf']","The upper bound for ε-perfect allocations with minimum cuts is shown in the third row of the table. For n ≥ 3 players, the upper bound is O(n^3 / ε).",1705.02946v3-Table1-1.png,The Query Complexity of Cake Cutting,"We study the query complexity of cake cutting and give lower and upper bounds
for computing approximately envy-free, perfect, and equitable allocations with
the minimum number of cuts. The lower bounds are tight for computing connected
envy-free allocations among n=3 players and for computing perfect and equitable
allocations with minimum number of cuts between n=2 players.
  We also formalize moving knife procedures and show that a large subclass of
this family, which captures all the known moving knife procedures, can be
simulated efficiently with arbitrarily small error in the Robertson-Webb query
model.","Query complexity in cake cutting in the standard query model. Our results are marked with (∗). The lower bounds for finding ε-perfect and ε-equitable allocations for n ≥ 3 players hold for any number of cuts [PW17]. The bounds for exact envy-free and proportional allocations hold for any number of cuts, except the upper bound for proportional works for connected pieces.",What is the upper bound on the query complexity for finding an ε-perfect allocation with minimum cuts for 3 or more players?
spiqa_346,1809.00263v5,"What is the value of the learning rate α for the BAIR dataset as shown in the figure titled ""Hyperparameters for training on different datasets"" in the ""Stochastic Dynamics for Video Infilling"" paper?",0.0002,1809.00263v5.pdf,"['1809.00263v5.pdf', '1705.09882v2.pdf', '1805.04687v2.pdf', '1809.01246v1.pdf', '1703.07015v3.pdf', '1705.08016v3.pdf', '1803.05776v2.pdf', '1804.05936v2.pdf', '1804.05995v2.pdf', '1809.03550v3.pdf', '1802.07351v2.pdf', '1703.10730v2.pdf']",The table shows the values of the hyperparameters for training on different datasets. The value of α for the BAIR dataset is listed as 0.0002.,1809.00263v5-Table3-1.png,Stochastic Dynamics for Video Infilling,"In this paper, we introduce a stochastic dynamics video infilling (SDVI)
framework to generate frames between long intervals in a video. Our task
differs from video interpolation which aims to produce transitional frames for
a short interval between every two frames and increase the temporal resolution.
Our task, namely video infilling, however, aims to infill long intervals with
plausible frame sequences. Our framework models the infilling as a constrained
stochastic generation process and sequentially samples dynamics from the
inferred distribution. SDVI consists of two parts: (1) a bi-directional
constraint propagation module to guarantee the spatial-temporal coherence among
frames, (2) a stochastic sampling process to generate dynamics from the
inferred distributions. Experimental results show that SDVI can generate clear
frame sequences with varying contents. Moreover, motions in the generated
sequence are realistic and able to transfer smoothly from the given start frame
to the terminal frame. Our project site is
https://xharlie.github.io/projects/project_sites/SDVI/video_results.html",Hyper parameters for training on different datasets,What is the value of the learning rate α for the BAIR dataset?
spiqa_347,1805.04687v2,"Based on the right-side figure illustrating the CDF of occlusion durations in the BDD100K dataset, what percentage of occlusions last for more than 10 frames?",Approximately 80%,1805.04687v2.pdf,"['1805.04687v2.pdf', '1705.08016v3.pdf', '1704.08615v2.pdf', '1812.00281v3.pdf', '1804.05936v2.pdf']","The figure on the right shows the cumulative distribution function (CDF) of occlusion durations. The CDF shows the percentage of occlusions that last for a certain number of frames or less. The x-axis shows the number of occluded frames, and the y-axis shows the percentage of occlusions. The CDF reaches 80% at approximately 10 frames, which means that 80% of occlusions last for more than 10 frames.",1805.04687v2-Figure8-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.",Number of occlusions by track (left) and number of occluded frames for each occlusion (right). Our dataset covers complicated occlusion and reappearing patterns.,What percentage of occlusions last for more than 10 frames?
spiqa_348,1811.02721v3,"Based on the figure displaying radio duty cycles in a lossy environment impacted by human activity in the paper ""Performant TCP for Low-Power Wireless Networks"", which protocol—TCP or CoAP—exhibits a higher radio duty cycle during the first 7 hours?",TCP,1811.02721v3.pdf,"['1811.02721v3.pdf', '1812.00281v3.pdf', '1707.08608v3.pdf', '1706.00827v2.pdf', '1702.08694v3.pdf', '1707.01922v5.pdf', '1805.01216v3.pdf', '1705.09296v2.pdf', '1710.05654v2.pdf', '1705.09882v2.pdf', '1703.10730v2.pdf', '1705.02798v6.pdf']","The green line, which represents TCP, is higher than the blue line, which represents CoAP, for the first 7 hours of the trial.",1811.02721v3-Figure11-1.png,Performant TCP for Low-Power Wireless Networks,"Low-power and lossy networks (LLNs) enable diverse applications integrating
many resource-constrained embedded devices, often requiring interconnectivity
with existing TCP/IP networks as part of the Internet of Things. But TCP has
received little attention in LLNs due to concerns about its overhead and
performance, leading to LLN-specific protocols that require specialized
gateways for interoperability. We present a systematic study of a well-designed
TCP stack in IEEE 802.15.4-based LLNs, based on the TCP protocol logic in
FreeBSD. Through careful implementation and extensive experiments, we show that
modern low-power sensor platforms are capable of running full-scale TCP and
that TCP, counter to common belief, performs well despite the lossy nature of
LLNs. By carefully studying the interaction between the transport and link
layers, we identify subtle but important modifications to both, achieving TCP
goodput within 25% of an upper bound (5-40x higher than prior results) and
low-power operation commensurate to CoAP in a practical LLN application
scenario. This suggests that a TCP-based transport layer, seamlessly
interoperable with existing TCP/IP networks, is viable and performant in LLNs.","Radio duty cycle of TCP and CoAP in a lossy wireless environment, in one representative trial (losses are caused by natural human activity)",What protocol has a higher radio duty cycle in the first 7 hours of the trial?
spiqa_349,1705.09966v2,"In the identity-preserving face superresolution results shown in Figure 9, how does the low-resolution input influence the head pose and facial expression of the generated high-resolution face when the identity image presents differing attributes?",The low-resolution input provides an overall shape constraint for the generated high-resolution image. The head pose and facial expression of the generated high-res images adopt those in the low-res inputs.,1705.09966v2.pdf,"['1705.09966v2.pdf', '1606.07384v2.pdf', '1804.01429v3.pdf', '1812.00108v4.pdf', '1804.07849v4.pdf', '1811.10673v1.pdf', '1707.01917v2.pdf', '1706.00633v4.pdf', '1811.08481v2.pdf', '1710.06177v2.pdf', '1710.05654v2.pdf']","The figure shows that the generated high-resolution images (c) have the same head pose and facial expression as the low-resolution inputs (a), even when the target identity images (b) have different poses and expressions. For example, in the example inside the blue box in the last row of Figure 2, the target identity image (b) shows a man smiling, while the low-resolution input (a) shows another man with a closed mouth. The generated high-resolution image (c) preserves the identity of the man in (b) while the pose of the head and the mouth follow the input.",1705.09966v2-Figure9-1.png,Attribute-Guided Face Generation Using Conditional CycleGAN,"We are interested in attribute-guided face generation: given a low-res face
input image, an attribute vector that can be extracted from a high-res image
(attribute image), our new method generates a high-res face image for the
low-res input that satisfies the given attributes. To address this problem, we
condition the CycleGAN and propose conditional CycleGAN, which is designed to
1) handle unpaired training data because the training low/high-res and high-res
attribute images may not necessarily align with each other, and to 2) allow
easy control of the appearance of the generated face via the input attributes.
We demonstrate impressive results on the attribute-guided conditional CycleGAN,
which can synthesize realistic face images with appearance easily controlled by
user-supplied attributes (e.g., gender, makeup, hair color, eyeglasses). Using
the attribute image as identity to produce the corresponding conditional vector
and by incorporating a face verification network, the attribute-guided network
becomes the identity-guided conditional CycleGAN which produces impressive and
interesting results on identity transfer. We demonstrate three applications on
identity-guided conditional CycleGAN: identity-preserving face superresolution,
face swapping, and frontal face generation, which consistently show the
advantage of our new method.","Fig. 9. Identity-guided face generation results on low-res input and high-res identity of the same person, i.e., identity-preserving face superresolution. (a) low-res inputs; (b) input identity of the same person; (c) our high-res face outputs (red boxes) from (a); (d) the high-res ground truth of (a).",What role does the low-resolution input play in the identity-guided face generation process?
spiqa_350,1906.10843v1,"According to Figure 6, which ATE estimator exhibits the largest increase in variance when noisy confounders are present in the user sentiment study for fully randomized A/B tests?",Outcome Regression (OR),1906.10843v1.pdf,"['1906.10843v1.pdf', '1608.02784v2.pdf', '1804.00863v3.pdf', '1705.09882v2.pdf', '1705.02798v6.pdf', '1805.02349v2.pdf']",The boxplot for OR shows the largest increase in variance compared to the other estimators. This indicates that OR is more sensitive to the presence of noisy confounders.,1906.10843v1-Figure6-1.png,User Sentiment as a Success Metric: Persistent Biases Under Full Randomization,"We study user sentiment (reported via optional surveys) as a metric for fully
randomized A/B tests. Both user-level covariates and treatment assignment can
impact response propensity. We propose a set of consistent estimators for the
average and local treatment effects on treated and respondent users. We show
that our problem can be mapped onto the intersection of the missing data
problem and observational causal inference, and we identify conditions under
which consistent estimators exist. We evaluate the performance of estimators
via simulation studies and find that more complicated models do not necessarily
provide superior performance.","Figure 6: Performance of different ATE estimators when noisy confounders are observed. Increase in variances of OR and DR, AB retains performance characteristics.",Which ATE estimator is most affected by the presence of noisy confounders?
spiqa_351,1811.09393v4,"In the figure comparing the video mappings between Trump and Obama, which GAN model demonstrates the most spatially detailed and temporally consistent blinking motions, outperforming RecycleGAN and STC-V2V?",TecoGAN,1811.09393v4.pdf,"['1811.09393v4.pdf', '1603.00286v5.pdf', '1804.01429v3.pdf', '1706.04269v2.pdf', '1803.02750v3.pdf', '1706.08146v3.pdf', '1809.01246v1.pdf', '1803.05776v2.pdf', '1803.04383v2.pdf', '1811.08481v2.pdf', '1705.07384v2.pdf', '1706.04284v3.pdf', '1704.04539v2.pdf', '1805.04687v2.pdf']","The caption states that our model, RecycleGAN, and STC-V2V are all able to generate correct blinking motions, but that our model outperforms the latter two in terms of coherent detail that is generated. This can be seen in the figure, where the blinking motions generated by our model are more realistic and less jerky than those generated by the other models.",1811.09393v4-Figure8-1.png,Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation,"Our work explores temporal self-supervision for GAN-based video generation
tasks. While adversarial training successfully yields generative models for a
variety of areas, temporal relationships in the generated data are much less
explored. Natural temporal changes are crucial for sequential generation tasks,
e.g. video super-resolution and unpaired video translation. For the former,
state-of-the-art methods often favor simpler norm losses such as $L^2$ over
adversarial training. However, their averaging nature easily leads to
temporally smooth results with an undesirable lack of spatial detail. For
unpaired video translation, existing approaches modify the generator networks
to form spatio-temporal cycle consistencies. In contrast, we focus on improving
learning objectives and propose a temporally self-supervised algorithm. For
both tasks, we show that temporal adversarial learning is key to achieving
temporally coherent solutions without sacrificing spatial detail. We also
propose a novel Ping-Pong loss to improve the long-term temporal consistency.
It effectively prevents recurrent networks from accumulating artifacts
temporally without depressing detailed features. Additionally, we propose a
first set of metrics to quantitatively evaluate the accuracy as well as the
perceptual quality of the temporal evolution. A series of user studies confirm
the rankings computed with these metrics. Code, data, models, and results are
provided at https://github.com/thunil/TecoGAN. The project page
https://ge.in.tum.de/publications/2019-tecogan-chu/ contains supplemental
materials.","When learning a mapping between Trump and Obama, the CycleGAN model gives good spatial features but collapses to essentially static outputs of Obama. It manages to transfer facial expressions back to Trump using tiny differences encoded in its Obama outputs, without understanding the cycle-consistency between the two domains. Being able to establish the correct temporal cycle-consistency between domains, ours, RecycleGAN and STC-V2V can generate correct blinking motions, shown in Sec. 4.7 of the supplemental web-page. Our model outperforms the latter two in terms of coherent detail that is generated. Obama and Trump video courtesy of the White House (public domain).",Which GAN model is able to generate the most realistic blinking motions?
spiqa_352,1708.05239v3,"Based on the figure comparing HMC, PE-HMC (N=2), and PE-HMC (N=5) that displays marginal posterior densities across different variables, which sampler demonstrates the most consistent and uniform exploration of the posterior distribution?",PE-HMC (N=5),1708.05239v3.pdf,"['1708.05239v3.pdf', '1906.10843v1.pdf', '1804.01429v3.pdf', '1804.04786v3.pdf', '1811.06635v1.pdf', '1805.02349v2.pdf', '1803.05776v2.pdf', '1706.00633v4.pdf', '1706.04269v2.pdf', '1811.02721v3.pdf', '1803.01128v3.pdf']","The marginal posterior densities for PE-HMC (N=5) are all relatively similar in shape and spread, suggesting that this sampler is able to effectively explore the posterior distribution for all of the variables. In contrast, the other two samplers show more variability in their performance across the different variables.",1708.05239v3-Figure9-1.png,Pseudo-extended Markov chain Monte Carlo,"Sampling from posterior distributions using Markov chain Monte Carlo (MCMC)
methods can require an exhaustive number of iterations, particularly when the
posterior is multi-modal as the MCMC sampler can become trapped in a local mode
for a large number of iterations. In this paper, we introduce the
pseudo-extended MCMC method as a simple approach for improving the mixing of
the MCMC sampler for multi-modal posterior distributions. The pseudo-extended
method augments the state-space of the posterior using pseudo-samples as
auxiliary variables. On the extended space, the modes of the posterior are
connected, which allows the MCMC sampler to easily move between well-separated
posterior modes. We demonstrate that the pseudo-extended approach delivers
improved MCMC sampling over the Hamiltonian Monte Carlo algorithm on
multi-modal posteriors, including Boltzmann machines and models with
sparsity-inducing priors.","Plots of marginal posterior densities for a random subsample of variables. Each column represents a different variable and each row is a different MCMC sampler, HMC, PE-HMC (N=2) and PE-HMC (N=5), respectively",Which MCMC sampler appears to have the most consistent performance across the different variables?
spiqa_353,1811.02721v3,"Referring to the detailed comparison in Table 9 of the paper ""Performant TCP for Low-Power Wireless Networks,"" which TCP stack demonstrates the most complete implementation of essential features such as flow control, congestion control, RTT estimation, and selective ACKs, and which stack lacks the majority of these critical functionalities, revealing significant gaps in its design?","The TCP stack presented in this paper (TCPlp) provides the most complete implementation of core TCP features, including flow control, congestion control, RTT estimation, MSS option, OOO reassembly, and various advanced features like timestamps and selective ACKs. In contrast, BLIP lacks the most features, as it does not implement congestion control, RTT estimation, or several other functionalities present in other stacks.",1811.02721v3.pdf,"['1811.02721v3.pdf', '1708.02153v2.pdf', '1809.04276v2.pdf', '1906.10843v1.pdf', '1704.00774v3.pdf']","The table directly compares the feature sets of different embedded TCP stacks. By analyzing the ""Yes"" and ""No"" entries, we can identify which functionalities are present or absent in each stack. The passage also provides additional information, highlighting the specific limitations of uIP and BLIP. Combining the table and passage allows us to comprehensively assess the completeness of each stack's implementation and identify the best and worst options in terms of feature richness.",1811.02721v3-Table9-1.png,Performant TCP for Low-Power Wireless Networks,"Low-power and lossy networks (LLNs) enable diverse applications integrating
many resource-constrained embedded devices, often requiring interconnectivity
with existing TCP/IP networks as part of the Internet of Things. But TCP has
received little attention in LLNs due to concerns about its overhead and
performance, leading to LLN-specific protocols that require specialized
gateways for interoperability. We present a systematic study of a well-designed
TCP stack in IEEE 802.15.4-based LLNs, based on the TCP protocol logic in
FreeBSD. Through careful implementation and extensive experiments, we show that
modern low-power sensor platforms are capable of running full-scale TCP and
that TCP, counter to common belief, performs well despite the lossy nature of
LLNs. By carefully studying the interaction between the transport and link
layers, we identify subtle but important modifications to both, achieving TCP
goodput within 25% of an upper bound (5-40x higher than prior results) and
low-power operation commensurate to CoAP in a practical LLN application
scenario. This suggests that a TCP-based transport layer, seamlessly
interoperable with existing TCP/IP networks, is viable and performant in LLNs.","Table 9: Comparison of core features among embedded TCP stacks: uIP (Contiki), BLIP (TinyOS), GNRC (RIOT), and","Which TCP stack provides the most complete implementation of core TCP features, and which stack lacks the most features?"
spiqa_354,1603.00286v5,"Referring to Fig. 3 in the ""Redividing the Cake"" paper, which agent is shown to have a positive value-density throughout their entire allocated share $Z_j$, consistent with their valuation of the rest of the cake at 0?",Agent $j$.,1603.00286v5.pdf,"['1603.00286v5.pdf', '1804.00863v3.pdf', '1706.04269v2.pdf', '1707.00189v3.pdf', '1703.10730v2.pdf', '1706.04284v3.pdf', '1805.01216v3.pdf', '1708.06832v3.pdf', '1706.08146v3.pdf', '1709.02418v2.pdf', '1805.04687v2.pdf', '1812.00108v4.pdf', '1906.10843v1.pdf']","The passage states that agent $j$ values $Z_j$ at $\lceil{n\over d}\rceil$, and the rest of the cake at $0$. This means that agent $j$ values the entire share $Z_j$. The figure shows that the value-density of agent $j$ is positive within the entire share $Z_j$, which is consistent with the passage.",1603.00286v5-Figure3-1.png,Redividing the Cake,"The paper considers fair allocation of resources that are already allocated
in an unfair way. This setting requires a careful balance between the fairness
considerations and the rights of the present owners.
  The paper presents re-division algorithms that attain various trade-off
points between fairness and ownership rights, in various settings differing in
the geometric constraints on the allotments: (a) no geometric constraints; (b)
connectivity -- the cake is a one-dimensional interval and each piece must be a
contiguous interval; (c) rectangularity -- the cake is a two-dimensional
rectangle or rectilinear polygon and the pieces should be rectangles; (d)
convexity -- the cake is a two-dimensional convex polygon and the pieces should
be convex.
  These re-division algorithms have implications on another problem: the
price-of-fairness -- the loss of social welfare caused by fairness
requirements. Each algorithm implies an upper bound on the price-of-fairness
with the respective geometric constraints.","Fig. 3 Solid boxes represent the value-density of agent j within Zj ; each dotted box represents a value-density of some other agent in the same group as agent j. In this example, dn d e = 5.",Which agent values the entire share $Z_j$?
spiqa_355,1805.07567v2,"Based on the figure comparing the performance of DSS and F-DSS under different thresholds, which algorithm demonstrates a superior F-measure by better balancing precision and recall?",F-DSS.,1805.07567v2.pdf,"['1805.07567v2.pdf', '1611.02654v2.pdf', '1705.09882v2.pdf', '1611.04684v1.pdf', '1705.09966v2.pdf', '1802.07459v2.pdf', '1804.07707v2.pdf', '1811.08481v2.pdf', '1811.06635v1.pdf', '1707.00189v3.pdf', '1809.00263v5.pdf', '1706.00633v4.pdf']","The plot shows that F-DSS has a higher F-measure than DSS for all thresholds. The F-measure is a harmonic mean of precision and recall, so a higher F-measure indicates a better balance between the two.",1805.07567v2-Figure5-1.png,Optimizing the F-measure for Threshold-free Salient Object Detection,"Current CNN-based solutions to salient object detection (SOD) mainly rely on
the optimization of cross-entropy loss (CELoss). Then the quality of detected
saliency maps is often evaluated in terms of F-measure. In this paper, we
investigate an interesting issue: can we consistently use the F-measure
formulation in both training and evaluation for SOD? By reformulating the
standard F-measure we propose the relaxed F-measure which is differentiable
w.r.t the posterior and can be easily appended to the back of CNNs as the loss
function. Compared to the conventional cross-entropy loss of which the
gradients decrease dramatically in the saturated area, our loss function, named
FLoss, holds considerable gradients even when the activation approaches the
target. Consequently, the FLoss can continuously force the network to produce
polarized activations. Comprehensive benchmarks on several popular datasets
show that FLoss outperforms the state-of-the-art with a considerable margin.
More specifically, due to the polarized predictions, our method is able to
obtain high-quality saliency maps without carefully tuning the optimal
threshold, showing significant advantages in real-world applications.","Precision, Recall, F-measure and Maximal F-measure (•) of DSS (- - -) and F-DSS (—) under different thresholds. DSS tends to predict unknown pixels as the majority class–the background, resulting in high precision but low recall. FLoss is able to find a better compromise between precision and recall.",Which algorithm achieves a better balance between precision and recall?
spiqa_356,1809.03550v3,"Based on Table 6 of this paper, which algorithm demonstrates the shortest mean processing time per frame on the ""baseline/highway"" video sequence from the changedetection.net dataset, and how does its speed compare to that of the slowest algorithm mentioned in the table?","Algorithm 12(SCDM with Geman-McLure) achieves the fastest processing time per frame at 0.103 seconds. This is approximately 100 times faster than the slowest algorithm, TTD_3WD, which takes 10.343 seconds per frame.",1809.03550v3.pdf,"['1809.03550v3.pdf', '1805.08751v2.pdf', '1804.04786v3.pdf', '1809.03449v3.pdf', '1704.04539v2.pdf', '1811.02553v4.pdf', '1703.00899v2.pdf', '1710.06177v2.pdf', '1803.04572v2.pdf', '1710.01507v4.pdf', '1901.00056v2.pdf', '1703.00060v2.pdf', '1811.08257v1.pdf', '1704.05426v4.pdf', '1704.07854v4.pdf']","Table 1 explicitly lists the mean processing time per frame for each algorithm. By comparing these values, we can identify the fastest and slowest algorithms. The caption clarifies that the implementation does not utilize parallelization, ensuring a fair comparison across algorithms.",1809.03550v3-Table6-1.png,Pursuit of Low-Rank Models of Time-Varying Matrices Robust to Sparse and Measurement Noise,"In tracking of time-varying low-rank models of time-varying matrices, we
present a method robust to both uniformly-distributed measurement noise and
arbitrarily-distributed ``sparse'' noise. In theory, we bound the tracking
error. In practice, our use of randomised coordinate descent is scalable and
allows for encouraging results on changedetection net, a benchmark.","Table 6: Mean processing time per input frame (in seconds) on the “baseline/highway” video-sequence from http://changedetection. net. Note, our implementation does not use any parallelisation at the moment. This was done on purpose to run on a machine serving multiple cameras simultaneously.",Which algorithm achieves the fastest processing time per frame and how much faster is it compared to the slowest algorithm listed?
spiqa_357,1803.04572v2,"In the figure comparing the convergence of COPA and SPARTan algorithms on CMS data with non-negativity constraints and target ranks R=15 and R=40, which algorithm demonstrated faster convergence in terms of time to reach the highest F-measure value?",SPARTan converged faster in both cases of target rank.,1803.04572v2.pdf,"['1803.04572v2.pdf', '1805.04687v2.pdf', '1805.06431v4.pdf', '1707.06320v2.pdf']",The figure shows that the SPARTan curve reaches a higher F-measure value in a shorter amount of time than the COPA curve in both cases of target rank.,1803.04572v2-Figure6-1.png,COPA: Constrained PARAFAC2 for Sparse & Large Datasets,"PARAFAC2 has demonstrated success in modeling irregular tensors, where the
tensor dimensions vary across one of the modes. An example scenario is modeling
treatments across a set of patients with the varying number of medical
encounters over time. Despite recent improvements on unconstrained PARAFAC2,
its model factors are usually dense and sensitive to noise which limits their
interpretability. As a result, the following open challenges remain: a) various
modeling constraints, such as temporal smoothness, sparsity and non-negativity,
are needed to be imposed for interpretable temporal modeling and b) a scalable
approach is required to support those constraints efficiently for large
datasets. To tackle these challenges, we propose a {\it CO}nstrained {\it
PA}RAFAC2 (COPA) method, which carefully incorporates optimization constraints
such as temporal smoothness, sparsity, and non-negativity in the resulting
factors. To efficiently support all those constraints, COPA adopts a hybrid
optimization framework using alternating optimization and alternating direction
method of multiplier (AO-ADMM). As evaluated on large electronic health record
(EHR) datasets with hundreds of thousands of patients, COPA achieves
significant speedups (up to 36 times faster) over prior PARAFAC2 approaches
that only attempt to handle a subset of the constraints that COPA enables.
Overall, our method outperforms all the baselines attempting to handle a subset
of the constraints in terms of speed, while achieving the same level of
accuracy. Through a case study on temporal phenotyping of medically complex
children, we demonstrate how the constraints imposed by COPA reveal concise
phenotypes and meaningful temporal profiles of patients. The clinical
interpretation of both the phenotypes and the temporal profiles was confirmed
by a medical expert.","The best convergence of COPA and SPARTan out of 5 different random initializations with non-negativity constraint on H, {Sk}, and V on CMS data with K=843,162, J=284 and maximum number of observations are 1500. Algorithms tested on different target ranks (two cases considered: R={15,40}).",Which algorithm converged faster in both cases of target rank?
spiqa_358,1805.02349v2,"In the figure comparing the runtime of various algorithms for recovering the ""ground truth"" permutation in correlated Erdős-Rényi random graphs, which algorithm demonstrates the fastest performance?",The algorithm proposed in this paper has the fastest runtime.,1805.02349v2.pdf,"['1805.02349v2.pdf', '1611.03780v2.pdf', '1709.08294v3.pdf', '1710.06177v2.pdf', '1701.06171v4.pdf', '1704.05426v4.pdf', '1703.02507v3.pdf', '1802.07459v2.pdf', '1704.05958v2.pdf', '1803.04383v2.pdf', '1901.00398v2.pdf', '1811.06635v1.pdf']",The table shows that the algorithm in this paper has a runtime of  while the other algorithms have runtimes of  or .,1805.02349v2-Figure1-1.png,(Nearly) Efficient Algorithms for the Graph Matching Problem on Correlated Random Graphs,"We give a quasipolynomial time algorithm for the graph matching problem (also
known as noisy or robust graph isomorphism) on correlated random graphs.
Specifically, for every $\gamma>0$, we give a $n^{O(\log n)}$ time algorithm
that given a pair of $\gamma$-correlated $G(n,p)$ graphs $G_0,G_1$ with average
degree between $n^{\varepsilon}$ and $n^{1/153}$ for $\varepsilon = o(1)$,
recovers the ""ground truth"" permutation $\pi\in S_n$ that matches the vertices
of $G_0$ to the vertices of $G_n$ in the way that minimizes the number of
mismatched edges. We also give a recovery algorithm for a denser regime, and a
polynomial-time algorithm for distinguishing between correlated and
uncorrelated graphs.
  Prior work showed that recovery is information-theoretically possible in this
model as long the average degree was at least $\log n$, but sub-exponential
time algorithms were only known in the dense case (i.e., for $p > n^{-o(1)}$).
Moreover, ""Percolation Graph Matching"", which is the most common heuristic for
this problem, has been shown to require knowledge of $n^{\Omega(1)}$ ""seeds""
(i.e., input/output pairs of the permutation $\pi$) to succeed in this regime.
In contrast our algorithms require no seed and succeed for $p$ which is as low
as $n^{o(1)-1}$.","A comparison of algorithms for recovery of the permutation in the correlated Erdös-Rényi model, when (G0,G1, π) ∼ Dstruct(n, p;γ).",Which algorithm has the fastest runtime?
spiqa_359,1805.04609v3,"According to Figure 3 in the ""Textual Membership Queries"" paper, which synthesis algorithm results in the highest percentage of switched labels across all datasets?",US-HC-MQ,1805.04609v3.pdf,"['1805.04609v3.pdf', '1703.10730v2.pdf', '1809.00263v5.pdf', '1802.07222v1.pdf', '1709.02418v2.pdf', '1707.08608v3.pdf', '1805.04687v2.pdf', '1701.06171v4.pdf', '1901.00056v2.pdf', '1809.01989v2.pdf', '1803.05776v2.pdf', '1906.06589v3.pdf']",Figure \ref{fig:effect_of_alg_on_label_switch} shows that US-HC-MQ has the highest percentage of switched instances for all five datasets.,1805.04609v3-Figure3-1.png,Textual Membership Queries,"Human labeling of data can be very time-consuming and expensive, yet, in many
cases it is critical for the success of the learning process. In order to
minimize human labeling efforts, we propose a novel active learning solution
that does not rely on existing sources of unlabeled data. It uses a small
amount of labeled data as the core set for the synthesis of useful membership
queries (MQs) - unlabeled instances generated by an algorithm for human
labeling. Our solution uses modification operators, functions that modify
instances to some extent. We apply the operators on a small set of instances
(core set), creating a set of new membership queries. Using this framework, we
look at the instance space as a search space and apply search algorithms in
order to generate new examples highly relevant to the learner. We implement
this framework in the textual domain and test it on several text classification
tasks and show improved classifier performance as more MQs are labeled and
incorporated into the training set. To the best of our knowledge, this is the
first work on membership queries in the textual domain.",Figure 3: The effect of the synthesis algorithm on the number of changed labels,Which algorithm has the highest percentage of switched instances?
spiqa_360,1706.00827v2,"Based on the data in Table 8 for plane and cylinder fitting, which algorithm, Multi-X or T-Linkage, consistently demonstrates faster processing times across all point sizes (100, 500, 1000)?",Multi-X is generally faster for fitting planes and cylinders compared to T-Linkage.,1706.00827v2.pdf,"['1706.00827v2.pdf', '1803.05776v2.pdf', '1704.05958v2.pdf', '1804.04410v2.pdf', '1707.01922v5.pdf', '1709.08294v3.pdf', '1802.07222v1.pdf', '1709.02418v2.pdf', '1611.07718v2.pdf', '1809.04276v2.pdf', '1812.10735v2.pdf']","Looking at column (5) in Table 1, which shows the processing times for fitting planes and cylinders, we can see that the processing time for Multi-X is consistently lower than that of T-Linkage for all three data sizes (100, 500, and 1000). For example, with 500 data points, Multi-X takes 3.8 seconds while T-Linkage takes 15.9 seconds. This trend holds true for the other data sizes as well, indicating that Multi-X offers superior performance in terms of speed for this specific fitting task.",1706.00827v2-Table8-1.png,Multi-Class Model Fitting by Energy Minimization and Mode-Seeking,"We propose a general formulation, called Multi-X, for multi-class
multi-instance model fitting - the problem of interpreting the input data as a
mixture of noisy observations originating from multiple instances of multiple
classes. We extend the commonly used alpha-expansion-based technique with a new
move in the label space. The move replaces a set of labels with the
corresponding density mode in the model parameter domain, thus achieving fast
and robust optimization. Key optimization parameters like the bandwidth of the
mode seeking are set automatically within the algorithm. Considering that a
group of outliers may form spatially coherent structures in the data, we
propose a cross-validation-based technique removing statistically insignificant
instances. Multi-X outperforms significantly the state-of-the-art on publicly
available datasets for diverse problems: multiple plane and rigid motion
detection; motion segmentation; simultaneous plane and cylinder fitting; circle
and line fitting.","Table 8: Processing times (sec) of Multi-X (M) and TLinkage (T) for the problem of fitting (1) lines and circles, (2) homographies, (3) two-view motions, (4) video motions, and (5) planes and cylinders. The number of data points is shown in the first column.",Which algorithm is generally faster for fitting planes and cylinders: Multi-X or T-Linkage?
spiqa_361,1605.07496v3,"Based on the figure showing the performance of Reinforce and TRPO on the Robotic Arm Simulator experiments, do both algorithms converge to a similar expected cost, or does one demonstrate a clear advantage in minimizing expected cost during the arm breakage task?",Both TRPO and Reinforce performed similarly on the arm breakage task.,1605.07496v3.pdf,"['1605.07496v3.pdf', '1809.03449v3.pdf', '1705.10667v4.pdf', '1809.00458v1.pdf', '1705.08016v3.pdf', '1705.09296v2.pdf', '1703.10730v2.pdf', '1709.00139v4.pdf', '1707.01922v5.pdf', '1811.09393v4.pdf', '1702.03584v3.pdf', '1708.00160v2.pdf']",The figure shows that the expected cost for both algorithms decreases to a similar level after around 400 simulator calls.,1605.07496v3-Figure4-1.png,Alternating Optimisation and Quadrature for Robust Control,"Bayesian optimisation has been successfully applied to a variety of
reinforcement learning problems. However, the traditional approach for learning
optimal policies in simulators does not utilise the opportunity to improve
learning by adjusting certain environment variables: state features that are
unobservable and randomly determined by the environment in a physical setting
but are controllable in a simulator. This paper considers the problem of
finding a robust policy while taking into account the impact of environment
variables. We present Alternating Optimisation and Quadrature (ALOQ), which
uses Bayesian optimisation and Bayesian quadrature to address such settings.
ALOQ is robust to the presence of significant rare events, which may not be
observable under random sampling, but play a substantial role in determining
the optimal policy. Experimental results across different domains show that
ALOQ can learn more efficiently and robustly than existing methods.",Performance of Reinforce and TRPO on the Robotic Arm Simulator experiments.,Which algorithm performed better on the arm breakage task?
spiqa_362,1809.00458v1,"In the figure comparing accuracy versus similarity threshold for containment similarity search, which algorithm achieves the highest F1 score on the ENRON dataset?",GB-KMV,1809.00458v1.pdf,"['1809.00458v1.pdf', '1611.04363v2.pdf', '1704.07121v2.pdf', '1708.00160v2.pdf', '1803.06506v3.pdf', '1612.02803v5.pdf', '1805.02349v2.pdf', '1803.01128v3.pdf', '1804.04786v3.pdf', '1705.09882v2.pdf', '1603.03833v4.pdf', '1611.07718v2.pdf', '1812.06589v2.pdf']",The figure shows the F1 score for different algorithms on different datasets. The GB-KMV algorithm has the highest F1 score on the ENRON dataset.,1809.00458v1-Figure15-1.png,GB-KMV: An Augmented KMV Sketch for Approximate Containment Similarity Search,"In this paper, we study the problem of approximate containment similarity
search. Given two records Q and X, the containment similarity between Q and X
with respect to Q is |Q intersect X|/ |Q|. Given a query record Q and a set of
records S, the containment similarity search finds a set of records from S
whose containment similarity regarding Q are not less than the given threshold.
This problem has many important applications in commercial and scientific
fields such as record matching and domain search. Existing solution relies on
the asymmetric LSH method by transforming the containment similarity to
well-studied Jaccard similarity. In this paper, we use a different framework by
transforming the containment similarity to set intersection. We propose a novel
augmented KMV sketch technique, namely GB-KMV, which is data-dependent and can
achieve a good trade-off between the sketch size and the accuracy. We provide a
set of theoretical analysis to underpin the proposed augmented KMV sketch
technique, and show that it outperforms the state-of-the-art technique LSH-E in
terms of estimation accuracy under practical assumption. Our comprehensive
experiments on real-life datasets verify that GB-KMV is superior to LSH-E in
terms of the space-accuracy trade-off, time-accuracy trade-off, and the sketch
construction time. For instance, with similar estimation accuracy (F-1 score),
GB-KMV is over 100 times faster than LSH-E on some real-life dataset.",Accuracy versus Similarity threshold,Which algorithm performs best on the ENRON dataset?
spiqa_363,1809.00458v1,"Based on the visualization of accuracy versus space on the ENRON dataset, how does GB-KMV compare to LSH-E in terms of F1 Score and Precision across varying levels of space usage?",GB-KMV performs better than LSH-E in terms of F1 Score and Precision.,1809.00458v1.pdf,"['1809.00458v1.pdf', '1605.07496v3.pdf', '1805.01216v3.pdf', '1708.03797v1.pdf', '1805.02349v2.pdf']",The figure shows that the F1 Score and Precision of GB-KMV are consistently higher than those of LSH-E across different SpaceUsed values.,1809.00458v1-Figure9-1.png,GB-KMV: An Augmented KMV Sketch for Approximate Containment Similarity Search,"In this paper, we study the problem of approximate containment similarity
search. Given two records Q and X, the containment similarity between Q and X
with respect to Q is |Q intersect X|/ |Q|. Given a query record Q and a set of
records S, the containment similarity search finds a set of records from S
whose containment similarity regarding Q are not less than the given threshold.
This problem has many important applications in commercial and scientific
fields such as record matching and domain search. Existing solution relies on
the asymmetric LSH method by transforming the containment similarity to
well-studied Jaccard similarity. In this paper, we use a different framework by
transforming the containment similarity to set intersection. We propose a novel
augmented KMV sketch technique, namely GB-KMV, which is data-dependent and can
achieve a good trade-off between the sketch size and the accuracy. We provide a
set of theoretical analysis to underpin the proposed augmented KMV sketch
technique, and show that it outperforms the state-of-the-art technique LSH-E in
terms of estimation accuracy under practical assumption. Our comprehensive
experiments on real-life datasets verify that GB-KMV is superior to LSH-E in
terms of the space-accuracy trade-off, time-accuracy trade-off, and the sketch
construction time. For instance, with similar estimation accuracy (F-1 score),
GB-KMV is over 100 times faster than LSH-E on some real-life dataset.",Accuracy versus Space on ENRON,Which algorithm performs better in terms of F1 Score and Precision on ENRON?
spiqa_364,1809.00458v1,"In the ""Accuracy versus Space on WEBSPAM"" figure from the paper *GB-KMV: An Augmented KMV Sketch for Approximate Containment Similarity Search*, which algorithm, GB-KMV or LSH-E, demonstrates better F1 score and precision when the space usage is limited to 5%?",GB-KMV performs better in terms of F1 score and precision when the space used is 5%.,1809.00458v1.pdf,"['1809.00458v1.pdf', '1705.02946v3.pdf', '1703.07015v3.pdf', '1704.07121v2.pdf', '1804.04410v2.pdf', '1705.09966v2.pdf', '1805.00912v4.pdf', '1704.04539v2.pdf', '1708.06832v3.pdf']","The plot on the left shows the F1 score for both algorithms, and the plot on the right shows the precision for both algorithms. At 5% space used, the GB-KMV line is higher than the LSH-E line in both plots, indicating that GB-KMV performs better in terms of both F1 score and precision.",1809.00458v1-Figure12-1.png,GB-KMV: An Augmented KMV Sketch for Approximate Containment Similarity Search,"In this paper, we study the problem of approximate containment similarity
search. Given two records Q and X, the containment similarity between Q and X
with respect to Q is |Q intersect X|/ |Q|. Given a query record Q and a set of
records S, the containment similarity search finds a set of records from S
whose containment similarity regarding Q are not less than the given threshold.
This problem has many important applications in commercial and scientific
fields such as record matching and domain search. Existing solution relies on
the asymmetric LSH method by transforming the containment similarity to
well-studied Jaccard similarity. In this paper, we use a different framework by
transforming the containment similarity to set intersection. We propose a novel
augmented KMV sketch technique, namely GB-KMV, which is data-dependent and can
achieve a good trade-off between the sketch size and the accuracy. We provide a
set of theoretical analysis to underpin the proposed augmented KMV sketch
technique, and show that it outperforms the state-of-the-art technique LSH-E in
terms of estimation accuracy under practical assumption. Our comprehensive
experiments on real-life datasets verify that GB-KMV is superior to LSH-E in
terms of the space-accuracy trade-off, time-accuracy trade-off, and the sketch
construction time. For instance, with similar estimation accuracy (F-1 score),
GB-KMV is over 100 times faster than LSH-E on some real-life dataset.",Accuracy versus Space on WEBSPAM,Which algorithm performs better in terms of F1 score and precision when the space used is 5%?
spiqa_365,1803.02750v3,"In the figure illustrating ""Average memory ratio with respect to BP+RR for GMap 10%"" under a mesh topology, which synchronization algorithm demonstrates the most efficient memory usage?",Delta-based BP+RR,1803.02750v3.pdf,"['1803.02750v3.pdf', '1611.07718v2.pdf', '1901.00056v2.pdf', '1811.10673v1.pdf', '1809.03149v2.pdf', '1706.04284v3.pdf', '1804.04410v2.pdf', '1901.00398v2.pdf']","The figure shows that the bar for Delta-based BP+RR is the shortest for GMap 10%, indicating that it has the lowest average memory ratio with respect to BP+RR.",1803.02750v3-Figure10-1.png,Efficient Synchronization of State-based CRDTs,"To ensure high availability in large scale distributed systems, Conflict-free
Replicated Data Types (CRDTs) relax consistency by allowing immediate query and
update operations at the local replica, with no need for remote
synchronization. State-based CRDTs synchronize replicas by periodically sending
their full state to other replicas, which can become extremely costly as the
CRDT state grows. Delta-based CRDTs address this problem by producing small
incremental states (deltas) to be used in synchronization instead of the full
state. However, current synchronisation algorithms for Delta-based CRDTs induce
redundant wasteful delta propagation, performing worse than expected, and
surprisingly, no better than State-based. In this paper we: 1) identify two
sources of inefficiency in current synchronization algorithms for delta-based
CRDTs; 2) bring the concept of join decomposition to state-based CRDTs; 3)
exploit join decompositions to obtain optimal deltas and 4) improve the
efficiency of synchronization algorithms; and finally, 5) evaluate the improved
algorithms.","Average memory ratio with respect to BP+RR for GCounter, GSet, GMap 10% and 100% – mesh topology",Which algorithm performs the best in terms of average memory ratio with respect to BP+RR for GMap 10%?
spiqa_366,1605.07496v3,"According to the figure illustrating quartile costs in the Robotic Arm Simulator's Joint Breakage experiment, which algorithm demonstrates the lowest expected cost across Q1, median, and Q2 values?",ALOQ,1605.07496v3.pdf,"['1605.07496v3.pdf', '1704.08615v2.pdf', '1802.07222v1.pdf', '1811.10673v1.pdf', '1805.07567v2.pdf', '1612.02803v5.pdf', '1701.06171v4.pdf', '1701.03077v10.pdf', '1705.02798v6.pdf', '1603.03833v4.pdf', '1706.04269v2.pdf']","The table shows the quartiles of the expected cost of the final π̂∗ estimated by each algorithm across 20 independent runs for the Robotic Arm Simulator experiments. The lower the cost, the better the performance. In the Joint Breakage experiment, ALOQ has the lowest Q1, median, and Q2 values.",1605.07496v3-Table2-1.png,Alternating Optimisation and Quadrature for Robust Control,"Bayesian optimisation has been successfully applied to a variety of
reinforcement learning problems. However, the traditional approach for learning
optimal policies in simulators does not utilise the opportunity to improve
learning by adjusting certain environment variables: state features that are
unobservable and randomly determined by the environment in a physical setting
but are controllable in a simulator. This paper considers the problem of
finding a robust policy while taking into account the impact of environment
variables. We present Alternating Optimisation and Quadrature (ALOQ), which
uses Bayesian optimisation and Bayesian quadrature to address such settings.
ALOQ is robust to the presence of significant rare events, which may not be
observable under random sampling, but play a substantial role in determining
the optimal policy. Experimental results across different domains show that
ALOQ can learn more efficiently and robustly than existing methods.",Quartiles of the expected cost of the final π̂∗ estimated by each algorithm across 20 independent runs for the Robotic Arm Simulator experiments.,Which algorithm performs the best in the Joint Breakage experiment?
spiqa_367,1708.01425v4,"Based on the figure reporting accuracy across methods, which approach delivers the highest performance on the development set for the argument reasoning comprehension task?",Intra-warrant attention with context.,1708.01425v4.pdf,"['1708.01425v4.pdf', '1611.03780v2.pdf', '1804.01429v3.pdf', '1809.02731v3.pdf', '1804.05938v2.pdf', '1705.02798v6.pdf', '1803.01128v3.pdf', '1611.05742v3.pdf', '1802.07351v2.pdf']","The table shows that the Intra-warrant attention with context approach has the highest accuracy on the development set, with a score of 0.638.",1708.01425v4-Table2-1.png,The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants,"Reasoning is a crucial part of natural language argumentation. To comprehend
an argument, one must analyze its warrant, which explains why its claim follows
from its premises. As arguments are highly contextualized, warrants are usually
presupposed and left implicit. Thus, the comprehension does not only require
language understanding and logic skills, but also depends on common sense. In
this paper we develop a methodology for reconstructing warrants systematically.
We operationalize it in a scalable crowdsourcing process, resulting in a freely
licensed dataset with warrants for 2k authentic arguments from news comments.
On this basis, we present a new challenging task, the argument reasoning
comprehension task. Given an argument with a claim and a premise, the goal is
to choose the correct implicit warrant from two options. Both warrants are
plausible and lexically close, but lead to contradicting claims. A solution to
this task will define a substantial step towards automatic warrant
reconstruction. However, experiments with several neural attention and language
models reveal that current approaches do not suffice.","Accuracy of each approach (humans and systems) on the development set and test set, respectively.",Which approach performs best on the development set?
spiqa_368,1706.00633v4,"Based on the figure in this paper, where the Resnet-32 model is evaluated on the MNIST dataset with the thresholding test strategy disabled, which adversarial attack method leads to the largest reduction in classification accuracy?",The most effective attack method at reducing the accuracy of the Resnet-32 model on the MNIST dataset is BIM/CE.,1706.00633v4.pdf,"['1706.00633v4.pdf', '1708.05239v3.pdf', '1704.07121v2.pdf', '1702.03584v3.pdf', '1704.08615v2.pdf', '1707.01917v2.pdf', '1606.07384v2.pdf', '1708.02153v2.pdf', '1811.08481v2.pdf', '1703.00060v2.pdf', '1706.08146v3.pdf', '1611.04684v1.pdf']","The figure shows the classification accuracy of the Resnet-32 model on the MNIST dataset under different attack methods. BIM/CE achieves the lowest accuracy of all the methods, indicating that it is the most effective at reducing the accuracy of the model.",1706.00633v4-Figure2-1.png,Towards Robust Detection of Adversarial Examples,"Although the recent progress is substantial, deep learning methods can be
vulnerable to the maliciously generated adversarial examples. In this paper, we
present a novel training procedure and a thresholding test strategy, towards
robust detection of adversarial examples. In training, we propose to minimize
the reverse cross-entropy (RCE), which encourages a deep network to learn
latent representations that better distinguish adversarial examples from normal
ones. In testing, we propose to use a thresholding strategy as the detector to
filter out adversarial examples for reliable predictions. Our method is simple
to implement using standard algorithms, with little extra training cost
compared to the common cross-entropy minimization. We apply our method to
defend various attacking methods on the widely used MNIST and CIFAR-10
datasets, and achieve significant improvements on robust predictions under all
the threat models in the adversarial setting.",Robustness with the thresholding test strategy disabled. The model of target networks is Resnet-32.,Which attack method is the most effective at reducing the accuracy of the Resnet-32 model on the MNIST dataset?
spiqa_369,1805.04687v2,"Based on the figure that illustrates the long-tail distribution of object categories in the BDD100K dataset, which specific category is the least frequent in terms of instance count?",Train,1805.04687v2.pdf,"['1805.04687v2.pdf', '1705.02946v3.pdf', '1703.07015v3.pdf', '1707.01922v5.pdf', '1707.00189v3.pdf', '1710.06177v2.pdf', '1805.07567v2.pdf', '1709.00139v4.pdf', '1809.03449v3.pdf', '1804.00863v3.pdf']","The figure shows the number of instances for each category of object. The category with the lowest bar is ""Train,"" indicating that it is the least common.",1805.04687v2-Figure3-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.","Instance statistics of our object categories. (a) Number of instances of each category, which follows a long-tail distribution. (b) Roughly half of the instances are occluded. (c) About 7% of the instances are truncated.",Which category of object is the least common in the dataset?
spiqa_370,1805.04687v2,"Referring to Table 12 of the BDD100K MOTS annotations, which object category has the highest total number of annotations, and does the high count of occluded annotations provide evidence that this category might be more challenging to annotate accurately?","The category with the highest total number of annotations is ""Masks,"" with 129K annotations. There is evidence that this category might be more challenging to annotate accurately because it also has the highest number of annotations in the ""Occluded"" subcategory, indicating that a large portion of these objects are partially hidden in the images.",1805.04687v2.pdf,"['1805.04687v2.pdf', '1706.08146v3.pdf', '1803.06506v3.pdf', '1812.00281v3.pdf', '1804.07849v4.pdf']","Table 1 provides the breakdown of annotations for different categories of objects in the BDD100K MOTS dataset. The table shows the total number of annotations as well as subcategories like ""Occluded,"" which refers to objects that are partially hidden. By comparing the total number of annotations and the number of occluded annotations for each category, we can gain insight into the potential difficulty of accurately annotating each category. The ""Masks"" category stands out with both the highest total number and the highest number of occluded annotations, suggesting that annotating masks accurately might be more challenging due to frequent occlusions.",1805.04687v2-Table12-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.",Table 12: Annotations of BDD100K MOTS by category.,"Which category of objects has the highest total number of annotations, and is there evidence that this category might be more challenging to annotate accurately?"
spiqa_371,1804.05936v2,"According to Table 4 of the paper ""Learning a Deep Listwise Context Model for Ranking Refinement,"" which specific combination of initial list, DLCM model, and loss function achieved the highest nDCG@10 (0.743) and ERR@10 (0.453) for Yahoo! set 1?","LambdaMART initial list, DLCM model, and AttRank loss function achieved the best overall performance on the Yahoo! set 1, with an nDCG@10 of 0.743 and an ERR@10 of 0.453.",1804.05936v2.pdf,"['1804.05936v2.pdf', '1704.08615v2.pdf', '1805.08751v2.pdf', '1802.07222v1.pdf', '1811.08481v2.pdf', '1603.03833v4.pdf', '1706.08146v3.pdf', '1706.00633v4.pdf']","The table presents the performance of various combinations of initial list, model, and loss function on the Yahoo! set 1, measured by several metrics including nDCG@10 and ERR@10. By looking at the last two columns of the table, we can identify the combination with the highest values for both metrics. In this case, the combination of LambdaMART initial list, DLCM model, and AttRank loss function has the highest values for both nDCG@10 (0.743) and ERR@10 (0.453), indicating the best overall performance.",1804.05936v2-Table4-1.png,Learning a Deep Listwise Context Model for Ranking Refinement,"Learning to rank has been intensively studied and widely applied in
information retrieval. Typically, a global ranking function is learned from a
set of labeled data, which can achieve good performance on average but may be
suboptimal for individual queries by ignoring the fact that relevant documents
for different queries may have different distributions in the feature space.
Inspired by the idea of pseudo relevance feedback where top ranked documents,
which we refer as the \textit{local ranking context}, can provide important
information about the query's characteristics, we propose to use the inherent
feature distributions of the top results to learn a Deep Listwise Context Model
that helps us fine tune the initial ranked list. Specifically, we employ a
recurrent neural network to sequentially encode the top results using their
feature vectors, learn a local context model and use it to re-rank the top
results. There are three merits with our model: (1) Our model can capture the
local ranking context based on the complex interactions between top results
using a deep neural network; (2) Our model can be built upon existing
learning-to-rank methods by directly using their extracted feature vectors; (3)
Our model is trained with an attention-based loss function, which is more
effective and efficient than many existing listwise methods. Experimental
results show that the proposed model can significantly improve the
state-of-the-art learning to rank methods on benchmark retrieval corpora.","Table 4: Comparison of baselines and the DLCMs on Yahoo! set 1. ∗, + and ‡ denotes significant improvements over the global ranking algorithm and the best corresponding re-ranking baseline (DNN) and LIDNN.","Which combination of initial list, model, and loss function achieved the best overall performance on the Yahoo! set 1, as measured by nDCG@10 and ERR@10?"
spiqa_372,1706.03847v3,"Referring to Table 2 in the ""Recurrent Neural Networks with Top-k Gains for Session-based Recommendations"" paper, which combination of dataset and loss function under the GRU4Rec with additional samples achieved the highest Recall@20 score, and by what percentage did it improve over the original GRU4Rec model with TOP1 loss for the same dataset?",The highest Recall@20 score was achieved by the GRU4Rec with additional samples and BPR-max loss function on the RSC15 dataset. This score was 42.37% higher than the Recall@20 score of the original GRU4Rec model on the same dataset.,1706.03847v3.pdf,"['1706.03847v3.pdf', '1708.00160v2.pdf', '1809.00263v5.pdf', '1706.00827v2.pdf', '1708.05239v3.pdf', '1805.08751v2.pdf', '1705.09882v2.pdf', '1702.08694v3.pdf', '1803.02750v3.pdf', '1804.07707v2.pdf', '1710.05654v2.pdf']","The table presents Recall@20 scores for different combinations of methods and datasets. By looking at the row for the RSC15 dataset and the column for the BPR-max loss function within the ""GRU4Rec with additional samples"" section, we can find the highest score (0.7211). The percentage improvement over the original GRU4Rec is also provided in parentheses within the same cell (+42.37%).",1706.03847v3-Table2-1.png,Recurrent Neural Networks with Top-k Gains for Session-based Recommendations,"RNNs have been shown to be excellent models for sequential data and in
particular for data that is generated by users in an session-based manner. The
use of RNNs provides impressive performance benefits over classical methods in
session-based recommendations. In this work we introduce novel ranking loss
functions tailored to RNNs in the recommendation setting. The improved
performance of these losses over alternatives, along with further tricks and
refinements described in this work, allow for an overall improvement of up to
35% in terms of MRR and Recall@20 over previous session-based RNN solutions and
up to 53% over classical collaborative filtering approaches. Unlike data
augmentation-based improvements, our method does not increase training times
significantly. We further demonstrate the performance gain of the RNN over
baselines in an online A/B test.",Table 2: Recommendation accuracy with additional samples and different loss functions compared to item-kNN and the original GRU4Rec. Improvements over item-kNN and the original GRU4Rec (with TOP1 loss) results are shown in parentheses. Best results are typeset bold.,"Which combination of method and dataset achieved the highest Recall@20 score, and how much higher was it compared to the original GRU4Rec model?"
spiqa_373,1706.00633v4,"Based on Table 2 of this paper, which combination of training procedure (CE or RCE) and thresholding metric consistently achieves the highest AUC-scores for adversarial detection across all attack types on both MNIST and CIFAR-10 datasets?",RCE training combined with the K-density metric consistently performs the best across both MNIST and CIFAR-10 datasets for all attack types.,1706.00633v4.pdf,"['1706.00633v4.pdf', '1708.03797v1.pdf', '1804.01429v3.pdf', '1804.04786v3.pdf', '1705.09966v2.pdf', '1809.02731v3.pdf', '1709.08294v3.pdf', '1802.07222v1.pdf', '1611.03780v2.pdf']","Table 2 presents the AUC-scores of different combinations of training procedures (CE and RCE) and thresholding metrics (Confidence, non-ME, and K-density) for various attack types. By comparing the scores across rows and columns, we can identify the best performing combination. In this case, the entries marked with (*) represent the RCE training and K-density metric combination, and they consistently show the highest or near-highest AUC-scores for all attack types on both datasets.",1706.00633v4-Table2-1.png,Towards Robust Detection of Adversarial Examples,"Although the recent progress is substantial, deep learning methods can be
vulnerable to the maliciously generated adversarial examples. In this paper, we
present a novel training procedure and a thresholding test strategy, towards
robust detection of adversarial examples. In training, we propose to minimize
the reverse cross-entropy (RCE), which encourages a deep network to learn
latent representations that better distinguish adversarial examples from normal
ones. In testing, we propose to use a thresholding strategy as the detector to
filter out adversarial examples for reliable predictions. Our method is simple
to implement using standard algorithms, with little extra training cost
compared to the common cross-entropy minimization. We apply our method to
defend various attacking methods on the widely used MNIST and CIFAR-10
datasets, and achieve significant improvements on robust predictions under all
the threat models in the adversarial setting.","Table 2: AUC-scores (10−2) of adversarial examples. The model of target networks is Resnet-32. Values are calculated on the examples which are correctly classified as normal examples and then misclassified as adversarial counterparts. Bandwidths used when calculating K-density are σ2 CE = 1/0.26 and σ2 RCE = 0.1/0.26. Here (-) indicates the strong baseline, and (*) indicates our defense method.",Which combination of training procedure and thresholding metric consistently performs the best across both MNIST and CIFAR-10 datasets for all attack types?
spiqa_374,1705.02798v6,"In the ablation study presented in Table 4 of the ""Reinforced Mnemonic Reader for Machine Reading Comprehension"" paper, which model component leads to the largest drop in F1 score when removed, and by how much does the score decrease on the SQuAD dev set?","The DCRL training method appears to have the biggest impact on the F1 score. Removing it leads to a drop of 0.9 points in F1, which is the largest decrease observed for any single component in the ablation study.",1705.02798v6.pdf,"['1705.02798v6.pdf', '1701.06171v4.pdf', '1705.08016v3.pdf', '1804.05936v2.pdf', '1802.07222v1.pdf']","The table shows the results of an ablation study, where different components of the model are removed or replaced to see how they affect performance. The ""ΔF1"" column specifically shows the change in F1 score compared to the baseline model (R.M-Reader). By comparing the values in this column, we can identify which component has the largest impact on the F1 score when removed. In this case, removing DCRL in ablation (2) leads to the biggest drop in F1 score, indicating its importance for achieving a high F1 score.",1705.02798v6-Table4-1.png,Reinforced Mnemonic Reader for Machine Reading Comprehension,"In this paper, we introduce the Reinforced Mnemonic Reader for machine
reading comprehension tasks, which enhances previous attentive readers in two
aspects. First, a reattention mechanism is proposed to refine current
attentions by directly accessing to past attentions that are temporally
memorized in a multi-round alignment architecture, so as to avoid the problems
of attention redundancy and attention deficiency. Second, a new optimization
approach, called dynamic-critical reinforcement learning, is introduced to
extend the standard supervised method. It always encourages to predict a more
acceptable answer so as to address the convergence suppression problem occurred
in traditional reinforcement learning algorithms. Extensive experiments on the
Stanford Question Answering Dataset (SQuAD) show that our model achieves
state-of-the-art results. Meanwhile, our model outperforms previous systems by
over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD
datasets.",Table 4: Ablation study on SQuAD dev set.,"Which component of the model seems to have the biggest impact on the F1 score on SQuAD dataset, and how much does removing it affect the score?"
spiqa_375,1803.04572v2,"According to Figure 0 in the COPA paper, which constraint on $\M{U_k}$ for the CMS dataset yields the highest FIT value when the target rank is set to 15?",The smoothness constraint on $\M{U_k}$ has the most significant impact on the FIT values for the CMS data set when the target rank is 15.,1803.04572v2.pdf,"['1803.04572v2.pdf', '1811.02721v3.pdf', '1703.00899v2.pdf', '1802.07222v1.pdf', '1710.05654v2.pdf', '1611.02654v2.pdf', '1708.06832v3.pdf', '1805.02349v2.pdf', '1701.06171v4.pdf', '1809.00458v1.pdf']","From Figure 0, we can see that the FIT value for COPA (Smoothness on $\M{U_k}$) is the highest among all the versions of COPA for the CMS data set when the target rank is 15. This indicates that the smoothness constraint on $\M{U_k}$ has the most significant impact on the FIT values.",1803.04572v2-Figure3-1.png,COPA: Constrained PARAFAC2 for Sparse & Large Datasets,"PARAFAC2 has demonstrated success in modeling irregular tensors, where the
tensor dimensions vary across one of the modes. An example scenario is modeling
treatments across a set of patients with the varying number of medical
encounters over time. Despite recent improvements on unconstrained PARAFAC2,
its model factors are usually dense and sensitive to noise which limits their
interpretability. As a result, the following open challenges remain: a) various
modeling constraints, such as temporal smoothness, sparsity and non-negativity,
are needed to be imposed for interpretable temporal modeling and b) a scalable
approach is required to support those constraints efficiently for large
datasets. To tackle these challenges, we propose a {\it CO}nstrained {\it
PA}RAFAC2 (COPA) method, which carefully incorporates optimization constraints
such as temporal smoothness, sparsity, and non-negativity in the resulting
factors. To efficiently support all those constraints, COPA adopts a hybrid
optimization framework using alternating optimization and alternating direction
method of multiplier (AO-ADMM). As evaluated on large electronic health record
(EHR) datasets with hundreds of thousands of patients, COPA achieves
significant speedups (up to 36 times faster) over prior PARAFAC2 approaches
that only attempt to handle a subset of the constraints that COPA enables.
Overall, our method outperforms all the baselines attempting to handle a subset
of the constraints in terms of speed, while achieving the same level of
accuracy. Through a case study on temporal phenotyping of medically complex
children, we demonstrate how the constraints imposed by COPA reveal concise
phenotypes and meaningful temporal profiles of patients. The clinical
interpretation of both the phenotypes and the temporal profiles was confirmed
by a medical expert.","Figure 3: Comparison of FIT for different approaches with various constraints on two target ranks R = 15 and R = 40 on real world datasets. Overall, COPA achieves comparable fit to SPARTan while supporting more constraints. The missing purple bar in the forth column is out of memory failure for Helwig method.",Which constraint has the most significant impact on the FIT values for the CMS data set when the target rank is 15?
spiqa_376,1708.00160v2,"Based on Table 4 in the paper discussing the use of linguistic features to improve generalization in neural coreference resolvers, which coreference model achieves the highest F$_1$ score on the CoNLL test set, and does the caption specify that this result is statistically significant compared to other models according to the approximate randomization test?","The ""ensemble"" model of e2ef achieves the highest F$_1$ score of 68.83 on the CoNLL test set. Yes, this performance is statistically significant compared to all other models listed in the table, as indicated by the caption and footnote referencing the approximate randomization test.",1708.00160v2.pdf,"['1708.00160v2.pdf', '1705.09296v2.pdf', '1704.07854v4.pdf', '1805.00912v4.pdf', '1809.02731v3.pdf', '1707.08608v3.pdf', '1805.06447v3.pdf', '1705.09966v2.pdf', '1809.01246v1.pdf']","The table presents the performance of different coreference models on the CoNLL test set using various metrics, including F$_1$ score. By looking at the ""CoNLL"" column and comparing the F$_1$ values, we can identify the ""ensemble"" model of e2e-coref as having the highest score. The caption further clarifies which performance differences are statistically significant, confirming that the ""ensemble"" model's superior performance is statistically meaningful.",1708.00160v2-Table4-1.png,Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers,"Coreference resolution is an intermediate step for text understanding. It is
used in tasks and domains for which we do not necessarily have coreference
annotated corpora. Therefore, generalization is of special importance for
coreference resolution. However, while recent coreference resolvers have
notable improvements on the CoNLL dataset, they struggle to generalize properly
to new domains or datasets. In this paper, we investigate the role of
linguistic features in building more generalizable coreference resolvers. We
show that generalization improves only slightly by merely using a set of
additional linguistic features. However, employing features and subsets of
their values that are informative for coreference resolution, considerably
improves generalization. Thanks to better generalization, our system achieves
state-of-the-art results in out-of-domain evaluations, e.g., on WikiCoref, our
system, which is trained on CoNLL, achieves on-par performance with a system
designed for this dataset.","Table 4: Comparisons on the CoNLL test set. The F1 gains that are statistically significant: (1) “+EPM” compared to “toppairs”, “ranking” and “JIM”, (2) “+EPM” compared to “reinforce” based on MUC, B3 and LEA, (3) “single” compared to “+EPM” based on MUC and B3, and (4) “ensemble” compared to other systems. Significance is measured based on the approximate randomization test (p < 0.05) (Noreen, 1989).",Which coreference model performs best on the CoNLL test set according to the F$_1$ score? Is this performance statistically significant compared to all other models in the table?
spiqa_377,1809.02731v3,"Based on the figure summarizing the corpus statistics in the paper ""Exploiting Invertible Decoders for Unsupervised Sentence Representation Learning,"" which of the two corpora contains more sentences, and by what exact margin?","The UMBC News corpus has more sentences, by approximately 60.5 million.",1809.02731v3.pdf,"['1809.02731v3.pdf', '1805.06447v3.pdf', '1705.02798v6.pdf', '1901.00398v2.pdf', '1611.07718v2.pdf', '1710.06177v2.pdf', '1703.00899v2.pdf', '1603.03833v4.pdf', '1811.09393v4.pdf', '1710.01507v4.pdf']","The table shows that the BookCorpus has 74 million sentences, while the UMBC News corpus has 134.5 million sentences.",1809.02731v3-Table1-1.png,Exploiting Invertible Decoders for Unsupervised Sentence Representation Learning,"The encoder-decoder models for unsupervised sentence representation learning
tend to discard the decoder after being trained on a large unlabelled corpus,
since only the encoder is needed to map the input sentence into a vector
representation. However, parameters learnt in the decoder also contain useful
information about language. In order to utilise the decoder after learning, we
present two types of decoding functions whose inverse can be easily derived
without expensive inverse calculation. Therefore, the inverse of the decoding
function serves as another encoder that produces sentence representations. We
show that, with careful design of the decoding functions, the model learns good
sentence representations, and the ensemble of the representations produced from
the encoder and the inverse of the decoder demonstrate even better
generalisation ability and solid transferability.","Summary statistics of the two corpora used. For simplicity, the two corpora are referred to as B and U in the following tables respectively.","Which corpus has more sentences, and by how much?"
spiqa_378,1804.05938v2,"Based on Table 1 of the ""Unbiased Learning to Rank with Unbiased Propensity Estimation"" paper, which correction method achieved the highest nDCG@10 and ERR@10 scores for DNN models, and what was the exact improvement over the NoCorrect baseline in these metrics?","The DNN trained with DLA achieved the best performance in terms of both nDCG@10 (0.421) and ERR@10 (0.582). Compared to not using any correction method (NoCorrect), DLA shows a significant improvement in both metrics, with nDCG@10 being higher by 0.063 and ERR@10 being higher by 0.082.",1804.05938v2.pdf,"['1804.05938v2.pdf', '1710.01507v4.pdf', '1811.02721v3.pdf', '1611.03780v2.pdf', '1809.01989v2.pdf', '1805.06431v4.pdf', '1704.08615v2.pdf', '1611.05742v3.pdf', '1703.04887v4.pdf', '1812.00108v4.pdf']","Table 1 presents the performance of different correction methods for DNN models trained on real click data. The nDCG@10 and ERR@10 columns specifically show the performance at a cutoff of 10, meaning they measure how well the models rank the top 10 most relevant items. By comparing the values in these columns across different methods, we can identify which method performs best. In this case, DLA shows the highest values for both metrics, indicating superior performance compared to other methods, including no correction at all.",1804.05938v2-Table4-1.png,Unbiased Learning to Rank with Unbiased Propensity Estimation,"Learning to rank with biased click data is a well-known challenge. A variety
of methods has been explored to debias click data for learning to rank such as
click models, result interleaving and, more recently, the unbiased
learning-to-rank framework based on inverse propensity weighting. Despite their
differences, most existing studies separate the estimation of click bias
(namely the \textit{propensity model}) from the learning of ranking algorithms.
To estimate click propensities, they either conduct online result
randomization, which can negatively affect the user experience, or offline
parameter estimation, which has special requirements for click data and is
optimized for objectives (e.g. click likelihood) that are not directly related
to the ranking performance of the system. In this work, we address those
problems by unifying the learning of propensity models and ranking models. We
find that the problem of estimating a propensity model from click data is a
dual problem of unbiased learning to rank. Based on this observation, we
propose a Dual Learning Algorithm (DLA) that jointly learns an unbiased ranker
and an \textit{unbiased propensity model}. DLA is an automatic unbiased
learning-to-rank framework as it directly learns unbiased ranking models from
biased click data without any preprocessing. It can adapt to the change of bias
distributions and is applicable to online learning. Our empirical experiments
with synthetic and real-world data show that the models trained with DLA
significantly outperformed the unbiased learning-to-rank algorithms based on
result randomization and the models trained with relevance signals extracted by
click models.",Table 4: Comparison of DNN trained with DLA and relevance signals extracted by click models. Significant improvements or degradations with respect to DLA are indicated with +/−.,"Which correction method resulted in the best performance in terms of nDCG@10 and ERR@10, and how does it compare to not using any correction method?"
spiqa_379,1809.01246v1,"Based on Table I in the ""Fast and Accurate Graph Stream Summarization"" paper, which data structure achieves the highest update speed (in MIPS) for the email-EuAll dataset?",GSS (no sampling),1809.01246v1.pdf,"['1809.01246v1.pdf', '1708.05239v3.pdf', '1611.04363v2.pdf', '1709.00139v4.pdf', '1709.02755v5.pdf']","The table shows the update speed of different data structures on different datasets. The update speed is measured in MIPS (millions of instructions per second). The higher the MIPS, the faster the update speed. For the email-EuAll dataset, GSS has the highest MIPS (2.2887), which means it is the fastest data structure for updating on this dataset.",1809.01246v1-TableI-1.png,Fast and Accurate Graph Stream Summarization,"A graph stream is a continuous sequence of data items, in which each item
indicates an edge, including its two endpoints and edge weight. It forms a
dynamic graph that changes with every item in the stream. Graph streams play
important roles in cyber security, social networks, cloud troubleshooting
systems and other fields. Due to the vast volume and high update speed of graph
streams, traditional data structures for graph storage such as the adjacency
matrix and the adjacency list are no longer sufficient. However, prior art of
graph stream summarization, like CM sketches, gSketches, TCM and gMatrix,
either supports limited kinds of queries or suffers from poor accuracy of query
results. In this paper, we propose a novel Graph Stream Sketch (GSS for short)
to summarize the graph streams, which has the linear space cost (O(|E|), E is
the edge set of the graph) and the constant update time complexity (O(1)) and
supports all kinds of queries over graph streams with the controllable errors.
Both theoretical analysis and experiment results confirm the superiority of our
solution with regard to the time/space complexity and query results' precision
compared with the state-of-the-art.",TABLE I UPDATE SPEED (MIPS),Which data structure is the fastest for updating on the email-EuAll dataset?
spiqa_380,1705.08016v3,"In the ""Pairwise Confusion for Fine-Grained Visual Classification"" paper, as detailed in Table 3, which dataset—ImageNet-Dogs or ImageNet-Random—exhibited a more significant improvement in classification accuracy after applying the Pairwise Confusion optimization method?",ImageNet-Dogs benefited more from the PC optimization method compared to ImageNet-Random.,1705.08016v3.pdf,"['1705.08016v3.pdf', '1803.04572v2.pdf', '1805.08751v2.pdf', '1706.00633v4.pdf', '1708.06832v3.pdf', '1809.03449v3.pdf', '1811.08257v1.pdf', '1703.04887v4.pdf']","The table shows the classification accuracy with and without PC for both datasets. While both datasets show improvement with PC, the gain in accuracy for ImageNet-Dogs (1.45%) is significantly higher than that for ImageNet-Random (0.54% ± 0.28%). This aligns with the passage's explanation that PC provides a larger benefit for datasets with higher inter-class similarity and intra-class variation, which is the case for ImageNet-Dogs compared to ImageNet-Random.",1705.08016v3-Table3-1.png,Pairwise Confusion for Fine-Grained Visual Classification,"Fine-Grained Visual Classification (FGVC) datasets contain small sample
sizes, along with significant intra-class variation and inter-class similarity.
While prior work has addressed intra-class variation using localization and
segmentation techniques, inter-class similarity may also affect feature
learning and reduce classification performance. In this work, we address this
problem using a novel optimization procedure for the end-to-end neural network
training on FGVC tasks. Our procedure, called Pairwise Confusion (PC) reduces
overfitting by intentionally {introducing confusion} in the activations. With
PC regularization, we obtain state-of-the-art performance on six of the most
widely-used FGVC datasets and demonstrate improved localization ability. {PC}
is easy to implement, does not need excessive hyperparameter tuning during
training, and does not add significant overhead during test time.",Table 3. Experiments with ImageNet and CIFAR show that datasets with large intraclass variation and high inter-class similarity benefit from optimization with Pairwise Confusion. Only the mean accuracy over 3 Imagenet-Random experiments is shown.,Which dataset benefited more from the Pairwise Confusion (PC) optimization method: ImageNet-Dogs or ImageNet-Random?
spiqa_381,1706.03847v3,"Based on the ""Events"" column in the training set section of Table 1, which dataset has the highest number of interactions, how many interactions does it have, and by what factor is it larger than the dataset with the fewest interactions?","The VIDXL dataset contains the most interactions (events) in the training set with 69,312,698 events. This is roughly 7.7 times larger than the RSC15 dataset, which has the least interactions (9,011,321) in the training set. ",1706.03847v3.pdf,"['1706.03847v3.pdf', '1804.05936v2.pdf', '1706.00633v4.pdf', '1709.00139v4.pdf', '1611.03780v2.pdf', '1611.07718v2.pdf', '1803.05776v2.pdf', '1611.05742v3.pdf', '1706.00827v2.pdf']","By looking at the ""Events"" column under the ""Train set"" section of Table 1, we can compare the number of interactions for each dataset. VIDXL clearly has the highest number, while RSC15 has the lowest. To determine the relative size difference, we simply divide the number of events in VIDXL by the number of events in RSC15 (69,312,698 / 9,011,321 ≈ 7.7).",1706.03847v3-Table1-1.png,Recurrent Neural Networks with Top-k Gains for Session-based Recommendations,"RNNs have been shown to be excellent models for sequential data and in
particular for data that is generated by users in an session-based manner. The
use of RNNs provides impressive performance benefits over classical methods in
session-based recommendations. In this work we introduce novel ranking loss
functions tailored to RNNs in the recommendation setting. The improved
performance of these losses over alternatives, along with further tricks and
refinements described in this work, allow for an overall improvement of up to
35% in terms of MRR and Recall@20 over previous session-based RNN solutions and
up to 53% over classical collaborative filtering approaches. Unlike data
augmentation-based improvements, our method does not increase training times
significantly. We further demonstrate the performance gain of the RNN over
baselines in an online A/B test.",Table 1: Properties of the datasets.,Which dataset contains the most interactions (events) in the training set and how much larger is it compared to the dataset with the least interactions in the training set?
spiqa_382,1703.07015v3,"Based on the autocorrelation graphs presented in the figure, which dataset shows the most distinct repeating 24-hour seasonality pattern?",The Traffic dataset.,1703.07015v3.pdf,"['1703.07015v3.pdf', '1802.07222v1.pdf', '1811.02553v4.pdf', '1812.00108v4.pdf', '1710.01507v4.pdf', '1803.05776v2.pdf']","The Traffic dataset's autocorrelation graph shows a clear, repeating pattern with a period of approximately 24 hours. This indicates that the data is strongly seasonal, with values tending to be similar at the same time of day on different days.",1703.07015v3-Figure3-1.png,Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks,"Multivariate time series forecasting is an important machine learning problem
across many domains, including predictions of solar plant energy output,
electricity consumption, and traffic jam situation. Temporal data arise in
these real-world applications often involves a mixture of long-term and
short-term patterns, for which traditional approaches such as Autoregressive
models and Gaussian Process may fail. In this paper, we proposed a novel deep
learning framework, namely Long- and Short-term Time-series network (LSTNet),
to address this open challenge. LSTNet uses the Convolution Neural Network
(CNN) and the Recurrent Neural Network (RNN) to extract short-term local
dependency patterns among variables and to discover long-term patterns for time
series trends. Furthermore, we leverage traditional autoregressive model to
tackle the scale insensitive problem of the neural network model. In our
evaluation on real-world data with complex mixtures of repetitive patterns,
LSTNet achieved significant performance improvements over that of several
state-of-the-art baseline methods. All the data and experiment codes are
available online.",Autocorrelation graphs of sampled variables form four datasets.,Which dataset exhibits the strongest seasonality?
spiqa_383,1812.10735v2,"Based on the data presented in Table 1 of the CAN paper, which dataset—Rest14 or Rest15—features a higher proportion of multi-aspect sentences relative to the total number of sentences?",Rest14 has a higher proportion of sentences containing multiple aspects compared to Rest15.,1812.10735v2.pdf,"['1812.10735v2.pdf', '1802.07351v2.pdf', '1811.10673v1.pdf', '1704.00774v3.pdf', '1803.05776v2.pdf', '1803.04572v2.pdf', '1812.06589v2.pdf', '1708.02153v2.pdf', '1811.08257v1.pdf', '1702.03584v3.pdf', '1809.01989v2.pdf', '1704.05426v4.pdf', '1804.05936v2.pdf']","While both datasets have a majority of single-aspect sentences, we can calculate the percentage of multi-aspect sentences in each dataset by dividing the total number of multi-aspect sentences by the total number of sentences. For Rest14, this is 482 (total multi-aspect) / 2535 (total sentences) = 19.01%. For Rest15, it is 309 (total multi-aspect) / 931 (total sentences) = 16.43%. Therefore, Rest14 has a slightly higher proportion of sentences with multiple aspects.",1812.10735v2-Table1-1.png,CAN: Constrained Attention Networks for Multi-Aspect Sentiment Analysis,"Aspect level sentiment classification is a fine-grained sentiment analysis
task. To detect the sentiment towards a particular aspect in a sentence,
previous studies have developed various attention-based methods for generating
aspect-specific sentence representations. However, the attention may inherently
introduce noise and downgrade the performance. In this paper, we propose
constrained attention networks (CAN), a simple yet effective solution, to
regularize the attention for multi-aspect sentiment analysis, which alleviates
the drawback of the attention mechanism. Specifically, we introduce orthogonal
regularization on multiple aspects and sparse regularization on each single
aspect. Experimental results on two public datasets demonstrate the
effectiveness of our approach. We further extend our approach to multi-task
settings and outperform the state-of-the-art methods.","Table 1: The numbers of single- and multi-aspect sentences. OL and NOL denote the overlapping and nonoverlapping multi-aspect sentences, respectively.",Which dataset has a higher proportion of sentences containing multiple aspects: Rest14 or Rest15?
spiqa_384,1803.03467v4,"Based on the figure in the RippleNet paper that shows AUC results across different ripple set sizes, which dataset consistently achieves the highest AUC across all tested scenarios?",MovieLens-1M,1803.03467v4.pdf,"['1803.03467v4.pdf', '1605.07496v3.pdf', '1804.07849v4.pdf', '1704.07854v4.pdf', '1811.10673v1.pdf', '1702.08694v3.pdf', '1803.05776v2.pdf', '1812.00108v4.pdf', '1705.02946v3.pdf', '1709.02755v5.pdf', '1706.03847v3.pdf', '1612.02803v5.pdf', '1710.01507v4.pdf']"," The table shows the AUC values for three datasets (MovieLens-1M, Book-Crossing, and Bing-News) for different ripple set sizes. The values in the MovieLens-1M row are consistently higher than the values in the other two rows, indicating that MovieLens-1M has the highest AUC for all ripple set sizes.",1803.03467v4-Table4-1.png,RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems,"To address the sparsity and cold start problem of collaborative filtering,
researchers usually make use of side information, such as social networks or
item attributes, to improve recommendation performance. This paper considers
the knowledge graph as the source of side information. To address the
limitations of existing embedding-based and path-based methods for
knowledge-graph-aware recommendation, we propose Ripple Network, an end-to-end
framework that naturally incorporates the knowledge graph into recommender
systems. Similar to actual ripples propagating on the surface of water, Ripple
Network stimulates the propagation of user preferences over the set of
knowledge entities by automatically and iteratively extending a user's
potential interests along links in the knowledge graph. The multiple ""ripples""
activated by a user's historically clicked items are thus superposed to form
the preference distribution of the user with respect to a candidate item, which
could be used for predicting the final clicking probability. Through extensive
experiments on real-world datasets, we demonstrate that Ripple Network achieves
substantial gains in a variety of scenarios, including movie, book and news
recommendation, over several state-of-the-art baselines.",The results of AUC w.r.t. different sizes of a user’s ripple set.,Which dataset has the highest AUC for all ripple set sizes?
spiqa_385,1706.03847v3,"Based on the figure comparing Recall@20 and MRR@20 across four datasets in the ""Recurrent Neural Networks with Top-k Gains for Session-based Recommendations"" paper, which dataset shows the highest performance using unified embeddings?",VIDXL has the highest Recall@20 and MRR@20.,1706.03847v3.pdf,"['1706.03847v3.pdf', '1706.08146v3.pdf', '1703.00060v2.pdf', '1805.04609v3.pdf', '1805.06447v3.pdf', '1705.09296v2.pdf', '1802.07351v2.pdf', '1805.01216v3.pdf', '1707.00189v3.pdf', '1705.02798v6.pdf', '1803.04572v2.pdf', '1606.07384v2.pdf', '1612.02803v5.pdf', '1705.09966v2.pdf', '1812.00108v4.pdf']",The table shows the Recall@20 and MRR@20 for four different datasets. VIDXL has the highest values for both metrics.,1706.03847v3-Table3-1.png,Recurrent Neural Networks with Top-k Gains for Session-based Recommendations,"RNNs have been shown to be excellent models for sequential data and in
particular for data that is generated by users in an session-based manner. The
use of RNNs provides impressive performance benefits over classical methods in
session-based recommendations. In this work we introduce novel ranking loss
functions tailored to RNNs in the recommendation setting. The improved
performance of these losses over alternatives, along with further tricks and
refinements described in this work, allow for an overall improvement of up to
35% in terms of MRR and Recall@20 over previous session-based RNN solutions and
up to 53% over classical collaborative filtering approaches. Unlike data
augmentation-based improvements, our method does not increase training times
significantly. We further demonstrate the performance gain of the RNN over
baselines in an online A/B test.",Results with unified embeddings. Relative improvement over the same network without unified item representation is shown in the parentheses.,Which dataset has the highest Recall@20 and MRR@20?
spiqa_386,1809.00458v1,"Which dataset, as reported in Table II of the paper ""GB-KMV: An Augmented KMV Sketch for Approximate Containment Similarity Search,"" has the highest average record length of 6284?",CaOpenData,1809.00458v1.pdf,"['1809.00458v1.pdf', '1809.01246v1.pdf', '1706.00633v4.pdf', '1703.00060v2.pdf', '1705.02946v3.pdf', '1611.04684v1.pdf', '1707.00524v2.pdf', '1906.10843v1.pdf']","The table shows the average record length for each dataset. The highest average record length is 6284, which is for the CaOpenData dataset.",1809.00458v1-TableII-1.png,GB-KMV: An Augmented KMV Sketch for Approximate Containment Similarity Search,"In this paper, we study the problem of approximate containment similarity
search. Given two records Q and X, the containment similarity between Q and X
with respect to Q is |Q intersect X|/ |Q|. Given a query record Q and a set of
records S, the containment similarity search finds a set of records from S
whose containment similarity regarding Q are not less than the given threshold.
This problem has many important applications in commercial and scientific
fields such as record matching and domain search. Existing solution relies on
the asymmetric LSH method by transforming the containment similarity to
well-studied Jaccard similarity. In this paper, we use a different framework by
transforming the containment similarity to set intersection. We propose a novel
augmented KMV sketch technique, namely GB-KMV, which is data-dependent and can
achieve a good trade-off between the sketch size and the accuracy. We provide a
set of theoretical analysis to underpin the proposed augmented KMV sketch
technique, and show that it outperforms the state-of-the-art technique LSH-E in
terms of estimation accuracy under practical assumption. Our comprehensive
experiments on real-life datasets verify that GB-KMV is superior to LSH-E in
terms of the space-accuracy trade-off, time-accuracy trade-off, and the sketch
construction time. For instance, with similar estimation accuracy (F-1 score),
GB-KMV is over 100 times faster than LSH-E on some real-life dataset.",TABLE II. CHARACTERISTICS OF DATASETS,Which dataset has the highest average record length?
spiqa_387,1705.08016v3,"According to the figure comparing FGVC and LSVC datasets in ""Pairwise Confusion for Fine-Grained Visual Classification,"" which dataset has the highest number of samples per class?",SVHN,1705.08016v3.pdf,"['1705.08016v3.pdf', '1805.04609v3.pdf', '1803.03467v4.pdf', '1705.02798v6.pdf', '1706.04269v2.pdf', '1805.00912v4.pdf', '1804.00863v3.pdf', '1805.02349v2.pdf', '1708.06832v3.pdf', '1703.00060v2.pdf', '1701.03077v10.pdf', '1709.02418v2.pdf', '1608.02784v2.pdf']",The table shows the number of samples per class for each dataset. SVHN has the highest number of samples per class with 7325.7.,1705.08016v3-Table1-1.png,Pairwise Confusion for Fine-Grained Visual Classification,"Fine-Grained Visual Classification (FGVC) datasets contain small sample
sizes, along with significant intra-class variation and inter-class similarity.
While prior work has addressed intra-class variation using localization and
segmentation techniques, inter-class similarity may also affect feature
learning and reduce classification performance. In this work, we address this
problem using a novel optimization procedure for the end-to-end neural network
training on FGVC tasks. Our procedure, called Pairwise Confusion (PC) reduces
overfitting by intentionally {introducing confusion} in the activations. With
PC regularization, we obtain state-of-the-art performance on six of the most
widely-used FGVC datasets and demonstrate improved localization ability. {PC}
is easy to implement, does not need excessive hyperparameter tuning during
training, and does not add significant overhead during test time.",A comparison of fine-grained visual classification (FGVC) datasets with largescale visual classification (LSVC) datasets. FGVC datasets are significantly smaller and noisier than LSVC datasets.,Which dataset has the highest number of samples per class?
spiqa_388,1703.07015v3,"Referring to Table 1, which dataset in the LSTNet experiments has the smallest sample rate ($L$), resulting in the highest temporal resolution and providing the most frequent data points?",The solar dataset has the highest temporal resolution.,1703.07015v3.pdf,"['1703.07015v3.pdf', '1611.02654v2.pdf', '1804.07849v4.pdf', '1710.05654v2.pdf', '1809.03449v3.pdf', '1702.03584v3.pdf', '1707.00524v2.pdf', '1611.03780v2.pdf', '1611.07718v2.pdf', '1805.04687v2.pdf', '1707.01922v5.pdf']","The sample rate, represented by $L$ in the table, indicates how frequently data points are collected for each dataset. The solar dataset has a sample rate of 10 minutes, which is the smallest interval compared to the other datasets. This means that the solar dataset provides more data points per unit of time than any other dataset in the table.",1703.07015v3-Table1-1.png,Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks,"Multivariate time series forecasting is an important machine learning problem
across many domains, including predictions of solar plant energy output,
electricity consumption, and traffic jam situation. Temporal data arise in
these real-world applications often involves a mixture of long-term and
short-term patterns, for which traditional approaches such as Autoregressive
models and Gaussian Process may fail. In this paper, we proposed a novel deep
learning framework, namely Long- and Short-term Time-series network (LSTNet),
to address this open challenge. LSTNet uses the Convolution Neural Network
(CNN) and the Recurrent Neural Network (RNN) to extract short-term local
dependency patterns among variables and to discover long-term patterns for time
series trends. Furthermore, we leverage traditional autoregressive model to
tackle the scale insensitive problem of the neural network model. In our
evaluation on real-world data with complex mixtures of repetitive patterns,
LSTNet achieved significant performance improvements over that of several
state-of-the-art baseline methods. All the data and experiment codes are
available online.","Table 1: Dataset Statistics, whereT is length of time series,D is number of variables, L is the sample rate.","Which dataset has the highest temporal resolution, meaning it provides data points at the most frequent intervals?"
spiqa_389,1707.01917v2,"In the figure detailing the hyperparameters for the datasets used in the Higher-order Relation Schema Induction experiments, which dataset has the highest λa value according to the TFBA model?",The NYT Sports dataset has the highest value for λa (0.9).,1707.01917v2.pdf,"['1707.01917v2.pdf', '1811.07073v3.pdf', '1706.08146v3.pdf', '1708.05239v3.pdf', '1901.00398v2.pdf', '1802.07459v2.pdf', '1805.06431v4.pdf', '1805.08751v2.pdf', '1803.03467v4.pdf']",The table shows the values of the hyperparameters for each dataset. The NYT Sports dataset has the highest value for λa.,1707.01917v2-Table3-1.png,Higher-order Relation Schema Induction using Tensor Factorization with Back-off and Aggregation,"Relation Schema Induction (RSI) is the problem of identifying type signatures
of arguments of relations from unlabeled text. Most of the previous work in
this area have focused only on binary RSI, i.e., inducing only the subject and
object type signatures per relation. However, in practice, many relations are
high-order, i.e., they have more than two arguments and inducing type
signatures of all arguments is necessary. For example, in the sports domain,
inducing a schema win(WinningPlayer, OpponentPlayer, Tournament, Location) is
more informative than inducing just win(WinningPlayer, OpponentPlayer). We
refer to this problem as Higher-order Relation Schema Induction (HRSI). In this
paper, we propose Tensor Factorization with Back-off and Aggregation (TFBA), a
novel framework for the HRSI problem. To the best of our knowledge, this is the
first attempt at inducing higher-order relation schemata from unlabeled text.
Using the experimental analysis on three real world datasets, we show how TFBA
helps in dealing with sparsity and induce higher order schemata.",Details of hyper-parameters set for different datasets.,Which dataset has the highest value for the hyperparameter  λa?
spiqa_390,1803.04572v2,"Based on the figure summarizing dataset statistics in the ""COPA: Constrained PARAFAC2 for Sparse & Large Datasets"" paper, which dataset records the highest maximum number of clinical visits per patient, reaching up to 1500?",CMS,1803.04572v2.pdf,"['1803.04572v2.pdf', '1804.05936v2.pdf', '1703.07015v3.pdf', '1706.00633v4.pdf', '1703.02507v3.pdf', '1708.00160v2.pdf', '1612.02803v5.pdf', '1707.00189v3.pdf', '1802.07351v2.pdf', '1702.08694v3.pdf', '1603.00286v5.pdf', '1704.04539v2.pdf', '1906.10843v1.pdf', '1809.03550v3.pdf', '1608.02784v2.pdf']","The table shows that the maximum number of clinical visits for the CMS dataset is 1500, which is larger than the maximum number of clinical visits for the CHOA dataset (857).",1803.04572v2-Table3-1.png,COPA: Constrained PARAFAC2 for Sparse & Large Datasets,"PARAFAC2 has demonstrated success in modeling irregular tensors, where the
tensor dimensions vary across one of the modes. An example scenario is modeling
treatments across a set of patients with the varying number of medical
encounters over time. Despite recent improvements on unconstrained PARAFAC2,
its model factors are usually dense and sensitive to noise which limits their
interpretability. As a result, the following open challenges remain: a) various
modeling constraints, such as temporal smoothness, sparsity and non-negativity,
are needed to be imposed for interpretable temporal modeling and b) a scalable
approach is required to support those constraints efficiently for large
datasets. To tackle these challenges, we propose a {\it CO}nstrained {\it
PA}RAFAC2 (COPA) method, which carefully incorporates optimization constraints
such as temporal smoothness, sparsity, and non-negativity in the resulting
factors. To efficiently support all those constraints, COPA adopts a hybrid
optimization framework using alternating optimization and alternating direction
method of multiplier (AO-ADMM). As evaluated on large electronic health record
(EHR) datasets with hundreds of thousands of patients, COPA achieves
significant speedups (up to 36 times faster) over prior PARAFAC2 approaches
that only attempt to handle a subset of the constraints that COPA enables.
Overall, our method outperforms all the baselines attempting to handle a subset
of the constraints in terms of speed, while achieving the same level of
accuracy. Through a case study on temporal phenotyping of medically complex
children, we demonstrate how the constraints imposed by COPA reveal concise
phenotypes and meaningful temporal profiles of patients. The clinical
interpretation of both the phenotypes and the temporal profiles was confirmed
by a medical expert.","Summary statistics of real datasets that we used in the experiments. K denotes the number of patients, J is the number of medical features and Ik denotes the number of clinical visits for kth patient.",Which dataset has the largest number of clinical visits per patient?
spiqa_391,1803.03467v4,"Referring to the figure showing the basic statistics of the datasets in the RippleNet paper, which dataset contains the highest number of 4-hop triples, exceeding six million?",Bing-News.,1803.03467v4.pdf,"['1803.03467v4.pdf', '1901.00056v2.pdf', '1705.02798v6.pdf', '1703.07015v3.pdf', '1809.04276v2.pdf', '1811.10673v1.pdf', '1812.10735v2.pdf']","The table shows the number of 4-hop triples for each dataset. Bing-News has the highest number of 4-hop triples, with 6,322,548.",1803.03467v4-Table1-1.png,RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems,"To address the sparsity and cold start problem of collaborative filtering,
researchers usually make use of side information, such as social networks or
item attributes, to improve recommendation performance. This paper considers
the knowledge graph as the source of side information. To address the
limitations of existing embedding-based and path-based methods for
knowledge-graph-aware recommendation, we propose Ripple Network, an end-to-end
framework that naturally incorporates the knowledge graph into recommender
systems. Similar to actual ripples propagating on the surface of water, Ripple
Network stimulates the propagation of user preferences over the set of
knowledge entities by automatically and iteratively extending a user's
potential interests along links in the knowledge graph. The multiple ""ripples""
activated by a user's historically clicked items are thus superposed to form
the preference distribution of the user with respect to a candidate item, which
could be used for predicting the final clicking probability. Through extensive
experiments on real-world datasets, we demonstrate that Ripple Network achieves
substantial gains in a variety of scenarios, including movie, book and news
recommendation, over several state-of-the-art baselines.",Basic statistics of the three datasets.,Which dataset has the most 4-hop triples?
spiqa_392,1812.00281v3,"Referring to the figure in this paper that compares the distribution of head, gaze, and eye poses across multiple datasets, which dataset shows the most concentrated clustering near the origin in the yaw-pitch angle space?",MPII-Gaze,1812.00281v3.pdf,"['1812.00281v3.pdf', '1708.06832v3.pdf', '1805.02349v2.pdf', '1809.03550v3.pdf', '1703.02507v3.pdf', '1611.05742v3.pdf', '1704.00774v3.pdf', '1804.04786v3.pdf', '1811.02721v3.pdf', '1803.03467v4.pdf', '1709.02418v2.pdf', '1811.06635v1.pdf']","The figure shows the distribution of gaze, head pose, and eye pose for four different datasets. The MPII-Gaze dataset has the most concentrated distribution of gaze and head pose, as the heatmap is more concentrated around the center.",1812.00281v3-Figure6-1.png,HUMBI: A Large Multiview Dataset of Human Body Expressions,"This paper presents a new large multiview dataset called HUMBI for human body
expressions with natural clothing. The goal of HUMBI is to facilitate modeling
view-specific appearance and geometry of gaze, face, hand, body, and garment
from assorted people. 107 synchronized HD cameras are used to capture 772
distinctive subjects across gender, ethnicity, age, and physical condition.
With the multiview image streams, we reconstruct high fidelity body expressions
using 3D mesh models, which allows representing view-specific appearance using
their canonical atlas. We demonstrate that HUMBI is highly effective in
learning and reconstructing a complete human model and is complementary to the
existing datasets of human body expressions with limited views and subjects
such as MPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets.","Distribution of head pose, gaze and eye pose in normalized space for MPII-Gaze, UT-Multiview, RT-GENE and HUMBI. Horizontal and vertical axis represent yaw and pitch angle respectively (unit: degree).",Which dataset has the most concentrated distribution of gaze and head pose?
spiqa_393,1901.00056v2,"According to the ""Dataset Statistics"" figure in the *Entity Synonym Discovery via Multipiece Bilateral Context Matching* paper, which dataset contains the largest number of entities?",MedBook + MKG,1901.00056v2.pdf,"['1901.00056v2.pdf', '1705.07384v2.pdf', '1802.07222v1.pdf', '1804.07849v4.pdf', '1809.02731v3.pdf', '1707.01917v2.pdf']","The table shows that the MedBook + MKG dataset has 32,002 entities, which is more than any other dataset.",1901.00056v2-Table1-1.png,Entity Synonym Discovery via Multipiece Bilateral Context Matching,"Being able to automatically discover synonymous entities in an open-world
setting benefits various tasks such as entity disambiguation or knowledge graph
canonicalization. Existing works either only utilize entity features, or rely
on structured annotations from a single piece of context where the entity is
mentioned. To leverage diverse contexts where entities are mentioned, in this
paper, we generalize the distributional hypothesis to a multi-context setting
and propose a synonym discovery framework that detects entity synonyms from
free-text corpora with considerations on effectiveness and robustness. As one
of the key components in synonym discovery, we introduce a neural network model
SYNONYMNET to determine whether or not two given entities are synonym with each
other. Instead of using entities features, SYNONYMNET makes use of multiple
pieces of contexts in which the entity is mentioned, and compares the
context-level similarity via a bilateral matching schema. Experimental results
demonstrate that the proposed model is able to detect synonym sets that are not
observed during training on both generic and domain-specific datasets:
Wiki+Freebase, PubMed+UMLS, and MedBook+MKG, with up to 4.16% improvement in
terms of Area Under the Curve and 3.19% in terms of Mean Average Precision
compared to the best baseline method.",Dataset Statistics.,Which dataset has the most entities?
spiqa_394,1805.04687v2,"Based on the lane marking statistics figure in the paper, which dataset is reported to have the highest number of annotated lane markings?",BDD100K,1805.04687v2.pdf,"['1805.04687v2.pdf', '1703.02507v3.pdf', '1709.02418v2.pdf', '1804.05995v2.pdf', '1708.06832v3.pdf', '1703.00060v2.pdf', '1702.08694v3.pdf', '1811.02553v4.pdf', '1710.01507v4.pdf', '1705.10667v4.pdf', '1809.03550v3.pdf', '1805.07567v2.pdf']","The table shows that BDD100K has 100,000 lane marking annotations, which is more than any other dataset in the table.",1805.04687v2-Table1-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.",Lane marking statistics. Our lane marking annotations are significantly richer and are more diverse.,Which dataset has the most lane marking annotations?
spiqa_395,1703.02507v3,"In the figure comparing average sentence lengths across datasets in the study on unsupervised sentence embeddings using compositional n-gram features, which dataset shows the shortest average sentence length?",Headlines.,1703.02507v3.pdf,"['1703.02507v3.pdf', '1805.07567v2.pdf', '1802.07351v2.pdf', '1809.02731v3.pdf', '1708.01425v4.pdf', '1707.06320v2.pdf', '1803.04572v2.pdf', '1812.06589v2.pdf']",The table shows the average sentence lengths for the different datasets. The Headlines dataset has the lowest average sentence length of 7.82.,1703.02507v3-Table8-1.png,Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features,"The recent tremendous success of unsupervised word embeddings in a multitude
of applications raises the obvious question if similar methods could be derived
to improve embeddings (i.e. semantic representations) of word sequences as
well. We present a simple but efficient unsupervised objective to train
distributed representations of sentences. Our method outperforms the
state-of-the-art unsupervised models on most benchmark tasks, highlighting the
robustness of the produced general-purpose sentence embeddings.",Average sentence lengths for the datasets used in the comparison.,Which dataset has the shortest average sentence length?
spiqa_396,1812.00281v3,"Based on Table 7 in the HUMBI dataset paper, which individual dataset achieves the highest AUC for 3D body keypoint prediction, and how does its performance compare to models trained on combined datasets, such as HUMBI+H36M or HUMBI+MI3D?","HUMBI performs best when used alone for training, with an average AUC of 0.399. While this is lower than the average AUC of models trained on combined datasets (0.433 for H36M+HUMBI and 0.413 for MI3D+HUMBI), HUMBI still achieves the highest score among the individual datasets.",1812.00281v3.pdf,"['1812.00281v3.pdf', '1812.00108v4.pdf', '1611.03780v2.pdf', '1803.05776v2.pdf', '1805.04687v2.pdf', '1809.00263v5.pdf', '1708.00160v2.pdf', '1803.01128v3.pdf', '1708.05239v3.pdf']","The table shows the cross-data evaluation results for 3D body keypoint prediction. The diagonal cells represent the performance of models trained and tested on the same dataset. By comparing the average AUC values in the last row, we can see that HUMBI outperforms H36M and MI3D when used alone. However, combining HUMBI with either H36M or MI3D further improves the performance, suggesting that these datasets offer complementary information for training.",1812.00281v3-Table7-1.png,HUMBI: A Large Multiview Dataset of Human Body Expressions,"This paper presents a new large multiview dataset called HUMBI for human body
expressions with natural clothing. The goal of HUMBI is to facilitate modeling
view-specific appearance and geometry of gaze, face, hand, body, and garment
from assorted people. 107 synchronized HD cameras are used to capture 772
distinctive subjects across gender, ethnicity, age, and physical condition.
With the multiview image streams, we reconstruct high fidelity body expressions
using 3D mesh models, which allows representing view-specific appearance using
their canonical atlas. We demonstrate that HUMBI is highly effective in
learning and reconstructing a complete human model and is complementary to the
existing datasets of human body expressions with limited views and subjects
such as MPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets.",Table 7: Cross-data evaluation results of 3D body keypoint prediction. Metric is AUC of PCK calculated over an error range of 0-150 mm.,"Which dataset performs best when used alone for training a 3D body keypoint prediction model, and how does its performance compare to models trained on combined datasets?"
spiqa_397,1704.07121v2,"Based on the summary in Table 2, which Visual QA dataset is identified as presenting the greatest challenge for models due to having the highest number of decoys per triplet, and how does this characteristic impact model performance in distinguishing correct answers from decoys?",The VQA dataset presents the biggest challenge.,1704.07121v2.pdf,"['1704.07121v2.pdf', '1809.04276v2.pdf', '1803.06506v3.pdf', '1811.07073v3.pdf', '1805.01216v3.pdf', '1906.10843v1.pdf', '1703.04887v4.pdf', '1805.02349v2.pdf', '1812.06589v2.pdf', '1707.06320v2.pdf', '1906.06589v3.pdf', '1812.10735v2.pdf', '1705.10667v4.pdf']","The table shows that VQA has the highest number of decoys per triplet (17), compared to Visual7W (3) and VG (-). This means that for each real triplet in VQA, there are 17 other plausible, but incorrect, triplets created as decoys. This makes it significantly harder for a model to accurately identify the true triplet among the many similar options.",1704.07121v2-Table2-1.png,Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets,"Visual question answering (Visual QA) has attracted a lot of attention
lately, seen essentially as a form of (visual) Turing test that artificial
intelligence should strive to achieve. In this paper, we study a crucial
component of this task: how can we design good datasets for the task? We focus
on the design of multiple-choice based datasets where the learner has to select
the right answer from a set of candidate ones including the target (\ie the
correct one) and the decoys (\ie the incorrect ones). Through careful analysis
of the results attained by state-of-the-art learning models and human
annotators on existing datasets, we show that the design of the decoy answers
has a significant impact on how and what the learning models learn from the
datasets. In particular, the resulting learner can ignore the visual
information, the question, or both while still doing well on the task. Inspired
by this, we propose automatic procedures to remedy such design deficiencies. We
apply the procedures to re-construct decoy answers for two popular Visual QA
datasets as well as to create a new Visual QA dataset from the Visual Genome
project, resulting in the largest dataset for this task. Extensive empirical
studies show that the design deficiencies have been alleviated in the remedied
datasets and the performance on them is likely a more faithful indicator of the
difference among learning models. The datasets are released and publicly
available via http://www.teds.usc.edu/website_vqa/.",Table 2: Summary of Visual QA datasets.,"Which dataset presents the biggest challenge for a model trying to distinguish true triplets from decoys, and why?"
spiqa_398,1812.00281v3,"Referring to Table 1 in this paper, which dataset uniquely captures facial expressions, full-body motion, and natural clothing in a real-world setting using multiple synchronized cameras?","HUMBI is the only dataset that provides data for both facial expressions and full-body motion capture, including clothing, in a natural setting. ",1812.00281v3.pdf,"['1812.00281v3.pdf', '1702.03584v3.pdf', '1605.07496v3.pdf', '1805.08751v2.pdf', '1803.06506v3.pdf', '1906.06589v3.pdf', '1906.10843v1.pdf', '1805.06447v3.pdf', '1706.04284v3.pdf', '1603.00286v5.pdf']","Table 1 summarizes various human body expression datasets and their features. By looking at the columns for ""Face,"" ""Body,"" and ""Cloth,"" we can identify datasets containing relevant data. However, we also need to consider the ""Measurement method"" and the information in the caption to determine if the data is captured in a natural setting or synthesized. 

- Several datasets provide facial expression data (e.g., CMU Multi-PIE, 3DMM), but they lack full-body motion capture. 
- Datasets like CMU Mocap and Human 3.6M offer body motion capture but lack facial expression data. 
- While datasets like INRIA and BUFF include both face and body data in a natural setting, they lack clothing information. 
- HUMBI is the only dataset that has checkmarks for ""Face,"" ""Body,"" and ""Cloth"" columns and explicitly mentions ""natural"" in the ""Cloth"" column, indicating that it captures all these aspects in a real-world setting.",1812.00281v3-Table1-1.png,HUMBI: A Large Multiview Dataset of Human Body Expressions,"This paper presents a new large multiview dataset called HUMBI for human body
expressions with natural clothing. The goal of HUMBI is to facilitate modeling
view-specific appearance and geometry of gaze, face, hand, body, and garment
from assorted people. 107 synchronized HD cameras are used to capture 772
distinctive subjects across gender, ethnicity, age, and physical condition.
With the multiview image streams, we reconstruct high fidelity body expressions
using 3D mesh models, which allows representing view-specific appearance using
their canonical atlas. We demonstrate that HUMBI is highly effective in
learning and reconstructing a complete human model and is complementary to the
existing datasets of human body expressions with limited views and subjects
such as MPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets.",Table 1: Human body expression datasets.,"Which dataset provides data for **both** facial expressions and full-body motion capture, including clothing, in a natural setting (not synthesized)? "
spiqa_399,1809.00458v1,"Based on the space usage (%) reported in TABLE III of the GB-KMV paper, which dataset demonstrates the highest storage demand when using the LSH-E method?",REUTERS,1809.00458v1.pdf,"['1809.00458v1.pdf', '1708.01425v4.pdf', '1703.02507v3.pdf', '1707.00524v2.pdf', '1705.02798v6.pdf', '1804.05936v2.pdf', '1805.06431v4.pdf', '1804.05995v2.pdf', '1811.07073v3.pdf']","The table shows the space usage (%) for each dataset and method. For the LSH-E method, the REUTERS dataset has the highest space usage of 329%.",1809.00458v1-TableIII-1.png,GB-KMV: An Augmented KMV Sketch for Approximate Containment Similarity Search,"In this paper, we study the problem of approximate containment similarity
search. Given two records Q and X, the containment similarity between Q and X
with respect to Q is |Q intersect X|/ |Q|. Given a query record Q and a set of
records S, the containment similarity search finds a set of records from S
whose containment similarity regarding Q are not less than the given threshold.
This problem has many important applications in commercial and scientific
fields such as record matching and domain search. Existing solution relies on
the asymmetric LSH method by transforming the containment similarity to
well-studied Jaccard similarity. In this paper, we use a different framework by
transforming the containment similarity to set intersection. We propose a novel
augmented KMV sketch technique, namely GB-KMV, which is data-dependent and can
achieve a good trade-off between the sketch size and the accuracy. We provide a
set of theoretical analysis to underpin the proposed augmented KMV sketch
technique, and show that it outperforms the state-of-the-art technique LSH-E in
terms of estimation accuracy under practical assumption. Our comprehensive
experiments on real-life datasets verify that GB-KMV is superior to LSH-E in
terms of the space-accuracy trade-off, time-accuracy trade-off, and the sketch
construction time. For instance, with similar estimation accuracy (F-1 score),
GB-KMV is over 100 times faster than LSH-E on some real-life dataset.",TABLE III. THE SPACE USAGE(%),Which dataset requires the most storage space when using the LSH-E method?
spiqa_400,1809.00458v1,"Referring to the figure on the effect of buffer size in the GB-KMV paper, which dataset reveals the greatest variance in F1 score as buffer size increases among the approximate algorithms tested?",ENRON,1809.00458v1.pdf,"['1809.00458v1.pdf', '1706.04269v2.pdf', '1809.04276v2.pdf', '1603.00286v5.pdf', '1705.09296v2.pdf', '1611.04684v1.pdf', '1704.05958v2.pdf', '1805.04687v2.pdf', '1906.06589v3.pdf', '1707.00524v2.pdf', '1707.06320v2.pdf', '1705.07164v8.pdf', '1802.07351v2.pdf']","The plot for ENRON shows a larger change in F1 score as the buffer size increases, indicating higher variance. This can be seen by comparing the slopes of the lines connecting the data points in the two plots.",1809.00458v1-Figure5-1.png,GB-KMV: An Augmented KMV Sketch for Approximate Containment Similarity Search,"In this paper, we study the problem of approximate containment similarity
search. Given two records Q and X, the containment similarity between Q and X
with respect to Q is |Q intersect X|/ |Q|. Given a query record Q and a set of
records S, the containment similarity search finds a set of records from S
whose containment similarity regarding Q are not less than the given threshold.
This problem has many important applications in commercial and scientific
fields such as record matching and domain search. Existing solution relies on
the asymmetric LSH method by transforming the containment similarity to
well-studied Jaccard similarity. In this paper, we use a different framework by
transforming the containment similarity to set intersection. We propose a novel
augmented KMV sketch technique, namely GB-KMV, which is data-dependent and can
achieve a good trade-off between the sketch size and the accuracy. We provide a
set of theoretical analysis to underpin the proposed augmented KMV sketch
technique, and show that it outperforms the state-of-the-art technique LSH-E in
terms of estimation accuracy under practical assumption. Our comprehensive
experiments on real-life datasets verify that GB-KMV is superior to LSH-E in
terms of the space-accuracy trade-off, time-accuracy trade-off, and the sketch
construction time. For instance, with similar estimation accuracy (F-1 score),
GB-KMV is over 100 times faster than LSH-E on some real-life dataset.","Effect of Buffer Size Approximate Algorithms. In the experiments, the approximate algorithms evaluated are as follows.",Which dataset shows a higher variance in F1 score with increasing buffer size?
spiqa_401,1701.03077v10,"Based on Figure 6, which dataset shows the largest variation in Adjusted Mutual Information (AMI) values across different values of the shape parameter α?",RCV1,1701.03077v10.pdf,"['1701.03077v10.pdf', '1809.03550v3.pdf', '1704.05958v2.pdf', '1706.08146v3.pdf', '1702.08694v3.pdf', '1611.07718v2.pdf', '1611.04684v1.pdf', '1703.07015v3.pdf', '1811.08257v1.pdf', '1709.02418v2.pdf', '1811.07073v3.pdf']","The RCV1 dataset shows the largest range of AMI values across the different values of $\power$, indicating that the performance of the algorithm on this dataset is highly dependent on the choice of $\power$.",1701.03077v10-Figure6-1.png,A General and Adaptive Robust Loss Function,"We present a generalization of the Cauchy/Lorentzian, Geman-McClure,
Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2
loss functions. By introducing robustness as a continuous parameter, our loss
function allows algorithms built around robust loss minimization to be
generalized, which improves performance on basic vision tasks such as
registration and clustering. Interpreting our loss as the negative log of a
univariate density yields a general probability distribution that includes
normal and Cauchy distributions as special cases. This probabilistic
interpretation enables the training of neural networks in which the robustness
of the loss automatically adapts itself during training, which improves
performance on learning-based tasks such as generative image synthesis and
unsupervised monocular depth estimation, without requiring any manual parameter
tuning.","Figure 6. Performance (higher is better) of our gRCC algorithm on the clustering task of [32], for different values of our shape parameter α, with the highest-accuracy point indicated by a dot. Because the baseline RCC algorithm is equivalent to gRCC with α = −2, we highlight that α value with a dashed line and a square.",Which dataset shows the greatest sensitivity to the choice of $\power$?
spiqa_402,1804.05936v2,"Referring to Table 1 in the paper, which dataset has the smallest number of queries and documents, making it the most suitable for training a learning-to-rank model under limited computational resources?",Microsoft 10k would be the most suitable dataset for training with limited computational resources.,1804.05936v2.pdf,"['1804.05936v2.pdf', '1703.04887v4.pdf', '1611.02654v2.pdf', '1804.05938v2.pdf', '1809.04276v2.pdf', '1704.07121v2.pdf', '1809.03550v3.pdf']","The table shows that Microsoft 10k has the smallest number of queries and documents compared to the other two datasets. This implies that training a model on this dataset would require less memory and processing power, making it a better choice when computational resources are limited.",1804.05936v2-Table1-1.png,Learning a Deep Listwise Context Model for Ranking Refinement,"Learning to rank has been intensively studied and widely applied in
information retrieval. Typically, a global ranking function is learned from a
set of labeled data, which can achieve good performance on average but may be
suboptimal for individual queries by ignoring the fact that relevant documents
for different queries may have different distributions in the feature space.
Inspired by the idea of pseudo relevance feedback where top ranked documents,
which we refer as the \textit{local ranking context}, can provide important
information about the query's characteristics, we propose to use the inherent
feature distributions of the top results to learn a Deep Listwise Context Model
that helps us fine tune the initial ranked list. Specifically, we employ a
recurrent neural network to sequentially encode the top results using their
feature vectors, learn a local context model and use it to re-rank the top
results. There are three merits with our model: (1) Our model can capture the
local ranking context based on the complex interactions between top results
using a deep neural network; (2) Our model can be built upon existing
learning-to-rank methods by directly using their extracted feature vectors; (3)
Our model is trained with an attention-based loss function, which is more
effective and efficient than many existing listwise methods. Experimental
results show that the proposed model can significantly improve the
state-of-the-art learning to rank methods on benchmark retrieval corpora.","Table 1: The characteristics of learning-to-rank datasets used in our experiments: number of queries, documents, relevance levels, features and year of release.","Which dataset would be most suitable for training a learning-to-rank model with limited computational resources, and why?"
spiqa_403,1706.08146v3,"""According to Table 1 and the impact of feature size on computational cost, which dataset with the highest number of features is predicted to benefit the most from the Fac.-Recover approach compared to Recover-Fac. for NMF on compressed gene expression data?""",The Leukemia dataset would likely benefit the most from using the Fac.-Recover approach.,1706.08146v3.pdf,"['1706.08146v3.pdf', '1709.02418v2.pdf', '1805.04687v2.pdf', '1802.07222v1.pdf', '1706.04284v3.pdf', '1710.05654v2.pdf', '1707.01922v5.pdf', '1706.00827v2.pdf', '1809.02731v3.pdf', '1603.03833v4.pdf', '1612.02803v5.pdf']","The passage states that the computation time for NMF is dominated by the cost of solving instances of the LP, which is directly related to the number of features in the dataset. Table 1 shows that the Leukemia dataset has by far the largest number of features (54,675) compared to the other two datasets. Since \textsc{Fac.-Recover} only needs to run sparse recovery for a smaller number of features (r) compared to the full set of features (m) required by \textsc{Recover-Fac.}, it would likely offer significant computational savings for the Leukemia dataset.",1706.08146v3-Table1-1.png,Compressed Factorization: Fast and Accurate Low-Rank Factorization of Compressively-Sensed Data,"What learning algorithms can be run directly on compressively-sensed data? In
this work, we consider the question of accurately and efficiently computing
low-rank matrix or tensor factorizations given data compressed via random
projections. We examine the approach of first performing factorization in the
compressed domain, and then reconstructing the original high-dimensional
factors from the recovered (compressed) factors. In both the matrix and tensor
settings, we establish conditions under which this natural approach will
provably recover the original factors. While it is well-known that random
projections preserve a number of geometric properties of a dataset, our work
can be viewed as showing that they can also preserve certain solutions of
non-convex, NP-Hard problems like non-negative matrix factorization. We support
these theoretical results with experiments on synthetic data and demonstrate
the practical applicability of compressed factorization on real-world gene
expression and EEG time series datasets.","Table 1: Summary of DNA microarray gene expression datasets, along with runtime (seconds) for each stage of the NMF pipeline on compressed data. Factorize-Recover runs only r instances of sparse recovery, as opposed to the m instances used by the alternative, Recover-Factorize.",Which dataset would likely benefit the most from using the Fac.-Recover approach instead of Recover-Fac. in terms of computational efficiency?
spiqa_404,1702.08694v3,"Referring to Table 2, which dataset, with 30 features and over a billion potential feature combinations (1,073,741,824), would require the most computational resources for analysis by C-Tarone?","The ""wdbc"" dataset would likely require the most computational resources for C-Tarone to analyze.",1702.08694v3.pdf,"['1702.08694v3.pdf', '1804.05995v2.pdf', '1906.06589v3.pdf', '1802.07222v1.pdf', '1803.04383v2.pdf', '1709.08294v3.pdf', '1703.00060v2.pdf', '1805.01216v3.pdf', '1804.07849v4.pdf', '1805.08751v2.pdf', '1611.04363v2.pdf', '1705.07164v8.pdf', '1704.04539v2.pdf', '1705.07384v2.pdf', '1703.02507v3.pdf']","The table shows that the ""wdbc"" dataset has the largest number of candidate combinations (search space) at 1,073,741,824. This means that C-Tarone needs to explore over a billion possible combinations of features during its analysis, which is significantly higher than any other dataset listed. The passage also mentions that the search space grows exponentially with the number of features (d). Since ""wdbc"" has a relatively high number of features (30), this contributes to its massive search space and the need for greater computational resources.",1702.08694v3-Table2-1.png,Finding Statistically Significant Interactions between Continuous Features,"The search for higher-order feature interactions that are statistically
significantly associated with a class variable is of high relevance in fields
such as Genetics or Healthcare, but the combinatorial explosion of the
candidate space makes this problem extremely challenging in terms of
computational efficiency and proper correction for multiple testing. While
recent progress has been made regarding this challenge for binary features, we
here present the first solution for continuous features. We propose an
algorithm which overcomes the combinatorial explosion of the search space of
higher-order interactions by deriving a lower bound on the p-value for each
interaction, which enables us to massively prune interactions that can never
reach significance and to thereby gain more statistical power. In our
experiments, our approach efficiently detects all significant interactions in a
variety of synthetic and real-world datasets.",Table 2: Statistics of real data.,Which dataset would likely require the most computational resources for C-Tarone to analyze?
spiqa_405,1803.06506v3,"Referring to Table 1 of the unsupervised visual grounding paper, which dataset—with the shortest average phrase length and lowest noun count—suggests the simplest localization task, and how do these factors contribute to this ease in model performance?","Flickr30k is likely the easiest dataset for a model to localize phrases in. 
Flickr30k has the shortest average phrase length (2.3 words) and the lowest average noun count (1.2) per phrase. This suggests that the phrases in this dataset are simpler and often directly refer to single objects present in the image. This makes the localization task easier, almost approaching a weakly supervised setting where the object to be localized is explicitly named in the phrase.",1803.06506v3.pdf,"['1803.06506v3.pdf', '1704.07121v2.pdf', '1707.01917v2.pdf', '1805.02349v2.pdf', '1707.08608v3.pdf', '1805.04687v2.pdf', '1611.04363v2.pdf', '1703.04887v4.pdf', '1705.02798v6.pdf', '1804.01429v3.pdf', '1704.00774v3.pdf', '1709.02418v2.pdf', '1702.08694v3.pdf', '1709.08294v3.pdf']","Table 1 shows that Flickr30k has the shortest average phrase length (2.3 words) and the lowest average noun count (1.2) per phrase. This suggests that the phrases in this dataset are simpler and often directly refer to single objects present in the image. This makes the localization task easier, almost approaching a weakly supervised setting where the object to be localized is explicitly named in the phrase.",1803.06506v3-Table1-1.png,Learning Unsupervised Visual Grounding Through Semantic Self-Supervision,"Localizing natural language phrases in images is a challenging problem that
requires joint understanding of both the textual and visual modalities. In the
unsupervised setting, lack of supervisory signals exacerbate this difficulty.
In this paper, we propose a novel framework for unsupervised visual grounding
which uses concept learning as a proxy task to obtain self-supervision. The
simple intuition behind this idea is to encourage the model to localize to
regions which can explain some semantic property in the data, in our case, the
property being the presence of a concept in a set of images. We present
thorough quantitative and qualitative experiments to demonstrate the efficacy
of our approach and show a 5.6% improvement over the current state of the art
on Visual Genome dataset, a 5.8% improvement on the ReferItGame dataset and
comparable to state-of-art performance on the Flickr30k dataset.",Table 1. Phrase-region related statistics for datasets used in evaluation. The numbers reflect the relative complexity of these datasets.,"Which dataset would you expect to be the easiest for a model to localize phrases in, and why?"
spiqa_406,1708.00160v2,"In the figure titled *Out-of-domain evaluation of deep-coref models on the WikiCoref dataset*, which deep-coref model, according to the +linguistic evaluation metric, achieved the highest F1 score?","The CoNLL model performed best on the WikiCoref dataset, with an F1 score of 53.40 when using the +linguistic evaluation metric.",1708.00160v2.pdf,"['1708.00160v2.pdf', '1809.03550v3.pdf', '1812.00281v3.pdf', '1703.07015v3.pdf', '1805.00912v4.pdf', '1804.00863v3.pdf', '1705.09882v2.pdf', '1709.08294v3.pdf', '1608.02784v2.pdf', '1710.01507v4.pdf', '1809.03449v3.pdf', '1705.10667v4.pdf', '1706.00827v2.pdf']","The table shows the performance of several deep-coref models on the WikiCoref dataset, measured using three different evaluation metrics: ranking, top-pairs, and +linguistic. The CoNLL model achieved the highest F1 score for all three metrics.",1708.00160v2-Table2-1.png,Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers,"Coreference resolution is an intermediate step for text understanding. It is
used in tasks and domains for which we do not necessarily have coreference
annotated corpora. Therefore, generalization is of special importance for
coreference resolution. However, while recent coreference resolvers have
notable improvements on the CoNLL dataset, they struggle to generalize properly
to new domains or datasets. In this paper, we investigate the role of
linguistic features in building more generalizable coreference resolvers. We
show that generalization improves only slightly by merely using a set of
additional linguistic features. However, employing features and subsets of
their values that are informative for coreference resolution, considerably
improves generalization. Thanks to better generalization, our system achieves
state-of-the-art results in out-of-domain evaluations, e.g., on WikiCoref, our
system, which is trained on CoNLL, achieves on-par performance with a system
designed for this dataset.",Out-of-domain evaluation of deep-coref models on the WikiCoref dataset.,"Which deep-coref model performed best on the WikiCoref dataset, according to the table?"
spiqa_407,1706.04284v3,"Based on the denoising results presented in Table 1 of the paper, which method achieves the highest average PSNR across noise levels σ = 25, 35, and 50 on the Kodak dataset?",The proposed method performs the best on average across all noise levels tested on the Kodak dataset.,1706.04284v3.pdf,"['1706.04284v3.pdf', '1809.03149v2.pdf', '1704.05426v4.pdf', '1809.01989v2.pdf', '1804.04410v2.pdf', '1901.00056v2.pdf', '1804.00863v3.pdf', '1708.00160v2.pdf', '1710.01507v4.pdf', '1811.09393v4.pdf', '1611.02654v2.pdf']","The last row of Table 1 shows the average PSNR for each method across all images in the Kodak dataset. For each noise level (σ = 25, 35, and 50), the proposed method achieves the highest average PSNR compared to the other methods listed. This indicates that, on average, the proposed method performs the best in terms of denoising images from the Kodak dataset for the tested noise levels.",1706.04284v3-Table1-1.png,When Image Denoising Meets High-Level Vision Tasks: A Deep Learning Approach,"Conventionally, image denoising and high-level vision tasks are handled
separately in computer vision. In this paper, we cope with the two jointly and
explore the mutual influence between them. First we propose a convolutional
neural network for image denoising which achieves the state-of-the-art
performance. Second we propose a deep neural network solution that cascades two
modules for image denoising and various high-level tasks, respectively, and use
the joint loss for updating only the denoising network via back-propagation. We
demonstrate that on one hand, the proposed denoiser has the generality to
overcome the performance degradation of different high-level vision tasks. On
the other hand, with the guidance of high-level vision information, the
denoising network can generate more visually appealing results. To the best of
our knowledge, this is the first work investigating the benefit of exploiting
image semantics simultaneously for image denoising and high-level vision tasks
via deep learning. The code is available online
https://github.com/Ding-Liu/DeepDenoising.",Table 1: Color image denoising results (PSNR) of different methods on Kodak dataset. The best result is shown in bold.,Which denoising method performs the best on average across all noise levels tested on the Kodak dataset?
spiqa_408,1710.05654v2,"According to the right plot in the figure labeled ""average squared distribution"" of MNIST digits in the ""Large Scale Graph Learning from Smooth Signals"" paper, which digit has the highest average squared distance from the others?","Digit ""1""",1710.05654v2.pdf,"['1710.05654v2.pdf', '1804.04410v2.pdf', '1802.07222v1.pdf', '1605.07496v3.pdf', '1703.02507v3.pdf', '1710.01507v4.pdf', '1704.07854v4.pdf']","The right plot shows the average squared distance for each digit. We can see that the bar for digit ""1"" is the highest, which means that on average, digit ""1"" is further away from other digits than any other digit.",1710.05654v2-Figure11-1.png,Large Scale Graph Learning from Smooth Signals,"Graphs are a prevalent tool in data science, as they model the inherent
structure of the data. They have been used successfully in unsupervised and
semi-supervised learning. Typically they are constructed either by connecting
nearest samples, or by learning them from data, solving an optimization
problem. While graph learning does achieve a better quality, it also comes with
a higher computational cost. In particular, the current state-of-the-art model
cost is $\mathcal{O}(n^2)$ for $n$ samples. In this paper, we show how to scale
it, obtaining an approximation with leading cost of $\mathcal{O}(n\log(n))$,
with quality that approaches the exact graph learning model. Our algorithm uses
known approximate nearest neighbor techniques to reduce the number of
variables, and automatically selects the correct parameters of the model,
requiring a single intuitive input: the desired edge density.",Label frequency (left) and average squared distribution (right) of MNIST train data (60000 nodes). The distances between digits “1” are significantly smaller than distances between other digits.,Which digit has the highest average squared distance to other digits in the MNIST dataset?
spiqa_409,1805.04687v2,"From the domain discrepancy experiments illustrated in Table 4 of the BDD100K paper, which domain shift leads to a greater decrease in object detection performance: city vs. non-city environments or daytime vs. nighttime conditions?",Daytime vs. nighttime has a larger impact on object detection performance.,1805.04687v2.pdf,"['1805.04687v2.pdf', '1703.04887v4.pdf', '1704.07121v2.pdf', '1811.08257v1.pdf', '1812.06589v2.pdf', '1811.08481v2.pdf', '1704.08615v2.pdf', '1707.01922v5.pdf', '1611.07718v2.pdf']","The table shows the Average Precision (AP) for object detection across different training and testing domains. While there is a noticeable difference in AP between city and non-city images, the drop in performance is much more significant when models trained on daytime images are tested on nighttime images, and vice versa. This observation is further supported by the passage, which explicitly mentions that ""the gap between daytime and nighttime is much bigger"" compared to the city vs. non-city discrepancy.",1805.04687v2-Table4-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.","Table 4: Domain discrepancy experiments with object detection. We take the images from one domain and report testing results in AP on the same domain or the opposite domain. We can observe significant domain discrepancies, especially between daytime and nighttime.",Which domain discrepancy has a larger impact on object detection performance: city vs. non-city or daytime vs. nighttime?
spiqa_410,1811.08481v2,"According to the figure that reports accuracy of different estimators on the CLEVR validation set in the paper ""VQA with no questions-answers training,"" which visual estimator achieved 100% accuracy?",Size estimator.,1811.08481v2.pdf,"['1811.08481v2.pdf', '1704.04539v2.pdf', '1701.03077v10.pdf', '1703.07015v3.pdf', '1705.09296v2.pdf', '1812.06589v2.pdf', '1611.05742v3.pdf', '1901.00398v2.pdf', '1704.05426v4.pdf', '1704.07854v4.pdf', '1803.06506v3.pdf', '1603.00286v5.pdf']","The table shows that the Size estimator achieves 100% accuracy, which is the highest among all the estimators.",1811.08481v2-Table1-1.png,VQA with no questions-answers training,"Methods for teaching machines to answer visual questions have made
significant progress in recent years, but current methods still lack important
human capabilities, including integrating new visual classes and concepts in a
modular manner, providing explanations for the answers and handling new domains
without explicit examples. We propose a novel method that consists of two main
parts: generating a question graph representation, and an answering procedure,
guided by the abstract structure of the question graph to invoke an extendable
set of visual estimators. Training is performed for the language part and the
visual part on their own, but unlike existing schemes, the method does not
require any training using images with associated questions and answers. This
approach is able to handle novel domains (extended question types and new
object classes, properties and relations) as long as corresponding visual
estimators are available. In addition, it can provide explanations to its
answers and suggest alternatives when questions are not grounded in the image.
We demonstrate that this approach achieves both high performance and domain
extensibility without any questions-answers training.",CLEVR estimators results on CLEVR validation set,Which estimator achieves the highest accuracy on the CLEVR validation set?
spiqa_411,1906.10843v1,"Based on Figure 7 of the paper ""User Sentiment as a Success Metric,"" which two ATETR estimators demonstrate the smallest bias and the best MSE performance when confounders are fully observed?",The Entropy Balancing (EB) and Covariate Control (CC) estimators.,1906.10843v1.pdf,"['1906.10843v1.pdf', '1704.05958v2.pdf', '1812.00281v3.pdf', '1811.10673v1.pdf', '1705.09882v2.pdf', '1809.02731v3.pdf', '1804.05936v2.pdf', '1804.00863v3.pdf', '1708.02153v2.pdf', '1612.02803v5.pdf', '1804.07849v4.pdf', '1611.05742v3.pdf', '1603.03833v4.pdf', '1804.04410v2.pdf']","The boxplots in Figure~\ref{figure_sim_ATETR_1} show the distribution of ATETR estimates for different estimators. The EB and CC estimators have boxplots that are centered close to the true ATETR value of 0, indicating that they have small biases. Additionally, the passage states that the EB and CC estimators have the best MSE performance across all estimators.",1906.10843v1-Figure7-1.png,User Sentiment as a Success Metric: Persistent Biases Under Full Randomization,"We study user sentiment (reported via optional surveys) as a metric for fully
randomized A/B tests. Both user-level covariates and treatment assignment can
impact response propensity. We propose a set of consistent estimators for the
average and local treatment effects on treated and respondent users. We show
that our problem can be mapped onto the intersection of the missing data
problem and observational causal inference, and we identify conditions under
which consistent estimators exist. We evaluate the performance of estimators
via simulation studies and find that more complicated models do not necessarily
provide superior performance.",Figure 7: Performance of different ATETR estimators when true confounders are fully observed. CC and EB outperforms AB in contrast to ATE.,Which estimator has the smallest bias and best MSE performance in the case of fully observed confounders?
spiqa_412,1906.10843v1,"Based on the results presented in Table 5, which ATETR estimator achieves the lowest Bias, MAE, and MSE in the presence of noisy confounders, and how does its performance compare against the Covariate Control (CC) estimator in terms of these metrics?","The Entropy Balancing (EB) estimator performs best across all measures (Bias, MAE, and MSE) when confounders are noisy. While the CC estimator also performs well, it exhibits slightly higher bias and MAE compared to EB.",1906.10843v1.pdf,"['1906.10843v1.pdf', '1706.00827v2.pdf', '1709.00139v4.pdf', '1709.02755v5.pdf', '1802.07222v1.pdf']","The table presents the performance of different ATETR estimators under the condition of noisy confounders. The performance is evaluated based on three metrics: Bias, Mean Absolute Error (MAE), and Mean Squared Error (MSE). By comparing the values in the table, we can see that EB has the lowest values for all three metrics, indicating its superior performance. Although CC also shows good performance, its metrics are slightly higher than those of EB, suggesting a slightly lower accuracy in this scenario.",1906.10843v1-Table5-1.png,User Sentiment as a Success Metric: Persistent Biases Under Full Randomization,"We study user sentiment (reported via optional surveys) as a metric for fully
randomized A/B tests. Both user-level covariates and treatment assignment can
impact response propensity. We propose a set of consistent estimators for the
average and local treatment effects on treated and respondent users. We show
that our problem can be mapped onto the intersection of the missing data
problem and observational causal inference, and we identify conditions under
which consistent estimators exist. We evaluate the performance of estimators
via simulation studies and find that more complicated models do not necessarily
provide superior performance.","Table 5: Performance of different ATETR estimators when noisy confounders are observed. Estimators are Adversarial Balancing (AB), Covariate Control (CC), Entrophy Balancing (EB), Inverse Propensity Weighing (IPW), Naive mean comparison and Outcome Regression (OR). Similar to the results in Table 4, EB outperforms across all measures. Similar to Table 4, simple CC estimator provides a comparable performance to EB.",Which estimator performs best in the presence of noisy confounders and how does it compare to the Covariate Control (CC) estimator?
spiqa_413,1708.02153v2,"In the context of the ""Shifted"" images shown in Table 1, which explanation method—LIME, MIM, or Parzen—demonstrates a more abrupt, ""shattered"" influence pattern, focusing on discrete, localized features rather than producing smoother, gradual changes in pixel intensity?","LIME appears to place the most emphasis on specific, localized features.",1708.02153v2.pdf,"['1708.02153v2.pdf', '1606.07384v2.pdf', '1706.08146v3.pdf', '1701.03077v10.pdf', '1805.06447v3.pdf', '1709.08294v3.pdf', '1811.08481v2.pdf']","The passage describes the LIME influence visualization as ""more 'shattered' compared to MIM and Parzen,"" indicating a less smooth and more focused influence pattern. This suggests that LIME assigns higher importance to specific pixels or small groups of pixels, rather than considering gradual changes in intensity across larger areas. 

This observation is further supported by the visual comparison of the ""Shifted"" images in the table. While MIM and Parzen result in relatively smooth shifts in brightness, LIME's shifted images show more abrupt changes and sharper contrasts, suggesting a stronger focus on specific features.",1708.02153v2-Table1-1.png,Axiomatic Characterization of Data-Driven Influence Measures for Classification,"We study the following problem: given a labeled dataset and a specific
datapoint x, how did the i-th feature influence the classification for x? We
identify a family of numerical influence measures - functions that, given a
datapoint x, assign a numeric value phi_i(x) to every feature i, corresponding
to how altering i's value would influence the outcome for x. This family, which
we term monotone influence measures (MIM), is uniquely derived from a set of
desirable properties, or axioms. The MIM family constitutes a provably sound
methodology for measuring feature influence in classification domains; the
values generated by MIM are based on the dataset alone, and do not make any
queries to the classifier. While this requirement naturally limits the scope of
our framework, we demonstrate its effectiveness on data.",Table 1: Influence of two different points of interest (POI),"Which explanation method seems to place the most emphasis on specific, localized features rather than smooth, gradual changes in pixel intensity?"
spiqa_414,1803.04383v2,"Based on Figure 5(a) in the ""Delayed Impact of Fair Machine Learning"" paper, which fairness criterion results in the highest loan approval rate for the Black group when the loss/profit ratio is set to -4?",The maximum profit criteria ($\maxprof$) results in the highest loan approval rate for the Black group when the loss/profit ratio is -4.,1803.04383v2.pdf,"['1803.04383v2.pdf', '1703.00899v2.pdf', '1705.08016v3.pdf', '1703.04887v4.pdf', '1702.03584v3.pdf', '1611.04363v2.pdf', '1709.08294v3.pdf', '1706.04284v3.pdf', '1812.06589v2.pdf', '1812.10735v2.pdf', '1708.03797v1.pdf', '1708.01425v4.pdf', '1706.00633v4.pdf', '1704.05426v4.pdf']","The figure shows the loan approval rates for different fairness criteria and different loss/profit ratios. The vertical lines represent the loan approval thresholds for each criteria. The higher the threshold, the more loans are approved. When the loss/profit ratio is -4, the $\maxprof$ threshold is the highest for the Black group, indicating that it approves the most loans for this group.",1803.04383v2-Figure5-1.png,Delayed Impact of Fair Machine Learning,"Fairness in machine learning has predominantly been studied in static
classification settings without concern for how decisions change the underlying
population over time. Conventional wisdom suggests that fairness criteria
promote the long-term well-being of those groups they aim to protect.
  We study how static fairness criteria interact with temporal indicators of
well-being, such as long-term improvement, stagnation, and decline in a
variable of interest. We demonstrate that even in a one-step feedback model,
common fairness criteria in general do not promote improvement over time, and
may in fact cause harm in cases where an unconstrained objective would not.
  We completely characterize the delayed impact of three standard criteria,
contrasting the regimes in which these exhibit qualitatively different
behavior. In addition, we find that a natural form of measurement error
broadens the regime in which fairness criteria perform favorably.
  Our results highlight the importance of measurement and temporal modeling in
the evaluation of fairness criteria, suggesting a range of new challenges and
trade-offs.","Figure 5: The empirical CDFs of both groups are plotted along with the decision thresholds resulting from MaxUtil, DemParity, and EqOpt for a model with bank utilities set to (a) u− u+ = −4 and (b) u− u+ = −10. The threshold for active harm is displayed; in (a) DemParity causes active harm while in (b) it does not. EqOpt and MaxUtil never cause active harm.",Which fairness criteria results in the highest loan approval rate for the Black group when the loss/profit ratio is -4?
spiqa_415,1809.00263v5,"Based on the figure illustrating the dimensionalities of various features in the SDVI framework, how do the first and second dimensions of these features compare in terms of size?",All features have the same dimensionality in the first two dimensions.,1809.00263v5.pdf,"['1809.00263v5.pdf', '1611.03780v2.pdf', '1705.02798v6.pdf', '1703.04887v4.pdf', '1802.07222v1.pdf', '1804.05995v2.pdf', '1805.04609v3.pdf', '1805.01216v3.pdf', '1603.00286v5.pdf', '1701.06171v4.pdf', '1703.00899v2.pdf']",The table shows that all features have a dimensionality of 4 in both the first and second dimensions.,1809.00263v5-Table2-1.png,Stochastic Dynamics for Video Infilling,"In this paper, we introduce a stochastic dynamics video infilling (SDVI)
framework to generate frames between long intervals in a video. Our task
differs from video interpolation which aims to produce transitional frames for
a short interval between every two frames and increase the temporal resolution.
Our task, namely video infilling, however, aims to infill long intervals with
plausible frame sequences. Our framework models the infilling as a constrained
stochastic generation process and sequentially samples dynamics from the
inferred distribution. SDVI consists of two parts: (1) a bi-directional
constraint propagation module to guarantee the spatial-temporal coherence among
frames, (2) a stochastic sampling process to generate dynamics from the
inferred distributions. Experimental results show that SDVI can generate clear
frame sequences with varying contents. Moreover, motions in the generated
sequence are realistic and able to transfer smoothly from the given start frame
to the terminal frame. Our project site is
https://xharlie.github.io/projects/project_sites/SDVI/video_results.html",The dimensionalities of different features,Which feature has the highest dimensionality in the first two dimensions?
spiqa_416,1811.08257v1,"Based on the figure titled ""Performance Comparison on MNIST and CIFAR10"" in the FALCON paper, which framework demonstrates the lowest total communication cost for MNIST classification?",FALCON,1811.08257v1.pdf,"['1811.08257v1.pdf', '1805.06431v4.pdf', '1802.07459v2.pdf', '1811.02721v3.pdf', '1804.07707v2.pdf', '1611.07718v2.pdf', '1804.05995v2.pdf', '1811.07073v3.pdf', '1611.03780v2.pdf', '1803.03467v4.pdf', '1707.06320v2.pdf', '1704.08615v2.pdf']","The table shows that FALCON has the lowest total communication cost for MNIST, with a total of 92.5 MB.",1811.08257v1-Table5-1.png,FALCON: A Fourier Transform Based Approach for Fast and Secure Convolutional Neural Network Predictions,"Machine learning as a service has been widely deployed to utilize deep neural
network models to provide prediction services. However, this raises privacy
concerns since clients need to send sensitive information to servers. In this
paper, we focus on the scenario where clients want to classify private images
with a convolutional neural network model hosted in the server, while both
parties keep their data private. We present FALCON, a fast and secure approach
for CNN predictions based on Fourier Transform. Our solution enables linear
layers of a CNN model to be evaluated simply and efficiently with fully
homomorphic encryption. We also introduce the first efficient and
privacy-preserving protocol for softmax function, which is an indispensable
component in CNNs and has not yet been evaluated in previous works due to its
high complexity. We implemented the FALCON and evaluated the performance on
real-world CNN models. The experimental results show that FALCON outperforms
the best known works in both computation and communication cost.",Performance Comparison on MNIST and CIFAR10.,Which framework has the lowest total communication cost for MNIST?
spiqa_417,1811.08257v1,"Referring to the table that benchmarks the setup and online times for convolutional and fully connected layers, which framework—FALCON or GAZELLE—demonstrates superior performance in the FC layer with faster setup (1.2 ms) and online times (0.1 ms)?",FALCON is faster for both setting up and running the FC layer.,1811.08257v1.pdf,"['1811.08257v1.pdf', '1705.09296v2.pdf', '1705.09882v2.pdf', '1812.00108v4.pdf']","The table shows that FALCON has a setup time of 1.2 ms and an online time of 0.1 ms for the FC layer, while GAZELLE has a setup time of 16.2 ms and an online time of 8.0 ms.",1811.08257v1-Table2-1.png,FALCON: A Fourier Transform Based Approach for Fast and Secure Convolutional Neural Network Predictions,"Machine learning as a service has been widely deployed to utilize deep neural
network models to provide prediction services. However, this raises privacy
concerns since clients need to send sensitive information to servers. In this
paper, we focus on the scenario where clients want to classify private images
with a convolutional neural network model hosted in the server, while both
parties keep their data private. We present FALCON, a fast and secure approach
for CNN predictions based on Fourier Transform. Our solution enables linear
layers of a CNN model to be evaluated simply and efficiently with fully
homomorphic encryption. We also introduce the first efficient and
privacy-preserving protocol for softmax function, which is an indispensable
component in CNNs and has not yet been evaluated in previous works due to its
high complexity. We implemented the FALCON and evaluated the performance on
real-world CNN models. The experimental results show that FALCON outperforms
the best known works in both computation and communication cost.",Benchmarks and Comparisons for Conv and FC.,Which framework is faster for setting up and running the FC layer?
spiqa_418,1707.00524v2,"Among the games shown in the figure comparing code loss and frame reconstruction errors, which game has the highest code loss during phase 2 under the convolutional autoencoder model used in this study?",Pacman,1707.00524v2.pdf,"['1707.00524v2.pdf', '1703.07015v3.pdf', '1705.07164v8.pdf', '1707.01922v5.pdf', '1811.02721v3.pdf', '1809.01989v2.pdf', '1805.06447v3.pdf', '1710.06177v2.pdf']","The figure shows the code loss for different games in phase 1 and phase 2. The height of the bars represents the code loss. In phase 2, the bar for Pacman is the highest, indicating that it has the highest code loss.",1707.00524v2-Figure5-1.png,Hashing over Predicted Future Frames for Informed Exploration of Deep Reinforcement Learning,"In deep reinforcement learning (RL) tasks, an efficient exploration mechanism
should be able to encourage an agent to take actions that lead to less frequent
states which may yield higher accumulative future return. However, both knowing
about the future and evaluating the frequentness of states are non-trivial
tasks, especially for deep RL domains, where a state is represented by
high-dimensional image frames. In this paper, we propose a novel informed
exploration framework for deep RL, where we build the capability for an RL
agent to predict over the future transitions and evaluate the frequentness for
the predicted future frames in a meaningful manner. To this end, we train a
deep prediction model to predict future frames given a state-action pair, and a
convolutional autoencoder model to hash over the seen frames. In addition, to
utilize the counts derived from the seen frames to evaluate the frequentness
for the predicted frames, we tackle the challenge of matching the predicted
future frames and their corresponding seen frames at the latent feature level.
In this way, we derive a reliable metric for evaluating the novelty of the
future direction pointed by each action, and hence inform the agent to explore
the least frequent one.",Comparison of the code loss and the frame reconstruction loss (MSE) for autoencoder after the training of phase 1 & phase 2.,Which game has the highest code loss in phase 2?
spiqa_419,1805.06447v3,"Referring to the progression of sample quality in Figure 2 of *Resisting Large Data Variations via Introspective Transformation Network*, which generative model, AC-GATN or ITN, produces more accurate and realistic MNIST samples by epoch 100?",ITN generates more accurate and realistic samples on the MNIST dataset compared to AC-GATN.,1805.06447v3.pdf,"['1805.06447v3.pdf', '1709.00139v4.pdf', '1812.10735v2.pdf', '1802.07351v2.pdf', '1805.06431v4.pdf']","Figure 2 shows the samples generated by AC-GATN and ITN on the MNIST dataset at different epochs. As the training progresses, the samples generated by ITN become increasingly clear and accurate, while some samples generated by AC-GATN remain misleading and inaccurate, even at epoch 100. This suggests that ITN is a better choice for generating realistic and accurate samples on the MNIST dataset.",1805.06447v3-Figure3-1.png,Resisting Large Data Variations via Introspective Transformation Network,"Training deep networks that generalize to a wide range of variations in test
data is essential to building accurate and robust image classifiers. One
standard strategy is to apply data augmentation to synthetically enlarge the
training set. However, data augmentation is essentially a brute-force method
which generates uniform samples from some pre-defined set of transformations.
In this paper, we propose a principled approach to train networks with
significantly improved resistance to large variations between training and
testing data. This is achieved by embedding a learnable transformation module
into the introspective network, which is a convolutional neural network (CNN)
classifier empowered with generative capabilities. Our approach alternates
between synthesizing pseudo-negative samples and transformed positive examples
based on the current model, and optimizing model predictions on these
synthesized samples. Experimental results verify that our approach
significantly improves the ability of deep networks to resist large variations
between training and testing data and achieves classification accuracy
improvements on several benchmark datasets, including MNIST, affNIST, SVHN,
CIFAR-10 and miniImageNet.",Figure 3. Testing errors of AC-GATN (B-CNN) and ITN (B-CNN) on the MNIST dataset.,"Which generative model generates more accurate and realistic samples on the MNIST dataset, AC-GATN or ITN?"
spiqa_421,1704.05426v4,"Based on Table 3 of the MultiNLI corpus paper, which genre exhibits the highest percentage of sentences parsed with an 'S' node by the Stanford Parser, and how does this percentage compare to the overall corpus average?","The genre with the highest percentage of 'S' parses is **9/11**, with **99%** of its sentences receiving this parse. This is higher than the overall average for the MultiNLI corpus, which sits at **91%**.",1704.05426v4.pdf,"['1704.05426v4.pdf', '1805.04609v3.pdf', '1701.03077v10.pdf', '1706.04284v3.pdf', '1705.09966v2.pdf', '1703.00899v2.pdf', '1703.02507v3.pdf', '1805.04687v2.pdf', '1702.03584v3.pdf', '1710.01507v4.pdf', '1707.01922v5.pdf']","The table provides the percentage of 'S' parses for each genre under the column ""`S' parses"". By comparing these values, we can identify which genre has the highest percentage. Additionally, the overall average for the corpus is provided in the last row of the table, allowing for a comparison between the 9/11 genre and the entire dataset.",1704.05426v4-Table3-1.png,A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference,"This paper introduces the Multi-Genre Natural Language Inference (MultiNLI)
corpus, a dataset designed for use in the development and evaluation of machine
learning models for sentence understanding. In addition to being one of the
largest corpora available for the task of NLI, at 433k examples, this corpus
improves upon available resources in its coverage: it offers data from ten
distinct genres of written and spoken English--making it possible to evaluate
systems on nearly the full complexity of the language--and it offers an
explicit setting for the evaluation of cross-genre domain adaptation.","Table 3: Key statistics for the corpus by genre. The first five genres represent the matched section of the development and test sets, and the remaining five represent the mismatched section. The first three statistics provide the number of examples in each genre. #Wds. Prem. is the mean token count among premise sentences. ‘S’ parses is the percentage of sentences for which the Stanford Parser produced a parse rooted with an ‘S’ (sentence) node. Agrmt. is the percent of individual labels that match the gold label in validated examples. Model Acc. gives the test accuracy for ESIM and CBOW models (trained on either SNLI or MultiNLI), as described in Section 3.","Which genre in the MultiNLI corpus has the highest percentage of sentences where the Stanford Parser produced a parse rooted with an 'S' (sentence) node, and how does this compare to the overall average for the corpus?"
spiqa_422,1707.08608v3,"Based on Table 11 of the SRL-NW network, which genre demonstrates the lowest failure rate, and how does its inference time across all three methods (GBI, Viterbi, and A*) compare to other genres in terms of efficiency?","The PT genre within the SRL-NW network has the lowest failure rate at 10.01%. Its inference time is also the lowest across all genres in the SRL-NW network for all three inference procedures (Viterbi, GBI, and A*).",1707.08608v3.pdf,"['1707.08608v3.pdf', '1803.01128v3.pdf', '1804.07849v4.pdf', '1809.00458v1.pdf', '1611.02654v2.pdf', '1603.03833v4.pdf', '1706.04269v2.pdf', '1811.02553v4.pdf', '1809.03550v3.pdf', '1804.05938v2.pdf', '1804.00863v3.pdf', '1611.03780v2.pdf']","The table shows the failure rate and inference time for each genre in both the SRL-100 and SRL-NW networks. By comparing the failure rate values within the SRL-NW network, we can identify PT as having the lowest rate. Additionally, by comparing the inference times for PT to other genres in the same network, we can see that it consistently requires the least amount of time for all three procedures.",1707.08608v3-Table11-1.png,Gradient-based Inference for Networks with Output Constraints,"Practitioners apply neural networks to increasingly complex problems in
natural language processing, such as syntactic parsing and semantic role
labeling that have rich output structures. Many such structured-prediction
problems require deterministic constraints on the output values; for example,
in sequence-to-sequence syntactic parsing, we require that the sequential
outputs encode valid trees. While hidden units might capture such properties,
the network is not always able to learn such constraints from the training data
alone, and practitioners must then resort to post-processing. In this paper, we
present an inference method for neural networks that enforces deterministic
constraints on outputs without performing rule-based post-processing or
expensive discrete search. Instead, in the spirit of gradient-based training,
we enforce constraints with gradient-based inference (GBI): for each input at
test-time, we nudge continuous model weights until the network's unconstrained
inference procedure generates an output that satisfies the constraints. We
study the efficacy of GBI on three tasks with hard constraints: semantic role
labeling, syntactic parsing, and sequence transduction. In each case, the
algorithm not only satisfies constraints but improves accuracy, even when the
underlying network is state-of-the-art.","Table 11: Comparison of runtime for difference inference procedures in the noise-free constraint setting: Viterbi, A*(He et al. 2017) and GBI. For SRL-100 refer Table 1 and SRL-NW is a model trained on NW genre.",Which genre in the SRL-NW network has the lowest failure rate and how does its inference time compare to other genres within the same network?
spiqa_423,1707.08608v3,"In the evaluation of out-of-domain data in Table 5, which genre exhibits the largest absolute improvement in F1 scores for both syntactic parsing and SRL after applying Gradient-based Inference (GBI) to address performance degradation on the failure set?",Pivot Corpus (PT) shows the largest absolute improvement in F1 score on the failure set after applying GBI for both syntactic parsing and SRL.,1707.08608v3.pdf,"['1707.08608v3.pdf', '1811.10673v1.pdf', '1805.00912v4.pdf', '1611.04684v1.pdf', '1605.07496v3.pdf']","Looking at the ""F1 on failure set"" columns in Table 1, we can compare the ""before"" and ""after"" scores for each genre and task. 

For syntactic parsing, PT improves by 4.4 points (75.8 - 71.4), which is the highest increase among all genres. 

Similarly, for SRL, PT again shows the biggest improvement, with a 16.5 point increase (63.69 - 47.19) after applying GBI. 

Therefore, PT demonstrates the largest absolute improvement in both tasks based on the F1 scores on the failure set.",1707.08608v3-Table5-1.png,Gradient-based Inference for Networks with Output Constraints,"Practitioners apply neural networks to increasingly complex problems in
natural language processing, such as syntactic parsing and semantic role
labeling that have rich output structures. Many such structured-prediction
problems require deterministic constraints on the output values; for example,
in sequence-to-sequence syntactic parsing, we require that the sequential
outputs encode valid trees. While hidden units might capture such properties,
the network is not always able to learn such constraints from the training data
alone, and practitioners must then resort to post-processing. In this paper, we
present an inference method for neural networks that enforces deterministic
constraints on outputs without performing rule-based post-processing or
expensive discrete search. Instead, in the spirit of gradient-based training,
we enforce constraints with gradient-based inference (GBI): for each input at
test-time, we nudge continuous model weights until the network's unconstrained
inference procedure generates an output that satisfies the constraints. We
study the efficacy of GBI on three tasks with hard constraints: semantic role
labeling, syntactic parsing, and sequence transduction. In each case, the
algorithm not only satisfies constraints but improves accuracy, even when the
underlying network is state-of-the-art.","Table 5: Evaluation of syntactic parser and SRL system on out-of-domain data. F1 scores are reported on the failure set. SRL model was trained on NW and the syntactic parser was trained on WSJ Section on OntoNote v5.0. Except PT, which is new and old Testament, all failure rate on out-domain data is higher than that of in-domain (11.9% for parsing and 18.1% for SRL) as suspected. The table shows that GBI can be successfully applied to resolve performance degradation on out-of-domain data.",Which genre shows the **largest absolute improvement** in F1 score on the failure set after applying GBI for **both** syntactic parsing and SRL?
spiqa_424,1704.07854v4,"According to the figure comparing gradient approximation methods in ""Generating Liquid Simulations with Deformation-aware Neural Networks,"" which approach—simplified advection or forward advection—leads to a more stable and lower validation loss during training?",The corrected gradient method leads to a more stable and lower loss value during training.,1704.07854v4.pdf,"['1704.07854v4.pdf', '1707.00524v2.pdf', '1710.05654v2.pdf', '1707.01922v5.pdf', '1804.05938v2.pdf']",The figure shows the validation loss for two different gradient approximation methods: naive gradient and corrected gradient. The corrected gradient method results in a much lower and more stable loss value than the naive gradient method.,1704.07854v4-Figure15-1.png,Generating Liquid Simulations with Deformation-aware Neural Networks,"We propose a novel approach for deformation-aware neural networks that learn
the weighting and synthesis of dense volumetric deformation fields. Our method
specifically targets the space-time representation of physical surfaces from
liquid simulations. Liquids exhibit highly complex, non-linear behavior under
changing simulation conditions such as different initial conditions. Our
algorithm captures these complex phenomena in two stages: a first neural
network computes a weighting function for a set of pre-computed deformations,
while a second network directly generates a deformation field for refining the
surface. Key for successful training runs in this setting is a suitable loss
function that encodes the effect of the deformations, and a robust calculation
of the corresponding gradients. To demonstrate the effectiveness of our
approach, we showcase our method with several complex examples of flowing
liquids with topology changes. Our representation makes it possible to rapidly
generate the desired implicit surfaces. We have implemented a mobile
application to demonstrate that real-time interactions with complex liquid
effects are possible with our approach.","Training with different gradient approximations: validation loss with a simplified advection (red), and the correct gradient from forward advection (green). The simplified version does not converge.",Which gradient approximation method leads to a more stable and lower loss value during training?
spiqa_425,1809.01246v1,"""Average Relative Error of Edge Queries,""",The graph for the Caida-networkflow dataset shows the largest improvement in accuracy for the TCM(8*memory) method compared to the GSS(fsize=12) method.,1809.01246v1.pdf,"['1809.01246v1.pdf', '1703.00060v2.pdf', '1704.08615v2.pdf', '1804.01429v3.pdf', '1811.08257v1.pdf', '1811.10673v1.pdf', '1805.08751v2.pdf', '1805.04687v2.pdf', '1901.00398v2.pdf', '1812.10735v2.pdf', '1710.01507v4.pdf', '1710.06177v2.pdf', '1803.02750v3.pdf', '1603.00286v5.pdf', '1709.02418v2.pdf']",The graph for the Caida-networkflow dataset shows the largest difference in the y-axis values between the two methods.,1809.01246v1-Figure8-1.png,Fast and Accurate Graph Stream Summarization,"A graph stream is a continuous sequence of data items, in which each item
indicates an edge, including its two endpoints and edge weight. It forms a
dynamic graph that changes with every item in the stream. Graph streams play
important roles in cyber security, social networks, cloud troubleshooting
systems and other fields. Due to the vast volume and high update speed of graph
streams, traditional data structures for graph storage such as the adjacency
matrix and the adjacency list are no longer sufficient. However, prior art of
graph stream summarization, like CM sketches, gSketches, TCM and gMatrix,
either supports limited kinds of queries or suffers from poor accuracy of query
results. In this paper, we propose a novel Graph Stream Sketch (GSS for short)
to summarize the graph streams, which has the linear space cost (O(|E|), E is
the edge set of the graph) and the constant update time complexity (O(1)) and
supports all kinds of queries over graph streams with the controllable errors.
Both theoretical analysis and experiment results confirm the superiority of our
solution with regard to the time/space complexity and query results' precision
compared with the state-of-the-art.",Average Relative Error of Edge Queries,Which graph shows the largest improvement in accuracy for the TCM(8*memory) method compared to the GSS(fsize=12) method?
spiqa_426,1701.03077v10,"In the research paper titled ""A General and Adaptive Robust Loss Function,"" which specific image representations, as shown in the figure where the loss function adapts between Cauchy-like and normal-like behavior, result in the sharpest and highest-quality samples?",DCT and wavelet representations.,1701.03077v10.pdf,"['1701.03077v10.pdf', '1811.07073v3.pdf', '1705.02798v6.pdf', '1809.03149v2.pdf', '1809.01246v1.pdf', '1705.09296v2.pdf', '1707.01917v2.pdf', '1706.00827v2.pdf']","The figure shows that the samples generated using our distribution are sharper and of higher quality when using DCT or wavelet representations than when using other representations. This is because our distribution can adaptively interpolate between Cauchy-like or normal-like behavior for each coefficient individually, which results in a better capture of low-frequency image content.",1701.03077v10-Figure3-1.png,A General and Adaptive Robust Loss Function,"We present a generalization of the Cauchy/Lorentzian, Geman-McClure,
Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2
loss functions. By introducing robustness as a continuous parameter, our loss
function allows algorithms built around robust loss minimization to be
generalized, which improves performance on basic vision tasks such as
registration and clustering. Interpreting our loss as the negative log of a
univariate density yields a general probability distribution that includes
normal and Cauchy distributions as special cases. This probabilistic
interpretation enables the training of neural networks in which the robustness
of the loss automatically adapts itself during training, which improves
performance on learning-based tasks such as generative image synthesis and
unsupervised monocular depth estimation, without requiring any manual parameter
tuning.","Random samples from our variational autoencoders. We use either normal, Cauchy, Student’s t, or our general distributions (columns) to model the coefficients of three different image representations (rows). Because our distribution can adaptively interpolate between Cauchy-like or normal-like behavior for each coefficient individually, using it results in sharper and higher-quality samples (particularly when using DCT or wavelet representations) and does a better job of capturing low-frequency image content than Student’s t-distribution.",Which image representation results in the sharpest and highest-quality samples?
spiqa_427,1707.08608v3,"Based on the evaluation in Table 4 of the paper ""Gradient-based Inference for Networks with Output Constraints,"" which inference method, specifically in terms of beam search width, consistently achieves the highest F1 score on the failure set across the architectures Net3, Net4, and Net5?",Beam search with a width of 9 consistently leads to the highest F1 score on the failure set across all three networks.,1707.08608v3.pdf,"['1707.08608v3.pdf', '1809.04276v2.pdf', '1703.04887v4.pdf', '1707.06320v2.pdf', '1707.00524v2.pdf', '1705.07384v2.pdf', '1612.02803v5.pdf']","The table shows the F1 scores on the failure set for different inference methods and network architectures. By comparing the F1 scores within each network, we can see that Beam 9 consistently achieves the highest score, indicating better performance in identifying and correcting failures compared to other methods like Greedy or Beam search with smaller widths.",1707.08608v3-Table4-1.png,Gradient-based Inference for Networks with Output Constraints,"Practitioners apply neural networks to increasingly complex problems in
natural language processing, such as syntactic parsing and semantic role
labeling that have rich output structures. Many such structured-prediction
problems require deterministic constraints on the output values; for example,
in sequence-to-sequence syntactic parsing, we require that the sequential
outputs encode valid trees. While hidden units might capture such properties,
the network is not always able to learn such constraints from the training data
alone, and practitioners must then resort to post-processing. In this paper, we
present an inference method for neural networks that enforces deterministic
constraints on outputs without performing rule-based post-processing or
expensive discrete search. Instead, in the spirit of gradient-based training,
we enforce constraints with gradient-based inference (GBI): for each input at
test-time, we nudge continuous model weights until the network's unconstrained
inference procedure generates an output that satisfies the constraints. We
study the efficacy of GBI on three tasks with hard constraints: semantic role
labeling, syntactic parsing, and sequence transduction. In each case, the
algorithm not only satisfies constraints but improves accuracy, even when the
underlying network is state-of-the-art.","Table 4: Evaluation of GBI on simpler, low-resource seq2seq networks. Here, we also evaluate whether GBI can be used in combination with different inference techniques: greedy and beam search of various widths.","Which inference method consistently leads to the highest F1 score on the failure set across all three networks (Net3, Net4, and Net5)?"
spiqa_428,1804.04786v3,"In the figure presenting the ablation study on loss functions, which combination of \(L_{rec}\), \(L_I\), and \(L_V\) results in the most realistic and accurate lip movements that are closely synchronized with the audio input, excluding the effects of \(L_l\)?","The combination of Lrec, LI, and LV is most important for generating realistic mouth movements.",1804.04786v3.pdf,"['1804.04786v3.pdf', '1805.06447v3.pdf', '1805.02349v2.pdf', '1704.05426v4.pdf']","The figure shows that when Lrec, LI, and LV are used together, the generated mouth movements are more realistic and match the audio input more closely. When Ll is added to the loss function combination, the generated mouth movements become less realistic and more exaggerated.",1804.04786v3-Figure4-1.png,Talking Face Generation by Conditional Recurrent Adversarial Network,"Given an arbitrary face image and an arbitrary speech clip, the proposed work
attempts to generating the talking face video with accurate lip synchronization
while maintaining smooth transition of both lip and facial movement over the
entire video clip. Existing works either do not consider temporal dependency on
face images across different video frames thus easily yielding
noticeable/abrupt facial and lip movement or are only limited to the generation
of talking face video for a specific person thus lacking generalization
capacity. We propose a novel conditional video generation network where the
audio input is treated as a condition for the recurrent adversarial network
such that temporal dependency is incorporated to realize smooth transition for
the lip and facial movement. In addition, we deploy a multi-task adversarial
training scheme in the context of video generation to improve both
photo-realism and the accuracy for lip synchronization. Finally, based on the
phoneme distribution information extracted from the audio clip, we develop a
sample selection method that effectively reduces the size of the training
dataset without sacrificing the quality of the generated video. Extensive
experiments on both controlled and uncontrolled datasets demonstrate the
superiority of the proposed approach in terms of visual quality, lip sync
accuracy, and smooth transition of lip and facial movement, as compared to the
state-of-the-art.","Ablation study on the loss functions used in the proposed method. The rows show the continuous frames generated from the same audio input but different loss combinations as denoted on the left. The subscripts r, I , V , and l indicates Lrec, LI , LV , and Ll in Eq. 5, respectively.",Which loss function combination is most important for generating realistic mouth movements?
spiqa_429,1803.06506v3,"According to the figure analyzing different surrogate losses in the paper *Learning Unsupervised Visual Grounding Through Semantic Self-Supervision*, which loss type has the highest performance when the concept batch size reaches 5,000?",Independent and common concept,1803.06506v3.pdf,"['1803.06506v3.pdf', '1611.04363v2.pdf', '1811.02553v4.pdf', '1705.07164v8.pdf', '1804.05936v2.pdf', '1611.02654v2.pdf', '1804.07849v4.pdf', '1706.03847v3.pdf', '1704.05958v2.pdf', '1812.10735v2.pdf', '1707.00189v3.pdf', '1707.08608v3.pdf', '1611.04684v1.pdf', '1703.00899v2.pdf']",The table shows that the independent and common concept loss type has the highest value (29.89) when the concept batch size is 5k.,1803.06506v3-Table3-1.png,Learning Unsupervised Visual Grounding Through Semantic Self-Supervision,"Localizing natural language phrases in images is a challenging problem that
requires joint understanding of both the textual and visual modalities. In the
unsupervised setting, lack of supervisory signals exacerbate this difficulty.
In this paper, we propose a novel framework for unsupervised visual grounding
which uses concept learning as a proxy task to obtain self-supervision. The
simple intuition behind this idea is to encourage the model to localize to
regions which can explain some semantic property in the data, in our case, the
property being the presence of a concept in a set of images. We present
thorough quantitative and qualitative experiments to demonstrate the efficacy
of our approach and show a 5.6% improvement over the current state of the art
on Visual Genome dataset, a 5.8% improvement on the ReferItGame dataset and
comparable to state-of-art performance on the Flickr30k dataset.",Analysis of different surrogate losses while varying the concept batch size.,Which loss type performs best when the concept batch size is 5k?
spiqa_430,1811.10673v1,"In Figure 1 of the paper ""Adversarial Video Compression Guided by Soft Edge Detection,"" which lossless compression scheme demonstrates the highest compression efficiency, achieving the lowest bits per pixel (BPP) for compressing the bi-level image?",The proposed scheme achieved the highest compression gain.,1811.10673v1.pdf,"['1811.10673v1.pdf', '1809.00458v1.pdf', '1707.01917v2.pdf', '1707.08608v3.pdf', '1710.05654v2.pdf', '1702.03584v3.pdf', '1703.00899v2.pdf', '1705.09882v2.pdf', '1709.02418v2.pdf', '1803.05776v2.pdf']","Figure 1 shows the BPP (bits per pixel) for different lossless compression schemes applied to a bi-level image. The proposed scheme has the lowest BPP, indicating the highest compression gain.",1811.10673v1-Figure5-1.png,Adversarial Video Compression Guided by Soft Edge Detection,"We propose a video compression framework using conditional Generative
Adversarial Networks (GANs). We rely on two encoders: one that deploys a
standard video codec and another which generates low-level maps via a pipeline
of down-sampling, a newly devised soft edge detector, and a novel lossless
compression scheme. For decoding, we use a standard video decoder as well as a
neural network based one, which is trained using a conditional GAN. Recent
""deep"" approaches to video compression require multiple videos to pre-train
generative networks to conduct interpolation. In contrast to this prior work,
our scheme trains a generative decoder on pairs of a very limited number of key
frames taken from a single video and corresponding low-level maps. The trained
decoder produces reconstructed frames relying on a guidance of low-level maps,
without any interpolation. Experiments on a diverse set of 131 videos
demonstrate that our proposed GAN-based compression engine achieves much higher
quality reconstructions at very low bitrates than prevailing standard codecs
such as H.264 or HEVC.",Figure 5: Efficiency in bits per pixel (BPP) achieved by different lossless compression schemes on a bi-level image.,Which lossless compression scheme achieved the highest compression gain in the example shown in Figure 1?
spiqa_431,1812.00108v4,"In the figure comparing the F1 scores across different methods when evaluating the Lobby dataset with models trained on the Multi-Ego dataset, which supervised method achieved the highest score?",Ours-supervised achieved the highest F1 score on the Lobby dataset with a score of 93.4.,1812.00108v4.pdf,"['1812.00108v4.pdf', '1803.02750v3.pdf', '1809.00458v1.pdf', '1708.01425v4.pdf', '1703.04887v4.pdf']","The table shows the F1 scores for different methods on three datasets: Office, Campus, and Lobby. The highest F1 score for the Lobby dataset is 93.4, which is achieved by the Ours-supervised method.",1812.00108v4-Table2-1.png,Multi-Stream Dynamic Video Summarization,"With vast amounts of video content being uploaded to the Internet every
minute, video summarization becomes critical for efficient browsing, searching,
and indexing of visual content. Nonetheless, the spread of social and
egocentric cameras creates an abundance of sparse scenarios captured by several
devices, and ultimately required to be jointly summarized. In this paper, we
discuss the problem of summarizing videos recorded independently by several
dynamic cameras that intermittently share the field of view. We present a
robust framework that (a) identifies a diverse set of important events among
moving cameras that often are not capturing the same scene, and (b) selects the
most representative view(s) at each event to be included in a universal
summary. Due to the lack of an applicable alternative, we collected a new
multi-view egocentric dataset, Multi-Ego. Our dataset is recorded
simultaneously by three cameras, covering a wide variety of real-life
scenarios. The footage is annotated by multiple individuals under various
summarization configurations, with a consensus analysis ensuring a reliable
ground truth. We conduct extensive experiments on the compiled dataset in
addition to three other standard benchmarks that show the robustness and the
advantage of our approach in both supervised and unsupervised settings.
Additionally, we show that our approach learns collectively from data of varied
number-of-views and orthogonal to other summarization methods, deeming it
scalable and generic.",Fixed-cameras multi-view f1-scores. We train our supervised model on Multi-Ego and test it on three datasets.,Which method achieved the highest F1 score on the Lobby dataset?
spiqa_432,1705.07164v8,"Based on Table 2 of the ""Relaxed Wasserstein with Applications to GANs"" paper, which method—RWGAN, WGAN, or WGAN(g)—achieved the highest Inception Score (IS) at the end of training for both CIFAR10 and ImageNet datasets, and how does this compare to the initial IS of the same method at the start of training?","For CIFAR10, WGAN achieved the highest IS at the end of training (2.42). However, RWGAN had a higher initial IS score (1.86) compared to WGAN's 1.63. 

For ImageNet, WGAN again achieved the highest final IS (2.80), while RWGAN had a slightly higher initial IS (2.04 vs. 2.00). ",1705.07164v8.pdf,"['1705.07164v8.pdf', '1805.00912v4.pdf', '1708.02153v2.pdf', '1705.02946v3.pdf', '1611.02654v2.pdf', '1803.04383v2.pdf', '1805.06447v3.pdf', '1703.02507v3.pdf', '1702.08694v3.pdf', '1804.07931v2.pdf', '1811.07073v3.pdf', '1703.10730v2.pdf']","The table presents the IS scores for different methods at both the beginning and end of the training process. By comparing the values in the ""end"" columns for both datasets, we can identify which method ultimately achieved the highest score. Additionally, comparing the values in the ""begin"" columns reveals which method started with the highest score.",1705.07164v8-Table2-1.png,Relaxed Wasserstein with Applications to GANs,"Wasserstein Generative Adversarial Networks (WGANs) provide a versatile class
of models, which have attracted great attention in various applications.
However, this framework has two main drawbacks: (i) Wasserstein-1 (or
Earth-Mover) distance is restrictive such that WGANs cannot always fit data
geometry well; (ii) It is difficult to achieve fast training of WGANs. In this
paper, we propose a new class of \textit{Relaxed Wasserstein} (RW) distances by
generalizing Wasserstein-1 distance with Bregman cost functions. We show that
RW distances achieve nice statistical properties while not sacrificing the
computational tractability. Combined with the GANs framework, we develop
Relaxed WGANs (RWGANs) which are not only statistically flexible but can be
approximated efficiently using heuristic approaches. Experiments on real images
demonstrate that the RWGAN with Kullback-Leibler (KL) cost function outperforms
other competing approaches, e.g., WGANs, even with gradient penalty.","Table 2: Inception scores (IS) obtained by running RWGAN, WGAN and WGAN(g). For cifar10, “begin"" and “end"" refer to IS averaged over first 5 and last 10 epochs. For imagenet, “begin"" and “end"" refer to IS averaged over first 3 and last 5 epochs.",Which method achieved the highest Inception Score (IS) at the end of training for both CIFAR10 and ImageNet datasets? Did this method also achieve the highest initial IS score?
spiqa_433,1804.07849v4,"According to Table 1 in the ""Mutual Information Maximization for Simple and Accurate Part-Of-Speech Induction"" paper, which method achieved the highest many-to-one accuracy on the 45-tag Penn WSJ dataset after 10 random restarts, and how does this accuracy compare to the method proposed by Berg-Kirkpatrick et al. (2010) and other approaches in terms of both accuracy and standard deviation?","The Variational $\wh{J}^{\mathrm{var}}$ method achieved the highest accuracy of 78.1% on the 45-tag Penn WSJ dataset. This is significantly higher than all other methods listed in the table, with the next best performing method (Berg-Kirkpatrick et al., 2010) achieving an accuracy of 74.9%.",1804.07849v4.pdf,"['1804.07849v4.pdf', '1702.03584v3.pdf', '1709.02418v2.pdf', '1805.02349v2.pdf', '1703.00899v2.pdf', '1704.08615v2.pdf', '1706.03847v3.pdf', '1803.03467v4.pdf', '1804.07707v2.pdf', '1707.00524v2.pdf', '1708.01425v4.pdf', '1705.07164v8.pdf']","Table 1 shows the accuracy of different methods on the 45-tag Penn WSJ dataset. The caption clarifies that the table presents the average accuracy over 10 random restarts with the best hyperparameter configurations for each method. The standard deviation is also provided, allowing for an assessment of the methods' performance stability. By comparing the accuracy values in the table, we can determine that Variational $\wh{J}^{\mathrm{var}}$  outperforms all other methods, achieving the highest average accuracy with a relatively low standard deviation.",1804.07849v4-Table1-1.png,Mutual Information Maximization for Simple and Accurate Part-Of-Speech Induction,"We address part-of-speech (POS) induction by maximizing the mutual
information between the induced label and its context. We focus on two training
objectives that are amenable to stochastic gradient descent (SGD): a novel
generalization of the classical Brown clustering objective and a recently
proposed variational lower bound. While both objectives are subject to noise in
gradient updates, we show through analysis and experiments that the variational
lower bound is robust whereas the generalized Brown objective is vulnerable. We
obtain competitive performance on a multitude of datasets and languages with a
simple architecture that encodes morphology and context.",Table 1: Many-to-one accuracy on the 45-tag Penn WSJ with the best hyperparameter configurations. The average accuracy over 10 random restarts is reported and the standard deviation is given in parentheses (except for deterministic methods).,"Which method achieved the highest accuracy on the 45-tag Penn WSJ dataset, and how does its performance compare to the other methods?"
spiqa_434,1804.07849v4,"Based on the figure titled ""Many-to-one accuracy on the 12-tag universal treebank dataset,"" which method achieves the highest reported accuracy of 77.4% for the Italian language data?",Variational J^var (7),1804.07849v4.pdf,"['1804.07849v4.pdf', '1708.06832v3.pdf', '1804.05995v2.pdf', '1611.03780v2.pdf', '1705.09296v2.pdf', '1809.02731v3.pdf', '1804.07931v2.pdf', '1704.07121v2.pdf', '1710.05654v2.pdf', '1805.06431v4.pdf', '1611.04363v2.pdf', '1603.00286v5.pdf', '1805.06447v3.pdf', '1811.09393v4.pdf']","The table shows the accuracy of different methods on different language data sets. The highest accuracy for the Italian data set is 77.4, which is achieved by the Variational J^var (7) method.",1804.07849v4-Table3-1.png,Mutual Information Maximization for Simple and Accurate Part-Of-Speech Induction,"We address part-of-speech (POS) induction by maximizing the mutual
information between the induced label and its context. We focus on two training
objectives that are amenable to stochastic gradient descent (SGD): a novel
generalization of the classical Brown clustering objective and a recently
proposed variational lower bound. While both objectives are subject to noise in
gradient updates, we show through analysis and experiments that the variational
lower bound is robust whereas the generalized Brown objective is vulnerable. We
obtain competitive performance on a multitude of datasets and languages with a
simple architecture that encodes morphology and context.",Many-to-one accuracy on the 12-tag universal treebank dataset. We use the same setting in Table 1. All models use a fixed hyperparameter configuration optimized on the 45-tag Penn WSJ.,Which method achieved the highest accuracy on the Italian language data set?
spiqa_435,1804.07849v4,"Referring to Table 4 in the paper, which part-of-speech induction method reported the highest mean V-measure (VM) score across multiple languages, and by how many points did it outperform the Baum-Welch method?","The Variational $\wh{J}^{\mathrm{var}}$ method achieved the highest average VM score (50.4). Its average score is 39.6 points higher than the Baum-Welch method, which achieved an average VM score of 10.8.",1804.07849v4.pdf,"['1804.07849v4.pdf', '1809.03149v2.pdf', '1901.00398v2.pdf', '1811.07073v3.pdf', '1804.07707v2.pdf', '1811.06635v1.pdf', '1611.03780v2.pdf']","The table shows the VM scores for different methods across various languages. By looking at the ""Mean"" column for the VM section, we can identify that Variational $\wh{J}^{\mathrm{var}}$ has the highest average score. The difference between the average scores of Variational $\wh{J}^{\mathrm{var}}$ and Baum-Welch can be calculated by simple subtraction (50.4 - 10.8 = 39.6).",1804.07849v4-Table4-1.png,Mutual Information Maximization for Simple and Accurate Part-Of-Speech Induction,"We address part-of-speech (POS) induction by maximizing the mutual
information between the induced label and its context. We focus on two training
objectives that are amenable to stochastic gradient descent (SGD): a novel
generalization of the classical Brown clustering objective and a recently
proposed variational lower bound. While both objectives are subject to noise in
gradient updates, we show through analysis and experiments that the variational
lower bound is robust whereas the generalized Brown objective is vulnerable. We
obtain competitive performance on a multitude of datasets and languages with a
simple architecture that encodes morphology and context.",Table 4: Comparison with the reported results with CRF autoencoders in many-to-one accuracy (M2O) and the V-measure (VM).,"Which method achieved the highest average V-measure (VM) across all languages, and how much higher was its average compared to the Baum-Welch method?"
spiqa_436,1809.01989v2,"Referring to Table 1, which method is reported to have the smallest sum of absolute percentage errors, and how does the comparison of positive versus negative errors highlight whether this method achieved the best overall market outperformance?","The Ridge method achieved the lowest sum of absolute percentage errors (136.84), indicating the highest tracking accuracy in terms of minimizing absolute deviations from the index. However, this doesn't necessarily translate to the best overall performance.",1809.01989v2.pdf,"['1809.01989v2.pdf', '1708.01425v4.pdf', '1704.07121v2.pdf', '1706.04269v2.pdf', '1811.10673v1.pdf', '1703.07015v3.pdf', '1809.03550v3.pdf', '1811.06635v1.pdf', '1707.06320v2.pdf', '1804.04410v2.pdf', '1705.08016v3.pdf']","While the sum/mean of absolute percentage errors in Table 2 reflects the tracking accuracy, the passage emphasizes the importance of considering the **sign** of the error. Positive errors, representing better returns than the market, are more desirable than negative errors. Although Ridge has the lowest overall error, the Cluster approach has a much higher proportion of positive errors (237.17) compared to its negative errors (21.42). This suggests that the Cluster approach, despite having a slightly higher total error than Ridge, might actually be achieving better overall performance due to its tendency to outperform the market.",1809.01989v2-Table1-1.png,Diversity and Sparsity: A New Perspective on Index Tracking,"We address the problem of partial index tracking, replicating a benchmark
index using a small number of assets. Accurate tracking with a sparse portfolio
is extensively studied as a classic finance problem. However in practice, a
tracking portfolio must also be diverse in order to minimise risk -- a
requirement which has only been dealt with by ad-hoc methods before. We
introduce the first index tracking method that explicitly optimises both
diversity and sparsity in a single joint framework. Diversity is realised by a
regulariser based on pairwise similarity of assets, and we demonstrate that
learning similarity from data can outperform some existing heuristics. Finally,
we show that the way we model diversity leads to an easy solution for sparsity,
allowing both constraints to be optimised easily and efficiently. we run
out-of-sample backtesting for a long interval of 15 years (2003 -- 2018), and
the results demonstrate the superiority of the proposed algorithm.",Table 1. Absolute percentage errors for different methods,Which method achieved the highest tracking accuracy in terms of minimizing the sum of absolute percentage errors? Does this necessarily mean it had the best overall performance?
spiqa_437,1805.06447v3,"Based on the figure presenting testing errors for the miniImageNet dataset in the ""Resisting Large Data Variations via Introspective Transformation Network"" paper, which method using ResNet-32 with data augmentation and introspective transformations achieved the lowest error rate?",ITTN (ResNet-32) (w/ DA) achieved the lowest testing error on the miniImageNet dataset with an error rate of 29.65%.,1805.06447v3.pdf,"['1805.06447v3.pdf', '1802.07222v1.pdf', '1710.05654v2.pdf', '1709.08294v3.pdf', '1906.06589v3.pdf', '1706.03847v3.pdf', '1709.02418v2.pdf', '1611.02654v2.pdf']",The table shows the testing errors of different methods on the miniImageNet dataset. The method with the lowest error rate is ITTN (ResNet-32) (w/ DA).,1805.06447v3-Table6-1.png,Resisting Large Data Variations via Introspective Transformation Network,"Training deep networks that generalize to a wide range of variations in test
data is essential to building accurate and robust image classifiers. One
standard strategy is to apply data augmentation to synthetically enlarge the
training set. However, data augmentation is essentially a brute-force method
which generates uniform samples from some pre-defined set of transformations.
In this paper, we propose a principled approach to train networks with
significantly improved resistance to large variations between training and
testing data. This is achieved by embedding a learnable transformation module
into the introspective network, which is a convolutional neural network (CNN)
classifier empowered with generative capabilities. Our approach alternates
between synthesizing pseudo-negative samples and transformed positive examples
based on the current model, and optimizing model predictions on these
synthesized samples. Experimental results verify that our approach
significantly improves the ability of deep networks to resist large variations
between training and testing data and achieves classification accuracy
improvements on several benchmark datasets, including MNIST, affNIST, SVHN,
CIFAR-10 and miniImageNet.",Testing errors on the miniImageNet dataset.,Which method achieved the lowest testing error on the miniImageNet dataset?
spiqa_438,1706.00827v2,"Referring to Table 1, which method consistently achieved zero false positives and the lowest false negatives across all test cases for simultaneous line and circle fitting, showcasing superior accuracy over the other methods?",Multi-X achieved the most accurate results for simultaneous line and circle fitting.,1706.00827v2.pdf,"['1706.00827v2.pdf', '1611.02654v2.pdf', '1809.03149v2.pdf', '1705.08016v3.pdf', '1906.10843v1.pdf', '1809.01246v1.pdf']","Table 1 shows the number of false positive (FP) and false negative (FN) instances for each method. Ideally, we want both FP and FN to be as low as possible, indicating fewer falsely detected and missed features. As shown in the table, Multi-X is the only method that achieved **zero** false positives across all test cases (columns 1, 2, and 3). Additionally, it had the lowest number of false negatives in two out of three cases and tied for the lowest in the remaining case. This demonstrates that Multi-X consistently produced the most accurate results compared to the other methods.",1706.00827v2-Table1-1.png,Multi-Class Model Fitting by Energy Minimization and Mode-Seeking,"We propose a general formulation, called Multi-X, for multi-class
multi-instance model fitting - the problem of interpreting the input data as a
mixture of noisy observations originating from multiple instances of multiple
classes. We extend the commonly used alpha-expansion-based technique with a new
move in the label space. The move replaces a set of labels with the
corresponding density mode in the model parameter domain, thus achieving fast
and robust optimization. Key optimization parameters like the bandwidth of the
mode seeking are set automatically within the algorithm. Considering that a
group of outliers may form spatially coherent structures in the data, we
propose a cross-validation-based technique removing statistically insignificant
instances. Multi-X outperforms significantly the state-of-the-art on publicly
available datasets for diverse problems: multiple plane and rigid motion
detection; motion segmentation; simultaneous plane and cylinder fitting; circle
and line fitting.",Table 1: The number of false positive (FP) and false negative (FN) instances for simultaneous line and circle fitting.,Which method achieved the most accurate results for simultaneous line and circle fitting?
spiqa_439,1706.08146v3,"Referring to Figure 3 of the paper on gene expression data with varying compression ratios n/d, which method—Factorize-Recover (FR) or Recover-Factorize (RF)—achieves a lower normalized reconstruction error when the compression factor exceeds 3?",Factorize-Recover,1706.08146v3.pdf,"['1706.08146v3.pdf', '1804.00863v3.pdf', '1706.04284v3.pdf', '1805.04609v3.pdf', '1812.10735v2.pdf']",The figure shows that the blue line (Factorize-Recover) is below the orange line (Recover-Factorize) when the compression factor is greater than 3.,1706.08146v3-Figure3-1.png,Compressed Factorization: Fast and Accurate Low-Rank Factorization of Compressively-Sensed Data,"What learning algorithms can be run directly on compressively-sensed data? In
this work, we consider the question of accurately and efficiently computing
low-rank matrix or tensor factorizations given data compressed via random
projections. We examine the approach of first performing factorization in the
compressed domain, and then reconstructing the original high-dimensional
factors from the recovered (compressed) factors. In both the matrix and tensor
settings, we establish conditions under which this natural approach will
provably recover the original factors. While it is well-known that random
projections preserve a number of geometric properties of a dataset, our work
can be viewed as showing that they can also preserve certain solutions of
non-convex, NP-Hard problems like non-negative matrix factorization. We support
these theoretical results with experiments on synthetic data and demonstrate
the practical applicability of compressed factorization on real-world gene
expression and EEG time series datasets.","Figure 3: Normalized reconstruction errors ‖Ŵ Ĥ −M‖F /‖M‖F for NMF on gene expression data with varying compression factors n/d. FR (blue, solid) is Factorize-Recover, RF (orange, dotted) is RecoverFactorize. The horizontal dashed line is the error when M is decomposed in the original space. Perhaps surprisingly, when n/d > 3, we observe a reduction in reconstruction error when compressed data is first factorized. See the text for further discussion.",Which method achieves lower approximation error when the compression factor is greater than 3?
spiqa_440,1811.09393v4,"Based on the figure that reports the averaged PSNR on 500 up-scaled frames from the Vid4 dataset (with the resolution increasing from 320x134 to 1280x536), which method achieves the highest pixel-wise accuracy?",DUF,1811.09393v4.pdf,"['1811.09393v4.pdf', '1809.01246v1.pdf', '1811.08481v2.pdf', '1710.05654v2.pdf', '1804.05936v2.pdf', '1805.06431v4.pdf', '1709.08294v3.pdf', '1809.02731v3.pdf', '1704.00774v3.pdf', '1804.04786v3.pdf', '1805.02349v2.pdf', '1704.07121v2.pdf', '1605.07496v3.pdf']",The table shows that DUF has the highest PSNR value of 27.38.,1811.09393v4-Table2-1.png,Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation,"Our work explores temporal self-supervision for GAN-based video generation
tasks. While adversarial training successfully yields generative models for a
variety of areas, temporal relationships in the generated data are much less
explored. Natural temporal changes are crucial for sequential generation tasks,
e.g. video super-resolution and unpaired video translation. For the former,
state-of-the-art methods often favor simpler norm losses such as $L^2$ over
adversarial training. However, their averaging nature easily leads to
temporally smooth results with an undesirable lack of spatial detail. For
unpaired video translation, existing approaches modify the generator networks
to form spatio-temporal cycle consistencies. In contrast, we focus on improving
learning objectives and propose a temporally self-supervised algorithm. For
both tasks, we show that temporal adversarial learning is key to achieving
temporally coherent solutions without sacrificing spatial detail. We also
propose a novel Ping-Pong loss to improve the long-term temporal consistency.
It effectively prevents recurrent networks from accumulating artifacts
temporally without depressing detailed features. Additionally, we propose a
first set of metrics to quantitatively evaluate the accuracy as well as the
perceptual quality of the temporal evolution. A series of user studies confirm
the rankings computed with these metrics. Code, data, models, and results are
provided at https://github.com/thunil/TecoGAN. The project page
https://ge.in.tum.de/publications/2019-tecogan-chu/ contains supplemental
materials.","Averaged VSR metric evaluations for the Vid4 data set with the following metrics, PSNR: pixel-wise accuracy. LPIPS (AlexNet): perceptual distance to the ground truth. T-diff: pixel-wise differences of warped frames. tOF: pixel-wise distance of estimated motions. tLP: perceptual distance between consecutive frames. User study: Bradley-Terry scores [Bradley and Terry 1952]. Performance is averaged over 500 images up-scaled from 320x134 to 1280x536. More details can be found in Appendix A and Sec. 3 of the supplemental web-page.",Which method achieves the highest PSNR on the Vid4 data set?
spiqa_441,1705.08016v3,"Based on the results shown in the figure from the Pairwise Confusion for Fine-Grained Visual Classification paper, which model achieves the best Top-1 accuracy on the CUB-200-2011 dataset with Pairwise Confusion regularization?",PC-DenseNet-161,1705.08016v3.pdf,"['1705.08016v3.pdf', '1809.04276v2.pdf', '1805.00912v4.pdf', '1707.01917v2.pdf', '1811.02553v4.pdf', '1805.06447v3.pdf', '1805.07567v2.pdf']",The table shows the Top-1 accuracy for each method on the CUB-200-2011 dataset. PC-DenseNet-161 has the highest Top-1 accuracy of 86.87.,1705.08016v3-Table2-1.png,Pairwise Confusion for Fine-Grained Visual Classification,"Fine-Grained Visual Classification (FGVC) datasets contain small sample
sizes, along with significant intra-class variation and inter-class similarity.
While prior work has addressed intra-class variation using localization and
segmentation techniques, inter-class similarity may also affect feature
learning and reduce classification performance. In this work, we address this
problem using a novel optimization procedure for the end-to-end neural network
training on FGVC tasks. Our procedure, called Pairwise Confusion (PC) reduces
overfitting by intentionally {introducing confusion} in the activations. With
PC regularization, we obtain state-of-the-art performance on six of the most
widely-used FGVC datasets and demonstrate improved localization ability. {PC}
is easy to implement, does not need excessive hyperparameter tuning during
training, and does not add significant overhead during test time.",Pairwise Confusion (PC) obtains state-of-the-art performance on six widelyused fine-grained visual classification datasets (A-F). Improvement over the baseline model is reported as (∆). All results averaged over 5 trials.,Which method achieves the highest Top-1 accuracy on the CUB-200-2011 dataset?
spiqa_442,1707.01917v2,"Based on the comparison of higher-order relation schema accuracies in the figure caption, which method achieves the highest average accuracy of 0.76 on the Shootings dataset?",TFBA,1707.01917v2.pdf,"['1707.01917v2.pdf', '1603.00286v5.pdf', '1611.02654v2.pdf', '1805.06431v4.pdf', '1703.10730v2.pdf', '1811.08257v1.pdf', '1708.01425v4.pdf', '1705.07384v2.pdf', '1611.05742v3.pdf', '1707.08608v3.pdf', '1804.00863v3.pdf', '1809.02731v3.pdf']","The table shows the accuracy of different methods on different datasets. For the Shootings dataset, TFBA has the highest average accuracy of 0.76.",1707.01917v2-Table5-1.png,Higher-order Relation Schema Induction using Tensor Factorization with Back-off and Aggregation,"Relation Schema Induction (RSI) is the problem of identifying type signatures
of arguments of relations from unlabeled text. Most of the previous work in
this area have focused only on binary RSI, i.e., inducing only the subject and
object type signatures per relation. However, in practice, many relations are
high-order, i.e., they have more than two arguments and inducing type
signatures of all arguments is necessary. For example, in the sports domain,
inducing a schema win(WinningPlayer, OpponentPlayer, Tournament, Location) is
more informative than inducing just win(WinningPlayer, OpponentPlayer). We
refer to this problem as Higher-order Relation Schema Induction (HRSI). In this
paper, we propose Tensor Factorization with Back-off and Aggregation (TFBA), a
novel framework for the HRSI problem. To the best of our knowledge, this is the
first attempt at inducing higher-order relation schemata from unlabeled text.
Using the experimental analysis on three real world datasets, we show how TFBA
helps in dealing with sparsity and induce higher order schemata.","Higher-order RSI accuracies of various methods on the three datasets. Induced schemata for each dataset and method are evaluated by three human evaluators, E1, E2, and E3. TFBA performs better than HardClust for Shootings and NYT Sports datasets. Even though HardClust achieves better accuracy on MUC dataset, it has several limitations, see Section 4 for more details. Chambers-13 solves a slightly different problem called event schema induction, for more details about the comparison with Chambers-13 see Section 4.1.",Which method achieves the highest accuracy on the Shootings dataset?
spiqa_443,1811.08481v2,"According to the figure titled ""CLEVR QA accuracy for state-of-the-art methods"" in the paper, which method achieves the highest overall validation accuracy of 99.8%?",UnCoRd-None-B.,1811.08481v2.pdf,"['1811.08481v2.pdf', '1803.06506v3.pdf', '1805.04609v3.pdf', '1803.05776v2.pdf', '1805.08751v2.pdf', '1703.00060v2.pdf', '1804.07849v4.pdf', '1708.05239v3.pdf', '1710.05654v2.pdf', '1804.05938v2.pdf']",The table shows the overall accuracy of different methods on the validation set. UnCoRd-None-B has the highest accuracy of 99.8%.,1811.08481v2-Table2-1.png,VQA with no questions-answers training,"Methods for teaching machines to answer visual questions have made
significant progress in recent years, but current methods still lack important
human capabilities, including integrating new visual classes and concepts in a
modular manner, providing explanations for the answers and handling new domains
without explicit examples. We propose a novel method that consists of two main
parts: generating a question graph representation, and an answering procedure,
guided by the abstract structure of the question graph to invoke an extendable
set of visual estimators. Training is performed for the language part and the
visual part on their own, but unlike existing schemes, the method does not
require any training using images with associated questions and answers. This
approach is able to handle novel domains (extended question types and new
object classes, properties and relations) as long as corresponding visual
estimators are available. In addition, it can provide explanations to its
answers and suggest alternatives when questions are not grounded in the image.
We demonstrate that this approach achieves both high performance and domain
extensibility without any questions-answers training.",CLEVR QA accuracy for state-of-the-art methods,Which method achieves the highest overall accuracy on the validation set?
spiqa_444,1708.06832v3,"Based on the data in Figure (b) for ILSVRC, which network architecture achieves the lowest test error rate of 28.0 at 1/4 of the total computational cost when utilizing the AdaLoss method?",MSDNNet38,1708.06832v3.pdf,"['1708.06832v3.pdf', '1703.04887v4.pdf', '1804.05995v2.pdf', '1703.07015v3.pdf', '1804.05938v2.pdf', '1809.00458v1.pdf']",The table in Figure (b) shows the error rates of different methods on ILSVRC at different fractions of the total cost. MSDNNet38 + AdaLoss has the lowest error rate of 28.0 at 1/4 of the total cost.,1708.06832v3-Figure3-1.png,Learning Anytime Predictions in Neural Networks via Adaptive Loss Balancing,"This work considers the trade-off between accuracy and test-time
computational cost of deep neural networks (DNNs) via \emph{anytime}
predictions from auxiliary predictions. Specifically, we optimize auxiliary
losses jointly in an \emph{adaptive} weighted sum, where the weights are
inversely proportional to average of each loss. Intuitively, this balances the
losses to have the same scale. We demonstrate theoretical considerations that
motivate this approach from multiple viewpoints, including connecting it to
optimizing the geometric mean of the expectation of each loss, an objective
that ignores the scale of losses. Experimentally, the adaptive weights induce
more competitive anytime predictions on multiple recognition data-sets and
models than non-adaptive approaches including weighing all losses equally. In
particular, anytime neural networks (ANNs) can achieve the same accuracy faster
using adaptive weights on a small network than using static constant weights on
a large one. For problems with high performance saturation, we also show a
sequence of exponentially deepening ANNscan achieve near-optimal anytime
results at any budget, at the cost of a const fraction of extra computation.","(a) Average relative percentage increase in error from OPT on CIFAR and SVHN at 1/4, 1/2, 3/4 and 1 of the total cost. E.g., the bottom right entry means that if OPT has a 10% final error rate, then AdaLoss has about 10.27%. (b) Test error rates at different fraction of the total costs on ResANN50 and DenseANN169.",Which method achieves the lowest error rate on ILSVRC at 1/4 of the total cost?
spiqa_445,1805.06431v4,"In the context of Table 1, which method demonstrates the most robust performance, consistently achieving the lowest RMSE across various outlier rates, particularly when noisy or corrupted outputs are present in the synthetic datasets?",ChoiceNet appears to be the most robust to outliers in the training data.,1805.06431v4.pdf,"['1805.06431v4.pdf', '1802.07459v2.pdf', '1811.06635v1.pdf', '1707.06320v2.pdf', '1804.01429v3.pdf', '1702.03584v3.pdf', '1804.05995v2.pdf', '1709.08294v3.pdf', '1906.06589v3.pdf', '1802.07351v2.pdf', '1701.03077v10.pdf', '1802.07222v1.pdf']","Table 1 shows the RMSEs of different methods for various percentages of outliers in the training data. While all methods perform well with no outliers (RMSE < 0.1), ChoiceNet consistently maintains the lowest RMSE even as the outlier rate increases. The passage also explicitly states that ChoiceNet successfully fits the target function even with outlier rates exceeding 40%, whereas other methods fail. This suggests that ChoiceNet is better able to handle noisy data and produce accurate predictions compared to the other methods.",1805.06431v4-Table4-1.png,Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks,"In this paper, we focus on weakly supervised learning with noisy training
data for both classification and regression problems.We assume that the
training outputs are collected from a mixture of a target and correlated noise
distributions.Our proposed method simultaneously estimates the target
distribution and the quality of each data which is defined as the correlation
between the target and data generating distributions.The cornerstone of the
proposed method is a Cholesky Block that enables modeling dependencies among
mixture distributions in a differentiable manner where we maintain the
distribution over the network weights.We first provide illustrative examples in
both regression and classification tasks to show the effectiveness of the
proposed method.Then, the proposed method is extensively evaluated in a number
of experiments where we show that it constantly shows comparable or superior
performances compared to existing baseline methods in the handling of noisy
data.",Table 4: The RMSEs of compared methods on synthetic toy examples,Which method appears to be most robust to the presence of outliers in the training data?
spiqa_446,1805.06431v4,"Based on Table 7, which method consistently achieves the lowest collision rates on straight lanes, even when up to 40% of the vehicles are outliers, indicating the safest performance for autonomous driving in the presence of noisy data?","ChoiceNet appears to be the safest method for autonomous driving on straight lanes, regardless of the percentage of outlier vehicles present.",1805.06431v4.pdf,"['1805.06431v4.pdf', '1809.00263v5.pdf', '1804.05938v2.pdf', '1703.00899v2.pdf', '1812.00281v3.pdf', '1809.03550v3.pdf', '1809.00458v1.pdf']","Table 1 shows the collision rates of different methods on straight lanes with varying percentages of outlier vehicles (0% to 40%). ChoiceNet consistently demonstrates the lowest collision rate (0% or close to 0%) across all outlier percentages. This is further supported by the passage, which explicitly states that ChoiceNet outperforms other methods in terms of safety, as evidenced by its low collision rates.",1805.06431v4-Table7-1.png,Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks,"In this paper, we focus on weakly supervised learning with noisy training
data for both classification and regression problems.We assume that the
training outputs are collected from a mixture of a target and correlated noise
distributions.Our proposed method simultaneously estimates the target
distribution and the quality of each data which is defined as the correlation
between the target and data generating distributions.The cornerstone of the
proposed method is a Cholesky Block that enables modeling dependencies among
mixture distributions in a differentiable manner where we maintain the
distribution over the network weights.We first provide illustrative examples in
both regression and classification tasks to show the effectiveness of the
proposed method.Then, the proposed method is extensively evaluated in a number
of experiments where we show that it constantly shows comparable or superior
performances compared to existing baseline methods in the handling of noisy
data.",Table 7: Collision rates of compared methods on straight lanes.,Which method appears to be the safest for autonomous driving on straight lanes with different levels of outlier vehicles?
spiqa_447,1701.03077v10,"Based on the experimental results presented in Table 2 of the paper on unsupervised monocular depth estimation using the KITTI dataset, which method for controlling the shape parameter of the proposed per-wavelet general loss function achieved the lowest average error, and by what percentage did it outperform the baseline model from [42]?","The ""adaptive $\power \in (0, 2)$"" strategy, where each wavelet coefficient has its own shape parameter that is optimized during training, achieved the best performance in terms of average error. It reduced the average error by approximately 17% compared to the reproduced baseline.",1701.03077v10.pdf,"['1701.03077v10.pdf', '1709.02755v5.pdf', '1901.00398v2.pdf', '1804.01429v3.pdf', '1707.01917v2.pdf', '1608.02784v2.pdf', '1708.02153v2.pdf', '1811.08257v1.pdf', '1704.05958v2.pdf', '1901.00056v2.pdf', '1708.01425v4.pdf', '1804.04786v3.pdf', '1809.03149v2.pdf']","The table presents various error and accuracy metrics for different methods of setting the shape parameter in the proposed loss function. The ""adaptive $\power \in (0, 2)$"" model shows the lowest average error (0.332) among all the listed methods. The reproduced baseline has an average error of 0.398. The percentage improvement can be calculated as: 

(0.398 - 0.332) / 0.398 * 100% ≈ 17%",1701.03077v10-Table2-1.png,A General and Adaptive Robust Loss Function,"We present a generalization of the Cauchy/Lorentzian, Geman-McClure,
Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2
loss functions. By introducing robustness as a continuous parameter, our loss
function allows algorithms built around robust loss minimization to be
generalized, which improves performance on basic vision tasks such as
registration and clustering. Interpreting our loss as the negative log of a
univariate density yields a general probability distribution that includes
normal and Cauchy distributions as special cases. This probabilistic
interpretation enables the training of neural networks in which the robustness
of the loss automatically adapts itself during training, which improves
performance on learning-based tasks such as generative image synthesis and
unsupervised monocular depth estimation, without requiring any manual parameter
tuning.","Table 2. Results on unsupervised monocular depth estimation using the KITTI dataset [13], building upon the model from [42] (“Baseline”). By replacing the per-pixel loss used by [42] with several variants of our own per-wavelet general loss function in which our loss’s shape parameters are fixed, annealed, or adaptive, we see a significant performance improvement. The top three techniques are colored red, orange, and yellow for each metric.",Which method for setting the shape parameter of the proposed loss function achieved the best performance in terms of average error? How much improvement did it offer compared to the reproduced baseline?
spiqa_448,1709.00139v4,"Based on Table 1 from the paper *""Fast Incremental SVDD Learning Algorithm with the Gaussian Kernel""*, which method—FISVDD or Incremental SVM—achieved a lower objective function value (OFV) across the datasets, and does the slight difference in OFV suggest that Incremental SVM is definitively better than FISVDD, considering the training time and overall efficiency?","For all datasets presented, Incremental SVM achieved a slightly lower OFV compared to FISVDD. However, this does not necessarily mean that Incremental SVM is definitively better.",1709.00139v4.pdf,"['1709.00139v4.pdf', '1612.02803v5.pdf', '1901.00398v2.pdf', '1703.04887v4.pdf', '1811.08257v1.pdf', '1809.04276v2.pdf', '1802.07351v2.pdf', '1703.02507v3.pdf']","The table shows that the OFV values for both methods are very close for each dataset. While Incremental SVM consistently has a lower value, the difference is minimal. Additionally, the passage mentions that FISVDD offers significant gains in efficiency, as evidenced by the significantly shorter training times shown in the table. Therefore, the slight decrease in OFV achieved by Incremental SVM might not outweigh its significantly longer training time, making FISVDD a potentially more attractive option depending on the specific needs of the application.",1709.00139v4-Table1-1.png,Fast Incremental SVDD Learning Algorithm with the Gaussian Kernel,"Support vector data description (SVDD) is a machine learning technique that
is used for single-class classification and outlier detection. The idea of SVDD
is to find a set of support vectors that defines a boundary around data. When
dealing with online or large data, existing batch SVDD methods have to be rerun
in each iteration. We propose an incremental learning algorithm for SVDD that
uses the Gaussian kernel. This algorithm builds on the observation that all
support vectors on the boundary have the same distance to the center of sphere
in a higher-dimensional feature space as mapped by the Gaussian kernel
function. Each iteration involves only the existing support vectors and the new
data point. Moreover, the algorithm is based solely on matrix manipulations;
the support vectors and their corresponding Lagrange multiplier $\alpha_i$'s
are automatically selected and determined in each iteration. It can be seen
that the complexity of our algorithm in each iteration is only $O(k^2)$, where
$k$ is the number of support vectors. Experimental results on some real data
sets indicate that FISVDD demonstrates significant gains in efficiency with
almost no loss in either outlier detection accuracy or objective function
value.",Table 1: Experimental Results of FISVDD and Incremental SVM on Different Data Sets,"Which method generally achieved a lower objective function value (OFV) for the different datasets, FISVDD or Incremental SVM? Does this imply that one method is definitively better than the other?"
spiqa_449,1805.06431v4,"Based on the results of Table 6 for the HalfCheetah behavior cloning task, how does ChoiceNet perform relative to MDN at different outlier percentages (10%, 20%, and 30%), and how does the performance gap between the two models change as the level of corrupt data increases?","ChoiceNet generally performed better than MDN in the HalfCheetah task. This is evident from the higher average returns of ChoiceNet across all outlier percentages (10%, 20%, and 30%).

The performance gap between ChoiceNet and MDN appears to decrease as the percentage of outliers increases. At 10% outliers, ChoiceNet has a significantly higher average return than MDN (2068.14 vs. 192.53). However, at 30% outliers, the difference in average return is smaller (2035.91 vs. 363.08).",1805.06431v4.pdf,"['1805.06431v4.pdf', '1804.07931v2.pdf', '1805.04609v3.pdf', '1708.03797v1.pdf', '1812.00108v4.pdf', '1811.02721v3.pdf', '1704.05426v4.pdf', '1611.07718v2.pdf', '1708.02153v2.pdf', '1809.01246v1.pdf', '1703.00060v2.pdf', '1812.10735v2.pdf']","The table shows the average returns of different methods in the HalfCheetah task for different outlier percentages. By comparing the values in the ChoiceNet and MDN columns, we can directly see which method performed better for each outlier percentage. The difference in average return values reflects the performance gap between the two methods.",1805.06431v4-Table6-1.png,Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks,"In this paper, we focus on weakly supervised learning with noisy training
data for both classification and regression problems.We assume that the
training outputs are collected from a mixture of a target and correlated noise
distributions.Our proposed method simultaneously estimates the target
distribution and the quality of each data which is defined as the correlation
between the target and data generating distributions.The cornerstone of the
proposed method is a Cholesky Block that enables modeling dependencies among
mixture distributions in a differentiable manner where we maintain the
distribution over the network weights.We first provide illustrative examples in
both regression and classification tasks to show the effectiveness of the
proposed method.Then, the proposed method is extensively evaluated in a number
of experiments where we show that it constantly shows comparable or superior
performances compared to existing baseline methods in the handling of noisy
data.",Table 6: Average returns of compared methods on behavior cloning problems using MuJoCo,"Which method generally performed better in the HalfCheetah task, ChoiceNet or MDN? How does the performance gap between these two methods change as the percentage of outliers increases?"
spiqa_450,1809.00263v5,"According to the figure in the Stochastic Dynamics for Video Infilling paper, which method accurately models the dynamics of the basketball video sequence and produces the most realistic moving objects compared to SuperSloMo and SepConv?",SDVI,1809.00263v5.pdf,"['1809.00263v5.pdf', '1703.00899v2.pdf', '1709.02418v2.pdf', '1704.04539v2.pdf']","The SDVI method is able to model the dynamic of the basketball video sequence correctly and generate better moving objects than SuperSloMo and SepConv. This can be seen in the figure, where the SDVI method is able to generate more realistic and accurate moving objects than the other two methods.",1809.00263v5-Figure16-1.png,Stochastic Dynamics for Video Infilling,"In this paper, we introduce a stochastic dynamics video infilling (SDVI)
framework to generate frames between long intervals in a video. Our task
differs from video interpolation which aims to produce transitional frames for
a short interval between every two frames and increase the temporal resolution.
Our task, namely video infilling, however, aims to infill long intervals with
plausible frame sequences. Our framework models the infilling as a constrained
stochastic generation process and sequentially samples dynamics from the
inferred distribution. SDVI consists of two parts: (1) a bi-directional
constraint propagation module to guarantee the spatial-temporal coherence among
frames, (2) a stochastic sampling process to generate dynamics from the
inferred distributions. Experimental results show that SDVI can generate clear
frame sequences with varying contents. Moreover, motions in the generated
sequence are realistic and able to transfer smoothly from the given start frame
to the terminal frame. Our project site is
https://xharlie.github.io/projects/project_sites/SDVI/video_results.html",A more complicated UCF101 example: a real basketball video sequence involving multiple objects. Our method can model the dynamic correctly and generate better moving objects than SuperSloMo and SepConv.,Which method generates the best moving objects?
spiqa_452,1811.09393v4,"In the figure comparing ENet, FRVSR, DUF, and TecoGAN on video super-resolution of Vid4 using PieAPP for perceptual metrics, which model is represented by the largest bubble, indicating the highest tOF score and best temporal coherence?",TecoGAN.,1811.09393v4.pdf,"['1811.09393v4.pdf', '1703.00899v2.pdf', '1705.02946v3.pdf', '1707.01917v2.pdf', '1704.07121v2.pdf', '1803.05776v2.pdf', '1606.07384v2.pdf']","The bubble size indicates the tOF score. TecoGAN has the largest bubble, which means it has the highest tOF score.",1811.09393v4-Figure18-1.png,Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation,"Our work explores temporal self-supervision for GAN-based video generation
tasks. While adversarial training successfully yields generative models for a
variety of areas, temporal relationships in the generated data are much less
explored. Natural temporal changes are crucial for sequential generation tasks,
e.g. video super-resolution and unpaired video translation. For the former,
state-of-the-art methods often favor simpler norm losses such as $L^2$ over
adversarial training. However, their averaging nature easily leads to
temporally smooth results with an undesirable lack of spatial detail. For
unpaired video translation, existing approaches modify the generator networks
to form spatio-temporal cycle consistencies. In contrast, we focus on improving
learning objectives and propose a temporally self-supervised algorithm. For
both tasks, we show that temporal adversarial learning is key to achieving
temporally coherent solutions without sacrificing spatial detail. We also
propose a novel Ping-Pong loss to improve the long-term temporal consistency.
It effectively prevents recurrent networks from accumulating artifacts
temporally without depressing detailed features. Additionally, we propose a
first set of metrics to quantitatively evaluate the accuracy as well as the
perceptual quality of the temporal evolution. A series of user studies confirm
the rankings computed with these metrics. Code, data, models, and results are
provided at https://github.com/thunil/TecoGAN. The project page
https://ge.in.tum.de/publications/2019-tecogan-chu/ contains supplemental
materials.","Tables and visualization of perceptual metrics computed with PieAPP [Prashnani et al. 2018] (instead of LPIPS used in Fig. 14 previously) on ENet, FRVSR, DUF and TecoGAN for the VSR of Vid4. Bubble size indicates the tOF score.",Which method has the highest T-Diff on average for the Vid4 dataset?
spiqa_453,1805.07567v2,"Which method, as illustrated by the red dashed line in the figure comparing performance versus training iterations, exhibits both the fastest convergence and the highest final F-measure score?",F-DSS,1805.07567v2.pdf,"['1805.07567v2.pdf', '1803.04383v2.pdf', '1811.10673v1.pdf', '1703.00899v2.pdf', '1705.08016v3.pdf', '1709.02755v5.pdf', '1705.09882v2.pdf', '1809.00263v5.pdf', '1707.00524v2.pdf', '1708.06832v3.pdf', '1603.03833v4.pdf']",The F-DSS line (red dashed line) reaches the highest F-measure value first and also has the highest F-measure value at the end of the training process.,1805.07567v2-Figure7-1.png,Optimizing the F-measure for Threshold-free Salient Object Detection,"Current CNN-based solutions to salient object detection (SOD) mainly rely on
the optimization of cross-entropy loss (CELoss). Then the quality of detected
saliency maps is often evaluated in terms of F-measure. In this paper, we
investigate an interesting issue: can we consistently use the F-measure
formulation in both training and evaluation for SOD? By reformulating the
standard F-measure we propose the relaxed F-measure which is differentiable
w.r.t the posterior and can be easily appended to the back of CNNs as the loss
function. Compared to the conventional cross-entropy loss of which the
gradients decrease dramatically in the saturated area, our loss function, named
FLoss, holds considerable gradients even when the activation approaches the
target. Consequently, the FLoss can continuously force the network to produce
polarized activations. Comprehensive benchmarks on several popular datasets
show that FLoss outperforms the state-of-the-art with a considerable margin.
More specifically, due to the polarized predictions, our method is able to
obtain high-quality saliency maps without carefully tuning the optimal
threshold, showing significant advantages in real-world applications.",Performance versus training iterations. Our method presents faster convergence and higher converged performance.,Which method has the fastest convergence and highest converged performance?
spiqa_456,1811.08481v2,"Based on the accuracy results shown in the figure comparing UnCoRd-VG-E and Pythia on 100 questions sampled from the VQA v2 dataset, which method demonstrates superior performance in answering visual questions using UnCoRd's visual estimators?",UnCoRd-VG-E,1811.08481v2.pdf,"['1811.08481v2.pdf', '1805.02349v2.pdf', '1608.02784v2.pdf', '1803.01128v3.pdf', '1809.03149v2.pdf', '1702.08694v3.pdf', '1703.10730v2.pdf', '1803.02750v3.pdf', '1707.08608v3.pdf', '1701.06171v4.pdf', '1708.05239v3.pdf', '1811.07073v3.pdf']","The table shows the accuracy of two methods for answering questions about images. The overall accuracy of UnCoRd-VG-E is 92.0, which is higher than the overall accuracy of Pythia (77.0).",1811.08481v2-Table8-1.png,VQA with no questions-answers training,"Methods for teaching machines to answer visual questions have made
significant progress in recent years, but current methods still lack important
human capabilities, including integrating new visual classes and concepts in a
modular manner, providing explanations for the answers and handling new domains
without explicit examples. We propose a novel method that consists of two main
parts: generating a question graph representation, and an answering procedure,
guided by the abstract structure of the question graph to invoke an extendable
set of visual estimators. Training is performed for the language part and the
visual part on their own, but unlike existing schemes, the method does not
require any training using images with associated questions and answers. This
approach is able to handle novel domains (extended question types and new
object classes, properties and relations) as long as corresponding visual
estimators are available. In addition, it can provide explanations to its
answers and suggest alternatives when questions are not grounded in the image.
We demonstrate that this approach achieves both high performance and domain
extensibility without any questions-answers training.",Answering accuracy for 100 questions sampled from VQA v2 dataset (on terms with visual estimators in UnCoRd).,Which method has the highest overall accuracy for answering questions about images on the 100 questions sampled from VQA v2 dataset?
spiqa_458,1805.06447v3,"Based on the figure illustrating testing errors for MNIST, affNIST, and TMTA, which version of the introspective transformation network achieves a lower testing error on the MNIST dataset?",ITN,1805.06447v3.pdf,"['1805.06447v3.pdf', '1804.04786v3.pdf', '1805.07567v2.pdf', '1704.00774v3.pdf']","The table shows that ITN has a testing error of 0.47% on the MNIST task, while ITN-NG has a testing error of 0.49%.",1805.06447v3-Table8-1.png,Resisting Large Data Variations via Introspective Transformation Network,"Training deep networks that generalize to a wide range of variations in test
data is essential to building accurate and robust image classifiers. One
standard strategy is to apply data augmentation to synthetically enlarge the
training set. However, data augmentation is essentially a brute-force method
which generates uniform samples from some pre-defined set of transformations.
In this paper, we propose a principled approach to train networks with
significantly improved resistance to large variations between training and
testing data. This is achieved by embedding a learnable transformation module
into the introspective network, which is a convolutional neural network (CNN)
classifier empowered with generative capabilities. Our approach alternates
between synthesizing pseudo-negative samples and transformed positive examples
based on the current model, and optimizing model predictions on these
synthesized samples. Experimental results verify that our approach
significantly improves the ability of deep networks to resist large variations
between training and testing data and achieves classification accuracy
improvements on several benchmark datasets, including MNIST, affNIST, SVHN,
CIFAR-10 and miniImageNet.","Testing errors of ITN and ITN-NG on MNIST, affNIST, and TMTA task, where ITN-NG is the version of ITN without generating pseudo-negative samples.",Which method has the lower testing error on the MNIST task?
spiqa_459,1706.00827v2,"According to the figure comparing misclassification errors across methods for two-view motion segmentation on the AdelaideRMF dataset, which method achieves the lowest average misclassification error for the cubechips image pair?",Multi-X,1706.00827v2.pdf,"['1706.00827v2.pdf', '1703.02507v3.pdf', '1704.04539v2.pdf', '1710.01507v4.pdf', '1809.00263v5.pdf', '1809.01246v1.pdf', '1706.04269v2.pdf', '1804.05938v2.pdf', '1708.06832v3.pdf', '1708.03797v1.pdf', '1803.01128v3.pdf', '1703.07015v3.pdf']","The table shows the average and minimum misclassification errors for each method on each image pair. For the cubechips image pair, Multi-X has the lowest average misclassification error of 3.45%.",1706.00827v2-Table4-1.png,Multi-Class Model Fitting by Energy Minimization and Mode-Seeking,"We propose a general formulation, called Multi-X, for multi-class
multi-instance model fitting - the problem of interpreting the input data as a
mixture of noisy observations originating from multiple instances of multiple
classes. We extend the commonly used alpha-expansion-based technique with a new
move in the label space. The move replaces a set of labels with the
corresponding density mode in the model parameter domain, thus achieving fast
and robust optimization. Key optimization parameters like the bandwidth of the
mode seeking are set automatically within the algorithm. Considering that a
group of outliers may form spatially coherent structures in the data, we
propose a cross-validation-based technique removing statistically insignificant
instances. Multi-X outperforms significantly the state-of-the-art on publicly
available datasets for diverse problems: multiple plane and rigid motion
detection; motion segmentation; simultaneous plane and cylinder fitting; circle
and line fitting.","Misclassification errors (%) for two-view motion segmentation on the AdelaideRMF dataset. All the methods were tuned separately for each video by the authors. Tested image pairs: (1) cubechips, (2) cubetoy, (3) breadcube, (4) gamebiscuit, (5) breadtoycar, (6) biscuitbookbox, (7) breadcubechips, (8) cubebreadtoychips.",Which method has the lowest average misclassification error for the cubechips image pair?
spiqa_461,1803.04572v2,"Based on the figure comparing iteration times across different values of R on both the CHOA and CMS datasets, which method, COPA or Helwig, demonstrates faster performance, especially considering the failure of Helwig on the CMS dataset due to memory constraints?",COPA is faster than Helwig.,1803.04572v2.pdf,"['1803.04572v2.pdf', '1611.04363v2.pdf', '1611.02654v2.pdf', '1708.00160v2.pdf', '1803.03467v4.pdf']",The figure shows that COPA has a lower time than Helwig for all values of R on both the CHOA and CMS data sets.,1803.04572v2-Figure7-1.png,COPA: Constrained PARAFAC2 for Sparse & Large Datasets,"PARAFAC2 has demonstrated success in modeling irregular tensors, where the
tensor dimensions vary across one of the modes. An example scenario is modeling
treatments across a set of patients with the varying number of medical
encounters over time. Despite recent improvements on unconstrained PARAFAC2,
its model factors are usually dense and sensitive to noise which limits their
interpretability. As a result, the following open challenges remain: a) various
modeling constraints, such as temporal smoothness, sparsity and non-negativity,
are needed to be imposed for interpretable temporal modeling and b) a scalable
approach is required to support those constraints efficiently for large
datasets. To tackle these challenges, we propose a {\it CO}nstrained {\it
PA}RAFAC2 (COPA) method, which carefully incorporates optimization constraints
such as temporal smoothness, sparsity, and non-negativity in the resulting
factors. To efficiently support all those constraints, COPA adopts a hybrid
optimization framework using alternating optimization and alternating direction
method of multiplier (AO-ADMM). As evaluated on large electronic health record
(EHR) datasets with hundreds of thousands of patients, COPA achieves
significant speedups (up to 36 times faster) over prior PARAFAC2 approaches
that only attempt to handle a subset of the constraints that COPA enables.
Overall, our method outperforms all the baselines attempting to handle a subset
of the constraints in terms of speed, while achieving the same level of
accuracy. Through a case study on temporal phenotyping of medically complex
children, we demonstrate how the constraints imposed by COPA reveal concise
phenotypes and meaningful temporal profiles of patients. The clinical
interpretation of both the phenotypes and the temporal profiles was confirmed
by a medical expert.","Time in seconds for one iteration (as an average of 5 different random initializations) for different values of R. The left figure is the comparison on CHOA and the right figure shows the comparison on CMS. For R=40 COPA achieves 32× over the Helwig approach on CHOA while for CMS dataset, execution in Helwig failed due to the excessive amount of memory request and COPA finished an iteration with the average of 224.21 seconds.","Which method is faster, COPA or Helwig?"
spiqa_462,1809.00458v1,"Based on the figure comparing accuracy versus space on the COD dataset, which method—GB-KMV or LSH-E—demonstrates superior performance in terms of space efficiency while maintaining a high accuracy for approximate containment similarity search?",GB-KMV is more efficient at utilizing space while maintaining high accuracy.,1809.00458v1.pdf,"['1809.00458v1.pdf', '1804.07707v2.pdf', '1804.04410v2.pdf', '1805.06431v4.pdf']",The plot on the left shows that GB-KMV achieves higher accuracy than LSH-E while using less space.,1809.00458v1-Figure7-1.png,GB-KMV: An Augmented KMV Sketch for Approximate Containment Similarity Search,"In this paper, we study the problem of approximate containment similarity
search. Given two records Q and X, the containment similarity between Q and X
with respect to Q is |Q intersect X|/ |Q|. Given a query record Q and a set of
records S, the containment similarity search finds a set of records from S
whose containment similarity regarding Q are not less than the given threshold.
This problem has many important applications in commercial and scientific
fields such as record matching and domain search. Existing solution relies on
the asymmetric LSH method by transforming the containment similarity to
well-studied Jaccard similarity. In this paper, we use a different framework by
transforming the containment similarity to set intersection. We propose a novel
augmented KMV sketch technique, namely GB-KMV, which is data-dependent and can
achieve a good trade-off between the sketch size and the accuracy. We provide a
set of theoretical analysis to underpin the proposed augmented KMV sketch
technique, and show that it outperforms the state-of-the-art technique LSH-E in
terms of estimation accuracy under practical assumption. Our comprehensive
experiments on real-life datasets verify that GB-KMV is superior to LSH-E in
terms of the space-accuracy trade-off, time-accuracy trade-off, and the sketch
construction time. For instance, with similar estimation accuracy (F-1 score),
GB-KMV is over 100 times faster than LSH-E on some real-life dataset.",Accuracy versus Space on COD,Which method is more efficient at utilizing space while maintaining high accuracy?
spiqa_463,1710.05654v2,"In the time comparison figure between different graph computation methods, which method achieves the fastest computation for a graph with 1,000,000 nodes and an average node degree of k=5, as shown in the right panel of the US Census 1990 dataset?",The proposed method in this paper (k=5) is the fastest for computing a graph with a small average node degree.,1710.05654v2.pdf,"['1710.05654v2.pdf', '1804.07849v4.pdf', '1706.00827v2.pdf', '1703.04887v4.pdf', '1703.10730v2.pdf', '1809.04276v2.pdf', '1804.04786v3.pdf', '1707.00524v2.pdf', '1709.08294v3.pdf', '1804.07707v2.pdf', '1906.06589v3.pdf', '1809.00458v1.pdf', '1706.04269v2.pdf', '1805.01216v3.pdf']",The figure on the right shows that the method in this paper (k=5) has the lowest time complexity for computing a graph with a small average node degree.,1710.05654v2-Figure1-1.png,Large Scale Graph Learning from Smooth Signals,"Graphs are a prevalent tool in data science, as they model the inherent
structure of the data. They have been used successfully in unsupervised and
semi-supervised learning. Typically they are constructed either by connecting
nearest samples, or by learning them from data, solving an optimization
problem. While graph learning does achieve a better quality, it also comes with
a higher computational cost. In particular, the current state-of-the-art model
cost is $\mathcal{O}(n^2)$ for $n$ samples. In this paper, we show how to scale
it, obtaining an approximation with leading cost of $\mathcal{O}(n\log(n))$,
with quality that approaches the exact graph learning model. Our algorithm uses
known approximate nearest neighbor techniques to reduce the number of
variables, and automatically selects the correct parameters of the model,
requiring a single intuitive input: the desired edge density.","Time comparison of different ways to compute a graph. Left: Graph between 10,000 most frequent English words using a word2vec representation. Right: Graph between 1,000,000 nodes from 68 features (US Census 1990). Scalable algorithms benefit from a small average node degree k.",Which method is the fastest for computing a graph with a small average node degree?
spiqa_464,1611.04363v2,"Based on Table 1 in the ""QA-Expert"" column of this paper, which method achieved the highest P@1 score, and by how much did it outperform the average P@1 score of D2V for the QA-Expert task?","The WeakFG method achieved the highest P@1 score for the QA-Expert task with a score of 52.8. This is 23.2% higher than the average P@1 score of the D2V method, which was 29.6. ",1611.04363v2.pdf,"['1611.04363v2.pdf', '1709.02418v2.pdf', '1710.01507v4.pdf', '1811.02721v3.pdf', '1702.08694v3.pdf', '1804.07849v4.pdf', '1703.00899v2.pdf', '1704.00774v3.pdf', '1710.06177v2.pdf', '1805.01216v3.pdf', '1804.07707v2.pdf']","The table provides the P@1 scores for all methods under the ""QA-Expert"" column. By comparing these values, we can identify WeakFG as the best performing method for this metric. We can then calculate the difference between WeakFG's score and D2V's average score to determine the performance gap.",1611.04363v2-Table1-1.png,Weakly Learning to Match Experts in Online Community,"In online question-and-answer (QA) websites like Quora, one central issue is
to find (invite) users who are able to provide answers to a given question and
at the same time would be unlikely to say ""no"" to the invitation. The challenge
is how to trade off the matching degree between users' expertise and the
question topic, and the likelihood of positive response from the invited users.
In this paper, we formally formulate the problem and develop a weakly
supervised factor graph (WeakFG) model to address the problem. The model
explicitly captures expertise matching degree between questions and users. To
model the likelihood that an invited user is willing to answer a specific
question, we incorporate a set of correlations based on social identity theory
into the WeakFG model. We use two different genres of datasets: QA-Expert and
Paper-Reviewer, to validate the proposed model. Our experimental results show
that the proposed model can significantly outperform (+1.5-10.7% by MAP) the
state-of-the-art algorithms for matching users (experts) with community
questions. We have also developed an online system to further demonstrate the
advantages of the proposed method.",Table 1: Performance comparison of different methods.,"Which method performed best according to the P@1 metric for the QA-Expert task, and how much better did it perform compared to the average P@1 score of the D2V method? "
spiqa_465,1706.00633v4,"Based on the classification error rates shown in the figure for the MNIST dataset in the paper ""Towards Robust Detection of Adversarial Examples,"" which model, ResNet-32 trained with cross-entropy (CE) or ResNet-56 trained with reverse cross-entropy (RCE), achieved a lower error rate and better performance?",ResNet-56 (RCE) performed better on the MNIST dataset with a classification error rate of 0.32% compared to ResNet-32 (CE) which had a classification error rate of 0.38%.,1706.00633v4.pdf,"['1706.00633v4.pdf', '1812.06589v2.pdf', '1606.07384v2.pdf', '1811.10673v1.pdf', '1809.02731v3.pdf', '1708.05239v3.pdf', '1703.07015v3.pdf', '1708.02153v2.pdf', '1812.10735v2.pdf', '1709.00139v4.pdf', '1705.10667v4.pdf', '1805.06431v4.pdf', '1611.07718v2.pdf']",The table shows the classification error rates for different methods on the MNIST and CIFAR-10 datasets. We can compare the error rates for the two methods on the MNIST dataset to see which one performed better.,1706.00633v4-Table1-1.png,Towards Robust Detection of Adversarial Examples,"Although the recent progress is substantial, deep learning methods can be
vulnerable to the maliciously generated adversarial examples. In this paper, we
present a novel training procedure and a thresholding test strategy, towards
robust detection of adversarial examples. In training, we propose to minimize
the reverse cross-entropy (RCE), which encourages a deep network to learn
latent representations that better distinguish adversarial examples from normal
ones. In testing, we propose to use a thresholding strategy as the detector to
filter out adversarial examples for reliable predictions. Our method is simple
to implement using standard algorithms, with little extra training cost
compared to the common cross-entropy minimization. We apply our method to
defend various attacking methods on the widely used MNIST and CIFAR-10
datasets, and achieve significant improvements on robust predictions under all
the threat models in the adversarial setting.",Classification error rates (%) on test sets.,"Which method performed better on the MNIST dataset, ResNet-32 (CE) or ResNet-56 (RCE)?"
spiqa_466,1812.06589v2,"In the figure illustrating quantitative results, which method, including the proposed approach, achieves the lowest LMD value, indicating the most accurate lip synchronization for arbitrary talking face generation?",AMIE (Ours),1812.06589v2.pdf,"['1812.06589v2.pdf', '1805.00912v4.pdf', '1703.02507v3.pdf', '1702.08694v3.pdf', '1809.01989v2.pdf', '1603.00286v5.pdf', '1701.03077v10.pdf']","The table shows the LMD values for different methods, and AMIE (Ours) has the lowest LMD value, which is desirable because lower LMD indicates better performance.",1812.06589v2-Table1-1.png,Arbitrary Talking Face Generation via Attentional Audio-Visual Coherence Learning,"Talking face generation aims to synthesize a face video with precise lip
synchronization as well as a smooth transition of facial motion over the entire
video via the given speech clip and facial image. Most existing methods mainly
focus on either disentangling the information in a single image or learning
temporal information between frames. However, cross-modality coherence between
audio and video information has not been well addressed during synthesis. In
this paper, we propose a novel arbitrary talking face generation framework by
discovering the audio-visual coherence via the proposed Asymmetric Mutual
Information Estimator (AMIE). In addition, we propose a Dynamic Attention (DA)
block by selectively focusing the lip area of the input image during the
training stage, to further enhance lip synchronization. Experimental results on
benchmark LRW dataset and GRID dataset transcend the state-of-the-art methods
on prevalent metrics with robust high-resolution synthesizing on gender and
pose variations.",Quantitative results.,Which method performed the best on the GRID dataset?
spiqa_468,1811.07073v3,"In Table 2 of the paper, what is the exact score of the ""Conv. Self-Corr."" method on the PASCAL VOC 2012 test set, and by how many points does it outperform the baseline method ""No Self-Corr."" in terms of segmentation accuracy?","The Conv. Self-Corr. method achieved the highest performance on the PASCAL VOC 2012 test set with a score of 82.72. This is approximately 1.11 points higher than the baseline model (""No Self-Corr."") which achieved a score of 81.61.",1811.07073v3.pdf,"['1811.07073v3.pdf', '1707.00189v3.pdf', '1705.09966v2.pdf', '1708.02153v2.pdf', '1812.06589v2.pdf']","The table shows the performance of different methods on the PASCAL VOC 2012 validation and test sets. The ""Test"" column provides the scores for the test set. By comparing the values in this column, we can identify the best performing method. The difference between the ""Conv. Self-Corr."" and ""No Self-Corr."" scores demonstrates the improvement gained by using the convolutional self-correction approach.",1811.07073v3-Table2-1.png,Semi-Supervised Semantic Image Segmentation with Self-correcting Networks,"Building a large image dataset with high-quality object masks for semantic
segmentation is costly and time consuming. In this paper, we introduce a
principled semi-supervised framework that only uses a small set of fully
supervised images (having semantic segmentation labels and box labels) and a
set of images with only object bounding box labels (we call it the weak set).
Our framework trains the primary segmentation model with the aid of an
ancillary model that generates initial segmentation labels for the weak set and
a self-correction module that improves the generated labels during training
using the increasingly accurate primary model. We introduce two variants of the
self-correction module using either linear or convolutional functions.
Experiments on the PASCAL VOC 2012 and Cityscape datasets show that our models
trained with a small fully supervised set perform similar to, or better than,
models trained with a large fully supervised set while requiring ~7x less
annotation effort.",Table 2: Results on PASCAL VOC 2012 validation and test sets. The last three rows report the performance of previous semi-supervised models with the same annotation.,Which method performed the best on the PASCAL VOC 2012 test set and how does it compare to the baseline model without self-correction?
spiqa_469,1805.06431v4,"In the figure comparing methods on MNIST with noise levels of 25%, 40%, 45%, and 47%, which model consistently achieves the highest accuracy across both the training and test sets?",ChoiceNet.,1805.06431v4.pdf,"['1805.06431v4.pdf', '1701.06171v4.pdf', '1901.00398v2.pdf', '1707.00524v2.pdf', '1611.04684v1.pdf', '1804.05938v2.pdf', '1703.00899v2.pdf', '1802.07459v2.pdf', '1706.03847v3.pdf', '1708.05239v3.pdf', '1811.02721v3.pdf', '1803.04572v2.pdf', '1802.07351v2.pdf', '1803.03467v4.pdf', '1812.06589v2.pdf']","The figure shows that ChoiceNet consistently achieves the highest accuracy across all noise levels (25%, 40%, 45%, and 47%) on both the training and test sets.",1805.06431v4-Figure13-1.png,Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks,"In this paper, we focus on weakly supervised learning with noisy training
data for both classification and regression problems.We assume that the
training outputs are collected from a mixture of a target and correlated noise
distributions.Our proposed method simultaneously estimates the target
distribution and the quality of each data which is defined as the correlation
between the target and data generating distributions.The cornerstone of the
proposed method is a Cholesky Block that enables modeling dependencies among
mixture distributions in a differentiable manner where we maintain the
distribution over the network weights.We first provide illustrative examples in
both regression and classification tasks to show the effectiveness of the
proposed method.Then, the proposed method is extensively evaluated in a number
of experiments where we show that it constantly shows comparable or superior
performances compared to existing baseline methods in the handling of noisy
data.",Learning curves of compared methods on random permutation experiments using MNIST with different noise levels.,Which method performs the best when there are a lot of outliers in the data?
spiqa_470,1611.03780v2,"In the ""Randomized Experimental Design via Geographic Clustering"" paper, according to Figure (b), how do GeoCUTS and DMA compare in terms of ensuring that 100% of queries from highly active users in the US have a Q-metric of at least 0.75?",Both GeoCUTS and DMA perform equally well for highly active users in the US.,1611.03780v2.pdf,"['1611.03780v2.pdf', '1606.07384v2.pdf', '1809.02731v3.pdf', '1804.07707v2.pdf', '1705.09966v2.pdf']","In Figure (b), for highly active users in the US, both GeoCUTS and DMA have 100% of queries from clusters with a Q-metric of at least 0.75 and 0.8.",1611.03780v2-Table1-1.png,Randomized Experimental Design via Geographic Clustering,"Web-based services often run randomized experiments to improve their
products. A popular way to run these experiments is to use geographical regions
as units of experimentation, since this does not require tracking of individual
users or browser cookies. Since users may issue queries from multiple
geographical locations, geo-regions cannot be considered independent and
interference may be present in the experiment. In this paper, we study this
problem, and first present GeoCUTS, a novel algorithm that forms geographical
clusters to minimize interference while preserving balance in cluster size. We
use a random sample of anonymized traffic from Google Search to form a graph
representing user movements, then construct a geographically coherent
clustering of the graph. Our main technical contribution is a statistical
framework to measure the effectiveness of clusterings. Furthermore, we perform
empirical evaluations showing that the performance of GeoCUTS is comparable to
hand-crafted geo-regions with respect to both novel and existing metrics.","(a) Average and query-weighted average (Query-w. avg) of Q-metric, (b) Percentage of queries from clusters with a Q-metric of at least x%. ∼ 200 clusters were used for the US and ∼ 50 for France. For both highly active and highly mobile graphs, GeoCUTS performs comparably to DMAs and outperforms the baseline grid clustering.",Which method performs best for highly active users in the US?
spiqa_471,1804.00863v3,"Which method achieves the best performance in terms of mean DSSIM error for the ""Representation"" task under the ""Novel"" view, as presented in the figure showing quantitative results on synthetic data in the Deep Appearance Maps paper?","The ""OUR"" method performs best for the ""Representation"" task when the view is ""Novel"".",1804.00863v3.pdf,"['1804.00863v3.pdf', '1804.01429v3.pdf', '1705.09966v2.pdf', '1803.03467v4.pdf', '1809.00458v1.pdf', '1704.05958v2.pdf', '1809.03550v3.pdf', '1707.00189v3.pdf', '1709.02418v2.pdf', '1703.00060v2.pdf', '1704.04539v2.pdf', '1710.06177v2.pdf', '1812.06589v2.pdf']","The table shows that the ""OUR"" method has a lower error rate (0.144) than the ""RM++"" method (0.181) for the ""Representation"" task when the view is ""Novel"".",1804.00863v3-Table1-1.png,Deep Appearance Maps,"We propose a deep representation of appearance, i. e., the relation of color,
surface orientation, viewer position, material and illumination. Previous
approaches have useddeep learning to extract classic appearance
representationsrelating to reflectance model parameters (e. g., Phong)
orillumination (e. g., HDR environment maps). We suggest todirectly represent
appearance itself as a network we call aDeep Appearance Map (DAM). This is a 4D
generalizationover 2D reflectance maps, which held the view direction fixed.
First, we show how a DAM can be learned from images or video frames and later
be used to synthesize appearance, given new surface orientations and viewer
positions. Second, we demonstrate how another network can be used to map from
an image or video frames to a DAM network to reproduce this appearance, without
using a lengthy optimization such as stochastic gradient descent
(learning-to-learn). Finally, we show the example of an appearance
estimation-and-segmentation task, mapping from an image showingmultiple
materials to multiple deep appearance maps.","Quantitative results on synthetic data. Rows are different combination of tasks and methods (three applications, two view protocols, our two methods). Columns are different data. Error is measured as mean DSSIM across the data set (less is better).","Which method performs best for the ""Representation"" task when the view is ""Novel""?"
spiqa_472,1710.06177v2,"Referring to the figure that tracks Top-1 accuracy across increasing shot numbers in the 10 classes 1-shot multi-class classification problem, which method consistently achieves the highest performance?",VAGER + Voting,1710.06177v2.pdf,"['1710.06177v2.pdf', '1707.06320v2.pdf', '1811.07073v3.pdf', '1703.02507v3.pdf', '1704.05958v2.pdf', '1603.00286v5.pdf', '1705.08016v3.pdf', '1706.00633v4.pdf', '1901.00056v2.pdf', '1805.08751v2.pdf', '1606.07384v2.pdf']",The figure shows the Top-1 Accuracy of different methods as the number of shots increases. VAGER + Voting has the highest Top-1 Accuracy for all numbers of shots.,1710.06177v2-Figure5-1.png,Learning to Learn Image Classifiers with Visual Analogy,"Humans are far better learners who can learn a new concept very fast with
only a few samples compared with machines. The plausible mystery making the
difference is two fundamental learning mechanisms: learning to learn and
learning by analogy. In this paper, we attempt to investigate a new human-like
learning method by organically combining these two mechanisms. In particular,
we study how to generalize the classification parameters from previously
learned concepts to a new concept. we first propose a novel Visual Analogy
Graph Embedded Regression (VAGER) model to jointly learn a low-dimensional
embedding space and a linear mapping function from the embedding space to
classification parameters for base classes. We then propose an out-of-sample
embedding method to learn the embedding of a new class represented by a few
samples through its visual analogy with base classes and derive the
classification parameters for the new class. We conduct extensive experiments
on ImageNet dataset and the results show that our method could consistently and
significantly outperform state-of-the-art baselines.",Change of performance as shot number increases in 10 classes 1-shot multi-class classification problem.,Which method performs best in the 10 classes 1-shot multi-class classification problem?
spiqa_473,1805.06447v3,"Based on the ""Testing errors on SVHN and CIFAR-10"" figure presented in the paper, which method demonstrates the lowest testing error on the CIFAR-10 dataset?",ITN (ResNet-32) with data augmentation performs best on the CIFAR-10 dataset with a testing error of 5.82%.,1805.06447v3.pdf,"['1805.06447v3.pdf', '1901.00398v2.pdf', '1706.00827v2.pdf', '1709.08294v3.pdf', '1704.08615v2.pdf', '1703.00899v2.pdf', '1804.07707v2.pdf', '1705.10667v4.pdf', '1707.00189v3.pdf', '1809.03449v3.pdf']",The table shows the testing errors for different methods on the SVHN and CIFAR-10 datasets. We can see that ITN (ResNet-32) with data augmentation has the lowest testing error for the CIFAR-10 dataset.,1805.06447v3-Table5-1.png,Resisting Large Data Variations via Introspective Transformation Network,"Training deep networks that generalize to a wide range of variations in test
data is essential to building accurate and robust image classifiers. One
standard strategy is to apply data augmentation to synthetically enlarge the
training set. However, data augmentation is essentially a brute-force method
which generates uniform samples from some pre-defined set of transformations.
In this paper, we propose a principled approach to train networks with
significantly improved resistance to large variations between training and
testing data. This is achieved by embedding a learnable transformation module
into the introspective network, which is a convolutional neural network (CNN)
classifier empowered with generative capabilities. Our approach alternates
between synthesizing pseudo-negative samples and transformed positive examples
based on the current model, and optimizing model predictions on these
synthesized samples. Experimental results verify that our approach
significantly improves the ability of deep networks to resist large variations
between training and testing data and achieves classification accuracy
improvements on several benchmark datasets, including MNIST, affNIST, SVHN,
CIFAR-10 and miniImageNet.",Testing errors on SVHN and CIFAR-10.,Which method performs best on the CIFAR-10 dataset?
spiqa_474,1611.05742v3,"Based on the handheld testing results in the PaSC dataset (PaSC2) shown in Table 1 of the paper ""*Building Deep Networks on Grassmann Manifolds,*"" which method achieves the highest accuracy, and how does this performance compare to other methods such as VGGDeepFace and DeepO2P?","The method that performs best on the PaSC dataset for the handheld testing scenario (PaSC2) is SPDNet, with an accuracy of 72.83%. This performance is slightly higher than GrNet-2Blocks (72.76%) and significantly higher than other methods like VGGDeepFace (68.24%) and DeepO2P (60.14%).",1611.05742v3.pdf,"['1611.05742v3.pdf', '1707.01917v2.pdf', '1701.03077v10.pdf', '1809.00263v5.pdf', '1804.04410v2.pdf', '1809.00458v1.pdf', '1811.08257v1.pdf', '1709.02418v2.pdf', '1704.05958v2.pdf', '1804.05936v2.pdf', '1605.07496v3.pdf', '1812.00281v3.pdf', '1809.04276v2.pdf', '1706.04284v3.pdf', '1706.00633v4.pdf']","By looking at the column corresponding to PaSC2 in Table 1, we can directly compare the accuracy of all the methods for this specific testing scenario. We can see that SPDNet has the highest accuracy value, indicating its superior performance compared to the other methods.",1611.05742v3-Table1-1.png,Building Deep Networks on Grassmann Manifolds,"Learning representations on Grassmann manifolds is popular in quite a few
visual recognition tasks. In order to enable deep learning on Grassmann
manifolds, this paper proposes a deep network architecture by generalizing the
Euclidean network paradigm to Grassmann manifolds. In particular, we design
full rank mapping layers to transform input Grassmannian data to more desirable
ones, exploit re-orthonormalization layers to normalize the resulting matrices,
study projection pooling layers to reduce the model complexity in the
Grassmannian context, and devise projection mapping layers to respect
Grassmannian geometry and meanwhile achieve Euclidean forms for regular output
layers. To train the Grassmann networks, we exploit a stochastic gradient
descent setting on manifolds of the connection weights, and study a matrix
generalization of backpropagation to update the structured data. The
evaluations on three visual recognition tasks show that our Grassmann networks
have clear advantages over existing Grassmann learning methods, and achieve
results comparable with state-of-the-art approaches.","Table 1: Results for the AFEW, HDM05 and PaSC datasets. PaSC1/PaSC2 are the control/handheld testings.","Which method performs best on the PaSC dataset for the handheld testing scenario (PaSC2), and how does its performance compare to other methods?"
spiqa_475,1805.06447v3,"Referring to Table 1, which shows the testing errors of various methods on the TMTA task, which method achieves the lowest error rate, and how does the use of data augmentation affect the performance compared to when it is not used?","The ITN (B-CNN) method with data augmentation (DA) performs best on the TMTA task, achieving a testing error of 21.31%. Data augmentation contributes significantly to its performance, as the ITN (B-CNN) method without DA has a higher testing error of 31.67%.",1805.06447v3.pdf,"['1805.06447v3.pdf', '1611.04684v1.pdf', '1811.06635v1.pdf', '1704.07854v4.pdf', '1812.00281v3.pdf']","Table 1 presents the testing errors of different methods on the TMTA task. The method with the lowest error rate is considered the best performing. We can see that ITN (B-CNN) with DA has the lowest error rate (21.31%) among all methods. Additionally, comparing the performance of ITN (B-CNN) with and without DA reveals a substantial improvement in error rate when DA is used, indicating its significant contribution to the performance.",1805.06447v3-Table1-1.png,Resisting Large Data Variations via Introspective Transformation Network,"Training deep networks that generalize to a wide range of variations in test
data is essential to building accurate and robust image classifiers. One
standard strategy is to apply data augmentation to synthetically enlarge the
training set. However, data augmentation is essentially a brute-force method
which generates uniform samples from some pre-defined set of transformations.
In this paper, we propose a principled approach to train networks with
significantly improved resistance to large variations between training and
testing data. This is achieved by embedding a learnable transformation module
into the introspective network, which is a convolutional neural network (CNN)
classifier empowered with generative capabilities. Our approach alternates
between synthesizing pseudo-negative samples and transformed positive examples
based on the current model, and optimizing model predictions on these
synthesized samples. Experimental results verify that our approach
significantly improves the ability of deep networks to resist large variations
between training and testing data and achieves classification accuracy
improvements on several benchmark datasets, including MNIST, affNIST, SVHN,
CIFAR-10 and miniImageNet.",Table 1. Testing errors of TMTA task.,Which method performs best on the TMTA task and how much does data augmentation contribute to its performance?
spiqa_476,1704.07121v2,"Based on the accuracy results presented in Table 7 of ""Being Negative but Constructively"", which machine learning method performs best overall on the VQA−-2014val dataset, and by how much does its accuracy fall short of the human benchmark?","MLP-IQA achieves the highest overall accuracy (46.5%) among the machine learning methods tested on VQA-2014val. However, this performance still falls short of human performance, which reaches an accuracy of 85.5% on the same dataset.",1704.07121v2.pdf,"['1704.07121v2.pdf', '1603.03833v4.pdf', '1803.05776v2.pdf', '1705.09296v2.pdf', '1811.06635v1.pdf', '1709.08294v3.pdf', '1809.03550v3.pdf', '1809.00458v1.pdf']","The table shows the accuracy of different methods on VQA$^-$-2014val, a dataset containing visual question-answering triplets. The ""All"" column presents the overall accuracy for each method. Comparing the values in this column reveals that MLP-IQA outperforms other methods like MLP-A, MLP-IA, and MLP-QA. However, when compared to the ""Human-IQA"" row, it becomes evident that even the best performing machine learning method still has a significant gap in accuracy compared to human performance.",1704.07121v2-Table7-1.png,Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets,"Visual question answering (Visual QA) has attracted a lot of attention
lately, seen essentially as a form of (visual) Turing test that artificial
intelligence should strive to achieve. In this paper, we study a crucial
component of this task: how can we design good datasets for the task? We focus
on the design of multiple-choice based datasets where the learner has to select
the right answer from a set of candidate ones including the target (\ie the
correct one) and the decoys (\ie the incorrect ones). Through careful analysis
of the results attained by state-of-the-art learning models and human
annotators on existing datasets, we show that the design of the decoy answers
has a significant impact on how and what the learning models learn from the
datasets. In particular, the resulting learner can ignore the visual
information, the question, or both while still doing well on the task. Inspired
by this, we propose automatic procedures to remedy such design deficiencies. We
apply the procedures to re-construct decoy answers for two popular Visual QA
datasets as well as to create a new Visual QA dataset from the Visual Genome
project, resulting in the largest dataset for this task. Extensive empirical
studies show that the design deficiencies have been alleviated in the remedied
datasets and the performance on them is likely a more faithful indicator of the
difference among learning models. The datasets are released and publicly
available via http://www.teds.usc.edu/website_vqa/.","Table 7: Accuracy (%) on VQA−-2014val, which contains 76,034 triplets.","Which method performs best overall on VQA-2014val, and how does its performance compare to human performance on the same dataset?"
spiqa_477,1805.06431v4,"Referring to Table 15, which method achieves the highest test accuracy on the Large Movie Review dataset when there is no label corruption (p = 0%), and how does its performance change as corruption levels increase to 40%?","When there is no label corruption (p = 0%), Mixup achieves the highest test accuracy of 79.77%. However, as the corruption level increases, Mixup's performance deteriorates more rapidly compared to other methods. ChoiceNet, on the other hand, demonstrates a more stable performance across different corruption levels, maintaining the highest accuracy when p is 10%, 20%, 30%, and 40%.",1805.06431v4.pdf,"['1805.06431v4.pdf', '1603.03833v4.pdf', '1706.03847v3.pdf', '1811.10673v1.pdf', '1804.05936v2.pdf', '1705.07384v2.pdf', '1805.01216v3.pdf', '1805.06447v3.pdf']","The table displays the test accuracies of different methods under varying degrees of label corruption. By comparing the values in the p = 0% column, we can identify the best performing method in the absence of corruption. Additionally, by observing the trend of each method's accuracy as p increases, we can assess their relative robustness to label noise.",1805.06431v4-Table15-1.png,Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks,"In this paper, we focus on weakly supervised learning with noisy training
data for both classification and regression problems.We assume that the
training outputs are collected from a mixture of a target and correlated noise
distributions.Our proposed method simultaneously estimates the target
distribution and the quality of each data which is defined as the correlation
between the target and data generating distributions.The cornerstone of the
proposed method is a Cholesky Block that enables modeling dependencies among
mixture distributions in a differentiable manner where we maintain the
distribution over the network weights.We first provide illustrative examples in
both regression and classification tasks to show the effectiveness of the
proposed method.Then, the proposed method is extensively evaluated in a number
of experiments where we show that it constantly shows comparable or superior
performances compared to existing baseline methods in the handling of noisy
data.",Table 15: Test accuracies on the Large Movie Review dataset with different corruption probabilities.,Which method performs best when there is no label corruption (p = 0%) on the Large Movie Review dataset and how does its performance change as the corruption level increases?
spiqa_478,1707.01922v5,"Based on Figure (c) of the paper, which sensor fusion method, ZDDA3 or naive fusion, demonstrates superior classification accuracy across varying noise levels in both RGB and depth testing data?",ZDDA3,1707.01922v5.pdf,"['1707.01922v5.pdf', '1704.05426v4.pdf', '1803.03467v4.pdf', '1709.02418v2.pdf', '1709.02755v5.pdf', '1803.01128v3.pdf', '1804.05938v2.pdf']","Figure (c) shows the difference in classification accuracy between ZDDA3 and naive fusion. In most cases, the difference is positive, which means that ZDDA3 performs better than naive fusion.",1707.01922v5-Figure4-1.png,Zero-Shot Deep Domain Adaptation,"Domain adaptation is an important tool to transfer knowledge about a task
(e.g. classification) learned in a source domain to a second, or target domain.
Current approaches assume that task-relevant target-domain data is available
during training. We demonstrate how to perform domain adaptation when no such
task-relevant target-domain data is available. To tackle this issue, we propose
zero-shot deep domain adaptation (ZDDA), which uses privileged information from
task-irrelevant dual-domain pairs. ZDDA learns a source-domain representation
which is not only tailored for the task of interest but also close to the
target-domain representation. Therefore, the source-domain task of interest
solution (e.g. a classifier for classification tasks) which is jointly trained
with the source-domain representation can be applicable to both the source and
target representations. Using the MNIST, Fashion-MNIST, NIST, EMNIST, and SUN
RGB-D datasets, we show that ZDDA can perform domain adaptation in
classification tasks without access to task-relevant target-domain training
data. We also extend ZDDA to perform sensor fusion in the SUN RGB-D scene
classification task by simulating task-relevant target-domain representations
with task-relevant source-domain data. To the best of our knowledge, ZDDA is
the first domain adaptation and sensor fusion method which requires no
task-relevant target-domain data. The underlying principle is not particular to
computer vision data, but should be extensible to other domains.",Performance comparison between the two sensor fusion methods with black images as the noisy images. We compare the classification accuracy (%) of (a) naive fusion and (b) ZDDA3 under different noise levels in both RGB and depth testing data. (c) shows that ZDDA3 outperforms the naive fusion under most conditions,Which method performs better in terms of classification accuracy?
spiqa_479,1804.05938v2,"Based on the figure comparing the MSE between DLA and RandList with respect to η, which algorithm provides more accurate click propensity estimates across the entire range?",DLA performs better than RandList in terms of mean square error.,1804.05938v2.pdf,"['1804.05938v2.pdf', '1809.01246v1.pdf', '1812.06589v2.pdf', '1705.09296v2.pdf', '1703.04887v4.pdf', '1803.04383v2.pdf', '1805.01216v3.pdf', '1805.04609v3.pdf', '1802.07351v2.pdf', '1802.07459v2.pdf', '1704.08615v2.pdf', '1707.01922v5.pdf', '1811.07073v3.pdf', '1612.02803v5.pdf']",The plot shows the mean square error (MSE) between the true click propensity and those estimated by DLA and RandList with respect to η. The MSE for DLA is consistently lower than that of RandList for all values of η.,1804.05938v2-Figure2-1.png,Unbiased Learning to Rank with Unbiased Propensity Estimation,"Learning to rank with biased click data is a well-known challenge. A variety
of methods has been explored to debias click data for learning to rank such as
click models, result interleaving and, more recently, the unbiased
learning-to-rank framework based on inverse propensity weighting. Despite their
differences, most existing studies separate the estimation of click bias
(namely the \textit{propensity model}) from the learning of ranking algorithms.
To estimate click propensities, they either conduct online result
randomization, which can negatively affect the user experience, or offline
parameter estimation, which has special requirements for click data and is
optimized for objectives (e.g. click likelihood) that are not directly related
to the ranking performance of the system. In this work, we address those
problems by unifying the learning of propensity models and ranking models. We
find that the problem of estimating a propensity model from click data is a
dual problem of unbiased learning to rank. Based on this observation, we
propose a Dual Learning Algorithm (DLA) that jointly learns an unbiased ranker
and an \textit{unbiased propensity model}. DLA is an automatic unbiased
learning-to-rank framework as it directly learns unbiased ranking models from
biased click data without any preprocessing. It can adapt to the change of bias
distributions and is applicable to online learning. Our empirical experiments
with synthetic and real-world data show that the models trained with DLA
significantly outperformed the unbiased learning-to-rank algorithms based on
result randomization and the models trained with relevance signals extracted by
click models.",The MSE between the true click propensity and those estimated by DLA and RandList with respect to η.,Which method performs better in terms of mean square error?
spiqa_480,1809.00458v1,"Based on the F1 scores presented in the figure, how does the performance of GB-KMV compare to LSH-E at different eleFreq values ranging from 0.4 to 1.2 and recSize values varying from 0.8 to 1.4, and what specific trends are observed as these parameters change?","GB-KMV generally performs better than LSH-E, as indicated by the higher F1 scores across the range of eleFreq and recSize values. For both methods, the F1 score tends to decrease as recSize increases, while the impact of eleFreq varies depending on the method.",1809.00458v1.pdf,"['1809.00458v1.pdf', '1803.06506v3.pdf', '1708.06832v3.pdf', '1710.06177v2.pdf', '1703.00060v2.pdf', '1706.08146v3.pdf', '1803.03467v4.pdf', '1705.02946v3.pdf', '1906.06589v3.pdf', '1705.07384v2.pdf', '1809.03149v2.pdf', '1611.05742v3.pdf']",The figure shows the F1 scores for GB-KMV and LSH-E at different values of eleFreq and recSize. The F1 score is a measure of performance that combines precision and recall. A higher F1 score indicates better performance.,1809.00458v1-Figure16-1.png,GB-KMV: An Augmented KMV Sketch for Approximate Containment Similarity Search,"In this paper, we study the problem of approximate containment similarity
search. Given two records Q and X, the containment similarity between Q and X
with respect to Q is |Q intersect X|/ |Q|. Given a query record Q and a set of
records S, the containment similarity search finds a set of records from S
whose containment similarity regarding Q are not less than the given threshold.
This problem has many important applications in commercial and scientific
fields such as record matching and domain search. Existing solution relies on
the asymmetric LSH method by transforming the containment similarity to
well-studied Jaccard similarity. In this paper, we use a different framework by
transforming the containment similarity to set intersection. We propose a novel
augmented KMV sketch technique, namely GB-KMV, which is data-dependent and can
achieve a good trade-off between the sketch size and the accuracy. We provide a
set of theoretical analysis to underpin the proposed augmented KMV sketch
technique, and show that it outperforms the state-of-the-art technique LSH-E in
terms of estimation accuracy under practical assumption. Our comprehensive
experiments on real-life datasets verify that GB-KMV is superior to LSH-E in
terms of the space-accuracy trade-off, time-accuracy trade-off, and the sketch
construction time. For instance, with similar estimation accuracy (F-1 score),
GB-KMV is over 100 times faster than LSH-E on some real-life dataset.",EleFreq z-value varying from 0.4 to 1.2 with recSize z-value 1.0; recSize z-value varying from 0.8 to 1.4 with eleFreq z-value 0.8,"Which method performs better, GB-KMV or LSH-E, and how does the performance change with different values of eleFreq and recSize?"
spiqa_481,1605.07496v3,"Which method, indicated by the red line in Figure (a) of the paper ""Alternating Optimisation and Quadrature for Robust Control"", achieves the lowest expected function value on the modified Branin function?",One Step ALOQ,1605.07496v3.pdf,"['1605.07496v3.pdf', '1705.07384v2.pdf', '1603.00286v5.pdf', '1804.04786v3.pdf', '1704.07854v4.pdf', '1812.10735v2.pdf', '1809.00458v1.pdf', '1704.07121v2.pdf', '1809.03149v2.pdf', '1804.07931v2.pdf', '1708.03797v1.pdf']","The One Step ALOQ method has the lowest expected function value for the Branin function, which is indicated by the red line in Figure (a).",1605.07496v3-Figure7-1.png,Alternating Optimisation and Quadrature for Robust Control,"Bayesian optimisation has been successfully applied to a variety of
reinforcement learning problems. However, the traditional approach for learning
optimal policies in simulators does not utilise the opportunity to improve
learning by adjusting certain environment variables: state features that are
unobservable and randomly determined by the environment in a physical setting
but are controllable in a simulator. This paper considers the problem of
finding a robust policy while taking into account the impact of environment
variables. We present Alternating Optimisation and Quadrature (ALOQ), which
uses Bayesian optimisation and Bayesian quadrature to address such settings.
ALOQ is robust to the presence of significant rare events, which may not be
observable under random sampling, but play a substantial role in determining
the optimal policy. Experimental results across different domains show that
ALOQ can learn more efficiently and robustly than existing methods.","Comparison of performance of all methods on the modified Branin and Hartmann 6 test functions used by Williams, Santner, and Notz.",Which method performs the best on the Branin function?
spiqa_482,1710.06177v2,"Based on the figure in the ""Learning to Learn Image Classifiers with Visual Analogy"" paper, which method demonstrates the highest AUC when the binary classification task uses exactly 50 shots?",VAGER+Voting,1710.06177v2.pdf,"['1710.06177v2.pdf', '1809.04276v2.pdf', '1704.00774v3.pdf', '1812.00108v4.pdf', '1811.02553v4.pdf', '1809.03149v2.pdf', '1705.08016v3.pdf', '1707.08608v3.pdf', '1901.00398v2.pdf', '1809.01246v1.pdf', '1705.10667v4.pdf', '1703.04887v4.pdf', '1804.07707v2.pdf']","The blue line, which represents VAGER+Voting, has the highest AUC value when the number of shots is 50.",1710.06177v2-Figure2-1.png,Learning to Learn Image Classifiers with Visual Analogy,"Humans are far better learners who can learn a new concept very fast with
only a few samples compared with machines. The plausible mystery making the
difference is two fundamental learning mechanisms: learning to learn and
learning by analogy. In this paper, we attempt to investigate a new human-like
learning method by organically combining these two mechanisms. In particular,
we study how to generalize the classification parameters from previously
learned concepts to a new concept. we first propose a novel Visual Analogy
Graph Embedded Regression (VAGER) model to jointly learn a low-dimensional
embedding space and a linear mapping function from the embedding space to
classification parameters for base classes. We then propose an out-of-sample
embedding method to learn the embedding of a new class represented by a few
samples through its visual analogy with base classes and derive the
classification parameters for the new class. We conduct extensive experiments
on ImageNet dataset and the results show that our method could consistently and
significantly outperform state-of-the-art baselines.",The change of performance as the number of shots increases in binary classification.,Which method performs the best when the number of shots is 50?
spiqa_484,1606.07384v2,"According to the experiments shown in Figure 1 of the ""Robust Learning of Fixed-Structure Bayesian Networks"" paper, which method achieves the lowest total variation error when the proportion of corrupted samples is high?",RANSAC,1606.07384v2.pdf,"['1606.07384v2.pdf', '1708.02153v2.pdf', '1706.04284v3.pdf', '1707.08608v3.pdf', '1804.05938v2.pdf', '1803.06506v3.pdf', '1706.00827v2.pdf', '1804.04786v3.pdf', '1811.06635v1.pdf', '1805.08751v2.pdf', '1809.02731v3.pdf', '1703.02507v3.pdf', '1804.05995v2.pdf', '1803.03467v4.pdf', '1702.08694v3.pdf']",The plot shows that RANSAC has the lowest error when the fraction of corrupted samples is high.,1606.07384v2-Figure2-1.png,Robust Learning of Fixed-Structure Bayesian Networks,"We investigate the problem of learning Bayesian networks in a robust model
where an $\epsilon$-fraction of the samples are adversarially corrupted. In
this work, we study the fully observable discrete case where the structure of
the network is given. Even in this basic setting, previous learning algorithms
either run in exponential time or lose dimension-dependent factors in their
error guarantees. We provide the first computationally efficient robust
learning algorithm for this problem with dimension-independent error
guarantees. Our algorithm has near-optimal sample complexity, runs in
polynomial time, and achieves error that scales nearly-linearly with the
fraction of adversarially corrupted samples. Finally, we show on both synthetic
and semi-synthetic data that our algorithm performs well in practice.","Experiments with semi-synthetic data: error is reported against the fraction of corrupted samples (lower is better). The error is the estimated total variation distance to the ALARM network. We use the sampling error without noise as a benchmark, and compare the performance of our algorithm (Filtering), empirical mean with noise (MLE), and RANSAC.",Which method performs the best when there is a high fraction of corrupted samples?
spiqa_485,1805.06447v3,"What is the testing error for ITN (B-CNN) with and without data augmentation when trained with 1% of MNIST training data, according to Table 2 in the paper ""Resisting Large Data Variations via Introspective Transformation Network,"" and how much does data augmentation improve the performance?","When trained with only 1% of the MNIST training data, ITN (B-CNN) (w/ DA) performs the best with a testing error of 2.78%. Data augmentation further improves its performance by 0.4%, bringing the testing error down to 2.78% from 3.18% achieved by ITN (B-CNN) without data augmentation.",1805.06447v3.pdf,"['1805.06447v3.pdf', '1703.04887v4.pdf', '1705.09882v2.pdf', '1708.05239v3.pdf', '1803.06506v3.pdf', '1703.00899v2.pdf', '1701.03077v10.pdf', '1805.00912v4.pdf']","The table presents the testing errors of different methods under various training data limitations. By looking at the row corresponding to 1% and comparing the values across different methods, we can identify the best performing model. The difference in testing error between ITN (B-CNN) with and without data augmentation (w/ DA) indicates the improvement achieved through data augmentation.",1805.06447v3-Table2-1.png,Resisting Large Data Variations via Introspective Transformation Network,"Training deep networks that generalize to a wide range of variations in test
data is essential to building accurate and robust image classifiers. One
standard strategy is to apply data augmentation to synthetically enlarge the
training set. However, data augmentation is essentially a brute-force method
which generates uniform samples from some pre-defined set of transformations.
In this paper, we propose a principled approach to train networks with
significantly improved resistance to large variations between training and
testing data. This is achieved by embedding a learnable transformation module
into the introspective network, which is a convolutional neural network (CNN)
classifier empowered with generative capabilities. Our approach alternates
between synthesizing pseudo-negative samples and transformed positive examples
based on the current model, and optimizing model predictions on these
synthesized samples. Experimental results verify that our approach
significantly improves the ability of deep networks to resist large variations
between training and testing data and achieves classification accuracy
improvements on several benchmark datasets, including MNIST, affNIST, SVHN,
CIFAR-10 and miniImageNet.","Table 2. Testing errors of the classification results with limited training data, where 0.1% means the training data is randomly selected 0.1% of the MNIST training data while the testing data is the entire MNIST testing data.","Which method performs the best when trained with only 1% of the MNIST training data, and how much does data augmentation improve its performance in this scenario?"
spiqa_486,1805.07567v2,"Based on the figure depicting example saliency maps in the paper *Optimizing the F-measure for Threshold-free Salient Object Detection*, which method, FLoss or Log-FLoss, produces saliency maps with sharper edges and more pronounced object separation?",FLoss,1805.07567v2.pdf,"['1805.07567v2.pdf', '1707.00189v3.pdf', '1611.03780v2.pdf', '1603.03833v4.pdf', '1708.01425v4.pdf', '1707.08608v3.pdf', '1803.06506v3.pdf', '1708.03797v1.pdf']","The figure shows example saliency maps produced by FLoss and Log-FLoss. The FLoss maps have higher contrast than the Log-FLoss maps, as can be seen by the sharper edges and more distinct foreground objects.",1805.07567v2-Figure2-1.png,Optimizing the F-measure for Threshold-free Salient Object Detection,"Current CNN-based solutions to salient object detection (SOD) mainly rely on
the optimization of cross-entropy loss (CELoss). Then the quality of detected
saliency maps is often evaluated in terms of F-measure. In this paper, we
investigate an interesting issue: can we consistently use the F-measure
formulation in both training and evaluation for SOD? By reformulating the
standard F-measure we propose the relaxed F-measure which is differentiable
w.r.t the posterior and can be easily appended to the back of CNNs as the loss
function. Compared to the conventional cross-entropy loss of which the
gradients decrease dramatically in the saturated area, our loss function, named
FLoss, holds considerable gradients even when the activation approaches the
target. Consequently, the FLoss can continuously force the network to produce
polarized activations. Comprehensive benchmarks on several popular datasets
show that FLoss outperforms the state-of-the-art with a considerable margin.
More specifically, due to the polarized predictions, our method is able to
obtain high-quality saliency maps without carefully tuning the optimal
threshold, showing significant advantages in real-world applications.",Example saliency maps by FLoss (bottom) and LogFLoss (middle). Our proposed FLoss method produces highcontrast saliency maps.,"Which method produces higher contrast saliency maps, FLoss or Log-FLoss?"
spiqa_487,1811.09393v4,"In the Foliage scene depicted in the figure, which model effectively removes recurrent artifacts by frame 40, unlike DsDt and PP-Augment?",TecoGAN⊖.,1811.09393v4.pdf,"['1811.09393v4.pdf', '1803.06506v3.pdf', '1803.02750v3.pdf', '1706.08146v3.pdf', '1708.02153v2.pdf', '1802.07222v1.pdf', '1809.00263v5.pdf', '1705.09882v2.pdf', '1802.07351v2.pdf', '1707.00189v3.pdf']","The figure shows that both DsDt and PP-Augment produce artifacts, while TecoGAN⊖ does not.",1811.09393v4-Figure23-1.png,Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation,"Our work explores temporal self-supervision for GAN-based video generation
tasks. While adversarial training successfully yields generative models for a
variety of areas, temporal relationships in the generated data are much less
explored. Natural temporal changes are crucial for sequential generation tasks,
e.g. video super-resolution and unpaired video translation. For the former,
state-of-the-art methods often favor simpler norm losses such as $L^2$ over
adversarial training. However, their averaging nature easily leads to
temporally smooth results with an undesirable lack of spatial detail. For
unpaired video translation, existing approaches modify the generator networks
to form spatio-temporal cycle consistencies. In contrast, we focus on improving
learning objectives and propose a temporally self-supervised algorithm. For
both tasks, we show that temporal adversarial learning is key to achieving
temporally coherent solutions without sacrificing spatial detail. We also
propose a novel Ping-Pong loss to improve the long-term temporal consistency.
It effectively prevents recurrent networks from accumulating artifacts
temporally without depressing detailed features. Additionally, we propose a
first set of metrics to quantitatively evaluate the accuracy as well as the
perceptual quality of the temporal evolution. A series of user studies confirm
the rankings computed with these metrics. Code, data, models, and results are
provided at https://github.com/thunil/TecoGAN. The project page
https://ge.in.tum.de/publications/2019-tecogan-chu/ contains supplemental
materials.","1st & 2nd row: Frame 15 & 40 of the Foliage scene. While DsDt leads to strong recurrent artifacts early on, PP-Augment shows similar artifacts later in time (2nd row, middle). TecoGAN⊖ model successfully removes these artifacts.",Which method produces the least amount of artifacts?
spiqa_489,1706.04269v2,"In Figure 4 of the paper, which method demonstrates superior efficiency by requiring approximately 120 observations on average to detect actions in videos with 2.5% action coverage, outperforming both the Direction Baseline and Random Baseline?",Action Search,1706.04269v2.pdf,"['1706.04269v2.pdf', '1805.07567v2.pdf', '1809.02731v3.pdf', '1707.08608v3.pdf']","The figure shows that the Action Search method (green bars) consistently requires fewer observations than the other two methods (blue and light blue bars) across all levels of action coverage. At 2.5% action coverage, the Action Search method requires about 120 observations on average, while the Direction Baseline and Random Baseline methods require about 150 and 175 observations, respectively.",1706.04269v2-Figure4-1.png,Action Search: Spotting Actions in Videos and Its Application to Temporal Action Localization,"State-of-the-art temporal action detectors inefficiently search the entire
video for specific actions. Despite the encouraging progress these methods
achieve, it is crucial to design automated approaches that only explore parts
of the video which are the most relevant to the actions being searched for. To
address this need, we propose the new problem of action spotting in video,
which we define as finding a specific action in a video while observing a small
portion of that video. Inspired by the observation that humans are extremely
efficient and accurate in spotting and finding action instances in video, we
propose Action Search, a novel Recurrent Neural Network approach that mimics
the way humans spot actions. Moreover, to address the absence of data recording
the behavior of human annotators, we put forward the Human Searches dataset,
which compiles the search sequences employed by human annotators spotting
actions in the AVA and THUMOS14 datasets. We consider temporal action
localization as an application of the action spotting problem. Experiments on
the THUMOS14 dataset reveal that our model is not only able to explore the
video efficiently (observing on average 17.3% of the video) but it also
accurately finds human activities with 30.8% mAP.","Fig. 4: Action spotting results for the AVA testing set for 1000 independent search trials per video. We report the cumulative spotting metric results on videos with action coverage (i.e. the percentage of video containing actions) ≤ 5%. Action Search takes 22%, 17%, and 13% fewer observations than the Direction Baseline on videos with at most 0.5%, 2.5%, and 5% action coverage, respectively.",Which method requires the fewest observations to spot an action in a video with 2.5% action coverage?
spiqa_490,1803.01128v3,"Referring to the figure comparing non-overlapping and targeted keyword attack methods for seq2seq models in machine translation, which specific adversarial example generation approach produced the highest BLEU score?",The 1-keyword method resulted in the highest BLEU score of 0.705.,1803.01128v3.pdf,"['1803.01128v3.pdf', '1906.10843v1.pdf', '1705.02946v3.pdf', '1804.01429v3.pdf', '1809.01989v2.pdf', '1805.04687v2.pdf', '1812.06589v2.pdf', '1708.01425v4.pdf', '1705.07384v2.pdf', '1809.00458v1.pdf', '1705.09882v2.pdf']",The table shows the BLEU score for each method. The 1-keyword method has the highest BLEU score.,1803.01128v3-Table5-1.png,Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples,"Crafting adversarial examples has become an important technique to evaluate
the robustness of deep neural networks (DNNs). However, most existing works
focus on attacking the image classification problem since its input space is
continuous and output space is finite.
  In this paper, we study the much more challenging problem of crafting
adversarial examples for sequence-to-sequence (seq2seq) models, whose inputs
are discrete text strings and outputs have an almost infinite number of
possibilities. To address the challenges caused by the discrete input space, we
propose a projected gradient method combined with group lasso and gradient
regularization. To handle the almost infinite output space, we design some
novel loss functions to conduct non-overlapping attack and targeted keyword
attack. We apply our algorithm to machine translation and text summarization
tasks, and verify the effectiveness of the proposed algorithm: by changing less
than 3 words, we can make seq2seq model to produce desired outputs with high
success rates. On the other hand, we recognize that, compared with the
well-evaluated CNN-based classifiers, seq2seq models are intrinsically more
robust to adversarial attacks.",Results of non-overlapping method and targeted keywords method in machine translation.,Which method resulted in the highest BLEU score?
spiqa_491,1812.06589v2,"In the figure visualizing the PCA-reduced distributions of real and generated frames, which method, AMIE or MINE, shows generated frames more closely aligned with the real frame distribution?",MINE,1812.06589v2.pdf,"['1812.06589v2.pdf', '1802.07351v2.pdf', '1811.06635v1.pdf', '1603.00286v5.pdf', '1702.03584v3.pdf']","The figure shows that the red dots, which represent the generated frames produced by MINE, are more closely clustered with the blue triangles, which represent the real frames, than the red dots produced by AMIE. This indicates that the MINE method produces generated frames that are closer in distribution to the real frames.",1812.06589v2-Figure3-1.png,Arbitrary Talking Face Generation via Attentional Audio-Visual Coherence Learning,"Talking face generation aims to synthesize a face video with precise lip
synchronization as well as a smooth transition of facial motion over the entire
video via the given speech clip and facial image. Most existing methods mainly
focus on either disentangling the information in a single image or learning
temporal information between frames. However, cross-modality coherence between
audio and video information has not been well addressed during synthesis. In
this paper, we propose a novel arbitrary talking face generation framework by
discovering the audio-visual coherence via the proposed Asymmetric Mutual
Information Estimator (AMIE). In addition, we propose a Dynamic Attention (DA)
block by selectively focusing the lip area of the input image during the
training stage, to further enhance lip synchronization. Experimental results on
benchmark LRW dataset and GRID dataset transcend the state-of-the-art methods
on prevalent metrics with robust high-resolution synthesizing on gender and
pose variations.",Visualization of distributions of real and generated frames. We reduce the dimension of frames into two-dimension via PCA for better demonstration. It is obvious that the generated samples are closer to the real samples than that with original MINE.,"Which method, AMIE or MINE, produces generated frames that are closer in distribution to the real frames?"
spiqa_492,1702.08694v3,"Based on the top left panel of Figure 2, when the number of features is approximately 1e+03 and the number of data points is around 1e+05, which method, represented by red circles, demonstrates superior precision?",C-Tarone.,1702.08694v3.pdf,"['1702.08694v3.pdf', '1709.00139v4.pdf', '1702.03584v3.pdf', '1701.03077v10.pdf', '1805.06431v4.pdf', '1804.05936v2.pdf', '1706.00633v4.pdf', '1603.00286v5.pdf', '1809.00263v5.pdf', '1812.06589v2.pdf', '1805.04687v2.pdf', '1703.07015v3.pdf', '1708.00160v2.pdf', '1803.04383v2.pdf']","The top left panel of Figure 2 shows that when the number of features is small (around 1e+03) and the number of data points is large (around 1e+05), the red circles (C-Tarone) are higher than the blue triangles (Binarization).",1702.08694v3-Figure5-1.png,Finding Statistically Significant Interactions between Continuous Features,"The search for higher-order feature interactions that are statistically
significantly associated with a class variable is of high relevance in fields
such as Genetics or Healthcare, but the combinatorial explosion of the
candidate space makes this problem extremely challenging in terms of
computational efficiency and proper correction for multiple testing. While
recent progress has been made regarding this challenge for binary features, we
here present the first solution for continuous features. We propose an
algorithm which overcomes the combinatorial explosion of the search space of
higher-order interactions by deriving a lower bound on the p-value for each
interaction, which enables us to massively prune interactions that can never
reach significance and to thereby gain more statistical power. In our
experiments, our approach efficiently detects all significant interactions in a
variety of synthetic and real-world datasets.","Figure 5: Results on synthetic data with the minor class ratio r1 = 0.2. The number of features is d = 20 in the left column and the sample size is N = 3,000 in the right column. Both x- and y-axes are in logarithmic scale. C-Tarone is shown in red circles, the binarization approach in blue triangles. Missing points in (b) mean that no significant combination is detected.","Which method, C-Tarone or Binarization, achieves higher precision when the number of features is small and the number of data points is large?"
spiqa_493,1611.03780v2,"Based on the clustering results displayed in Figure 4 of the paper ""Randomized Experimental Design via Geographic Clustering,"" which metropolitan area is identified by the GeoCUTS algorithm as containing San Francisco, Berkeley, and Palo Alto, while excluding Sacramento?",The Bay Area.,1611.03780v2.pdf,"['1611.03780v2.pdf', '1809.04276v2.pdf', '1611.04684v1.pdf', '1803.01128v3.pdf', '1809.03449v3.pdf', '1901.00056v2.pdf', '1704.07854v4.pdf', '1703.00899v2.pdf', '1804.01429v3.pdf', '1603.00286v5.pdf', '1811.10673v1.pdf', '1706.00827v2.pdf', '1803.04572v2.pdf', '1704.00774v3.pdf', '1704.07121v2.pdf']","The figure shows the clusters generated by the GeoCUTS algorithm for data from the United States. The Bay Area is shown as a single cluster that includes San Francisco, Berkeley, and Palo Alto, but not Sacramento.",1611.03780v2-Figure4-1.png,Randomized Experimental Design via Geographic Clustering,"Web-based services often run randomized experiments to improve their
products. A popular way to run these experiments is to use geographical regions
as units of experimentation, since this does not require tracking of individual
users or browser cookies. Since users may issue queries from multiple
geographical locations, geo-regions cannot be considered independent and
interference may be present in the experiment. In this paper, we study this
problem, and first present GeoCUTS, a novel algorithm that forms geographical
clusters to minimize interference while preserving balance in cluster size. We
use a random sample of anonymized traffic from Google Search to form a graph
representing user movements, then construct a geographically coherent
clustering of the graph. Our main technical contribution is a statistical
framework to measure the effectiveness of clusterings. Furthermore, we perform
empirical evaluations showing that the performance of GeoCUTS is comparable to
hand-crafted geo-regions with respect to both novel and existing metrics.","Figure 4: The GeoCUTS algorithm applied to user queries from the United States. The algorithm automatically identifies metropolitan areas, correctly predicting, for example, that the Bay Area includes San Francisco, Berkeley, and Palo Alto, but not Sacramento.","Which metropolitan area is predicted by the GeoCUTS algorithm to include San Francisco, Berkeley, and Palo Alto, but not Sacramento?"
spiqa_494,1611.03780v2,"Based on Figure 5 in the paper ""Randomized Experimental Design via Geographic Clustering,"" which metropolitan areas in France are highlighted as correctly clustered by the GeoCUTS algorithm?","Paris, Bordeaux, and Lyon.",1611.03780v2.pdf,"['1611.03780v2.pdf', '1805.07567v2.pdf', '1901.00056v2.pdf', '1702.03584v3.pdf', '1603.03833v4.pdf', '1611.04684v1.pdf', '1705.10667v4.pdf', '1811.10673v1.pdf', '1805.06431v4.pdf', '1804.07707v2.pdf', '1809.03449v3.pdf', '1803.01128v3.pdf', '1705.02798v6.pdf']","The figure shows the clusters generated by the GeoCUTS algorithm for data from France. The clusters corresponding to Paris, Bordeaux, and Lyon are clearly visible.",1611.03780v2-Figure5-1.png,Randomized Experimental Design via Geographic Clustering,"Web-based services often run randomized experiments to improve their
products. A popular way to run these experiments is to use geographical regions
as units of experimentation, since this does not require tracking of individual
users or browser cookies. Since users may issue queries from multiple
geographical locations, geo-regions cannot be considered independent and
interference may be present in the experiment. In this paper, we study this
problem, and first present GeoCUTS, a novel algorithm that forms geographical
clusters to minimize interference while preserving balance in cluster size. We
use a random sample of anonymized traffic from Google Search to form a graph
representing user movements, then construct a geographically coherent
clustering of the graph. Our main technical contribution is a statistical
framework to measure the effectiveness of clusterings. Furthermore, we perform
empirical evaluations showing that the performance of GeoCUTS is comparable to
hand-crafted geo-regions with respect to both novel and existing metrics.","Figure 5: The GeoCUTS algorithm applied to user queries from France. It correctly identifies metropolitan areas such as Paris, Bordeaux, and Lyon, and regions such as Alsace and Normandy.",Which metropolitan areas are correctly identified by the GeoCUTS algorithm in France?
spiqa_495,1705.09882v2,"In the multi-shot evaluation on the TUM-GAID new clothes scenario, as shown in the figure reporting both single-shot and multi-shot accuracies, which modality combining Body Depth and Head RGB with LSTM and Reinforced Temporal Attention achieved the highest top-1 accuracy of 88.1%?",Body Depth & Head RGB (ms: LSTM & RTA),1705.09882v2.pdf,"['1705.09882v2.pdf', '1809.03149v2.pdf', '1703.10730v2.pdf', '1709.02418v2.pdf', '1803.04383v2.pdf', '1804.07849v4.pdf', '1802.07459v2.pdf', '1702.08694v3.pdf', '1804.04786v3.pdf', '1803.03467v4.pdf', '1707.01917v2.pdf', '1703.07015v3.pdf', '1705.07164v8.pdf', '1705.07384v2.pdf', '1805.04609v3.pdf']","The table shows the top-1 accuracy for each modality in both single-shot and multi-shot evaluations. The highest top-1 accuracy in the multi-shot evaluation is 88.1%, which is achieved by the Body Depth & Head RGB (ms: LSTM & RTA) modality.",1705.09882v2-Table2-1.png,Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-Identification,"We address the problem of person re-identification from commodity depth
sensors. One challenge for depth-based recognition is data scarcity. Our first
contribution addresses this problem by introducing split-rate RGB-to-Depth
transfer, which leverages large RGB datasets more effectively than popular
fine-tuning approaches. Our transfer scheme is based on the observation that
the model parameters at the bottom layers of a deep convolutional neural
network can be directly shared between RGB and depth data while the remaining
layers need to be fine-tuned rapidly. Our second contribution enhances
re-identification for video by implementing temporal attention as a
Bernoulli-Sigmoid unit acting upon frame-level features. Since this unit is
stochastic, the temporal attention parameters are trained using reinforcement
learning. Extensive experiments validate the accuracy of our method in person
re-identification from depth sequences. Finally, in a scenario where subjects
wear unseen clothes, we show large performance gains compared to a
state-of-the-art model which relies on RGB data.","Top-1 re-identification accuracy (top-1, %) and normalized Area Under the Curve (nAUC, %) on TUM-GAID in newclothes scenario with single-shot (ss) and multi-shot (ms) evaluation",Which modality achieved the highest top-1 accuracy in the multi-shot evaluation on TUM-GAID?
spiqa_496,1708.03797v1,"Based on the data presented in Table 2 of the ""Hybrid Deep-Semantic Matrix Factorization"" paper, which model consistently achieved the highest performance across all ranking metrics, including Precision (P@k), Recall (R@k), F-Score (F@k), Mean Reciprocal Rank (MRR), and Mean Average Precision (MAP), in the context of tag-aware personalized recommendation?",HDMF achieved the best overall performance.,1708.03797v1.pdf,"['1708.03797v1.pdf', '1703.07015v3.pdf', '1804.07931v2.pdf', '1701.03077v10.pdf', '1705.07384v2.pdf']","The table shows the performance of various models on several metrics, including Precision at different cut-off ranks (P@k), Recall at different cut-off ranks (R@k), F-score at different cut-off ranks (F@k), Mean Average Precision (MAP), and Mean Reciprocal Rank (MRR). Higher values indicate better performance for all metrics. Observing the bolded values in the table, which represent the highest scores achieved for each metric, we can see that HDMF consistently outperforms all other models across all the listed metrics. This suggests that HDMF is most effective at ranking relevant tags for users compared to the other models considered.",1708.03797v1-Table2-1.png,Hybrid Deep-Semantic Matrix Factorization for Tag-Aware Personalized Recommendation,"Matrix factorization has now become a dominant solution for personalized
recommendation on the Social Web. To alleviate the cold start problem, previous
approaches have incorporated various additional sources of information into
traditional matrix factorization models. These upgraded models, however,
achieve only ""marginal"" enhancements on the performance of personalized
recommendation. Therefore, inspired by the recent development of deep-semantic
modeling, we propose a hybrid deep-semantic matrix factorization (HDMF) model
to further improve the performance of tag-aware personalized recommendation by
integrating the techniques of deep-semantic modeling, hybrid learning, and
matrix factorization. Experimental results show that HDMF significantly
outperforms the state-of-the-art baselines in tag-aware personalized
recommendation, in terms of all evaluation metrics, e.g., its mean reciprocal
rank (resp., mean average precision) is 1.52 (resp., 1.66) times as high as
that of the best baseline.",Table 2: Recommendation Performance of Various Models (in %),Which model achieved the best overall performance in terms of ranking relevant tags for users?
spiqa_497,1805.01216v3,"Based on Table 3 in the ""AMT Evaluations on CamRest and SMD,"" which model achieved the highest combined score for informativeness and grammatical correctness on the CamRest dataset?",The BOSSNET model achieved the highest combined score for informativeness and grammatical correctness on the CamRest dataset.,1805.01216v3.pdf,"['1805.01216v3.pdf', '1811.02553v4.pdf', '1707.00189v3.pdf', '1703.07015v3.pdf', '1705.10667v4.pdf', '1812.06589v2.pdf', '1804.01429v3.pdf', '1805.06431v4.pdf', '1709.08294v3.pdf', '1611.04684v1.pdf', '1812.10735v2.pdf', '1704.00774v3.pdf', '1705.08016v3.pdf', '1812.00281v3.pdf']","The table shows separate scores for informativeness (""Info"") and grammatical correctness (""Grammar"") for each model on both the CamRest and SMD datasets. To find the combined score, we simply add the two individual scores. For CamRest, \sys\ has an ""Info"" score of 80 and a ""Grammar"" score of 2.44, resulting in a combined score of 82.44, which is higher than any other model on that dataset.",1805.01216v3-Table3-1.png,Disentangling Language and Knowledge in Task-Oriented Dialogs,"The Knowledge Base (KB) used for real-world applications, such as booking a
movie or restaurant reservation, keeps changing over time. End-to-end neural
networks trained for these task-oriented dialogs are expected to be immune to
any changes in the KB. However, existing approaches breakdown when asked to
handle such changes. We propose an encoder-decoder architecture (BoSsNet) with
a novel Bag-of-Sequences (BoSs) memory, which facilitates the disentangled
learning of the response's language model and its knowledge incorporation.
Consequently, the KB can be modified with new knowledge without a drop in
interpretability. We find that BoSsNet outperforms state-of-the-art models,
with considerable improvements (> 10\%) on bAbI OOV test sets and other
human-human datasets. We also systematically modify existing datasets to
measure disentanglement and show BoSsNet to be robust to KB modifications.",Table 3: AMT Evaluations on CamRest and SMD,Which model achieved the highest combined score for informativeness and grammatical correctness on the CamRest dataset?
spiqa_498,1707.00524v2,"Referring to the figure comparing the performance scores of baseline RL models across different games, which model achieved the highest score specifically for the Breakout game?",A3C-CTS,1707.00524v2.pdf,"['1707.00524v2.pdf', '1704.05958v2.pdf', '1611.04684v1.pdf', '1803.06506v3.pdf']","The table shows the performance scores for different models on various games. The highest score for the Breakout game is 473.93, which is achieved by the A3C-CTS model.",1707.00524v2-Table2-1.png,Hashing over Predicted Future Frames for Informed Exploration of Deep Reinforcement Learning,"In deep reinforcement learning (RL) tasks, an efficient exploration mechanism
should be able to encourage an agent to take actions that lead to less frequent
states which may yield higher accumulative future return. However, both knowing
about the future and evaluating the frequentness of states are non-trivial
tasks, especially for deep RL domains, where a state is represented by
high-dimensional image frames. In this paper, we propose a novel informed
exploration framework for deep RL, where we build the capability for an RL
agent to predict over the future transitions and evaluate the frequentness for
the predicted future frames in a meaningful manner. To this end, we train a
deep prediction model to predict future frames given a state-action pair, and a
convolutional autoencoder model to hash over the seen frames. In addition, to
utilize the counts derived from the seen frames to evaluate the frequentness
for the predicted frames, we tackle the challenge of matching the predicted
future frames and their corresponding seen frames at the latent feature level.
In this way, we derive a reliable metric for evaluating the novelty of the
future direction pointed by each action, and hence inform the agent to explore
the least frequent one.",Performance score for the proposed approach and baseline RL approaches.,Which model achieved the highest performance score on the Breakout game?
spiqa_499,1705.09296v2,"Based on the results presented in Table 1 of your paper ""Neural Models for Documents with Metadata,"" which model achieves the highest internal and external NPMI scores in the unsupervised setting on the IMDB dataset, and how does this model compare to others in terms of parameter complexity?","The Scholar + w.v. model achieves the best NPMI scores (both internal and external) in the unsupervised setting. However, this model also has the highest number of people parameters, indicating a trade-off between topic coherence and model complexity.",1705.09296v2.pdf,"['1705.09296v2.pdf', '1707.00189v3.pdf', '1803.04383v2.pdf', '1709.08294v3.pdf', '1803.03467v4.pdf']","Table 1 shows the performance of various topic models on the IMDB dataset in an unsupervised setting. The NPMI scores (both internal and external) measure the coherence of the topics generated by each model, with higher scores indicating more coherent topics. The ""Ppl."" column indicates the number of people parameters in each model, which reflects its complexity. By comparing the NPMI scores and the number of people parameters across the models, we can identify the trade-offs between topic coherence and model complexity. In this case, Scholar + w.v. achieves the best coherence but also requires the most parameters.",1705.09296v2-Table1-1.png,Neural Models for Documents with Metadata,"Most real-world document collections involve various types of metadata, such
as author, source, and date, and yet the most commonly-used approaches to
modeling text corpora ignore this information. While specialized models have
been developed for particular applications, few are widely used in practice, as
customization typically requires derivation of a custom inference algorithm. In
this paper, we build on recent advances in variational inference methods and
propose a general neural framework, based on topic models, to enable flexible
incorporation of metadata and allow for rapid exploration of alternative
models. Our approach achieves strong performance, with a manageable tradeoff
between perplexity, coherence, and sparsity. Finally, we demonstrate the
potential of our framework through an exploration of a corpus of articles about
US immigration.","Table 1: Performance of our various models in an unsupervised setting (i.e., without labels or covariates) on the IMDB dataset using a 5,000-word vocabulary and 50 topics. The supplementary materials contain additional results for 20 newsgroups and Yahoo answers.","Which model achieves the best NPMI scores (both internal and external) in the unsupervised setting, and what trade-off does this model exhibit compared to other models?"
spiqa_500,1703.04887v4,"Referring to the BLEU scores in Table 1 of the paper ""Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets,"" which model configuration using BR-CSGAN yields the highest BLEU score for the Chinese-English translation task, and by how many points does it exceed the baseline RNNSearch model?",The Transformer+BR-CSGAN model with λ=0.8 achieves the best performance on the Chinese-English translation task with an average BLEU score of 42.61. This represents an improvement of 0.81 BLEU points compared to the baseline RNNSearch model.,1703.04887v4.pdf,"['1703.04887v4.pdf', '1804.05995v2.pdf', '1809.01989v2.pdf', '1606.07384v2.pdf', '1709.00139v4.pdf', '1710.06177v2.pdf', '1805.01216v3.pdf', '1709.02418v2.pdf', '1706.00827v2.pdf', '1804.05938v2.pdf']",Table 1 shows the BLEU scores for different models and configurations on the Chinese-English translation task. We can compare the average BLEU scores across different models to identify the best performing one. The table also allows us to compare the performance of the BR-CSGAN augmented models with their respective baseline models (RNNSearch and Transformer) by looking at the difference in BLEU scores.,1703.04887v4-Table1-1.png,Improving Neural Machine Translation with Conditional Sequence Generative Adversarial Nets,"This paper proposes an approach for applying GANs to NMT. We build a
conditional sequence generative adversarial net which comprises of two
adversarial sub models, a generator and a discriminator. The generator aims to
generate sentences which are hard to be discriminated from human-translated
sentences (i.e., the golden target sentences), And the discriminator makes
efforts to discriminate the machine-generated sentences from human-translated
ones. The two sub models play a mini-max game and achieve the win-win situation
when they reach a Nash Equilibrium. Additionally, the static sentence-level
BLEU is utilized as the reinforced objective for the generator, which biases
the generation towards high BLEU points. During training, both the dynamic
discriminator and the static BLEU objective are employed to evaluate the
generated sentences and feedback the evaluations to guide the learning of the
generator. Experimental results show that the proposed model consistently
outperforms the traditional RNNSearch and the newly emerged state-of-the-art
Transformer on English-German and Chinese-English translation tasks.","Table 1: BLEU score on Chinese-English and English-German translation tasks. The hyper-parameter λ is selected according to the development set. For the Transformer, following (Vaswani et al., 2017), we report the result of a single model obtained by averaging the 5 checkpoints around the best model selected on the development set.","Which model and configuration achieves the best performance on the Chinese-English translation task, and how much improvement does it offer compared to the baseline RNNSearch model?"
spiqa_501,1901.00056v2,"Based on Table 2 of the paper ""Entity Synonym Discovery via Multipiece Bilateral Context Matching,"" which model and training objective combination achieves the highest AUC and MAP on the PubMed + UMLS dataset, and how does this performance statistically compare to the DPE baseline?","The SYNONYMNET(Pairwise) model with Leaky Unit performs best on the PubMed + UMLS dataset, achieving an AUC of 0.9838 and a MAP of 0.9872. This is a statistically significant improvement over the DPE baseline, which achieved an AUC of 0.9513 and a MAP of 0.9623.",1901.00056v2.pdf,"['1901.00056v2.pdf', '1803.03467v4.pdf', '1707.00189v3.pdf', '1703.02507v3.pdf', '1709.00139v4.pdf', '1802.07351v2.pdf', '1811.02721v3.pdf', '1707.00524v2.pdf', '1603.03833v4.pdf', '1804.05936v2.pdf', '1606.07384v2.pdf', '1611.07718v2.pdf', '1901.00398v2.pdf', '1805.00912v4.pdf']","The table shows the performance of different models and training objectives on three datasets, including PubMed + UMLS. We can identify the best performing model and training objective combination for this dataset by looking at the highest AUC and MAP values in the corresponding column. The table also indicates statistically significant improvements with the symbol †, confirming that \modelname (Pairwise) with Leaky Unit significantly outperforms DPE on this dataset.",1901.00056v2-Table2-1.png,Entity Synonym Discovery via Multipiece Bilateral Context Matching,"Being able to automatically discover synonymous entities in an open-world
setting benefits various tasks such as entity disambiguation or knowledge graph
canonicalization. Existing works either only utilize entity features, or rely
on structured annotations from a single piece of context where the entity is
mentioned. To leverage diverse contexts where entities are mentioned, in this
paper, we generalize the distributional hypothesis to a multi-context setting
and propose a synonym discovery framework that detects entity synonyms from
free-text corpora with considerations on effectiveness and robustness. As one
of the key components in synonym discovery, we introduce a neural network model
SYNONYMNET to determine whether or not two given entities are synonym with each
other. Instead of using entities features, SYNONYMNET makes use of multiple
pieces of contexts in which the entity is mentioned, and compares the
context-level similarity via a bilateral matching schema. Experimental results
demonstrate that the proposed model is able to detect synonym sets that are not
observed during training on both generic and domain-specific datasets:
Wiki+Freebase, PubMed+UMLS, and MedBook+MKG, with up to 4.16% improvement in
terms of Area Under the Curve and 3.19% in terms of Mean Average Precision
compared to the best baseline method.",Table 2: Test performance in AUC and MAP on three datasets. † indicates the significant improvement over all baselines (p < 0.05).,"Which model and training objective combination performs best on the PubMed + UMLS dataset, and how does it compare to the DPE baseline?"
spiqa_502,1805.00912v4,"Referring to Table 1 of the ""Tensorized Self-Attention: Efficiently Modeling Pairwise and Global Dependencies Together"" paper, which model achieves the highest test accuracy on the SNLI dataset, and how does its training time per epoch compare to the MTSA model in terms of both training efficiency and accuracy?","The model with the highest test accuracy is MTSA, with an accuracy of 86.3%. Its training time per epoch is 180s, which is faster than the training time of several other models with lower accuracy, such as Bi-LSTM (854s), Bi-GRU (850s), and DiSA (390s).",1805.00912v4.pdf,"['1805.00912v4.pdf', '1708.05239v3.pdf', '1706.00633v4.pdf', '1805.02349v2.pdf', '1706.03847v3.pdf', '1611.07718v2.pdf', '1809.01246v1.pdf', '1707.08608v3.pdf', '1710.06177v2.pdf', '1804.05995v2.pdf', '1612.02803v5.pdf', '1703.10730v2.pdf']","The table shows the test accuracy and training time per epoch for various models. By comparing these values across different models, we can identify the model with the best performance and understand its relative training efficiency. In this case, MTSA achieves the highest test accuracy while also having a relatively fast training time per epoch.",1805.00912v4-Table1-1.png,Tensorized Self-Attention: Efficiently Modeling Pairwise and Global Dependencies Together,"Neural networks equipped with self-attention have parallelizable computation,
light-weight structure, and the ability to capture both long-range and local
dependencies. Further, their expressive power and performance can be boosted by
using a vector to measure pairwise dependency, but this requires to expand the
alignment matrix to a tensor, which results in memory and computation
bottlenecks. In this paper, we propose a novel attention mechanism called
""Multi-mask Tensorized Self-Attention"" (MTSA), which is as fast and as
memory-efficient as a CNN, but significantly outperforms previous
CNN-/RNN-/attention-based models. MTSA 1) captures both pairwise (token2token)
and global (source2token) dependencies by a novel compatibility function
composed of dot-product and additive attentions, 2) uses a tensor to represent
the feature-wise alignment scores for better expressive power but only requires
parallelizable matrix multiplications, and 3) combines multi-head with
multi-dimensional attentions, and applies a distinct positional mask to each
head (subspace), so the memory and computation can be distributed to multiple
heads, each with sequential information encoded independently. The experiments
show that a CNN/RNN-free model based on MTSA achieves state-of-the-art or
competitive performance on nine NLP benchmarks with compelling memory- and
time-efficiency.",Table 1: Experimental results for different methods with comparative parameter number on SNLI. |θ|: the number of parameters (excluding word embedding part); Time/Epoch: averaged training time per epoch with batch size 128; Inf. Time: averaged dev inference time with batch size 128; Memory: memory load on synthetic data of sequence length 64 and batch size 64 with back-propagation considered; Train Acc. and Test Acc.: the accuracies on training/test sets. All state-of-the-art methods in leaderboard are listed in Table 1&2 up to Sep. 2018.,"Which model has the highest test accuracy on the SNLI dataset, and how does its training time per epoch compare to the MTSA model?"
spiqa_503,1611.07718v2,"Based on the figure comparing ResNet-101 (44.5M) and DMRNet (43.3M) presented in this paper, which model demonstrates the lowest Top-1 validation error on the ImageNet dataset?",ResNet-101 from the reference paper reports top-1 validation error of 23.60 which is lower than ResNet-101 reevaluated (26.41) and DMRNet (23.66),1611.07718v2.pdf,"['1611.07718v2.pdf', '1709.08294v3.pdf', '1811.09393v4.pdf', '1608.02784v2.pdf', '1708.06832v3.pdf', '1805.04687v2.pdf', '1703.10730v2.pdf', '1803.01128v3.pdf', '1809.01989v2.pdf']","The table shows that DMRNet has a Top-1 validation error of 23.66%, which is lower than the 26.41% error of ResNet-101.",1611.07718v2-Table4-1.png,Deep Convolutional Neural Networks with Merge-and-Run Mappings,"A deep residual network, built by stacking a sequence of residual blocks, is
easy to train, because identity mappings skip residual branches and thus
improve information flow. To further reduce the training difficulty, we present
a simple network architecture, deep merge-and-run neural networks. The novelty
lies in a modularized building block, merge-and-run block, which assembles
residual branches in parallel through a merge-and-run mapping: Average the
inputs of these residual branches (Merge), and add the average to the output of
each residual branch as the input of the subsequent residual branch (Run),
respectively. We show that the merge-and-run mapping is a linear idempotent
function in which the transformation matrix is idempotent, and thus improves
information flow, making training easy. In comparison to residual networks, our
networks enjoy compelling advantages: they contain much shorter paths, and the
width, i.e., the number of channels, is increased. We evaluate the performance
on the standard recognition tasks. Our approach demonstrates consistent
improvements over ResNets with the comparable setup, and achieves competitive
results (e.g., $3.57\%$ testing error on CIFAR-$10$, $19.00\%$ on CIFAR-$100$,
$1.51\%$ on SVHN).",The validation (single 224×224 center crop) and training errors (%) of ResNet-101 (44.5M) and our DMRNet (43.3M) on ImageNet.,Which model has the lowest Top-1 validation error on ImageNet?
spiqa_504,1805.00912v4,"Based on the experiments visualized in Figure (a) and (b) of the paper, which model demonstrates the best memory efficiency and reduced time cost across all sequence lengths on synthetic data?",MTSA,1805.00912v4.pdf,"['1805.00912v4.pdf', '1811.08257v1.pdf', '1708.05239v3.pdf', '1702.03584v3.pdf', '1704.05958v2.pdf', '1809.04276v2.pdf', '1707.01922v5.pdf', '1804.05936v2.pdf', '1701.03077v10.pdf']",Figure (a) and (b) show that the MTSA model has the lowest memory consumption and time cost for all sequence lengths.,1805.00912v4-Figure1-1.png,Tensorized Self-Attention: Efficiently Modeling Pairwise and Global Dependencies Together,"Neural networks equipped with self-attention have parallelizable computation,
light-weight structure, and the ability to capture both long-range and local
dependencies. Further, their expressive power and performance can be boosted by
using a vector to measure pairwise dependency, but this requires to expand the
alignment matrix to a tensor, which results in memory and computation
bottlenecks. In this paper, we propose a novel attention mechanism called
""Multi-mask Tensorized Self-Attention"" (MTSA), which is as fast and as
memory-efficient as a CNN, but significantly outperforms previous
CNN-/RNN-/attention-based models. MTSA 1) captures both pairwise (token2token)
and global (source2token) dependencies by a novel compatibility function
composed of dot-product and additive attentions, 2) uses a tensor to represent
the feature-wise alignment scores for better expressive power but only requires
parallelizable matrix multiplications, and 3) combines multi-head with
multi-dimensional attentions, and applies a distinct positional mask to each
head (subspace), so the memory and computation can be distributed to multiple
heads, each with sequential information encoded independently. The experiments
show that a CNN/RNN-free model based on MTSA achieves state-of-the-art or
competitive performance on nine NLP benchmarks with compelling memory- and
time-efficiency.","(a) Memory consumption and (b) time cost vs. sequence length on synthetic data; (c) memory load (x-axis), inference time on dev set (y-axis) and test accuracy on the SNLI dataset.",Which model has the lowest memory consumption and time cost on synthetic data?
spiqa_505,1710.05654v2,"In the figure titled ""Connectivity across classes of MNIST,"" which model, shown in the rightmost plot, effectively connects digit clusters even at larger distances in very sparse graphs, with 5 edges per node?",The log model.,1710.05654v2.pdf,"['1710.05654v2.pdf', '1805.08751v2.pdf', '1706.04269v2.pdf', '1804.04786v3.pdf', '1805.06431v4.pdf', '1701.03077v10.pdf']","The rightmost plot shows that the log model has the highest percentage of weight for connected pairs of each label, even for very sparse graphs of 5 edges per node. This indicates that the log model is the most effective at connecting digits with larger distances.",1710.05654v2-Figure5-1.png,Large Scale Graph Learning from Smooth Signals,"Graphs are a prevalent tool in data science, as they model the inherent
structure of the data. They have been used successfully in unsupervised and
semi-supervised learning. Typically they are constructed either by connecting
nearest samples, or by learning them from data, solving an optimization
problem. While graph learning does achieve a better quality, it also comes with
a higher computational cost. In particular, the current state-of-the-art model
cost is $\mathcal{O}(n^2)$ for $n$ samples. In this paper, we show how to scale
it, obtaining an approximation with leading cost of $\mathcal{O}(n\log(n))$,
with quality that approaches the exact graph learning model. Our algorithm uses
known approximate nearest neighbor techniques to reduce the number of
variables, and automatically selects the correct parameters of the model,
requiring a single intuitive input: the desired edge density.","Connectivity across classes of MNIST. The graph is normalized so that ‖W‖1,1 = 1. We measure the percentage of the total weight for connected pairs of each label. The last columns correspond to the total of the wrong edges, between images of different labels. Left: A-NN graph. Middle: `2 model (4) neglects digits with larger distance. Right: log model (5) does not neglect to connect any cluster even for very sparse graphs of 5 edges per node.",Which model is the most effective at connecting digits with larger distances?
spiqa_506,1809.02731v3,"In the figure comparing the performance of three models on seven unsupervised evaluation tasks, as indicated in the last row of the table, which model had the best average score?",The Linear model performed best on average with a score of 70.0.,1809.02731v3.pdf,"['1809.02731v3.pdf', '1906.10843v1.pdf', '1611.05742v3.pdf', '1803.06506v3.pdf', '1804.05938v2.pdf', '1705.02946v3.pdf']",The table shows the performance of three models on seven different tasks. The average performance is shown in the last row of the table.,1809.02731v3-Table4-1.png,Exploiting Invertible Decoders for Unsupervised Sentence Representation Learning,"The encoder-decoder models for unsupervised sentence representation learning
tend to discard the decoder after being trained on a large unlabelled corpus,
since only the encoder is needed to map the input sentence into a vector
representation. However, parameters learnt in the decoder also contain useful
information about language. In order to utilise the decoder after learning, we
present two types of decoding functions whose inverse can be easily derived
without expensive inverse calculation. Therefore, the inverse of the decoding
function serves as another encoder that produces sentence representations. We
show that, with careful design of the decoding functions, the model learns good
sentence representations, and the ensemble of the representations produced from
the encoder and the inverse of the decoder demonstrate even better
generalisation ability and solid transferability.","Comparison of the learnt representations in our system with the same dimensionality as pretrained word vectors on unsupervised evaluation tasks. The encoding function that is learnt to compose a sentence representation from pretrained word vectors outperforms averaging word vectors, which supports our argument that learning helps to produce higher-quality sentence representations.",Which model performed best on average across all tasks?
spiqa_507,1704.07121v2,"Referring to Table 3 in the paper ""Being Negative but Constructively,"" which model achieved the highest accuracy in the ""All"" category of the Visual7W dataset, and how large is the performance gap between this model and human accuracy, as shown in the table?","The MLP-IQA model achieved the highest accuracy in the ""All"" category of Visual7W, with a score of 45.1%. However, this performance still falls significantly short of human performance, which stands at 84.1% for the same category.",1704.07121v2.pdf,"['1704.07121v2.pdf', '1803.04572v2.pdf', '1812.00108v4.pdf', '1701.03077v10.pdf', '1804.04786v3.pdf', '1804.07707v2.pdf']","The table displays the test accuracy of various models and humans on different categories of the Visual7W dataset. The ""All"" category likely refers to the combined performance across all question types. By looking at the final column of the table, we can identify that MLP-IQA achieved the highest accuracy among the models. Comparing this value to the human performance shown in the same column reveals the performance gap between the best model and humans.",1704.07121v2-Table3-1.png,Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets,"Visual question answering (Visual QA) has attracted a lot of attention
lately, seen essentially as a form of (visual) Turing test that artificial
intelligence should strive to achieve. In this paper, we study a crucial
component of this task: how can we design good datasets for the task? We focus
on the design of multiple-choice based datasets where the learner has to select
the right answer from a set of candidate ones including the target (\ie the
correct one) and the decoys (\ie the incorrect ones). Through careful analysis
of the results attained by state-of-the-art learning models and human
annotators on existing datasets, we show that the design of the decoy answers
has a significant impact on how and what the learning models learn from the
datasets. In particular, the resulting learner can ignore the visual
information, the question, or both while still doing well on the task. Inspired
by this, we propose automatic procedures to remedy such design deficiencies. We
apply the procedures to re-construct decoy answers for two popular Visual QA
datasets as well as to create a new Visual QA dataset from the Visual Genome
project, resulting in the largest dataset for this task. Extensive empirical
studies show that the design deficiencies have been alleviated in the remedied
datasets and the performance on them is likely a more faithful indicator of the
difference among learning models. The datasets are released and publicly
available via http://www.teds.usc.edu/website_vqa/.",Table 3: Test accuracy (%) on Visual7W.,"Which model performed best on the ""All"" category of Visual7W, and how did its performance compare to human performance?"
spiqa_508,1705.09296v2,"According to Table 2 in the ""Neural Models for Documents with Metadata"" paper, which model(s) achieved the highest accuracy on the IMDB dataset for classifying documents into categorical labels, and how much did they outperform the SLDA model?","Both the SCHOLAR (covariates) and Logistic Regression models achieved the highest accuracy of 0.87 on the IMDB dataset. This represents a 0.23 improvement over the SLDA model, which achieved an accuracy of 0.64.",1705.09296v2.pdf,"['1705.09296v2.pdf', '1805.08751v2.pdf', '1803.05776v2.pdf', '1706.00827v2.pdf', '1802.07351v2.pdf', '1805.06447v3.pdf', '1704.07854v4.pdf', '1606.07384v2.pdf', '1804.01429v3.pdf', '1705.02798v6.pdf', '1906.06589v3.pdf', '1812.10735v2.pdf', '1804.00863v3.pdf', '1703.00060v2.pdf']","Table 1 displays the accuracy of different models on three datasets, including IMDB. By comparing the accuracy values in the IMDB column, we can identify the best performing model(s). The difference in accuracy between the top model(s) and SLDA can be calculated by subtracting the SLDA accuracy from the highest accuracy value.",1705.09296v2-Table2-1.png,Neural Models for Documents with Metadata,"Most real-world document collections involve various types of metadata, such
as author, source, and date, and yet the most commonly-used approaches to
modeling text corpora ignore this information. While specialized models have
been developed for particular applications, few are widely used in practice, as
customization typically requires derivation of a custom inference algorithm. In
this paper, we build on recent advances in variational inference methods and
propose a general neural framework, based on topic models, to enable flexible
incorporation of metadata and allow for rapid exploration of alternative
models. Our approach achieves strong performance, with a manageable tradeoff
between perplexity, coherence, and sparsity. Finally, we demonstrate the
potential of our framework through an exploration of a corpus of articles about
US immigration.",Table 2: Accuracy of various models on three datasets with categorical labels.,"Which model performed best on the IMDB dataset for classifying documents with categorical labels, and how much better did it perform compared to the SLDA model?"
spiqa_509,1812.10735v2,"Referring to Table 1 in the ""CAN: Constrained Attention Networks for Multi-Aspect Sentiment Analysis"" paper, which model achieved the highest accuracy and Macro-F1 scores for binary classification on the Rest15 dataset, and how do these results compare to the performance of the best model for 3-way classification on the same dataset?","For binary classification on the Rest15 dataset, M-CAN-2$R_o$ achieved the highest performance with an accuracy of 82.14% and Macro-F1 of 81.58%. In comparison, the best performing model for 3-way classification on Rest15 was M-CAN-2$R_s$, achieving an accuracy of 78.22% and Macro-F1 of 55.80%. This indicates that M-CAN-2$R_o$ performed better in both accuracy and Macro-F1 for binary classification compared to the best model for 3-way classification on the same dataset.",1812.10735v2.pdf,"['1812.10735v2.pdf', '1803.02750v3.pdf', '1811.08481v2.pdf', '1709.08294v3.pdf', '1704.00774v3.pdf', '1706.04284v3.pdf', '1705.02798v6.pdf']","Table 1 provides the performance results of different models on both Rest14 and Rest15 datasets for both 3-way and binary classifications. By comparing the accuracy and Macro-F1 values for each model on the Rest15 dataset, we can identify the best performing models for each classification task. The table clearly shows the values for each model and allows for direct comparison of their performance.",1812.10735v2-Table3-1.png,CAN: Constrained Attention Networks for Multi-Aspect Sentiment Analysis,"Aspect level sentiment classification is a fine-grained sentiment analysis
task. To detect the sentiment towards a particular aspect in a sentence,
previous studies have developed various attention-based methods for generating
aspect-specific sentence representations. However, the attention may inherently
introduce noise and downgrade the performance. In this paper, we propose
constrained attention networks (CAN), a simple yet effective solution, to
regularize the attention for multi-aspect sentiment analysis, which alleviates
the drawback of the attention mechanism. Specifically, we introduce orthogonal
regularization on multiple aspects and sparse regularization on each single
aspect. Experimental results on two public datasets demonstrate the
effectiveness of our approach. We further extend our approach to multi-task
settings and outperform the state-of-the-art methods.",Table 3: Results of the ALSC task in multi-task settings in terms of accuracy (%) and Macro-F1 (%).,"Which model performed best on the Rest15 dataset for binary classification, and how does its performance compare to the best model for 3-way classification on the same dataset?"
spiqa_510,1805.00912v4,"According to the figure presenting the benchmarking results for SNLI and MultiNLI in the ""Tensorized Self-Attention"" paper, which model achieved the highest accuracy on the SNLI test set?",The Transfer + MTSA model performed best on the SNLI test set with an accuracy of 86.9%.,1805.00912v4.pdf,"['1805.00912v4.pdf', '1703.02507v3.pdf', '1811.08257v1.pdf', '1710.06177v2.pdf', '1605.07496v3.pdf']",The table shows the accuracy of different models on the SNLI and MultiNLI benchmark tasks. The Transfer + MTSA model has the highest accuracy on the SNLI test set.,1805.00912v4-Table2-1.png,Tensorized Self-Attention: Efficiently Modeling Pairwise and Global Dependencies Together,"Neural networks equipped with self-attention have parallelizable computation,
light-weight structure, and the ability to capture both long-range and local
dependencies. Further, their expressive power and performance can be boosted by
using a vector to measure pairwise dependency, but this requires to expand the
alignment matrix to a tensor, which results in memory and computation
bottlenecks. In this paper, we propose a novel attention mechanism called
""Multi-mask Tensorized Self-Attention"" (MTSA), which is as fast and as
memory-efficient as a CNN, but significantly outperforms previous
CNN-/RNN-/attention-based models. MTSA 1) captures both pairwise (token2token)
and global (source2token) dependencies by a novel compatibility function
composed of dot-product and additive attentions, 2) uses a tensor to represent
the feature-wise alignment scores for better expressive power but only requires
parallelizable matrix multiplications, and 3) combines multi-head with
multi-dimensional attentions, and applies a distinct positional mask to each
head (subspace), so the memory and computation can be distributed to multiple
heads, each with sequential information encoded independently. The experiments
show that a CNN/RNN-free model based on MTSA achieves state-of-the-art or
competitive performance on nine NLP benchmarks with compelling memory- and
time-efficiency.","Experimental results on sentence-encoding based SNLI and MultiNLI benchmark tasks. “Transfer” denotes pretrained language model on large corpus for transfer learning, which detailed by Radford et al. (2018). References: a(Nie and Bansal, 2017), b(Chen et al., 2018), c(Talman et al., 2018).",Which model performed best on the SNLI test set?
spiqa_511,1809.03449v3,"Based on the figure comparing model performance in the ""Explicit Utilization of General Knowledge in Machine Reading Comprehension"" paper, which model achieved the highest F1 score on the AddOneSent adversarial dataset as of October 18, 2018?",KAR,1809.03449v3.pdf,"['1809.03449v3.pdf', '1709.02755v5.pdf', '1811.07073v3.pdf', '1805.07567v2.pdf', '1708.02153v2.pdf', '1707.00189v3.pdf', '1707.00524v2.pdf', '1809.01989v2.pdf', '1803.04572v2.pdf', '1603.00286v5.pdf', '1804.05936v2.pdf']",The table shows that KAR achieved the highest F1 score of 72.3 on the AddOneSent dataset.,1809.03449v3-Table2-1.png,Explicit Utilization of General Knowledge in Machine Reading Comprehension,"To bridge the gap between Machine Reading Comprehension (MRC) models and
human beings, which is mainly reflected in the hunger for data and the
robustness to noise, in this paper, we explore how to integrate the neural
networks of MRC models with the general knowledge of human beings. On the one
hand, we propose a data enrichment method, which uses WordNet to extract
inter-word semantic connections as general knowledge from each given
passage-question pair. On the other hand, we propose an end-to-end MRC model
named as Knowledge Aided Reader (KAR), which explicitly uses the above
extracted general knowledge to assist its attention mechanisms. Based on the
data enrichment method, KAR is comparable in performance with the
state-of-the-art MRC models, and significantly more robust to noise than them.
When only a subset (20%-80%) of the training examples are available, KAR
outperforms the state-of-the-art MRC models by a large margin, and is still
reasonably robust to noise.","Model comparison based on SQuAD 1.1 and two of its adversarial sets: AddSent and AddOneSent. All the numbers are up to date as of October 18, 2018. Note that SQuAD 2.0 (Rajpurkar et al., 2018) is not involved in this paper, because it requires MRC models to deal with the problem of answer triggering, but this paper is aimed at improving the hunger for data and robustness to noise of MRC models.",Which model performed the best on the AddOneSent dataset?
spiqa_512,1703.02507v3,"According to the figure comparing model performances on supervised evaluation tasks in the ""Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features"" paper, which model achieved the highest accuracy for the MSRP task in the Ordered Sentences dataset?",SkipThought,1703.02507v3.pdf,"['1703.02507v3.pdf', '1709.00139v4.pdf', '1805.01216v3.pdf', '1803.04572v2.pdf']","The table shows the performance of different models on different supervised evaluation tasks. For the Ordered Sentences dataset, the model with the highest accuracy for the MSRP task is SkipThought, with an accuracy of 83.0.",1703.02507v3-Table1-1.png,Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features,"The recent tremendous success of unsupervised word embeddings in a multitude
of applications raises the obvious question if similar methods could be derived
to improve embeddings (i.e. semantic representations) of word sequences as
well. We present a simple but efficient unsupervised objective to train
distributed representations of sentences. Our method outperforms the
state-of-the-art unsupervised models on most benchmark tasks, highlighting the
robustness of the produced general-purpose sentence embeddings.","Comparison of the performance of different models on different supervised evaluation tasks. An underline indicates the best performance for the dataset. Top 3 performances in each data category are shown in bold. The average is calculated as the average of accuracy for each category (For MSRP, we take the accuracy). )",Which model performed the best on the MSRP task for the Ordered Sentences dataset?
spiqa_513,1703.02507v3,"Based on the performance comparison in the figure from the paper ""Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features,"" which Sent2Vec model achieved the highest score on the MSRP task for the Twitter dataset?",The Sent2Vec uni. + bi. model performed the best on the MSRP task for the Twitter dataset.,1703.02507v3.pdf,"['1703.02507v3.pdf', '1611.04684v1.pdf', '1805.06447v3.pdf', '1705.08016v3.pdf', '1707.01922v5.pdf', '1804.05938v2.pdf', '1611.03780v2.pdf', '1611.07718v2.pdf', '1802.07222v1.pdf', '1707.08608v3.pdf', '1809.04276v2.pdf', '1802.07351v2.pdf', '1606.07384v2.pdf', '1804.07931v2.pdf', '1811.10673v1.pdf']","The table shows the performance of different models on different downstream supervised evaluation tasks. The MSRP task is one of these tasks, and the Twitter dataset is one of the datasets used. The Sent2Vec uni. + bi. model achieved the highest score (72.4 / 80.6) on the MSRP task for the Twitter dataset.",1703.02507v3-Table6-1.png,Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features,"The recent tremendous success of unsupervised word embeddings in a multitude
of applications raises the obvious question if similar methods could be derived
to improve embeddings (i.e. semantic representations) of word sequences as
well. We present a simple but efficient unsupervised objective to train
distributed representations of sentences. Our method outperforms the
state-of-the-art unsupervised models on most benchmark tasks, highlighting the
robustness of the produced general-purpose sentence embeddings.","Comparison of the performance of different Sent2Vec models with different semisupervised/supervised models on different downstream supervised evaluation tasks. An underline indicates the best performance for the dataset and Sent2Vec model performances are bold if they perform as well or better than all other non-Sent2Vec models, including those presented in Table 1.",Which model performed the best on the MSRP task for the Twitter dataset?
spiqa_514,1611.02654v2,"In the figure titled ""Performance comparison for semantic similarity and paraphrase detection,"" which model achieves the lowest MSE score on the SICK dataset?",The supervised model performed the best on the SICK dataset according to the MSE metric.,1611.02654v2.pdf,"['1611.02654v2.pdf', '1804.01429v3.pdf', '1603.03833v4.pdf', '1811.02721v3.pdf', '1707.06320v2.pdf', '1809.01989v2.pdf', '1603.00286v5.pdf']",The table shows the MSE for each model on the SICK dataset. The supervised model has the lowest MSE of 0.253.,1611.02654v2-Table4-1.png,Sentence Ordering and Coherence Modeling using Recurrent Neural Networks,"Modeling the structure of coherent texts is a key NLP problem. The task of
coherently organizing a given set of sentences has been commonly used to build
and evaluate models that understand such structure. We propose an end-to-end
unsupervised deep learning approach based on the set-to-sequence framework to
address this problem. Our model strongly outperforms prior methods in the order
discrimination task and a novel task of ordering abstracts from scientific
articles. Furthermore, our work shows that useful text representations can be
obtained by learning to order sentences. Visualizing the learned sentence
representations shows that the model captures high-level logical structure in
paragraphs. Our representations perform comparably to state-of-the-art
pre-training methods on sentence similarity and paraphrase detection tasks.",Performance comparison for semantic similarity and paraphrase detection. The first row shows the best performing purely supervised methods. The last section shows our models.,Which model performed the best on the SICK dataset according to the MSE metric?
spiqa_515,1906.06589v3,"Which model achieved the highest test accuracy (Atest) on the dataset Dtest, as presented in the figure captioned in the paper's evaluation of protected models using DMP?",P-FC,1906.06589v3.pdf,"['1906.06589v3.pdf', '1710.06177v2.pdf', '1603.00286v5.pdf', '1805.06447v3.pdf', '1707.01917v2.pdf', '1802.07459v2.pdf', '1706.03847v3.pdf']",The table shows that P-FC achieved the highest test accuracy (Atest) of 74.1.,1906.06589v3-Table10-1.png,Membership Privacy for Machine Learning Models Through Knowledge Transfer,"Large capacity machine learning (ML) models are prone to membership inference
attacks (MIAs), which aim to infer whether the target sample is a member of the
target model's training dataset. The serious privacy concerns due to the
membership inference have motivated multiple defenses against MIAs, e.g.,
differential privacy and adversarial regularization. Unfortunately, these
defenses produce ML models with unacceptably low classification performances.
Our work proposes a new defense, called distillation for membership privacy
(DMP), against MIAs that preserves the utility of the resulting models
significantly better than prior defenses. DMP leverages knowledge distillation
to train ML models with membership privacy. We provide a novel criterion to
tune the data used for knowledge transfer in order to amplify the membership
privacy of DMP. Our extensive evaluation shows that DMP provides significantly
better tradeoffs between membership privacy and classification accuracies
compared to state-of-the-art MIA defenses. For instance, DMP achieves ~100%
accuracy improvement over adversarial regularization for DenseNet trained on
CIFAR100, for similar membership privacy (measured using MIA risk): when the
MIA risk is 53.7%, adversarially regularized DenseNet is 33.6% accurate, while
DMP-trained DenseNet is 65.3% accurate.","DMP does not pose membership inference risk to the possibly sensitive reference data. Aref and Atest are accuracies of protected model, θp, on Xref and Dtest, respectively.",Which model performed the best on the test data?
spiqa_516,1805.07567v2,"Referring to Table 3 in the paper ""Optimizing the F-measure for Threshold-free Salient Object Detection,"" which model achieves the highest Mean F-measure on the DUT-OMRON dataset when trained on the MB dataset, and how does the incorporation of the FLoss compared to the base version of the model improve its performance?","The DSS+FLoss model performs best in terms of Mean F-measure on the DUT-OMRON dataset when trained on the MB dataset, achieving a score of 0.755.

The FLoss variant of the DSS model shows a clear improvement over the base DSS model, with a Mean F-measure increase from 0.738 to 0.755.",1805.07567v2.pdf,"['1805.07567v2.pdf', '1811.02721v3.pdf', '1708.00160v2.pdf', '1703.02507v3.pdf', '1804.07707v2.pdf', '1705.09882v2.pdf', '1611.02654v2.pdf', '1805.02349v2.pdf', '1805.06431v4.pdf', '1701.03077v10.pdf', '1704.07854v4.pdf', '1809.03149v2.pdf', '1705.09296v2.pdf']","The table provides the Mean F-measure scores for different models on various datasets, including DUT-OMRON, when trained on the MB dataset. By comparing the scores within the DUT-OMRON column and focusing on the MB training data rows, we can identify DSS+FLoss as the top performer. Additionally, comparing DSS+FLoss to DSS within the same dataset and training data conditions shows the positive impact of the FLoss variant.",1805.07567v2-Table3-1.png,Optimizing the F-measure for Threshold-free Salient Object Detection,"Current CNN-based solutions to salient object detection (SOD) mainly rely on
the optimization of cross-entropy loss (CELoss). Then the quality of detected
saliency maps is often evaluated in terms of F-measure. In this paper, we
investigate an interesting issue: can we consistently use the F-measure
formulation in both training and evaluation for SOD? By reformulating the
standard F-measure we propose the relaxed F-measure which is differentiable
w.r.t the posterior and can be easily appended to the back of CNNs as the loss
function. Compared to the conventional cross-entropy loss of which the
gradients decrease dramatically in the saturated area, our loss function, named
FLoss, holds considerable gradients even when the activation approaches the
target. Consequently, the FLoss can continuously force the network to produce
polarized activations. Comprehensive benchmarks on several popular datasets
show that FLoss outperforms the state-of-the-art with a considerable margin.
More specifically, due to the polarized predictions, our method is able to
obtain high-quality saliency maps without carefully tuning the optimal
threshold, showing significant advantages in real-world applications.","Table 3. Quantitative comparison of different methods on 6 popular datasets. Our proposed FLoss consistently improves performance in terms of both MAE (the smaller the better) and F-measure (the larger the better). Especially in terms of Mean F-measure, we outperform the state-of-the-art with very clear margins, because our method is able to produce high-contrast predictions that can achieve high F-measure under a wide range of thresholds.",Which model performs best in terms of Mean F-measure on the DUT-OMRON dataset when trained on the MB dataset? How does the FLoss variant of this model compare to its base version?
spiqa_517,1703.02507v3,"Referring to the ""Unsupervised Evaluation Tasks"" figure in this paper, which model achieves the highest average Spearman and Pearson correlation score on the SICK 2014 dataset?",C-PHRASE,1703.02507v3.pdf,"['1703.02507v3.pdf', '1704.07854v4.pdf', '1906.06589v3.pdf', '1611.03780v2.pdf', '1811.07073v3.pdf']",The table shows the performance of different models on the SICK 2014 dataset in terms of Spearman and Pearson correlation. The C-PHRASE model has the highest average score of 63.67.,1703.02507v3-Table2-1.png,Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features,"The recent tremendous success of unsupervised word embeddings in a multitude
of applications raises the obvious question if similar methods could be derived
to improve embeddings (i.e. semantic representations) of word sequences as
well. We present a simple but efficient unsupervised objective to train
distributed representations of sentences. Our method outperforms the
state-of-the-art unsupervised models on most benchmark tasks, highlighting the
robustness of the produced general-purpose sentence embeddings.",Unsupervised Evaluation Tasks: Comparison of the performance of different models on Spearman/Pearson correlation measures. An underline indicates the best performance for the dataset. Top 3 performances in each data category are shown in bold. The average is calculated as the average of entries for each correlation measure.,Which model performs best on the SICK 2014 dataset in terms of average Spearman and Pearson correlation?
spiqa_518,1707.06320v2,"Based on Table 3 of *""Learning Visually Grounded Sentence Representations""*, how much does the GroundSent-Both model outperform the STb-1024 model in terms of accuracy on the SNLI dataset, and what role does the grounding mechanism (caption and image grounding) play in this performance difference?","The GroundSent-Both model performs best on the SNLI dataset, achieving an accuracy of 72.0%. Grounding contributes to an improvement of 4.7% compared to the baseline STb-1024 model, which achieves 67.3%.",1707.06320v2.pdf,"['1707.06320v2.pdf', '1705.09966v2.pdf', '1811.02553v4.pdf', '1704.05958v2.pdf', '1705.07164v8.pdf', '1707.01922v5.pdf', '1811.06635v1.pdf', '1704.07854v4.pdf', '1805.04609v3.pdf', '1710.05654v2.pdf', '1708.02153v2.pdf', '1803.05776v2.pdf', '1705.10667v4.pdf', '1708.00160v2.pdf', '1805.00912v4.pdf']","The table displays the performance of different models on various datasets, including SNLI. By comparing the accuracy of the GroundSent-Both model (with grounding) to the STb-1024 model (without grounding), we can determine the contribution of grounding to the performance improvement.",1707.06320v2-Table3-1.png,Learning Visually Grounded Sentence Representations,"We introduce a variety of models, trained on a supervised image captioning
corpus to predict the image features for a given caption, to perform sentence
representation grounding. We train a grounded sentence encoder that achieves
good performance on COCO caption and image retrieval and subsequently show that
this encoder can successfully be transferred to various NLP tasks, with
improved performance over text-only models. Lastly, we analyze the contribution
of grounding, and show that word embeddings learned by this system outperform
non-grounded ones.","Table 3: Thorough investigation of the contribution of grounding, ensuring equal number of components and identical architectures, on the variety of sentence-level semantic benchmark tasks. STb=SkipThought-like model with bidirectional LSTM+max. 2×STb-1024=ensemble of 2 different STb models with different initializations. GroundSent is STb-1024+Cap2Cap/Img/Both. We find that performance improvements are sometimes due to having more parameters, but in most cases due to grounding.","Which model performs best on the SNLI dataset, and how much does grounding contribute to its performance compared to the baseline STb-1024 model?"
spiqa_519,1611.04684v1,"According to the figure titled ""Accuracy on different lengths of text,"" comparing LSTM, MV-LSTM, and KEHNN on the Ubuntu dataset, which model achieves the highest accuracy for text lengths between 60 and 90 words?",KEHNN,1611.04684v1.pdf,"['1611.04684v1.pdf', '1708.06832v3.pdf', '1709.00139v4.pdf', '1803.03467v4.pdf', '1611.07718v2.pdf', '1709.02755v5.pdf', '1811.02553v4.pdf', '1803.05776v2.pdf', '1603.00286v5.pdf', '1811.06635v1.pdf', '1802.07222v1.pdf']","The table shows the accuracy of three models (LSTM, MV-LSTM, and KEHNN) on two datasets (QA and Ubuntu) for different text lengths. For text lengths between 60 and 90 words on the Ubuntu dataset, KEHNN has the highest accuracy of 0.785.",1611.04684v1-Table5-1.png,Knowledge Enhanced Hybrid Neural Network for Text Matching,"Long text brings a big challenge to semantic matching due to their
complicated semantic and syntactic structures. To tackle the challenge, we
consider using prior knowledge to help identify useful information and filter
out noise to matching in long text. To this end, we propose a knowledge
enhanced hybrid neural network (KEHNN). The model fuses prior knowledge into
word representations by knowledge gates and establishes three matching channels
with words, sequential structures of sentences given by Gated Recurrent Units
(GRU), and knowledge enhanced representations. The three channels are processed
by a convolutional neural network to generate high level features for matching,
and the features are synthesized as a matching score by a multilayer
perceptron. The model extends the existing methods by conducting matching on
words, local structures of sentences, and global context of sentences.
Evaluation results from extensive experiments on public data sets for question
answering and conversation show that KEHNN can significantly outperform
the-state-of-the-art matching models and particularly improve the performance
on pairs with long text.",Accuracy on different length of text,Which model performs best on the Ubuntu dataset for text lengths between 60 and 90 words?
spiqa_520,1804.07931v2,"Based on the AUC values in Table 1, which model demonstrates the best performance for both CVR and CTCVR tasks in this study?",The ESMM model performs the best overall on both the CVR and CTCVR tasks.,1804.07931v2.pdf,"['1804.07931v2.pdf', '1803.04572v2.pdf', '1703.00060v2.pdf', '1611.04684v1.pdf', '1704.04539v2.pdf']","Table 1 shows the AUC (Area Under the Curve) values for different models on both the CVR and CTCVR tasks. The higher the AUC value, the better the model performs. We can see that ESMM has the highest AUC values for both tasks (68.56 ± 0.37 for CVR and 65.32 ± 0.49 for CTCVR), indicating superior performance compared to other models.",1804.07931v2-Table2-1.png,Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate,"Estimating post-click conversion rate (CVR) accurately is crucial for ranking
systems in industrial applications such as recommendation and advertising.
Conventional CVR modeling applies popular deep learning methods and achieves
state-of-the-art performance. However it encounters several task-specific
problems in practice, making CVR modeling challenging. For example,
conventional CVR models are trained with samples of clicked impressions while
utilized to make inference on the entire space with samples of all impressions.
This causes a sample selection bias problem. Besides, there exists an extreme
data sparsity problem, making the model fitting rather difficult. In this
paper, we model CVR in a brand-new perspective by making good use of sequential
pattern of user actions, i.e., impression -> click -> conversion. The proposed
Entire Space Multi-task Model (ESMM) can eliminate the two problems
simultaneously by i) modeling CVR directly over the entire space, ii) employing
a feature representation transfer learning strategy. Experiments on dataset
gathered from Taobao's recommender system demonstrate that ESMM significantly
outperforms competitive methods. We also release a sampling version of this
dataset to enable future research. To the best of our knowledge, this is the
first public dataset which contains samples with sequential dependence of click
and conversion labels for CVR modeling.",Table 2: Comparison of different models on Public Dataset.,Which model performs best overall on both the CVR and CTCVR tasks?
spiqa_521,1805.01216v3,"Based on the figure evaluating bAbI Task 1 in the ""Disentangling Language and Knowledge in Task-Oriented Dialogs"" paper, which model achieves the highest per-response accuracy when the percentage of unseen entities in the response is low?",BoSsNet,1805.01216v3.pdf,"['1805.01216v3.pdf', '1709.00139v4.pdf', '1901.00398v2.pdf', '1811.08481v2.pdf', '1708.05239v3.pdf', '1812.06589v2.pdf', '1704.05958v2.pdf', '1703.02507v3.pdf', '1710.01507v4.pdf', '1809.02731v3.pdf', '1812.10735v2.pdf']",The figure shows that the BoSsNet line is at the top when the percentage of unseen entities in the response is low.,1805.01216v3-Figure3-1.png,Disentangling Language and Knowledge in Task-Oriented Dialogs,"The Knowledge Base (KB) used for real-world applications, such as booking a
movie or restaurant reservation, keeps changing over time. End-to-end neural
networks trained for these task-oriented dialogs are expected to be immune to
any changes in the KB. However, existing approaches breakdown when asked to
handle such changes. We propose an encoder-decoder architecture (BoSsNet) with
a novel Bag-of-Sequences (BoSs) memory, which facilitates the disentangled
learning of the response's language model and its knowledge incorporation.
Consequently, the KB can be modified with new knowledge without a drop in
interpretability. We find that BoSsNet outperforms state-of-the-art models,
with considerable improvements (> 10\%) on bAbI OOV test sets and other
human-human datasets. We also systematically modify existing datasets to
measure disentanglement and show BoSsNet to be robust to KB modifications.",bAbI Task 1: Per-response accuracy comparison on KA sets,Which task-oriented dialog system performs the best when the percentage of unseen information in the KB is high?
spiqa_522,1707.00189v3,"According to Table 1 in the paper, which neural ranking model achieves the highest nDCG@20 score on the WT14 dataset when trained with the NYT data, and how does this performance statistically compare to BM25, WT10, and AOL based on the significant differences indicated by the paired t-test arrows?","The Conv-KNRM model performs best when trained on the NYT dataset and evaluated on the WT14 dataset, achieving an nDCG@20 score of 0.3215. This performance is significantly better than all the baselines: BM25 (B), WT10 (W), and AOL (A).",1707.00189v3.pdf,"['1707.00189v3.pdf', '1611.05742v3.pdf', '1805.08751v2.pdf', '1707.08608v3.pdf', '1805.04687v2.pdf']","Looking at the ""WT14"" column and the row corresponding to ""NYT"" under each model, we can compare the nDCG@20 scores. Conv-KNRM has the highest score (0.3215) in that column. The upward arrows next to the score indicate that this performance is statistically significantly better than all the baselines according to a paired t-test.",1707.00189v3-Table1-1.png,Content-Based Weak Supervision for Ad-Hoc Re-Ranking,"One challenge with neural ranking is the need for a large amount of
manually-labeled relevance judgments for training. In contrast with prior work,
we examine the use of weak supervision sources for training that yield pseudo
query-document pairs that already exhibit relevance (e.g., newswire
headline-content pairs and encyclopedic heading-paragraph pairs). We also
propose filtering techniques to eliminate training samples that are too far out
of domain using two techniques: a heuristic-based approach and novel supervised
filter that re-purposes a neural ranker. Using several leading neural ranking
architectures and multiple weak supervision datasets, we show that these
sources of training pairs are effective on their own (outperforming prior weak
supervision techniques), and that filtering can further improve performance.","Table 1: Ranking performance when trained using contentbased sources (NYT and Wiki). Significant differences compared to the baselines ([B]M25, [W]T10, [A]OL) are indicated with ↑ and ↓ (paired t-test, p < 0.05).","Which model performs best when trained on the NYT dataset and evaluated on the WT14 dataset, and how does its performance compare to the baselines?"
spiqa_523,1705.07164v8,"Based on the training curves comparing generator and discriminator losses in Relaxed WGANs and competing models shown in the figure, which model demonstrates superior performance in terms of lower and more stable loss reductions?","It is difficult to say definitively which model performs better based on the training curves alone. However, it appears that the WGAN(g) model may be performing better than the other models, as its generator and discriminator losses are both lower than the other models.",1705.07164v8.pdf,"['1705.07164v8.pdf', '1805.04687v2.pdf', '1804.04786v3.pdf', '1809.02731v3.pdf', '1707.01922v5.pdf', '1611.07718v2.pdf']","The training curves show the generator and discriminator losses for each model over time. The lower the loss, the better the model is performing.",1705.07164v8-Figure3-1.png,Relaxed Wasserstein with Applications to GANs,"Wasserstein Generative Adversarial Networks (WGANs) provide a versatile class
of models, which have attracted great attention in various applications.
However, this framework has two main drawbacks: (i) Wasserstein-1 (or
Earth-Mover) distance is restrictive such that WGANs cannot always fit data
geometry well; (ii) It is difficult to achieve fast training of WGANs. In this
paper, we propose a new class of \textit{Relaxed Wasserstein} (RW) distances by
generalizing Wasserstein-1 distance with Bregman cost functions. We show that
RW distances achieve nice statistical properties while not sacrificing the
computational tractability. Combined with the GANs framework, we develop
Relaxed WGANs (RWGANs) which are not only statistically flexible but can be
approximated efficiently using heuristic approaches. Experiments on real images
demonstrate that the RWGAN with Kullback-Leibler (KL) cost function outperforms
other competing approaches, e.g., WGANs, even with gradient penalty.",Training curves by running all approaches. Generator and discriminator losses are plotted in orange and blue lines. cifar10 imagenet,Which model performs better based on the training curves?
spiqa_524,1704.05426v4,"According to the figure comparing the key validation statistics of SNLI and MultiNLI, which dataset demonstrates a higher percentage of individual labels that match the author's label, and what are the exact percentages for each?","SNLI performs better than MultiNLI when considering the percentage of individual labels that match the author's label. SNLI has a score of 85.8%, while MultiNLI has a score of 85.2%.",1704.05426v4.pdf,"['1704.05426v4.pdf', '1805.02349v2.pdf', '1809.03149v2.pdf', '1811.08481v2.pdf', '1803.04572v2.pdf', '1704.05958v2.pdf', '1611.02654v2.pdf']","The table shows the validation statistics for SNLI and MultiNLI. The ""Individual label = author's label"" row shows the percentage of individual labels that match the author's label. SNLI has a higher percentage than MultiNLI, which indicates that SNLI performs better on this metric.",1704.05426v4-Table2-1.png,A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference,"This paper introduces the Multi-Genre Natural Language Inference (MultiNLI)
corpus, a dataset designed for use in the development and evaluation of machine
learning models for sentence understanding. In addition to being one of the
largest corpora available for the task of NLI, at 433k examples, this corpus
improves upon available resources in its coverage: it offers data from ten
distinct genres of written and spoken English--making it possible to evaluate
systems on nearly the full complexity of the language--and it offers an
explicit setting for the evaluation of cross-genre domain adaptation.","Key validation statistics for SNLI (copied from Bowman et al., 2015) and MultiNLI.",Which model performs better on the MultiNLI dataset when considering the percentage of individual labels that match the author's label?
spiqa_525,1803.05776v2,"Referring to Figure (c) in the cerebellum data results at SNR=0dB, which model achieves the lowest NMSE?",GPG-K,1803.05776v2.pdf,"['1803.05776v2.pdf', '1804.05936v2.pdf', '1704.05426v4.pdf', '1709.00139v4.pdf', '1906.10843v1.pdf', '1805.02349v2.pdf', '1811.02721v3.pdf', '1710.06177v2.pdf', '1706.08146v3.pdf']","The NMSE for GPG-K is the lowest at SNR=0dB, as shown in Figure (c).",1803.05776v2-Figure1-1.png,Gaussian Processes Over Graphs,"We propose Gaussian processes for signals over graphs (GPG) using the apriori
knowledge that the target vectors lie over a graph. We incorporate this
information using a graph- Laplacian based regularization which enforces the
target vectors to have a specific profile in terms of graph Fourier transform
coeffcients, for example lowpass or bandpass graph signals. We discuss how the
regularization affects the mean and the variance in the prediction output. In
particular, we prove that the predictive variance of the GPG is strictly
smaller than the conventional Gaussian process (GP) for any non-trivial graph.
We validate our concepts by application to various real-world graph signals.
Our experiments show that the performance of the GPG is superior to GP for
small training data sizes and under noisy training.","Results for the cerebellum data (a) Adjacency matrix, (b) NMSE for testing data as a function of training data size at SNR=10dB, and (c) at SNR=0dB.",Which model performs the best at SNR=0dB?
spiqa_526,1804.07707v2,"According to Table 1 in the paper, which model achieves the highest unlabelled F1 score for predicting the delexicalised constituency tree of the LDC2017T10 dev set, and by how many points does it outperform the Unconditional model that does not leverage text or AMR graph information?","The Text-to-parse model performs the best at predicting the delexicalised constituency tree, achieving an unlabelled F1 score of 87.5. This is significantly higher than the baseline Unconditional model, which achieves an unlabelled F1 score of 38.5. The Text-to-parse model therefore performs approximately 49 points better than the baseline.",1804.07707v2.pdf,"['1804.07707v2.pdf', '1709.02755v5.pdf', '1805.08751v2.pdf', '1706.04269v2.pdf', '1706.08146v3.pdf']","Table 1 presents the parsing scores of different models on both labelled and unlabelled F1 metrics. By comparing the F1 scores across the models, we can identify which model performs best. The passage further clarifies that the Unconditional model serves as a baseline for comparison, as it does not leverage any information from the text or AMR graph. Therefore, the difference in unlabelled F1 scores between the Text-to-parse and Unconditional models demonstrates the improvement gained by utilizing textual information for predicting the syntactic structure.",1804.07707v2-Table1-1.png,Factorising AMR generation through syntax,"Generating from Abstract Meaning Representation (AMR) is an underspecified
problem, as many syntactic decisions are not constrained by the semantic graph.
To explicitly account for this underspecification, we break down generating
from AMR into two steps: first generate a syntactic structure, and then
generate the surface form. We show that decomposing the generation process this
way leads to state-of-the-art single model performance generating from AMR
without additional unlabelled data. We also demonstrate that we can generate
meaning-preserving syntactic paraphrases of the same AMR graph, as judged by
humans.",Table 1: Parsing scores on LDC2017T10 dev set.,"Which model performs the best at predicting the delexicalised constituency tree of an example, and how much better does it perform compared to the baseline model in terms of unlabelled F1 score?"
spiqa_527,1611.04684v1,"In Table 1 of the paper *Knowledge Enhanced Hybrid Neural Network for Text Matching*, which model achieves the highest scores for response selection across the metrics R$2$@1, R${10}$@1, R${10}$@2, and R${10}$@5, and how do the results justify the model's superior performance?","The KEHNN model performs the best for response selection. This is evident because it achieves the highest scores across all metrics (R$2$@1, R${10}$@1, R${10}$@2, and R${10}$@5) compared to all other models in the table.",1611.04684v1.pdf,"['1611.04684v1.pdf', '1803.04383v2.pdf', '1702.08694v3.pdf', '1611.03780v2.pdf', '1703.00060v2.pdf', '1804.04786v3.pdf', '1709.02755v5.pdf', '1805.06431v4.pdf', '1709.00139v4.pdf', '1805.04609v3.pdf', '1804.05936v2.pdf', '1708.05239v3.pdf']","Table 1 presents the performance of different models on the task of response selection. Each column represents a specific evaluation metric, and each row represents a different model. By comparing the values in each column, we can identify the model with the best performance for each metric. In this case, KEHNN consistently outperforms all other models, indicating its superior performance in response selection.",1611.04684v1-Table4-1.png,Knowledge Enhanced Hybrid Neural Network for Text Matching,"Long text brings a big challenge to semantic matching due to their
complicated semantic and syntactic structures. To tackle the challenge, we
consider using prior knowledge to help identify useful information and filter
out noise to matching in long text. To this end, we propose a knowledge
enhanced hybrid neural network (KEHNN). The model fuses prior knowledge into
word representations by knowledge gates and establishes three matching channels
with words, sequential structures of sentences given by Gated Recurrent Units
(GRU), and knowledge enhanced representations. The three channels are processed
by a convolutional neural network to generate high level features for matching,
and the features are synthesized as a matching score by a multilayer
perceptron. The model extends the existing methods by conducting matching on
words, local structures of sentences, and global context of sentences.
Evaluation results from extensive experiments on public data sets for question
answering and conversation show that KEHNN can significantly outperform
the-state-of-the-art matching models and particularly improve the performance
on pairs with long text.",Table 4: Evaluation results on response selection,"Which model performs the best for response selection, and how can we tell?"
spiqa_528,1611.02654v2,"Based on the results presented in Table 1 of the paper, which model achieves the highest accuracy for the order discrimination task on the Accidents dataset, and how does its performance compare quantitatively to the Seq2seq and Window-based models?","The proposed model in this paper achieves the best performance for the order discrimination task on the Accidents dataset with an accuracy of 0.944. It outperforms the other data-driven approaches, namely Window (Recurrent) with 0.840, Window (Recursive) with 0.864, and Seq2seq with 0.930.",1611.02654v2.pdf,"['1611.02654v2.pdf', '1809.03449v3.pdf', '1804.00863v3.pdf', '1709.02418v2.pdf', '1811.09393v4.pdf', '1803.05776v2.pdf', '1811.10673v1.pdf', '1706.04284v3.pdf', '1703.00060v2.pdf', '1805.04687v2.pdf', '1703.10730v2.pdf', '1906.06589v3.pdf']","Table 1 presents the mean accuracy of different models on the Accidents and Earthquakes datasets for the order discrimination task. By comparing the accuracy values within the Accidents row, we can identify which model performs best. Our model has the highest value (0.944) compared to the other data-driven models listed in the table.",1611.02654v2-Table1-1.png,Sentence Ordering and Coherence Modeling using Recurrent Neural Networks,"Modeling the structure of coherent texts is a key NLP problem. The task of
coherently organizing a given set of sentences has been commonly used to build
and evaluate models that understand such structure. We propose an end-to-end
unsupervised deep learning approach based on the set-to-sequence framework to
address this problem. Our model strongly outperforms prior methods in the order
discrimination task and a novel task of ordering abstracts from scientific
articles. Furthermore, our work shows that useful text representations can be
obtained by learning to order sentences. Visualizing the learned sentence
representations shows that the model captures high-level logical structure in
paragraphs. Our representations perform comparably to state-of-the-art
pre-training methods on sentence similarity and paraphrase detection tasks.","Table 1: Mean Accuracy comparison on the Accidents and Earthquakes data for the order discrimination task. The reference models are Entity-Grid (Barzilay and Lapata 2008), HMM (Louis and Nenkova 2012), Graph (Guinaudeau and Strube 2013), Window network (Li and Hovy 2014) and sequence-to-sequence (Li and Jurafsky 2016), respectively.",Which model performs the best for the order discrimination task on the Accidents dataset and how does it compare to the other data-driven approaches?
spiqa_529,1803.03467v4,Which model achieves the highest AUC on the MovieLens-1M dataset according to the figure summarizing AUC and Accuracy in CTR prediction in the RippleNet paper?,RippleNet*,1803.03467v4.pdf,"['1803.03467v4.pdf', '1803.04383v2.pdf', '1804.05938v2.pdf', '1811.07073v3.pdf', '1704.08615v2.pdf', '1805.08751v2.pdf', '1804.07849v4.pdf', '1603.00286v5.pdf', '1811.06635v1.pdf']"," The table shows the AUC and ACC values for different models on three datasets. The highest AUC value for the MovieLens-1M dataset is 0.921, which corresponds to the RippleNet* model.",1803.03467v4-Table3-1.png,RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems,"To address the sparsity and cold start problem of collaborative filtering,
researchers usually make use of side information, such as social networks or
item attributes, to improve recommendation performance. This paper considers
the knowledge graph as the source of side information. To address the
limitations of existing embedding-based and path-based methods for
knowledge-graph-aware recommendation, we propose Ripple Network, an end-to-end
framework that naturally incorporates the knowledge graph into recommender
systems. Similar to actual ripples propagating on the surface of water, Ripple
Network stimulates the propagation of user preferences over the set of
knowledge entities by automatically and iteratively extending a user's
potential interests along links in the knowledge graph. The multiple ""ripples""
activated by a user's historically clicked items are thus superposed to form
the preference distribution of the user with respect to a candidate item, which
could be used for predicting the final clicking probability. Through extensive
experiments on real-world datasets, we demonstrate that Ripple Network achieves
substantial gains in a variety of scenarios, including movie, book and news
recommendation, over several state-of-the-art baselines.",The results of AUC and Accuracy in CTR prediction.,Which model performs the best in terms of AUC on the MovieLens-1M dataset?
spiqa_531,1705.10667v4,"Referring to the test error comparisons visualized in Figure (d) of the Conditional Adversarial Domain Adaptation paper, which model achieves the lowest test error among all evaluated models?",CDAN (M),1705.10667v4.pdf,"['1705.10667v4.pdf', '1611.07718v2.pdf', '1706.04269v2.pdf', '1706.00633v4.pdf', '1708.00160v2.pdf', '1705.07384v2.pdf', '1605.07496v3.pdf', '1706.04284v3.pdf', '1809.01989v2.pdf']",Figure (d) shows the test error for different models. CDAN (M) has the lowest test error.,1705.10667v4-Figure2-1.png,Conditional Adversarial Domain Adaptation,"Adversarial learning has been embedded into deep networks to learn
disentangled and transferable representations for domain adaptation. Existing
adversarial domain adaptation methods may not effectively align different
domains of multimodal distributions native in classification problems. In this
paper, we present conditional adversarial domain adaptation, a principled
framework that conditions the adversarial adaptation models on discriminative
information conveyed in the classifier predictions. Conditional domain
adversarial networks (CDANs) are designed with two novel conditioning
strategies: multilinear conditioning that captures the cross-covariance between
feature representations and classifier predictions to improve the
discriminability, and entropy conditioning that controls the uncertainty of
classifier predictions to guarantee the transferability. With theoretical
guarantees and a few lines of codes, the approach has exceeded state-of-the-art
results on five datasets.","Analysis of conditioning strategies, distribution discrepancy, and convergence.",Which model performs the best in terms of test error?
spiqa_532,1708.06832v3,"Based on Figures (a) and (b) in the paper ""Learning Anytime Predictions in Neural Networks via Adaptive Loss Balancing,"" which model achieves the best accuracy and lowest error rate on CIFAR100 and ILSVRC datasets when using AdaLoss compared to static constant weights?",EANN with AdaLoss performs the best on both CIFAR100 and ILSVRC datasets.,1708.06832v3.pdf,"['1708.06832v3.pdf', '1809.00263v5.pdf', '1811.06635v1.pdf', '1704.07121v2.pdf', '1703.07015v3.pdf', '1707.00189v3.pdf', '1805.06431v4.pdf', '1611.07718v2.pdf', '1708.03797v1.pdf', '1805.01216v3.pdf', '1805.02349v2.pdf', '1707.00524v2.pdf', '1705.10667v4.pdf', '1603.03833v4.pdf']","This can be seen in Figure (a) and (b), where EANN with AdaLoss consistently has the lowest error rate compared to other models.",1708.06832v3-Figure5-1.png,Learning Anytime Predictions in Neural Networks via Adaptive Loss Balancing,"This work considers the trade-off between accuracy and test-time
computational cost of deep neural networks (DNNs) via \emph{anytime}
predictions from auxiliary predictions. Specifically, we optimize auxiliary
losses jointly in an \emph{adaptive} weighted sum, where the weights are
inversely proportional to average of each loss. Intuitively, this balances the
losses to have the same scale. We demonstrate theoretical considerations that
motivate this approach from multiple viewpoints, including connecting it to
optimizing the geometric mean of the expectation of each loss, an objective
that ignores the scale of losses. Experimentally, the adaptive weights induce
more competitive anytime predictions on multiple recognition data-sets and
models than non-adaptive approaches including weighing all losses equally. In
particular, anytime neural networks (ANNs) can achieve the same accuracy faster
using adaptive weights on a small network than using static constant weights on
a large one. For problems with high performance saturation, we also show a
sequence of exponentially deepening ANNscan achieve near-optimal anytime
results at any budget, at the cost of a const fraction of extra computation.",(a) EANN performs better if the ANNs use AdaLoss instead of CONST. (b) EANN outperforms linear ensembles of DNNs on ILSVRC. (c) The learned adaptive weights of the same model on three data-sets.,Which model performs the best on CIFAR100 and ILSVRC datasets?
spiqa_533,1705.02798v6,"Based on the figure comparing model performance on adversarial SQuAD datasets in the Reinforced Mnemonic Reader paper, which model achieves the highest F1 score of 67.0 on the AddOneSent dataset?",R.M.-Reader.,1705.02798v6.pdf,"['1705.02798v6.pdf', '1702.03584v3.pdf', '1809.03550v3.pdf', '1704.07121v2.pdf', '1802.07222v1.pdf', '1706.04284v3.pdf', '1605.07496v3.pdf', '1708.01425v4.pdf', '1608.02784v2.pdf', '1811.10673v1.pdf', '1803.04572v2.pdf', '1805.06431v4.pdf', '1805.04609v3.pdf', '1803.03467v4.pdf', '1706.00827v2.pdf']",The table shows the performance of different models on the AddOneSent dataset in terms of EM and F1 score. The R.M.-Reader model has the highest F1 score of 67.0.,1705.02798v6-Table3-1.png,Reinforced Mnemonic Reader for Machine Reading Comprehension,"In this paper, we introduce the Reinforced Mnemonic Reader for machine
reading comprehension tasks, which enhances previous attentive readers in two
aspects. First, a reattention mechanism is proposed to refine current
attentions by directly accessing to past attentions that are temporally
memorized in a multi-round alignment architecture, so as to avoid the problems
of attention redundancy and attention deficiency. Second, a new optimization
approach, called dynamic-critical reinforcement learning, is introduced to
extend the standard supervised method. It always encourages to predict a more
acceptable answer so as to address the convergence suppression problem occurred
in traditional reinforcement learning algorithms. Extensive experiments on the
Stanford Question Answering Dataset (SQuAD) show that our model achieves
state-of-the-art results. Meanwhile, our model outperforms previous systems by
over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD
datasets.","Performance comparison on two adversarial SQuAD datasets. Wang & Jiang[2017]1, Seo et al.[2017]2, Liu et al.[2017a]3, Shen et al.[2016]4 and Huang et al.[2017]5. ∗ indicates ensemble models.",Which model performs the best on the AddOneSent dataset in terms of F1 score?
spiqa_534,1809.02731v3,"According to the figure summarizing the results on unsupervised evaluation tasks in *Exploiting Invertible Decoders for Unsupervised Sentence Representation Learning*, which model achieves the highest score on the STS16 task with unsupervised training?",The Bijective model performs the best on the STS16 task with unsupervised training.,1809.02731v3.pdf,"['1809.02731v3.pdf', '1906.06589v3.pdf', '1803.06506v3.pdf', '1805.04687v2.pdf', '1804.05995v2.pdf', '1703.10730v2.pdf', '1804.01429v3.pdf', '1703.00060v2.pdf']","The table shows the performance of different models on the STS16 task with unsupervised training. The Bijective model has the highest score of 75.8, which indicates that it performs the best.",1809.02731v3-Table3-1.png,Exploiting Invertible Decoders for Unsupervised Sentence Representation Learning,"The encoder-decoder models for unsupervised sentence representation learning
tend to discard the decoder after being trained on a large unlabelled corpus,
since only the encoder is needed to map the input sentence into a vector
representation. However, parameters learnt in the decoder also contain useful
information about language. In order to utilise the decoder after learning, we
present two types of decoding functions whose inverse can be easily derived
without expensive inverse calculation. Therefore, the inverse of the decoding
function serves as another encoder that produces sentence representations. We
show that, with careful design of the decoding functions, the model learns good
sentence representations, and the ensemble of the representations produced from
the encoder and the inverse of the decoder demonstrate even better
generalisation ability and solid transferability.","Results on unsupervised evaluation tasks (Pearson’s r × 100) . Bold numbers are the best results among unsupervised transfer models, and underlined numbers are the best ones among all models. ‘WR’ refers to",Which model performs the best on the STS16 task with unsupervised training?
spiqa_536,1703.07015v3,"In the figure where the true (blue) and predicted (red) time series for the Traffic occupation dataset are compared, with the X-axis representing week days and the forecasting horizon set to 24, which model, VAR or LSTNet, more effectively captures both daily and weekly repeating patterns?",LSTNet,1703.07015v3.pdf,"['1703.07015v3.pdf', '1802.07459v2.pdf', '1705.09966v2.pdf', '1703.00060v2.pdf', '1708.05239v3.pdf', '1705.09296v2.pdf', '1704.08615v2.pdf', '1706.03847v3.pdf', '1611.07718v2.pdf', '1705.08016v3.pdf', '1811.08481v2.pdf', '1704.07854v4.pdf', '1708.02153v2.pdf']","The figure shows that LSTNet is able to predict similar patterns for Fridays and Saturdays, and ones for Sundays and Mondays, while VAR is not. This suggests that LSTNet is better at capturing both daily and weekly repeating patterns in the data.",1703.07015v3-Figure7-1.png,Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks,"Multivariate time series forecasting is an important machine learning problem
across many domains, including predictions of solar plant energy output,
electricity consumption, and traffic jam situation. Temporal data arise in
these real-world applications often involves a mixture of long-term and
short-term patterns, for which traditional approaches such as Autoregressive
models and Gaussian Process may fail. In this paper, we proposed a novel deep
learning framework, namely Long- and Short-term Time-series network (LSTNet),
to address this open challenge. LSTNet uses the Convolution Neural Network
(CNN) and the Recurrent Neural Network (RNN) to extract short-term local
dependency patterns among variables and to discover long-term patterns for time
series trends. Furthermore, we leverage traditional autoregressive model to
tackle the scale insensitive problem of the neural network model. In our
evaluation on real-world data with complex mixtures of repetitive patterns,
LSTNet achieved significant performance improvements over that of several
state-of-the-art baseline methods. All the data and experiment codes are
available online.","The true time series (blue) and the predicted ones (red) by VAR (a) and by LSTNet (b) for one variable in the Traffic occupation dataset. The X axis indicates the week days and the forecasting horizon = 24. VAR inadequately predicts similar patterns for Fridays and Saturdays, and ones for Sundays andMondays, while LSTNet successfully captures both the daily and weekly repeating patterns.","Which model, VAR or LSTNet, is better at capturing both daily and weekly repeating patterns in the data?"
spiqa_537,1802.07351v2,"Based on Table 1 of the *Devon* paper, which configuration shows the fastest processing times for both the forward and backward passes, and what is the exact difference in milliseconds between the ""Without dilation"" configuration and the full model for the backward pass?","The ""Without dilation"" configuration resulted in the fastest processing time for both forward and backward passes. It was approximately 29.43 ms faster than the full model in terms of the backward pass (147.74 ms vs. 177.17 ms).",1802.07351v2.pdf,"['1802.07351v2.pdf', '1805.06431v4.pdf', '1705.02946v3.pdf', '1705.09296v2.pdf', '1811.08257v1.pdf', '1703.07015v3.pdf', '1706.08146v3.pdf', '1703.04887v4.pdf', '1608.02784v2.pdf']","Table 1 shows the runtime (in milliseconds) for different configurations of the Devon model. The ""Without dilation"" configuration has the lowest values in both the ""Forward"" and ""Backward"" columns, indicating the fastest processing time in both directions. The difference between the ""Without dilation"" and ""Full model"" values in the ""Backward"" column (177.17 - 147.74) gives us the improvement in processing time.",1802.07351v2-Table6-1.png,Devon: Deformable Volume Network for Learning Optical Flow,"State-of-the-art neural network models estimate large displacement optical
flow in multi-resolution and use warping to propagate the estimation between
two resolutions. Despite their impressive results, it is known that there are
two problems with the approach. First, the multi-resolution estimation of
optical flow fails in situations where small objects move fast. Second, warping
creates artifacts when occlusion or dis-occlusion happens. In this paper, we
propose a new neural network module, Deformable Cost Volume, which alleviates
the two problems. Based on this module, we designed the Deformable Volume
Network (Devon) which can estimate multi-scale optical flow in a single high
resolution. Experiments show Devon is more suitable in handling small objects
moving fast and achieves comparable results to the state-of-the-art methods in
public benchmarks.",Table 6. Runtime (ms).,"Which modification to the Devon model resulted in the fastest processing time for both forward and backward passes, and how much faster was it compared to the full model in terms of the backward pass? "
spiqa_538,1811.02721v3,"According to Table 8 in the paper ""Performant TCP for Low-Power Wireless Networks,"" which module of TCPlp on TinyOS consumes the most active RAM, and what is the exact memory consumption?","The protocol implementation module consumes the most memory in the active RAM on TinyOS, utilizing 488 bytes.",1811.02721v3.pdf,"['1811.02721v3.pdf', '1704.00774v3.pdf', '1603.03833v4.pdf', '1805.08751v2.pdf', '1812.10735v2.pdf', '1701.03077v10.pdf', '1811.07073v3.pdf', '1804.07707v2.pdf']","Table 2 provides a breakdown of \sys{}'s memory usage on TinyOS, categorized by module and memory type. Looking at the ""RAM (Active)"" row, we can compare the memory consumption of each module. The ""Protocol"" column shows the highest value of 488 bytes, indicating that the protocol implementation module uses the most active RAM compared to the event scheduler and user library modules.",1811.02721v3-Table8-1.png,Performant TCP for Low-Power Wireless Networks,"Low-power and lossy networks (LLNs) enable diverse applications integrating
many resource-constrained embedded devices, often requiring interconnectivity
with existing TCP/IP networks as part of the Internet of Things. But TCP has
received little attention in LLNs due to concerns about its overhead and
performance, leading to LLN-specific protocols that require specialized
gateways for interoperability. We present a systematic study of a well-designed
TCP stack in IEEE 802.15.4-based LLNs, based on the TCP protocol logic in
FreeBSD. Through careful implementation and extensive experiments, we show that
modern low-power sensor platforms are capable of running full-scale TCP and
that TCP, counter to common belief, performs well despite the lossy nature of
LLNs. By carefully studying the interaction between the transport and link
layers, we identify subtle but important modifications to both, achieving TCP
goodput within 25% of an upper bound (5-40x higher than prior results) and
low-power operation commensurate to CoAP in a practical LLN application
scenario. This suggests that a TCP-based transport layer, seamlessly
interoperable with existing TCP/IP networks, is viable and performant in LLNs.","Table 8: Memory usage of TCPlp on TinyOS. Our implementation of TCPlp spans three modules: (1) protocol implementation, (2) event scheduler that injects callbacks into userspace, and (3) userland library.","Which module of TCPlp consumes the most memory in the active RAM on TinyOS, and how much memory does it utilize?"
spiqa_539,1611.07718v2,"Based on the figure comparing classification error rates with DenseNet and other state-of-the-art models, which architecture in the paper achieves the lowest error rate on CIFAR-10?",DMRNet-Wide,1611.07718v2.pdf,"['1611.07718v2.pdf', '1804.05936v2.pdf', '1804.05938v2.pdf', '1708.05239v3.pdf', '1811.08257v1.pdf', '1809.00263v5.pdf', '1708.06832v3.pdf', '1803.05776v2.pdf', '1803.02750v3.pdf', '1811.07073v3.pdf', '1703.02507v3.pdf', '1703.04887v4.pdf', '1804.04410v2.pdf', '1812.00281v3.pdf', '1611.04684v1.pdf']","The table shows the classification error rates for different network architectures on the CIFAR-10 dataset. The network with the lowest error rate is DMRNet-Wide (ours), which has an error rate of 19.00%.",1611.07718v2-Table3-1.png,Deep Convolutional Neural Networks with Merge-and-Run Mappings,"A deep residual network, built by stacking a sequence of residual blocks, is
easy to train, because identity mappings skip residual branches and thus
improve information flow. To further reduce the training difficulty, we present
a simple network architecture, deep merge-and-run neural networks. The novelty
lies in a modularized building block, merge-and-run block, which assembles
residual branches in parallel through a merge-and-run mapping: Average the
inputs of these residual branches (Merge), and add the average to the output of
each residual branch as the input of the subsequent residual branch (Run),
respectively. We show that the merge-and-run mapping is a linear idempotent
function in which the transformation matrix is idempotent, and thus improves
information flow, making training easy. In comparison to residual networks, our
networks enjoy compelling advantages: they contain much shorter paths, and the
width, i.e., the number of channels, is increased. We evaluate the performance
on the standard recognition tasks. Our approach demonstrates consistent
improvements over ResNets with the comparable setup, and achieves competitive
results (e.g., $3.57\%$ testing error on CIFAR-$10$, $19.00\%$ on CIFAR-$100$,
$1.51\%$ on SVHN).","Classification error comparison with state-of-the-arts. The results of DenseNets are based on the networks without bottlenecks. The DMRNet-Wide is the wide version of a DMRNet, 4× wider, i.e., the widths of the threes stages are 64, 128, and 256, respectively.",Which network architecture has the highest accuracy on the CIFAR-10 dataset?
spiqa_540,1611.07718v2,"According to the figure in the Deep Merge-and-Run Networks paper, which network exhibits the shortest average path length when L = 9?",DMRNet,1611.07718v2.pdf,"['1611.07718v2.pdf', '1803.04383v2.pdf', '1706.04284v3.pdf', '1705.07384v2.pdf', '1803.01128v3.pdf', '1804.07931v2.pdf', '1611.02654v2.pdf', '1804.00863v3.pdf', '1804.04410v2.pdf', '1809.01989v2.pdf', '1811.08481v2.pdf', '1809.00458v1.pdf', '1709.08294v3.pdf']","The figure shows the distribution of path lengths for three networks. The average path length for each network is shown in the legend. When L = 9, DMRNet has the shortest average path length (8.0 ± 2.83).",1611.07718v2-Figure3-1.png,Deep Convolutional Neural Networks with Merge-and-Run Mappings,"A deep residual network, built by stacking a sequence of residual blocks, is
easy to train, because identity mappings skip residual branches and thus
improve information flow. To further reduce the training difficulty, we present
a simple network architecture, deep merge-and-run neural networks. The novelty
lies in a modularized building block, merge-and-run block, which assembles
residual branches in parallel through a merge-and-run mapping: Average the
inputs of these residual branches (Merge), and add the average to the output of
each residual branch as the input of the subsequent residual branch (Run),
respectively. We show that the merge-and-run mapping is a linear idempotent
function in which the transformation matrix is idempotent, and thus improves
information flow, making training easy. In comparison to residual networks, our
networks enjoy compelling advantages: they contain much shorter paths, and the
width, i.e., the number of channels, is increased. We evaluate the performance
on the standard recognition tasks. Our approach demonstrates consistent
improvements over ResNets with the comparable setup, and achieves competitive
results (e.g., $3.57\%$ testing error on CIFAR-$10$, $19.00\%$ on CIFAR-$100$,
$1.51\%$ on SVHN).",Comparing the distributions of the path lengths for three networks. Different networks: (avg length ± std). Left: L = 9. Right: L = 24.,Which network has the shortest average path length when L = 9?
spiqa_541,1705.09296v2,"In Figure 1a of the paper ""Neural Models for Documents with Metadata,"" which unshaded node is used to represent the latent variable in the generative model?",The node labeled η represents the latent variable.,1705.09296v2.pdf,"['1705.09296v2.pdf', '1704.04539v2.pdf', '1706.04284v3.pdf', '1805.01216v3.pdf', '1603.00286v5.pdf']","Latent variables are not directly observed, but are inferred from the observed data. In the generative model,  η  is not shaded, indicating that it is not observed.",1705.09296v2-Figure1-1.png,Neural Models for Documents with Metadata,"Most real-world document collections involve various types of metadata, such
as author, source, and date, and yet the most commonly-used approaches to
modeling text corpora ignore this information. While specialized models have
been developed for particular applications, few are widely used in practice, as
customization typically requires derivation of a custom inference algorithm. In
this paper, we build on recent advances in variational inference methods and
propose a general neural framework, based on topic models, to enable flexible
incorporation of metadata and allow for rapid exploration of alternative
models. Our approach achieves strong performance, with a manageable tradeoff
between perplexity, coherence, and sparsity. Finally, we demonstrate the
potential of our framework through an exploration of a corpus of articles about
US immigration.",Figure 1a presents the generative story of our model. Figure 1b illustrates the inference network using the reparametrization trick to perform variational inference on our model. Shaded nodes are observed; double circles indicate deterministic transformations of parent nodes.,Which node in the generative model represents the latent variable?
spiqa_542,1706.00633v4,"Based on the figure comparing Resnet-32's performance under the C&W-wb attack on the MNIST dataset, which loss function achieves a f2(x∗) > 0 ratio of 0.77, significantly surpassing the alternative?",RCE,1706.00633v4.pdf,"['1706.00633v4.pdf', '1809.01989v2.pdf', '1709.02755v5.pdf', '1708.00160v2.pdf', '1803.06506v3.pdf', '1710.05654v2.pdf', '1611.04363v2.pdf']","The table shows that the ratio of f2(x∗) > 0 for RCE is 0.77, while the ratio for CE is 0.01.",1706.00633v4-Table3-1.png,Towards Robust Detection of Adversarial Examples,"Although the recent progress is substantial, deep learning methods can be
vulnerable to the maliciously generated adversarial examples. In this paper, we
present a novel training procedure and a thresholding test strategy, towards
robust detection of adversarial examples. In training, we propose to minimize
the reverse cross-entropy (RCE), which encourages a deep network to learn
latent representations that better distinguish adversarial examples from normal
ones. In testing, we propose to use a thresholding strategy as the detector to
filter out adversarial examples for reliable predictions. Our method is simple
to implement using standard algorithms, with little extra training cost
compared to the common cross-entropy minimization. We apply our method to
defend various attacking methods on the widely used MNIST and CIFAR-10
datasets, and achieve significant improvements on robust predictions under all
the threat models in the adversarial setting.",The ratios of f2(x∗) > 0 and minimal distortions of the adversarial examples crafted by C&W-wb. Model is Resnet-32.,Which objective function resulted in a higher ratio of f2(x∗) > 0 for the MNIST dataset?
spiqa_544,1803.02750v3,"According to the figure illustrating transmission rates under delta-based BP+RR synchronization in tree topology, which algorithm achieves the lowest transmission ratio for GSet?",Op-based GSet.,1803.02750v3.pdf,"['1803.02750v3.pdf', '1704.07121v2.pdf', '1703.02507v3.pdf', '1708.02153v2.pdf', '1603.03833v4.pdf', '1608.02784v2.pdf']",The figure shows that Op-based GSet has the lowest transmission ratio compared to all other algorithms in a tree topology.,1803.02750v3-Figure7-1.png,Efficient Synchronization of State-based CRDTs,"To ensure high availability in large scale distributed systems, Conflict-free
Replicated Data Types (CRDTs) relax consistency by allowing immediate query and
update operations at the local replica, with no need for remote
synchronization. State-based CRDTs synchronize replicas by periodically sending
their full state to other replicas, which can become extremely costly as the
CRDT state grows. Delta-based CRDTs address this problem by producing small
incremental states (deltas) to be used in synchronization instead of the full
state. However, current synchronisation algorithms for Delta-based CRDTs induce
redundant wasteful delta propagation, performing worse than expected, and
surprisingly, no better than State-based. In this paper we: 1) identify two
sources of inefficiency in current synchronization algorithms for delta-based
CRDTs; 2) bring the concept of join decomposition to state-based CRDTs; 3)
exploit join decompositions to obtain optimal deltas and 4) improve the
efficiency of synchronization algorithms; and finally, 5) evaluate the improved
algorithms.",Transmission of GSet and GCounter with respect to delta-based BP+RR – tree and mesh topologies.,Which of the algorithms is most efficient in terms of transmission in a tree topology?
spiqa_545,1605.07496v3,"Based on Figure (a) of the paper, which algorithm demonstrates the lowest expected cost and performs best on the robotic arm joint breakage task, particularly in handling rare events?",ALOQ.,1605.07496v3.pdf,"['1605.07496v3.pdf', '1805.02349v2.pdf', '1706.04269v2.pdf', '1705.09966v2.pdf', '1803.05776v2.pdf', '1804.05936v2.pdf', '1611.05742v3.pdf', '1708.06832v3.pdf', '1611.03780v2.pdf', '1805.04609v3.pdf', '1811.07073v3.pdf']",Figure (a) shows that ALOQ has the lowest expected cost compared to the other algorithms.,1605.07496v3-Figure3-1.png,Alternating Optimisation and Quadrature for Robust Control,"Bayesian optimisation has been successfully applied to a variety of
reinforcement learning problems. However, the traditional approach for learning
optimal policies in simulators does not utilise the opportunity to improve
learning by adjusting certain environment variables: state features that are
unobservable and randomly determined by the environment in a physical setting
but are controllable in a simulator. This paper considers the problem of
finding a robust policy while taking into account the impact of environment
variables. We present Alternating Optimisation and Quadrature (ALOQ), which
uses Bayesian optimisation and Bayesian quadrature to address such settings.
ALOQ is robust to the presence of significant rare events, which may not be
observable under random sampling, but play a substantial role in determining
the optimal policy. Experimental results across different domains show that
ALOQ can learn more efficiently and robustly than existing methods.",Performance and learned configurations on the robotic arm joint breakage task.,Which of the algorithms performs the best on the robotic arm joint breakage task?
spiqa_546,1805.06431v4,"In the figure comparing resulting trajectories of methods trained with mixed demonstrations, which method exhibits the safest behavior by staying closest to the center of the lane and avoiding the oncoming vehicle?",ChoiceNet,1805.06431v4.pdf,"['1805.06431v4.pdf', '1706.04269v2.pdf', '1809.01989v2.pdf', '1611.04684v1.pdf', '1703.02507v3.pdf']",The trajectories of the different methods are shown in the figure. ChoiceNet's trajectory is the one that stays closest to the center of the lane and avoids the oncoming car.,1805.06431v4-Figure8-1.png,Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks,"In this paper, we focus on weakly supervised learning with noisy training
data for both classification and regression problems.We assume that the
training outputs are collected from a mixture of a target and correlated noise
distributions.Our proposed method simultaneously estimates the target
distribution and the quality of each data which is defined as the correlation
between the target and data generating distributions.The cornerstone of the
proposed method is a Cholesky Block that enables modeling dependencies among
mixture distributions in a differentiable manner where we maintain the
distribution over the network weights.We first provide illustrative examples in
both regression and classification tasks to show the effectiveness of the
proposed method.Then, the proposed method is extensively evaluated in a number
of experiments where we show that it constantly shows comparable or superior
performances compared to existing baseline methods in the handling of noisy
data.",Resulting trajectories of compared methods trained with mixed demonstrations. (best viewed in color).,Which of the compared methods is most likely to be the safest?
spiqa_548,1706.00633v4,"Based on the CIFAR-10 classification error rates presented in the figure from ""Towards Robust Detection of Adversarial Examples,"" which algorithm has the lowest error rate when using cross-entropy (CE) for training in the ResNet-32 model?",C&W-hc,1706.00633v4.pdf,"['1706.00633v4.pdf', '1707.01917v2.pdf', '1708.01425v4.pdf', '1704.07854v4.pdf', '1705.02798v6.pdf', '1809.04276v2.pdf', '1811.02721v3.pdf', '1705.09296v2.pdf', '1812.10735v2.pdf', '1704.07121v2.pdf', '1906.10843v1.pdf', '1704.00774v3.pdf', '1804.07707v2.pdf', '1708.00160v2.pdf']",The figure shows the error rates for different algorithms when trained via the CE and RCE. The C&W-hc algorithm has the lowest error rate when trained via the CE.,1706.00633v4-Figure4-1.png,Towards Robust Detection of Adversarial Examples,"Although the recent progress is substantial, deep learning methods can be
vulnerable to the maliciously generated adversarial examples. In this paper, we
present a novel training procedure and a thresholding test strategy, towards
robust detection of adversarial examples. In training, we propose to minimize
the reverse cross-entropy (RCE), which encourages a deep network to learn
latent representations that better distinguish adversarial examples from normal
ones. In testing, we propose to use a thresholding strategy as the detector to
filter out adversarial examples for reliable predictions. Our method is simple
to implement using standard algorithms, with little extra training cost
compared to the common cross-entropy minimization. We apply our method to
defend various attacking methods on the widely used MNIST and CIFAR-10
datasets, and achieve significant improvements on robust predictions under all
the threat models in the adversarial setting.",Classification error rates on CIFAR-10. Two panels separately show the results when the networks are trained via the CE and RCE. The models is Resnet-32.,Which of the following algorithms performs the best when trained via the CE?
spiqa_549,1804.05936v2,"Based on the figure illustrating model performance on the Microsoft 30k dataset under different hyper-parameter settings, which method consistently registered the lowest ERR@10 across all tests?",LambdaMART,1804.05936v2.pdf,"['1804.05936v2.pdf', '1805.04609v3.pdf', '1611.07718v2.pdf', '1803.01128v3.pdf', '1809.03550v3.pdf', '1709.02755v5.pdf', '1803.06506v3.pdf', '1704.04539v2.pdf', '1706.03847v3.pdf', '1707.06320v2.pdf', '1802.07459v2.pdf', '1703.04887v4.pdf', '1709.00139v4.pdf', '1705.10667v4.pdf', '1803.05776v2.pdf']","LambdaMART consistently has the lowest ERR@10 values in all the figures, regardless of the hyper-parameter being tested.",1804.05936v2-Figure4-1.png,Learning a Deep Listwise Context Model for Ranking Refinement,"Learning to rank has been intensively studied and widely applied in
information retrieval. Typically, a global ranking function is learned from a
set of labeled data, which can achieve good performance on average but may be
suboptimal for individual queries by ignoring the fact that relevant documents
for different queries may have different distributions in the feature space.
Inspired by the idea of pseudo relevance feedback where top ranked documents,
which we refer as the \textit{local ranking context}, can provide important
information about the query's characteristics, we propose to use the inherent
feature distributions of the top results to learn a Deep Listwise Context Model
that helps us fine tune the initial ranked list. Specifically, we employ a
recurrent neural network to sequentially encode the top results using their
feature vectors, learn a local context model and use it to re-rank the top
results. There are three merits with our model: (1) Our model can capture the
local ranking context based on the complex interactions between top results
using a deep neural network; (2) Our model can be built upon existing
learning-to-rank methods by directly using their extracted feature vectors; (3)
Our model is trained with an attention-based loss function, which is more
effective and efficient than many existing listwise methods. Experimental
results show that the proposed model can significantly improve the
state-of-the-art learning to rank methods on benchmark retrieval corpora.",The performance of the DLCMs on Microsoft 30k with different hyper-parameters.,Which of the following methods has the best performance?
spiqa_550,1707.01922v5,"Referring to Fig. 2 in the paper ""Zero-Shot Deep Domain Adaptation,"" how does ZDDA simulate the target-domain representation in step 1, and what specific data is used in this simulation?",(a) ZDDA simulates the target-domain representation using the source-domain data.,1707.01922v5.pdf,"['1707.01922v5.pdf', '1803.03467v4.pdf', '1812.06589v2.pdf', '1703.10730v2.pdf', '1906.06589v3.pdf', '1809.01989v2.pdf', '1809.03149v2.pdf', '1803.02750v3.pdf', '1708.05239v3.pdf', '1708.01425v4.pdf', '1812.10735v2.pdf', '1809.04276v2.pdf', '1708.02153v2.pdf', '1705.07164v8.pdf']","The passage states that ""ZDDA simulates the target-domain representation using the source-domain data."" This is shown in step 1 of the figure, where the target CNN is used to simulate the target-domain representation using the source-domain data.",1707.01922v5-Figure2-1.png,Zero-Shot Deep Domain Adaptation,"Domain adaptation is an important tool to transfer knowledge about a task
(e.g. classification) learned in a source domain to a second, or target domain.
Current approaches assume that task-relevant target-domain data is available
during training. We demonstrate how to perform domain adaptation when no such
task-relevant target-domain data is available. To tackle this issue, we propose
zero-shot deep domain adaptation (ZDDA), which uses privileged information from
task-irrelevant dual-domain pairs. ZDDA learns a source-domain representation
which is not only tailored for the task of interest but also close to the
target-domain representation. Therefore, the source-domain task of interest
solution (e.g. a classifier for classification tasks) which is jointly trained
with the source-domain representation can be applicable to both the source and
target representations. Using the MNIST, Fashion-MNIST, NIST, EMNIST, and SUN
RGB-D datasets, we show that ZDDA can perform domain adaptation in
classification tasks without access to task-relevant target-domain training
data. We also extend ZDDA to perform sensor fusion in the SUN RGB-D scene
classification task by simulating task-relevant target-domain representations
with task-relevant source-domain data. To the best of our knowledge, ZDDA is
the first domain adaptation and sensor fusion method which requires no
task-relevant target-domain data. The underlying principle is not particular to
computer vision data, but should be extensible to other domains.","Fig. 2. An overview of the ZDDA training procedure. We use the images from the SUN RGB-D [36] dataset for illustration. ZDDA simulates the target-domain representation using the source-domain data, builds a joint network with the supervision from the source domain, and trains a sensor fusion network. In step 1, we choose to train s1 and fix t, but we can also train t and fix s1 to simulate the target-domain representation. In step 2, t can also be trainable instead of being fixed, but we choose to fix it to make the number of trainable parameters manageable. The details are explained in Sec. 3","Which of the following statements about the training procedure of ZDDA is true?

(a) ZDDA simulates the target-domain representation using the source-domain data. (b) ZDDA builds a joint network with the supervision from the target domain. (c) ZDDA trains a sensor fusion network in step 1. (d) ZDDA trains a sensor fusion network in step 2."
spiqa_551,1705.10667v4,"Referring to the T-SNE visualizations in the figure of the ""Conditional Adversarial Domain Adaptation"" paper, which method among ResNet, DANN, CDAN-f, and CDAN-fg shows the most distinct separation between the two domains A (red) and W (blue)?",CDAN-fg,1705.10667v4.pdf,"['1705.10667v4.pdf', '1605.07496v3.pdf', '1811.10673v1.pdf', '1803.06506v3.pdf', '1707.06320v2.pdf', '1805.06447v3.pdf', '1906.06589v3.pdf']","In the figure, we can see that CDAN-fg produces the clearest separation between the red and blue data points. The other methods show some overlap between the two classes, indicating that they are not as effective at separating them.",1705.10667v4-Figure3-1.png,Conditional Adversarial Domain Adaptation,"Adversarial learning has been embedded into deep networks to learn
disentangled and transferable representations for domain adaptation. Existing
adversarial domain adaptation methods may not effectively align different
domains of multimodal distributions native in classification problems. In this
paper, we present conditional adversarial domain adaptation, a principled
framework that conditions the adversarial adaptation models on discriminative
information conveyed in the classifier predictions. Conditional domain
adversarial networks (CDANs) are designed with two novel conditioning
strategies: multilinear conditioning that captures the cross-covariance between
feature representations and classifier predictions to improve the
discriminability, and entropy conditioning that controls the uncertainty of
classifier predictions to guarantee the transferability. With theoretical
guarantees and a few lines of codes, the approach has exceeded state-of-the-art
results on five datasets.","T-SNE of (a) ResNet, (b) DANN, (c) CDAN-f, (d) CDAN-fg (red: A; blue: W).","Which of the four methods (ResNet, DANN, CDAN-f, CDAN-fg) is most effective at separating the two classes of data points?"
spiqa_552,1805.06431v4,"According to Figure (c) of the paper ""Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks,"" which method achieves the lowest average fitting error across varying outlier rates when fitting the step function?",The proposed method.,1805.06431v4.pdf,"['1805.06431v4.pdf', '1704.07121v2.pdf', '1805.04687v2.pdf', '1606.07384v2.pdf', '1709.08294v3.pdf', '1811.09393v4.pdf', '1703.10730v2.pdf', '1901.00398v2.pdf', '1804.04786v3.pdf', '1804.01429v3.pdf', '1804.05995v2.pdf']","From the plot (c), it can be seen that the proposed method has the lowest average error for all outlier rates compared to the other three methods.",1805.06431v4-Figure6-1.png,Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks,"In this paper, we focus on weakly supervised learning with noisy training
data for both classification and regression problems.We assume that the
training outputs are collected from a mixture of a target and correlated noise
distributions.Our proposed method simultaneously estimates the target
distribution and the quality of each data which is defined as the correlation
between the target and data generating distributions.The cornerstone of the
proposed method is a Cholesky Block that enables modeling dependencies among
mixture distributions in a differentiable manner where we maintain the
distribution over the network weights.We first provide illustrative examples in
both regression and classification tasks to show the effectiveness of the
proposed method.Then, the proposed method is extensively evaluated in a number
of experiments where we show that it constantly shows comparable or superior
performances compared to existing baseline methods in the handling of noisy
data.","(a-c) Average fitting errors while varying the outlier rates and (e-f) fitting results of the compared methods with 60% outliers using cosexp, linear, and step functions.",Which of the four methods has the best performance in terms of average error for the step function?
spiqa_553,1705.09966v2,"Referring specifically to Table 1 in this paper, which image generation method achieved the highest SSIM score, indicating the closest visual similarity to real images in the CelebA dataset?",The Conditional CycleGAN method is expected to produce images most visually similar to the real images.,1705.09966v2.pdf,"['1705.09966v2.pdf', '1706.08146v3.pdf', '1803.05776v2.pdf', '1811.08481v2.pdf', '1707.01917v2.pdf']","Table 1 presents the SSIM scores for different image generation methods on the CelebA test set. SSIM measures the structural similarity between two images, with higher values indicating greater similarity. In this case, the Conditional CycleGAN method achieved the highest SSIM score of 0.92, suggesting that its generated images are most similar to the real images in terms of visual perception.",1705.09966v2-Table1-1.png,Attribute-Guided Face Generation Using Conditional CycleGAN,"We are interested in attribute-guided face generation: given a low-res face
input image, an attribute vector that can be extracted from a high-res image
(attribute image), our new method generates a high-res face image for the
low-res input that satisfies the given attributes. To address this problem, we
condition the CycleGAN and propose conditional CycleGAN, which is designed to
1) handle unpaired training data because the training low/high-res and high-res
attribute images may not necessarily align with each other, and to 2) allow
easy control of the appearance of the generated face via the input attributes.
We demonstrate impressive results on the attribute-guided conditional CycleGAN,
which can synthesize realistic face images with appearance easily controlled by
user-supplied attributes (e.g., gender, makeup, hair color, eyeglasses). Using
the attribute image as identity to produce the corresponding conditional vector
and by incorporating a face verification network, the attribute-guided network
becomes the identity-guided conditional CycleGAN which produces impressive and
interesting results on identity transfer. We demonstrate three applications on
identity-guided conditional CycleGAN: identity-preserving face superresolution,
face swapping, and frontal face generation, which consistently show the
advantage of our new method.",Table 1. SSIM on CelebA test sets.,"Which of the methods among Conditional GAN, Unsupervised GAN and Consitional CycleGAN would you expect to produce images that are most visually similar to the real images in the CelebA dataset?"
spiqa_555,1704.07854v4,"Based on the figure in the paper, which method is the only one capable of accurately reconstructing both arms of the liquid in the first row and the left sheet in the bottom row, fully matching the reference surfaces?",Only the full method with a deformation network is able to produce a perfect reconstruction.,1704.07854v4.pdf,"['1704.07854v4.pdf', '1803.03467v4.pdf', '1705.08016v3.pdf', '1809.01246v1.pdf', '1709.00139v4.pdf', '1705.10667v4.pdf', '1706.00633v4.pdf', '1901.00056v2.pdf', '1703.04887v4.pdf', '1703.00899v2.pdf', '1606.07384v2.pdf', '1708.06832v3.pdf', '1707.06320v2.pdf', '1709.02418v2.pdf']","The figure shows that the PCA reconstruction (purple) and the weighted deformations using a trained parameter network (pink) are not able to reconstruct both arms of liquid in the first row. The reference surfaces (brown) are shown in the background for each version, and it is clear that only the full method with a deformation network is able to match the reference surface.",1704.07854v4-Figure16-1.png,Generating Liquid Simulations with Deformation-aware Neural Networks,"We propose a novel approach for deformation-aware neural networks that learn
the weighting and synthesis of dense volumetric deformation fields. Our method
specifically targets the space-time representation of physical surfaces from
liquid simulations. Liquids exhibit highly complex, non-linear behavior under
changing simulation conditions such as different initial conditions. Our
algorithm captures these complex phenomena in two stages: a first neural
network computes a weighting function for a set of pre-computed deformations,
while a second network directly generates a deformation field for refining the
surface. Key for successful training runs in this setting is a suitable loss
function that encodes the effect of the deformations, and a robust calculation
of the corresponding gradients. To demonstrate the effectiveness of our
approach, we showcase our method with several complex examples of flowing
liquids with topology changes. Our representation makes it possible to rapidly
generate the desired implicit surfaces. We have implemented a mobile
application to demonstrate that real-time interactions with complex liquid
effects are possible with our approach.","Different example surfaces from the 2D parameter space of Fig. 13. From left to right: surfaces reconstructed with PCA (purple), weighted deformations using a trained parameter network (pink), the reference surfaces (brown), and on the far right the output of our full method with a deformation network (teal). Note that none of the other methods is able to reconstruct both arms of liquid in the first row, as well as the left sheet in the bottom row. The reference surfaces are shown in light brown in the background for each version.",Which of the methods is able to reconstruct the shape of the liquid properly?
spiqa_556,1802.07351v2,"In the figure from the FlyingChairs validation set, which optical flow estimation method—Devon, LiteFlowNet, or PWC-Net—provides the most accurate prediction of the fast-moving small object's motion, as indicated by the green arrows, relative to the ground truth?",Devon.,1802.07351v2.pdf,"['1802.07351v2.pdf', '1805.01216v3.pdf', '1803.03467v4.pdf', '1701.03077v10.pdf', '1804.07849v4.pdf', '1803.05776v2.pdf']","The ground truth image shows the actual motion of the small object, which is indicated by the green arrows. Devon's prediction is closest to the ground truth, as it correctly predicts the direction and magnitude of the object's motion. LiteFlowNet and PWC-Net, on the other hand, underestimate the object's motion.",1802.07351v2-Figure8-1.png,Devon: Deformable Volume Network for Learning Optical Flow,"State-of-the-art neural network models estimate large displacement optical
flow in multi-resolution and use warping to propagate the estimation between
two resolutions. Despite their impressive results, it is known that there are
two problems with the approach. First, the multi-resolution estimation of
optical flow fails in situations where small objects move fast. Second, warping
creates artifacts when occlusion or dis-occlusion happens. In this paper, we
propose a new neural network module, Deformable Cost Volume, which alleviates
the two problems. Based on this module, we designed the Deformable Volume
Network (Devon) which can estimate multi-scale optical flow in a single high
resolution. Experiments show Devon is more suitable in handling small objects
moving fast and achieves comparable results to the state-of-the-art methods in
public benchmarks.",FlyingChairs (validation set). Green arrows indicate the small object that moves fast.,"Which of the three methods, LiteFlowNet, PWC-Net, or Devon, most accurately predicts the motion of the small object in the scene?"
spiqa_558,1809.00458v1,"Based on the time vs. F-1 score distribution shown in the figure, which algorithm—GB-KMV or LSH-E—demonstrates consistently faster execution times across different accuracy thresholds?",LSH-E,1809.00458v1.pdf,"['1809.00458v1.pdf', '1708.00160v2.pdf', '1803.05776v2.pdf', '1611.02654v2.pdf', '1705.02946v3.pdf', '1811.08257v1.pdf', '1809.03550v3.pdf', '1702.03584v3.pdf', '1705.07164v8.pdf', '1705.08016v3.pdf', '1705.09296v2.pdf', '1805.04609v3.pdf', '1708.03797v1.pdf']","The figure shows the time taken by each algorithm for different F-1 scores. The LSH-E line is consistently below the other two lines, indicating that it is the fastest on average.",1809.00458v1-Figure14-1.png,GB-KMV: An Augmented KMV Sketch for Approximate Containment Similarity Search,"In this paper, we study the problem of approximate containment similarity
search. Given two records Q and X, the containment similarity between Q and X
with respect to Q is |Q intersect X|/ |Q|. Given a query record Q and a set of
records S, the containment similarity search finds a set of records from S
whose containment similarity regarding Q are not less than the given threshold.
This problem has many important applications in commercial and scientific
fields such as record matching and domain search. Existing solution relies on
the asymmetric LSH method by transforming the containment similarity to
well-studied Jaccard similarity. In this paper, we use a different framework by
transforming the containment similarity to set intersection. We propose a novel
augmented KMV sketch technique, namely GB-KMV, which is data-dependent and can
achieve a good trade-off between the sketch size and the accuracy. We provide a
set of theoretical analysis to underpin the proposed augmented KMV sketch
technique, and show that it outperforms the state-of-the-art technique LSH-E in
terms of estimation accuracy under practical assumption. Our comprehensive
experiments on real-life datasets verify that GB-KMV is superior to LSH-E in
terms of the space-accuracy trade-off, time-accuracy trade-off, and the sketch
construction time. For instance, with similar estimation accuracy (F-1 score),
GB-KMV is over 100 times faster than LSH-E on some real-life dataset.",The distribution of Accuracy,Which of the two algorithms (GB-KMV or LSH-E) is the fastest on average across all datasets?
spiqa_559,1805.06431v4,"Based on the figure illustrating binary classification on corrupt data using both a mixture of densities and a mixture of classifiers, which method is shown to more effectively handle outliers and accurately classify the data points?",Mixture of classifiers.,1805.06431v4.pdf,"['1805.06431v4.pdf', '1605.07496v3.pdf', '1704.07121v2.pdf', '1704.08615v2.pdf', '1702.03584v3.pdf', '1703.10730v2.pdf', '1708.01425v4.pdf', '1705.07164v8.pdf']","The figure shows that the mixture of classifiers approach is able to correctly classify the data points even when there are outliers present. This is because the mixture of classifiers approach is able to learn the different modes of the data distribution, while the density estimation approach is not.",1805.06431v4-Figure1-1.png,Task Agnostic Robust Learning on Corrupt Outputs by Correlation-Guided Mixture Density Networks,"In this paper, we focus on weakly supervised learning with noisy training
data for both classification and regression problems.We assume that the
training outputs are collected from a mixture of a target and correlated noise
distributions.Our proposed method simultaneously estimates the target
distribution and the quality of each data which is defined as the correlation
between the target and data generating distributions.The cornerstone of the
proposed method is a Cholesky Block that enables modeling dependencies among
mixture distributions in a differentiable manner where we maintain the
distribution over the network weights.We first provide illustrative examples in
both regression and classification tasks to show the effectiveness of the
proposed method.Then, the proposed method is extensively evaluated in a number
of experiments where we show that it constantly shows comparable or superior
performances compared to existing baseline methods in the handling of noisy
data.",A process of binary classification on corrupt data using the mixture of (a) densities and (b) classifiers through (4).,"Which of the two approaches, density estimation or mixture of classifiers, is more robust to outliers?"
spiqa_560,1703.07015v3,"Based on the figure in the paper that compares the predicted time series of LSTw/oAR and LST-Skip against the true electricity data on the Electricity dataset with a forecasting horizon of 24, which model demonstrates closer alignment with the actual data?",LST-Skip seems to perform better in predicting electricity consumption.,1703.07015v3.pdf,"['1703.07015v3.pdf', '1706.04269v2.pdf', '1707.01917v2.pdf', '1809.03449v3.pdf', '1705.02946v3.pdf', '1803.03467v4.pdf']",The figure shows the predicted time series (red) by LSTw/oAR (a) and by LST-Skip (b) vs. the true data (blue) on Electricity dataset with horizon = 24. It can be seen that the predicted time series by LST-Skip is closer to the true data than the predicted time series by LSTw/oAR.,1703.07015v3-Figure6-1.png,Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks,"Multivariate time series forecasting is an important machine learning problem
across many domains, including predictions of solar plant energy output,
electricity consumption, and traffic jam situation. Temporal data arise in
these real-world applications often involves a mixture of long-term and
short-term patterns, for which traditional approaches such as Autoregressive
models and Gaussian Process may fail. In this paper, we proposed a novel deep
learning framework, namely Long- and Short-term Time-series network (LSTNet),
to address this open challenge. LSTNet uses the Convolution Neural Network
(CNN) and the Recurrent Neural Network (RNN) to extract short-term local
dependency patterns among variables and to discover long-term patterns for time
series trends. Furthermore, we leverage traditional autoregressive model to
tackle the scale insensitive problem of the neural network model. In our
evaluation on real-world data with complex mixtures of repetitive patterns,
LSTNet achieved significant performance improvements over that of several
state-of-the-art baseline methods. All the data and experiment codes are
available online.",The predicted time series (red) by LSTw/oAR (a) and by LST-Skip (b) vs. the true data (blue) on Electricity dataset with horizon = 24,"Which of the two models, LSTw/oAR or LST-Skip, seems to perform better in predicting electricity consumption?"
spiqa_561,1704.07854v4,"Based on the performance measurements depicted in the figure showing setup details and rendering times for liquid simulations on a Samsung S8, which scene, Staris or Drop, exhibits a longer computation time for rendering?",Staris,1704.07854v4.pdf,"['1704.07854v4.pdf', '1811.10673v1.pdf', '1804.05995v2.pdf', '1704.07121v2.pdf', '1708.03797v1.pdf', '1612.02803v5.pdf', '1611.04684v1.pdf', '1707.00524v2.pdf']","The table shows that the rendering time for Staris is 35ms, while the rendering time for Drop is 21ms.",1704.07854v4-Table1-1.png,Generating Liquid Simulations with Deformation-aware Neural Networks,"We propose a novel approach for deformation-aware neural networks that learn
the weighting and synthesis of dense volumetric deformation fields. Our method
specifically targets the space-time representation of physical surfaces from
liquid simulations. Liquids exhibit highly complex, non-linear behavior under
changing simulation conditions such as different initial conditions. Our
algorithm captures these complex phenomena in two stages: a first neural
network computes a weighting function for a set of pre-computed deformations,
while a second network directly generates a deformation field for refining the
surface. Key for successful training runs in this setting is a suitable loss
function that encodes the effect of the deformations, and a robust calculation
of the corresponding gradients. To demonstrate the effectiveness of our
approach, we showcase our method with several complex examples of flowing
liquids with topology changes. Our representation makes it possible to rapidly
generate the desired implicit surfaces. We have implemented a mobile
application to demonstrate that real-time interactions with complex liquid
effects are possible with our approach.",Performance and setup details of our 4D data sets in the Android app measured on a Samsung S8 device. The ”defo. align” step contains alignment and rescaling of the deformations.,"Which of the two scenes, Drop or Staris, requires more computation time for rendering?"
spiqa_562,1811.08257v1,Which operation in the FALCON performance benchmarks for ReLU and Max Pooling demonstrates the lowest online processing time as shown in the figure?,ReLU,1811.08257v1.pdf,"['1811.08257v1.pdf', '1703.10730v2.pdf', '1611.05742v3.pdf', '1709.02418v2.pdf', '1707.00524v2.pdf', '1709.00139v4.pdf', '1705.02946v3.pdf', '1803.02750v3.pdf', '1704.05426v4.pdf', '1809.03550v3.pdf', '1901.00398v2.pdf', '1706.00633v4.pdf', '1805.08751v2.pdf', '1811.07073v3.pdf']","The online time for ReLU is 4.20 ms, which is the lowest value in the ""online"" column of the table.",1811.08257v1-Table3-1.png,FALCON: A Fourier Transform Based Approach for Fast and Secure Convolutional Neural Network Predictions,"Machine learning as a service has been widely deployed to utilize deep neural
network models to provide prediction services. However, this raises privacy
concerns since clients need to send sensitive information to servers. In this
paper, we focus on the scenario where clients want to classify private images
with a convolutional neural network model hosted in the server, while both
parties keep their data private. We present FALCON, a fast and secure approach
for CNN predictions based on Fourier Transform. Our solution enables linear
layers of a CNN model to be evaluated simply and efficiently with fully
homomorphic encryption. We also introduce the first efficient and
privacy-preserving protocol for softmax function, which is an indispensable
component in CNNs and has not yet been evaluated in previous works due to its
high complexity. We implemented the FALCON and evaluated the performance on
real-world CNN models. The experimental results show that FALCON outperforms
the best known works in both computation and communication cost.",Benchmarks for ReLU and Max Pooling.,Which operation has the lowest online time?
spiqa_563,1705.07384v2,"In the figure depicting the results of Experiment 2, which policy learning method is shown to have the lowest regret value of 0.18, according to the caption stating ""numbers denote regret""?","The DR-SVM method achieved the lowest regret in Ex. 2, with a regret of 0.18.",1705.07384v2.pdf,"['1705.07384v2.pdf', '1809.01246v1.pdf', '1701.06171v4.pdf', '1707.08608v3.pdf', '1809.00263v5.pdf', '1706.04269v2.pdf', '1704.08615v2.pdf', '1804.05936v2.pdf', '1705.10667v4.pdf']","The figure shows the regret for each policy learning method in Ex. 2. The DR-SVM method has the lowest regret, as indicated by the number 0.18 below its corresponding plot.",1705.07384v2-Figure2-1.png,Balanced Policy Evaluation and Learning,"We present a new approach to the problems of evaluating and learning
personalized decision policies from observational data of past contexts,
decisions, and outcomes. Only the outcome of the enacted decision is available
and the historical policy is unknown. These problems arise in personalized
medicine using electronic health records and in internet advertising. Existing
approaches use inverse propensity weighting (or, doubly robust versions) to
make historical outcome (or, residual) data look like it were generated by a
new policy being evaluated or learned. But this relies on a plug-in approach
that rejects data points with a decision that disagrees with the new policy,
leading to high variance estimates and ineffective learning. We propose a new,
balance-based approach that too makes the data look like the new policy but
does so directly by finding weights that optimize for balance between the
weighted data and the target policy in the given, finite sample, which is
equivalent to minimizing worst-case or posterior conditional mean square error.
Our policy learner proceeds as a two-level optimization problem over policies
and weights. We demonstrate that this approach markedly outperforms existing
ones both in evaluation and learning, which is unsurprising given the wider
support of balance-based weights. We establish extensive theoretical
consistency guarantees and regret bounds that support this empirical success.",Policy learning results in Ex. 2; numbers denote regret,Which policy learning method achieved the lowest regret in Ex. 2?
spiqa_564,1605.07496v3,"Among the policies compared in the figure captioned ""Comparison of the performance of ALOQ, MAP, and RQ-ALOQ policies when \(p(\theta)\) must be estimated,"" which policy exhibited the highest average cost?",MAP Policy,1605.07496v3.pdf,"['1605.07496v3.pdf', '1809.01246v1.pdf', '1702.03584v3.pdf', '1704.05958v2.pdf', '1705.09882v2.pdf', '1811.07073v3.pdf']",The table shows the average cost for each policy. The MAP Policy has the highest average cost of 28.76.,1605.07496v3-Table1-1.png,Alternating Optimisation and Quadrature for Robust Control,"Bayesian optimisation has been successfully applied to a variety of
reinforcement learning problems. However, the traditional approach for learning
optimal policies in simulators does not utilise the opportunity to improve
learning by adjusting certain environment variables: state features that are
unobservable and randomly determined by the environment in a physical setting
but are controllable in a simulator. This paper considers the problem of
finding a robust policy while taking into account the impact of environment
variables. We present Alternating Optimisation and Quadrature (ALOQ), which
uses Bayesian optimisation and Bayesian quadrature to address such settings.
ALOQ is robust to the presence of significant rare events, which may not be
observable under random sampling, but play a substantial role in determining
the optimal policy. Experimental results across different domains show that
ALOQ can learn more efficiently and robustly than existing methods.","Comparison of the performance of ALOQ, MAP and RQ-ALOQ policies when p(θ) must be estimated",Which policy resulted in the highest average cost?
spiqa_565,1611.05742v3,"According to the bar graph in Figure (a) displaying results for the AFEW database, which projection pooling method achieves the highest accuracy among S-FRMap, M-FRMap, A-ProjPooling, and W-ProjPooling?",W-ProjPooling,1611.05742v3.pdf,"['1611.05742v3.pdf', '1706.08146v3.pdf', '1803.06506v3.pdf', '1812.06589v2.pdf', '1708.03797v1.pdf', '1803.05776v2.pdf']","The accuracy of each pooling method for each database is shown in the bar graph in part (a) of the figure. The bar for W-ProjPooling for the AFEW database is the highest, indicating that it is the most accurate pooling method for that database.",1611.05742v3-Figure2-1.png,Building Deep Networks on Grassmann Manifolds,"Learning representations on Grassmann manifolds is popular in quite a few
visual recognition tasks. In order to enable deep learning on Grassmann
manifolds, this paper proposes a deep network architecture by generalizing the
Euclidean network paradigm to Grassmann manifolds. In particular, we design
full rank mapping layers to transform input Grassmannian data to more desirable
ones, exploit re-orthonormalization layers to normalize the resulting matrices,
study projection pooling layers to reduce the model complexity in the
Grassmannian context, and devise projection mapping layers to respect
Grassmannian geometry and meanwhile achieve Euclidean forms for regular output
layers. To train the Grassmann networks, we exploit a stochastic gradient
descent setting on manifolds of the connection weights, and study a matrix
generalization of backpropagation to update the structured data. The
evaluations on three visual recognition tasks show that our Grassmann networks
have clear advantages over existing Grassmann learning methods, and achieve
results comparable with state-of-the-art approaches.","(a) Results of using single and multiple FRMap (S-FRMap, M-FRMap), ProjPoolings across or within projections (A-ProjPooling, W-ProjPooling) for the three used databases. (b) (c) Convergence and accuracy curves of SPDNet and the proposed GrNet for the AFEW.",Which pooling method is the most accurate for the AFEW database?
spiqa_566,1811.10673v1,"According to the results depicted in Figure 7 and its accompanying table, which video quality metric exhibits the most significant improvement as the parameter *k* is increased in the proposed GAN-based video compression method?",MS-SSIM,1811.10673v1.pdf,"['1811.10673v1.pdf', '1705.09966v2.pdf', '1703.00899v2.pdf', '1703.00060v2.pdf', '1710.01507v4.pdf', '1705.02798v6.pdf', '1811.02553v4.pdf', '1710.05654v2.pdf', '1709.08294v3.pdf', '1611.04684v1.pdf']",The table shows that the MS-SSIM values increase the most as k is increased.,1811.10673v1-Table2-1.png,Adversarial Video Compression Guided by Soft Edge Detection,"We propose a video compression framework using conditional Generative
Adversarial Networks (GANs). We rely on two encoders: one that deploys a
standard video codec and another which generates low-level maps via a pipeline
of down-sampling, a newly devised soft edge detector, and a novel lossless
compression scheme. For decoding, we use a standard video decoder as well as a
neural network based one, which is trained using a conditional GAN. Recent
""deep"" approaches to video compression require multiple videos to pre-train
generative networks to conduct interpolation. In contrast to this prior work,
our scheme trains a generative decoder on pairs of a very limited number of key
frames taken from a single video and corresponding low-level maps. The trained
decoder produces reconstructed frames relying on a guidance of low-level maps,
without any interpolation. Experiments on a diverse set of 131 videos
demonstrate that our proposed GAN-based compression engine achieves much higher
quality reconstructions at very low bitrates than prevailing standard codecs
such as H.264 or HEVC.","Video quality assessment of reconstructed frames in Figure 7. As k is increased, the quality of the reconstructed frames becomes improve.",Which quality factor improves the most as k is increased?
spiqa_567,1809.01246v1,"Referring to the figure in the ""Fast and Accurate Graph Stream Summarization"" paper illustrating the influence of M/|V| on query accuracy, which query type demonstrates the highest accuracy when M/|V| is small?",Edge query.,1809.01246v1.pdf,"['1809.01246v1.pdf', '1704.05426v4.pdf', '1809.04276v2.pdf', '1804.07707v2.pdf', '1705.08016v3.pdf', '1802.07222v1.pdf', '1703.10730v2.pdf', '1901.00398v2.pdf', '1809.00458v1.pdf', '1901.00056v2.pdf', '1803.04572v2.pdf', '1701.03077v10.pdf', '1805.06447v3.pdf', '1805.08751v2.pdf', '1804.05995v2.pdf']",The figure shows that the edge query has a higher correct rate than the other two query types when M/|V| is small. This can be seen by comparing the heights of the surfaces in the three plots.,1809.01246v1-Figure3-1.png,Fast and Accurate Graph Stream Summarization,"A graph stream is a continuous sequence of data items, in which each item
indicates an edge, including its two endpoints and edge weight. It forms a
dynamic graph that changes with every item in the stream. Graph streams play
important roles in cyber security, social networks, cloud troubleshooting
systems and other fields. Due to the vast volume and high update speed of graph
streams, traditional data structures for graph storage such as the adjacency
matrix and the adjacency list are no longer sufficient. However, prior art of
graph stream summarization, like CM sketches, gSketches, TCM and gMatrix,
either supports limited kinds of queries or suffers from poor accuracy of query
results. In this paper, we propose a novel Graph Stream Sketch (GSS for short)
to summarize the graph streams, which has the linear space cost (O(|E|), E is
the edge set of the graph) and the constant update time complexity (O(1)) and
supports all kinds of queries over graph streams with the controllable errors.
Both theoretical analysis and experiment results confirm the superiority of our
solution with regard to the time/space complexity and query results' precision
compared with the state-of-the-art.",Influence of M on Accuracy,Which query type has the highest accuracy when M/|V| is small?
spiqa_568,1804.05936v2,"Based on the data visualization in Figure 2, which relevance label category of documents benefited the most from the application of the Deep Listwise Context Model in terms of rank promotion over LambdaMART, as measured by the NegPair reduction?",The perfect results received the largest promotions in rank.,1804.05936v2.pdf,"['1804.05936v2.pdf', '1805.02349v2.pdf', '1805.06447v3.pdf', '1706.00827v2.pdf', '1706.04269v2.pdf', '1706.08146v3.pdf', '1705.09296v2.pdf', '1805.00912v4.pdf', '1703.00899v2.pdf', '1703.04887v4.pdf']","Figure 0 shows that the average NegPair reduction for documents with the **perfect** relevance label is the highest among all categories, with a value of 1.88. This indicates that the positions of these documents have been effectively increased by nearly 2 in their ranked lists.",1804.05936v2-Figure2-1.png,Learning a Deep Listwise Context Model for Ranking Refinement,"Learning to rank has been intensively studied and widely applied in
information retrieval. Typically, a global ranking function is learned from a
set of labeled data, which can achieve good performance on average but may be
suboptimal for individual queries by ignoring the fact that relevant documents
for different queries may have different distributions in the feature space.
Inspired by the idea of pseudo relevance feedback where top ranked documents,
which we refer as the \textit{local ranking context}, can provide important
information about the query's characteristics, we propose to use the inherent
feature distributions of the top results to learn a Deep Listwise Context Model
that helps us fine tune the initial ranked list. Specifically, we employ a
recurrent neural network to sequentially encode the top results using their
feature vectors, learn a local context model and use it to re-rank the top
results. There are three merits with our model: (1) Our model can capture the
local ranking context based on the complex interactions between top results
using a deep neural network; (2) Our model can be built upon existing
learning-to-rank methods by directly using their extracted feature vectors; (3)
Our model is trained with an attention-based loss function, which is more
effective and efficient than many existing listwise methods. Experimental
results show that the proposed model can significantly improve the
state-of-the-art learning to rank methods on benchmark retrieval corpora.","Figure 2: The NegPair reduction (NP(d,LambdaMART )- NP(d,DLCM)) on documents with different relevance labels.",Which relevance label category of documents received the most significant rank promotion according to the NegPair reduction metric?
spiqa_569,1704.08615v2,"Referring to Table 1 in this paper, which saliency map method achieved the highest score for the sAUC metric, and how does its performance differ from the other methods listed based on this specific benchmark?",The saliency map method with the highest sAUC score is **SIM**. Its sAUC score appears to be significantly higher than all other methods listed in the table.,1704.08615v2.pdf,"['1704.08615v2.pdf', '1811.06635v1.pdf', '1706.04269v2.pdf', '1704.07854v4.pdf', '1705.08016v3.pdf', '1811.02721v3.pdf']","Table 1 presents the performance of different saliency map methods across various metrics, including sAUC. By examining the sAUC column, we can identify which method achieved the highest score and compare its performance to others. The table shows that SIM has the highest sAUC value, indicating superior performance compared to the other methods in terms of this specific metric.",1704.08615v2-Table3-1.png,"Saliency Benchmarking Made Easy: Separating Models, Maps and Metrics","Dozens of new models on fixation prediction are published every year and
compared on open benchmarks such as MIT300 and LSUN. However, progress in the
field can be difficult to judge because models are compared using a variety of
inconsistent metrics. Here we show that no single saliency map can perform well
under all metrics. Instead, we propose a principled approach to solve the
benchmarking problem by separating the notions of saliency models, maps and
metrics. Inspired by Bayesian decision theory, we define a saliency model to be
a probabilistic model of fixation density prediction and a saliency map to be a
metric-specific prediction derived from the model density which maximizes the
expected performance on that metric given the model density. We derive these
optimal saliency maps for the most commonly used saliency metrics (AUC, sAUC,
NSS, CC, SIM, KL-Div) and show that they can be computed analytically or
approximated with high precision. We show that this leads to consistent
rankings in all metrics and avoids the penalties of using one saliency map for
all metrics. Our method allows researchers to have their model compete on many
different metrics with state-of-the-art in those metrics: ""good"" models will
perform well in all metrics.",Table 3: The raw data plotted in Figure 3,"Which saliency map method achieved the highest score for the sAUC metric, and how does its performance compare to other methods based on this metric? "
spiqa_570,1705.10667v4,"Based on the results in Table 5 for the Office-31 domain adaptation tasks, which variant of CDAN+E achieves the highest average accuracy, and how does it compare to variants utilizing uniform or Gaussian random matrix sampling?",The table shows that CDAN+E (w/o random sampling) achieves the highest average accuracy of 87.7% across all domain adaptation tasks. This is slightly higher than the performance of CDAN+E with uniform sampling (87.0%) and Gaussian sampling (86.4%).,1705.10667v4.pdf,"['1705.10667v4.pdf', '1709.08294v3.pdf', '1706.08146v3.pdf', '1811.07073v3.pdf', '1705.09966v2.pdf', '1805.06447v3.pdf', '1706.00827v2.pdf', '1708.00160v2.pdf', '1803.04572v2.pdf']","The table presents the accuracy of different CDAN variants on various domain adaptation tasks. The ""Avg"" column provides the average accuracy across all tasks. By comparing the values in this column, we can see that not using random sampling leads to the best overall performance. This suggests that relying on pre-defined matrices might be more effective than introducing randomness in this specific context.",1705.10667v4-Table5-1.png,Conditional Adversarial Domain Adaptation,"Adversarial learning has been embedded into deep networks to learn
disentangled and transferable representations for domain adaptation. Existing
adversarial domain adaptation methods may not effectively align different
domains of multimodal distributions native in classification problems. In this
paper, we present conditional adversarial domain adaptation, a principled
framework that conditions the adversarial adaptation models on discriminative
information conveyed in the classifier predictions. Conditional domain
adversarial networks (CDANs) are designed with two novel conditioning
strategies: multilinear conditioning that captures the cross-covariance between
feature representations and classifier predictions to improve the
discriminability, and entropy conditioning that controls the uncertainty of
classifier predictions to guarantee the transferability. With theoretical
guarantees and a few lines of codes, the approach has exceeded state-of-the-art
results on five datasets.",Table 5: Accuracy (%) of CDAN variants on Office-31 for unsupervised domain adaptation (ResNet),Which sampling strategy for random matrices in CDAN+E leads to the highest average accuracy across all domain adaptation tasks on Office-31? How does this compare to the performance of CDAN+E variants that use random sampling?
spiqa_571,1708.01425v4,"As indicated in the figure outlining the dataset size changes throughout the eight-step crowdsourcing process, which specific step resulted in the most significant reduction, where the input instances decreased from 5,119 to 1,955, representing over a 60% drop?","Step 4, Reason disambiguation.",1708.01425v4.pdf,"['1708.01425v4.pdf', '1708.05239v3.pdf', '1705.07164v8.pdf', '1708.02153v2.pdf', '1803.04572v2.pdf', '1803.06506v3.pdf', '1805.04687v2.pdf', '1705.09966v2.pdf', '1704.00774v3.pdf']","The table shows that the input data for Step 4 had a size of 5,119, while the output data had a size of 1,955. This is a decrease of over 60%.",1708.01425v4-Table1-1.png,The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants,"Reasoning is a crucial part of natural language argumentation. To comprehend
an argument, one must analyze its warrant, which explains why its claim follows
from its premises. As arguments are highly contextualized, warrants are usually
presupposed and left implicit. Thus, the comprehension does not only require
language understanding and logic skills, but also depends on common sense. In
this paper we develop a methodology for reconstructing warrants systematically.
We operationalize it in a scalable crowdsourcing process, resulting in a freely
licensed dataset with warrants for 2k authentic arguments from news comments.
On this basis, we present a new challenging task, the argument reasoning
comprehension task. Given an argument with a claim and a premise, the goal is
to choose the correct implicit warrant from two options. Both warrants are
plausible and lexically close, but lead to contradicting claims. A solution to
this task will define a substantial step towards automatic warrant
reconstruction. However, experiments with several neural attention and language
models reveal that current approaches do not suffice.","Details and statistics of the datasets resulting from the eight steps of our methodology implemented in a crowdsourcing process. *Input instances were filtered by their ‘logic score’ assigned in Step 6, such that the weakest 30% were discarded. A more detailed description is available in the readme file of the source code.",Which step in the methodology resulted in the largest decrease in the size of the dataset?
spiqa_572,1803.02750v3,"According to the right plot in the experiment setup discussed in the paper ""Efficient Synchronization of State-based CRDTs"", which synchronization method—state-based, delta-based, or the proposed method—exhibits the lowest CPU processing time?",The proposed method compared to state-based and delta-based methods.,1803.02750v3.pdf,"['1803.02750v3.pdf', '1705.07164v8.pdf', '1803.04572v2.pdf', '1811.08257v1.pdf', '1608.02784v2.pdf', '1605.07496v3.pdf', '1811.02721v3.pdf']",The right plot shows that this paper's method has the lowest CPU processing time ratio compared to the other two methods.,1803.02750v3-Figure1-1.png,Efficient Synchronization of State-based CRDTs,"To ensure high availability in large scale distributed systems, Conflict-free
Replicated Data Types (CRDTs) relax consistency by allowing immediate query and
update operations at the local replica, with no need for remote
synchronization. State-based CRDTs synchronize replicas by periodically sending
their full state to other replicas, which can become extremely costly as the
CRDT state grows. Delta-based CRDTs address this problem by producing small
incremental states (deltas) to be used in synchronization instead of the full
state. However, current synchronisation algorithms for Delta-based CRDTs induce
redundant wasteful delta propagation, performing worse than expected, and
surprisingly, no better than State-based. In this paper we: 1) identify two
sources of inefficiency in current synchronization algorithms for delta-based
CRDTs; 2) bring the concept of join decomposition to state-based CRDTs; 3)
exploit join decompositions to obtain optimal deltas and 4) improve the
efficiency of synchronization algorithms; and finally, 5) evaluate the improved
algorithms.","Experiment setup: 15 nodes in a partial mesh topology replicating an always-growing set. The left plot depicts the number of elements being sent throughout the experiment, while the right plot shows the CPU processing time ratio with respect to state-based. Not only does delta-based synchronization not improve state-based in terms of state transmission, it even incurs a substantial processing overhead.",Which synchronization method is the most efficient in terms of CPU processing time?
spiqa_573,1704.04539v2,"Based on Table 1, in the IT domain, which system (GT or projection-based) achieves the highest full-cycle Smatch score, and by how many points does it outperform the other system?","The GT system achieves the highest full-cycle Smatch score in the IT domain with a score of 59. This is 14 points higher than the projection-based system in the same domain, which scored 45.",1704.04539v2.pdf,"['1704.04539v2.pdf', '1809.03149v2.pdf', '1709.02418v2.pdf', '1811.08481v2.pdf', '1705.08016v3.pdf', '1605.07496v3.pdf', '1811.07073v3.pdf', '1611.04363v2.pdf', '1709.00139v4.pdf', '1710.05654v2.pdf', '1812.06589v2.pdf', '1802.07351v2.pdf', '1705.09296v2.pdf']","The table presents the Smatch scores for different systems across various domains, including IT. The ""Cycle"" column specifies the type of evaluation, with ""full-cycle"" referring to the complete parsing process. By looking at the row corresponding to the IT domain and the ""GT"" system under the ""full-cycle"" column, we can identify its score of 59. Comparing this to the score of 45 for the ""Projection"" system in the same column reveals the performance difference.",1704.04539v2-Table1-1.png,Cross-lingual Abstract Meaning Representation Parsing,"Abstract Meaning Representation (AMR) annotation efforts have mostly focused
on English. In order to train parsers on other languages, we propose a method
based on annotation projection, which involves exploiting annotations in a
source language and a parallel corpus of the source language and a target
language. Using English as the source language, we show promising results for
Italian, Spanish, German and Chinese as target languages. Besides evaluating
the target parsers on non-gold datasets, we further propose an evaluation
method that exploits the English gold annotations and does not require access
to gold annotations for the target languages. This is achieved by inverting the
projection process: a new English parser is learned from the target language
parser and evaluated on the existing English gold standard.","Table 1: Silver, gold and full-cycle Smatch scores for projection-based and MT-based systems.","Which system performs best on the IT domain in terms of full-cycle Smatch score, and how does its performance compare to the projection-based system in the same domain?"
spiqa_574,1608.02784v2,"Referring specifically to Table 3, when human judges rate captions with an average rank of less than 3, how do the average rankings for both CCA and SMT compare, and can any definitive conclusions be drawn from the data about which system performs better for low-quality captions?","It is difficult to definitively say which system performs better for low-quality captions based solely on the provided data. However, we can observe some trends:

1. When the SMT average rank is below 3, the CCA average rank is also lower (1.64 vs. 1.77).
2. When the SMT average rank is 3 or higher, the CCA average rank is significantly higher (3.54 vs. 3.46).

This suggests that CCA might perform relatively better for low-quality captions, but more data and analysis are needed for a conclusive answer.",1608.02784v2.pdf,"['1608.02784v2.pdf', '1906.10843v1.pdf', '1705.09966v2.pdf', '1706.03847v3.pdf', '1809.03149v2.pdf', '1804.00863v3.pdf', '1703.02507v3.pdf']","Table 1 presents the average ranking by human judges for both CCA and SMT systems, categorized by whether the caption has an average rank above or below 3. By comparing the average ranks within each category, we can gain insights into the relative performance of the two systems for different caption quality levels. However, due to the limited data points and potential variations within each category, a definitive conclusion about which system performs better for low-quality captions requires further investigation.",1608.02784v2-Table3-1.png,Canonical Correlation Inference for Mapping Abstract Scenes to Text,"We describe a technique for structured prediction, based on canonical
correlation analysis. Our learning algorithm finds two projections for the
input and the output spaces that aim at projecting a given input and its
correct output into points close to each other. We demonstrate our technique on
a language-vision problem, namely the problem of giving a textual description
to an ""abstract scene"".",Table 3: Average ranking by human judges for cases in which the caption has an average rank of 3 or higher (for both CCA and SMT) and when its average rank is lower than 3. “M” stands for SMT average rank and “C” stands for CCA average rank.,"Which system, CCA or SMT, generally performs better when the caption is of low quality (average rank less than 3)?"
spiqa_575,1805.01216v3,"Referencing the hyperparameters from Table 9 in the ""Disentangling Language and Knowledge in Task-Oriented Dialogs"" paper, which tasks required the highest learning rate, and how does this value compare specifically to the learning rate used for training BoSsNet on the CamRest dataset?","Task T1 and T2 required the highest learning rate of 0.001. This is twice the learning rate used for CamRest, which was trained with a learning rate of 0.0005.",1805.01216v3.pdf,"['1805.01216v3.pdf', '1701.03077v10.pdf', '1812.00281v3.pdf', '1704.08615v2.pdf', '1709.00139v4.pdf', '1705.02946v3.pdf', '1707.01922v5.pdf', '1705.08016v3.pdf', '1811.09393v4.pdf', '1701.06171v4.pdf', '1708.00160v2.pdf', '1706.04284v3.pdf', '1707.00524v2.pdf', '1611.07718v2.pdf', '1704.05426v4.pdf']","The table shows the hyperparameters used for training \sys\ on different datasets. The ""Learning Rate"" column directly provides the information needed to answer the question. By comparing the values in this column for different tasks, we can determine which task required the highest learning rate and how it compares to the learning rate used for CamRest.",1805.01216v3-Table9-1.png,Disentangling Language and Knowledge in Task-Oriented Dialogs,"The Knowledge Base (KB) used for real-world applications, such as booking a
movie or restaurant reservation, keeps changing over time. End-to-end neural
networks trained for these task-oriented dialogs are expected to be immune to
any changes in the KB. However, existing approaches breakdown when asked to
handle such changes. We propose an encoder-decoder architecture (BoSsNet) with
a novel Bag-of-Sequences (BoSs) memory, which facilitates the disentangled
learning of the response's language model and its knowledge incorporation.
Consequently, the KB can be modified with new knowledge without a drop in
interpretability. We find that BoSsNet outperforms state-of-the-art models,
with considerable improvements (> 10\%) on bAbI OOV test sets and other
human-human datasets. We also systematically modify existing datasets to
measure disentanglement and show BoSsNet to be robust to KB modifications.",Table 9: The hyperparameters used to train BOSSNET on the different datasets .,Which task required the highest learning rate and how does this compare to the learning rate used for CamRest?
spiqa_577,1811.02721v3,"According to Table 1 in *Performant TCP for Low-Power Wireless Networks*, which technique addressing the *Resource Constraints* challenge provided the greatest overall memory reduction across both send and receive buffers for low-power sensor platforms?","The ""Resource Constraints"" challenge was addressed with two techniques: ""Zero-Copy Send"" and ""In-Place Reass."" The first led to a 50% reduction in send buffer memory usage, while the second achieved a 38% reduction in receive buffer memory. Therefore, Zero-Copy Send was slightly more effective in reducing overall memory consumption.",1811.02721v3.pdf,"['1811.02721v3.pdf', '1804.04410v2.pdf', '1612.02803v5.pdf', '1805.01216v3.pdf', '1809.01246v1.pdf', '1709.02418v2.pdf', '1708.01425v4.pdf', '1605.07496v3.pdf']","The table explicitly lists the observed improvements for each technique applied to specific challenges. By comparing the percentage reductions in memory usage for both send and receive buffers, we can determine which technique had a greater overall impact on memory consumption.",1811.02721v3-Table1-1.png,Performant TCP for Low-Power Wireless Networks,"Low-power and lossy networks (LLNs) enable diverse applications integrating
many resource-constrained embedded devices, often requiring interconnectivity
with existing TCP/IP networks as part of the Internet of Things. But TCP has
received little attention in LLNs due to concerns about its overhead and
performance, leading to LLN-specific protocols that require specialized
gateways for interoperability. We present a systematic study of a well-designed
TCP stack in IEEE 802.15.4-based LLNs, based on the TCP protocol logic in
FreeBSD. Through careful implementation and extensive experiments, we show that
modern low-power sensor platforms are capable of running full-scale TCP and
that TCP, counter to common belief, performs well despite the lossy nature of
LLNs. By carefully studying the interaction between the transport and link
layers, we identify subtle but important modifications to both, achieving TCP
goodput within 25% of an upper bound (5-40x higher than prior results) and
low-power operation commensurate to CoAP in a practical LLN application
scenario. This suggests that a TCP-based transport layer, seamlessly
interoperable with existing TCP/IP networks, is viable and performant in LLNs.",Table 1: Impact of techniques to run full-scale TCP in LLNs,Which technique was most effective at reducing memory consumption in both send and receive buffers?
spiqa_578,1705.09296v2,"Based on the internal coherence values displayed in the figure for topics generated by the SCHOLAR model on the 20 newsgroups dataset, which topic ranks the highest in internal coherence?","The topic with the highest internal coherence value is ""turks armenian armenia turkish roads escape soviet muslim mountain soul"".",1705.09296v2.pdf,"['1705.09296v2.pdf', '1705.10667v4.pdf', '1606.07384v2.pdf', '1704.08615v2.pdf', '1710.01507v4.pdf', '1706.08146v3.pdf', '1805.02349v2.pdf', '1811.08481v2.pdf', '1701.03077v10.pdf', '1704.05958v2.pdf', '1706.00827v2.pdf']",The table shows the internal coherence values for each topic. The topic with the highest value is listed at the top of the table.,1705.09296v2-Table6-1.png,Neural Models for Documents with Metadata,"Most real-world document collections involve various types of metadata, such
as author, source, and date, and yet the most commonly-used approaches to
modeling text corpora ignore this information. While specialized models have
been developed for particular applications, few are widely used in practice, as
customization typically requires derivation of a custom inference algorithm. In
this paper, we build on recent advances in variational inference methods and
propose a general neural framework, based on topic models, to enable flexible
incorporation of metadata and allow for rapid exploration of alternative
models. Our approach achieves strong performance, with a manageable tradeoff
between perplexity, coherence, and sparsity. Finally, we demonstrate the
potential of our framework through an exploration of a corpus of articles about
US immigration.","Topics from the unsupervised SCHOLAR on the 20 newsgroups dataset, and the corresponding internal coherence values.",Which topic has the highest internal coherence value?
spiqa_579,1705.09296v2,"According to Figure 2, which topic—represented by terms such as ""english language city spanish community""—has the highest probability of being associated with a pro-immigration tone in the joint model of words and tone?","""english language city spanish community""",1705.09296v2.pdf,"['1705.09296v2.pdf', '1812.00108v4.pdf', '1804.05938v2.pdf', '1705.02946v3.pdf', '1804.00863v3.pdf', '1809.04276v2.pdf', '1809.02731v3.pdf', '1703.04887v4.pdf', '1805.08751v2.pdf', '1605.07496v3.pdf', '1706.00633v4.pdf', '1703.07015v3.pdf', '1705.10667v4.pdf', '1805.02349v2.pdf']"," The figure shows the probability of pro-immigration tone for each topic, and ""english language city spanish community"" has the highest probability (close to 1). This suggests that articles that discuss topics related to English language, city, Spanish community are more likely to have a pro-immigration stance.",1705.09296v2-Figure2-1.png,Neural Models for Documents with Metadata,"Most real-world document collections involve various types of metadata, such
as author, source, and date, and yet the most commonly-used approaches to
modeling text corpora ignore this information. While specialized models have
been developed for particular applications, few are widely used in practice, as
customization typically requires derivation of a custom inference algorithm. In
this paper, we build on recent advances in variational inference methods and
propose a general neural framework, based on topic models, to enable flexible
incorporation of metadata and allow for rapid exploration of alternative
models. Our approach achieves strong performance, with a manageable tradeoff
between perplexity, coherence, and sparsity. Finally, we demonstrate the
potential of our framework through an exploration of a corpus of articles about
US immigration.","Figure 2: Topics inferred by a joint model of words and tone, and the corresponding probability of proimmigration tone for each topic. A topic is represented by the top words sorted by word probability throughout the paper.",Which topic is most likely to be associated with a pro-immigration stance?
spiqa_580,1803.02750v3,"Based on the figure comparing transmission rates for tree and mesh topologies in the ""Efficient Synchronization of State-based CRDTs"" paper, which topology exhibits a higher transmission rate when GMap is at 100% utilization?",Mesh,1803.02750v3.pdf,"['1803.02750v3.pdf', '1805.06447v3.pdf', '1709.02755v5.pdf', '1701.03077v10.pdf', '1804.05936v2.pdf', '1809.03550v3.pdf', '1804.07707v2.pdf', '1708.05239v3.pdf', '1812.00281v3.pdf']",The figure shows that the transmission rate for GMap 100% is higher for the mesh topology than for the tree topology. This can be seen by comparing the curves for the two topologies in the top right panel of the figure.,1803.02750v3-Figure8-1.png,Efficient Synchronization of State-based CRDTs,"To ensure high availability in large scale distributed systems, Conflict-free
Replicated Data Types (CRDTs) relax consistency by allowing immediate query and
update operations at the local replica, with no need for remote
synchronization. State-based CRDTs synchronize replicas by periodically sending
their full state to other replicas, which can become extremely costly as the
CRDT state grows. Delta-based CRDTs address this problem by producing small
incremental states (deltas) to be used in synchronization instead of the full
state. However, current synchronisation algorithms for Delta-based CRDTs induce
redundant wasteful delta propagation, performing worse than expected, and
surprisingly, no better than State-based. In this paper we: 1) identify two
sources of inefficiency in current synchronization algorithms for delta-based
CRDTs; 2) bring the concept of join decomposition to state-based CRDTs; 3)
exploit join decompositions to obtain optimal deltas and 4) improve the
efficiency of synchronization algorithms; and finally, 5) evaluate the improved
algorithms.","Transmission of GMap 10%, 30%, 60% and 100% – tree and mesh topologies.",Which topology has the highest transmission rate for GMap 100%?
spiqa_581,1805.04687v2,"Referencing Table 9 in the BDD100K paper, which specific training approach achieved the optimal balance between minimizing false negatives (FN) and false positives (FP), while also achieving the highest MOTSA score in the multi-object tracking and segmentation task?","The training approach ""Det + T + I + S"" achieved the best balance between minimizing false negatives (FN) and false positives (FP) in object detection, while also maintaining a high MOTSA score.",1805.04687v2.pdf,"['1805.04687v2.pdf', '1804.00863v3.pdf', '1804.04786v3.pdf', '1705.02798v6.pdf', '1804.07931v2.pdf', '1811.07073v3.pdf', '1705.07164v8.pdf']","The table shows that ""Det + T + I + S"" achieved the lowest FN count (5132) while having a relatively low FP count (6228) compared to other approaches. Although ""MOT (T) + MOTS"" has a slightly lower FP count, its FN count is significantly higher. Additionally, ""Det + T + I + S"" achieved the highest MOTSA score (41.4), indicating good overall performance in multi-object tracking and segmentation. Therefore, considering both FN, FP, and MOTSA, ""Det + T + I + S"" demonstrates the best balance.",1805.04687v2-Table9-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.","Table 9: MOTS evaluation results. Both instance segmentation AP and MOTS evaluation metrics are reported. Instance segmentation tracking is very hard to label, but we are able to use object detection, tracking, and instance segmentation to improve segmentation tracking accuracy significantly.","Which training approach achieved the best balance between minimizing false negatives (FN) and false positives (FP) in object detection, while also maintaining a high MOTSA score?"
spiqa_582,1812.00281v3,"According to Table 1 in the HUMBI paper, which combination of training datasets resulted in the lowest prediction error for both the UP-3D and HUMBI test sets in cross-data evaluation?",Training with UP-3D + HUMBI resulted in the lowest prediction error for both UP-3D and HUMBI test sets.,1812.00281v3.pdf,"['1812.00281v3.pdf', '1805.07567v2.pdf', '1707.00524v2.pdf', '1805.06447v3.pdf', '1709.02418v2.pdf', '1803.03467v4.pdf']","Table 1 shows the mean error of 3D body mesh prediction for different combinations of training and testing data. For both UP-3D and HUMBI test sets, the lowest error values (**18.4±13.8** and **12.5±8.4** respectively) are observed when the model is trained with **UP-3D + HUMBI**. This suggests that combining both datasets during training leads to better generalization and lower prediction errors compared to using either dataset alone.",1812.00281v3-Table8-1.png,HUMBI: A Large Multiview Dataset of Human Body Expressions,"This paper presents a new large multiview dataset called HUMBI for human body
expressions with natural clothing. The goal of HUMBI is to facilitate modeling
view-specific appearance and geometry of gaze, face, hand, body, and garment
from assorted people. 107 synchronized HD cameras are used to capture 772
distinctive subjects across gender, ethnicity, age, and physical condition.
With the multiview image streams, we reconstruct high fidelity body expressions
using 3D mesh models, which allows representing view-specific appearance using
their canonical atlas. We demonstrate that HUMBI is highly effective in
learning and reconstructing a complete human model and is complementary to the
existing datasets of human body expressions with limited views and subjects
such as MPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets.",Table 8: The mean error of 3D body mesh prediction for cross-data evaluation (unit: pixel).,Which training data configuration resulted in the lowest prediction error for both UP-3D and HUMBI test sets?
spiqa_583,1704.04539v2,"Referring to Table 2 in the *Cross-lingual Abstract Meaning Representation Parsing* paper, which translation system between Moses and Nematus achieves the highest BLEU scores, and how does its performance compare against Google Translate (GT)? Is the comparison unbiased, considering the differences in training data sizes mentioned in the study?","According to Table 2, Moses achieves the highest BLEU scores among the listed translation systems (Moses, Nematus) across all language pairs. However, its performance still falls behind Google Translate (GT) in every case.

The comparison with GT might not be entirely fair because, as GT has the advantage of being trained on a significantly larger dataset. This suggests that GT's performance advantage might be partially due to its training data rather than solely its inherent capabilities.",1704.04539v2.pdf,"['1704.04539v2.pdf', '1804.07849v4.pdf', '1710.01507v4.pdf', '1812.06589v2.pdf', '1803.05776v2.pdf', '1705.09296v2.pdf', '1708.01425v4.pdf', '1811.09393v4.pdf', '1811.08481v2.pdf', '1804.04786v3.pdf', '1809.00458v1.pdf', '1901.00398v2.pdf', '1710.05654v2.pdf']"," Question: 

Based on Table 2 and the passage, which translation system performs the best and how does its performance compare to Google Translate (GT)? Is the comparison with GT completely fair? Explain your answer. 

## Answer: 

According to Table 2, Moses achieves the highest BLEU scores among the listed translation systems (Moses, Nematus) across all language pairs. However, its performance still falls behind Google Translate (GT) in every case. 

The comparison with GT might not be entirely fair because, as mentioned in the passage, GT has the advantage of being trained on a significantly larger dataset. This suggests that GT's performance advantage might be partially due to its training data rather than solely its inherent capabilities.",1704.04539v2-Table2-1.png,Cross-lingual Abstract Meaning Representation Parsing,"Abstract Meaning Representation (AMR) annotation efforts have mostly focused
on English. In order to train parsers on other languages, we propose a method
based on annotation projection, which involves exploiting annotations in a
source language and a parallel corpus of the source language and a target
language. Using English as the source language, we show promising results for
Italian, Spanish, German and Chinese as target languages. Besides evaluating
the target parsers on non-gold datasets, we further propose an evaluation
method that exploits the English gold annotations and does not require access
to gold annotations for the target languages. This is achieved by inverting the
projection process: a new English parser is learned from the target language
parser and evaluated on the existing English gold standard.","Table 2: BLEU scores for Moses, Nematus and Google Translate (GT) on the (out-of-domain) LDC2015E86 test set",Which translation system performs the best and how does its performance compare to Google Translate (GT)? Is the comparison with GT completely fair? Explain your answer.
spiqa_584,1706.00633v4,"In reference to the figure comparing adversarial examples generated by Resnet-32 (CE) and Resnet-32 (RCE) on the MNIST and CIFAR-10 datasets, which method produces images that are visually more similar to the original test images, based on the observed distortions in CE versus RCE examples?",The CE method results in images that are visually more similar to the original images than the RCE method.,1706.00633v4.pdf,"['1706.00633v4.pdf', '1702.08694v3.pdf', '1611.05742v3.pdf', '1705.07384v2.pdf', '1811.02721v3.pdf', '1805.06431v4.pdf', '1708.03797v1.pdf', '1803.06506v3.pdf', '1707.01922v5.pdf', '1803.01128v3.pdf', '1710.06177v2.pdf', '1606.07384v2.pdf']"," The figure shows that the CE images have less distortion than the RCE images. This is evident in the MNIST and CIFAR-10 datasets, where the CE images are visually very similar to the original images, while the RCE images have more noticeable distortions.",1706.00633v4-Figure3-1.png,Towards Robust Detection of Adversarial Examples,"Although the recent progress is substantial, deep learning methods can be
vulnerable to the maliciously generated adversarial examples. In this paper, we
present a novel training procedure and a thresholding test strategy, towards
robust detection of adversarial examples. In training, we propose to minimize
the reverse cross-entropy (RCE), which encourages a deep network to learn
latent representations that better distinguish adversarial examples from normal
ones. In testing, we propose to use a thresholding strategy as the detector to
filter out adversarial examples for reliable predictions. Our method is simple
to implement using standard algorithms, with little extra training cost
compared to the common cross-entropy minimization. We apply our method to
defend various attacking methods on the widely used MNIST and CIFAR-10
datasets, and achieve significant improvements on robust predictions under all
the threat models in the adversarial setting.","The normal test images are termed as Normal, and adversarial examples generated on Resnet-32 (CE) and Resnet-32 (RCE) are separately termed as CE / RCE. Adversarial examples are generated by C&W-wb with minimal distortions.",Which type of adversarial example generation method results in images that are visually more similar to the original images?
spiqa_585,1901.00398v2,"In the context of the ""Judge the Judges"" study, as shown in Table 4, which specific generators produced reviews that were the hardest for meta-adversarial evaluators to distinguish from human-written ones, achieving the lowest accuracy?",MLE SeqGAN and Word LSTM with temperature 1.0.,1901.00398v2.pdf,"['1901.00398v2.pdf', '1802.07351v2.pdf', '1705.10667v4.pdf', '1709.00139v4.pdf', '1704.07854v4.pdf', '1709.08294v3.pdf', '1611.03780v2.pdf', '1707.01922v5.pdf']","The table shows the accuracy of meta-adversarial evaluators in identifying machine-generated reviews. Lower accuracy indicates that the reviews are more difficult to distinguish from human-written ones. We can see that MLE SeqGAN and Word LSTM with temperature 1.0 consistently achieve the lowest accuracy across most evaluators, implying that these generators produced reviews that were most similar to human-written ones and thus hardest to detect as machine-generated.",1901.00398v2-Table4-1.png,Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation,"We conduct a large-scale, systematic study to evaluate the existing
evaluation methods for natural language generation in the context of generating
online product reviews. We compare human-based evaluators with a variety of
automated evaluation procedures, including discriminative evaluators that
measure how well machine-generated text can be distinguished from human-written
text, as well as word overlap metrics that assess how similar the generated
text compares to human-written references. We determine to what extent these
different evaluators agree on the ranking of a dozen of state-of-the-art
generators for online product reviews. We find that human evaluators do not
correlate well with discriminative evaluators, leaving a bigger question of
whether adversarial accuracy is the correct objective for natural language
generation. In general, distinguishing machine-generated text is challenging
even for human evaluators, and human decisions correlate better with lexical
overlaps. We find lexical diversity an intriguing metric that is indicative of
the assessments of different evaluators. A post-experiment survey of
participants provides insights into how to evaluate and improve the quality of
natural language generation systems.","Table 4: Accuracy of deep (LSTM, CNN, CNN & LSTM) and shallow (SVM, RF, NB, XGBoost) meta-adversarial evaluators. The lower the better. Meta-adversarial evaluators do better than humans on individual reviews, with less bias between the two classes. GAN-based generators are considered best by meta-adversarial evaluators.",Which type of generator generally produced reviews that were most easily identified as machine-generated by the meta-adversarial evaluators?
spiqa_586,1707.06320v2,"Based on the results presented in Table 1, which visually grounded model achieves the highest accuracy on the MRPC task, and by how much does its performance surpass the baseline ST-LN model in terms of accuracy?","GroundSent-Cap appears to be most beneficial for the MRPC task, achieving an accuracy of 72.9/82.2 compared to the baseline model ST-LN's 69.6/81.2.",1707.06320v2.pdf,"['1707.06320v2.pdf', '1804.04786v3.pdf', '1811.09393v4.pdf', '1809.03149v2.pdf', '1812.10735v2.pdf', '1703.00899v2.pdf', '1804.07849v4.pdf', '1707.00189v3.pdf', '1611.02654v2.pdf', '1809.04276v2.pdf', '1703.04887v4.pdf', '1802.07459v2.pdf']","Table 1 presents the accuracy results for different models on various semantic classification and entailment tasks. The MRPC task results are shown in the fifth column. Comparing the scores of different models on this task, we can see that GroundSent-Cap outperforms the others, including the baseline model ST-LN, by a small margin. This suggests that grounding with captions is particularly helpful for the MRPC task.",1707.06320v2-Table2-1.png,Learning Visually Grounded Sentence Representations,"We introduce a variety of models, trained on a supervised image captioning
corpus to predict the image features for a given caption, to perform sentence
representation grounding. We train a grounded sentence encoder that achieves
good performance on COCO caption and image retrieval and subsequently show that
this encoder can successfully be transferred to various NLP tasks, with
improved performance over text-only models. Lastly, we analyze the contribution
of grounding, and show that word embeddings learned by this system outperform
non-grounded ones.",Table 2: Accuracy results on sentence classification and entailment tasks.,"Which type of grounding appears to be most beneficial for the MRPC task, and how does its performance compare to the baseline model (ST-LN)?"
spiqa_587,1805.04687v2,"Based on the class distribution presented in Figure 14 of the BDD100K paper for semantic instance segmentation, which object type appears most frequently in the dataset?",Cars are the most common object in the dataset.,1805.04687v2.pdf,"['1805.04687v2.pdf', '1804.05938v2.pdf', '1812.10735v2.pdf', '1703.02507v3.pdf', '1703.10730v2.pdf', '1802.07351v2.pdf', '1809.00458v1.pdf', '1710.01507v4.pdf', '1804.04786v3.pdf', '1812.00281v3.pdf']","The figure shows that there are almost 60 thousand car instances, which is more than any other type of object.",1805.04687v2-Figure14-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.","Figure 14: Distribution of classes in semantic instance segmentation. It presents a long-tail effect with more than 10 cars and poles per image, but only tens of trains in the whole dataset.",Which type of object is the most common in the dataset?
spiqa_588,1704.05426v4,"Referring to the figure comparing MultiNLI and SNLI for word-related phenomena, which Penn Treebank-style tag exhibits the largest difference in frequency, with a discrepancy of 26 in the ""Diff."" column?",Negation (PTB),1704.05426v4.pdf,"['1704.05426v4.pdf', '1703.07015v3.pdf', '1805.01216v3.pdf', '1707.00524v2.pdf', '1804.07931v2.pdf', '1804.00863v3.pdf', '1709.02755v5.pdf', '1809.01989v2.pdf', '1809.01246v1.pdf', '1809.00458v1.pdf', '1809.02731v3.pdf', '1802.07459v2.pdf', '1708.00160v2.pdf']","The table shows the difference in frequency of occurrence between MultiNLI and SNLI for each type of word. The ""Diff."" column shows that negation has the greatest difference, with a difference of 26.",1704.05426v4-Table5-1.png,A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference,"This paper introduces the Multi-Genre Natural Language Inference (MultiNLI)
corpus, a dataset designed for use in the development and evaluation of machine
learning models for sentence understanding. In addition to being one of the
largest corpora available for the task of NLI, at 433k examples, this corpus
improves upon available resources in its coverage: it offers data from ten
distinct genres of written and spoken English--making it possible to evaluate
systems on nearly the full complexity of the language--and it offers an
explicit setting for the evaluation of cross-genre domain adaptation.","Dev. Freq. is the percentage of dev. set examples that include each phenomenon, ordered by greatest difference in frequency of occurrence (Diff.) between MultiNLI and SNLI. Most Frequent Label specifies which label is the most frequent for each tag in the MultiNLI dev. set, and % is its incidence. Model Acc. is the dev. set accuracy (%) by annotation tag for each baseline model (trained on MultiNLI only). (PTB) marks a tag as derived from Penn Treebank-style parser output tags (Marcus et al., 1993).",Which type of word has the greatest difference in frequency of occurrence between MultiNLI and SNLI?
spiqa_589,1805.04687v2,"In the BDD100K paper, as depicted in Figure 4, which weather condition yields the highest classification accuracy for the image tagging task using the DLA-34 model?",Clear weather.,1805.04687v2.pdf,"['1805.04687v2.pdf', '1705.02946v3.pdf', '1703.00060v2.pdf', '1707.01917v2.pdf', '1809.03550v3.pdf']","The figure shows the classification accuracy for different weather conditions. The bar for clear weather is the highest, indicating that it has the highest accuracy.",1805.04687v2-Figure4-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.",Figure 4: Image tagging classification results using DLA-34.,Which weather condition has the highest classification accuracy?
spiqa_590,1705.09296v2,"According to Figure 3's learned embeddings of publication years in the neural model for US immigration news articles, which year is most closely associated with the terms ""sept"", ""hijackers"", and ""attacks""?",2001,1705.09296v2.pdf,"['1705.09296v2.pdf', '1803.04572v2.pdf', '1708.05239v3.pdf', '1710.05654v2.pdf']","The figure shows that the year 2001 is located highest on the vertical axis, which is associated with the terms ""sept"", ""hijackers"", and ""attacks"". This suggests that these terms are most prevalent in articles published in 2001.",1705.09296v2-Figure3-1.png,Neural Models for Documents with Metadata,"Most real-world document collections involve various types of metadata, such
as author, source, and date, and yet the most commonly-used approaches to
modeling text corpora ignore this information. While specialized models have
been developed for particular applications, few are widely used in practice, as
customization typically requires derivation of a custom inference algorithm. In
this paper, we build on recent advances in variational inference methods and
propose a general neural framework, based on topic models, to enable flexible
incorporation of metadata and allow for rapid exploration of alternative
models. Our approach achieves strong performance, with a manageable tradeoff
between perplexity, coherence, and sparsity. Finally, we demonstrate the
potential of our framework through an exploration of a corpus of articles about
US immigration.",Figure 3: Learned embeddings of year-ofpublication (treated as a covariate) from combined model of news articles about immigration.,"Which year is most associated with the terms ""sept"", ""hijackers"", and ""attacks""?"
spiqa_591,1805.04687v2,"Based on the data presented in Table 3, why do MOTS datasets like KITTI MOTS and MOTS Challenge require significantly denser annotations per frame than YouTube VOS, despite having fewer frames and sequences, and how does this correlate with the multitask learning objectives in autonomous driving?","MOTS datasets require denser annotations per frame because they involve both segmentation and tracking of multiple objects in crowded scenes. This results in smaller dataset sizes compared to VOS datasets, which typically focus on segmenting a single or fewer objects. ",1805.04687v2.pdf,"['1805.04687v2.pdf', '1906.06589v3.pdf', '1804.07707v2.pdf', '1906.10843v1.pdf', '1803.04383v2.pdf', '1706.04269v2.pdf', '1802.07222v1.pdf', '1709.02418v2.pdf', '1705.02946v3.pdf', '1804.07849v4.pdf']","The table shows that KITTI MOTS and MOTS Challenge have significantly fewer frames and sequences compared to YouTube VOS. However, their ""Ann./Fr."" (annotations per frame) values are much higher, indicating denser annotations. BDD100K MOTS, while being larger than other MOTS datasets, maintains a high ""Ann./Fr."" value, explaining why its size is comparable to YouTube VOS despite having fewer frames and sequences.",1805.04687v2-Table3-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.",Table 3: Comparisons with other MOTS and VOS datasets.,"Why are MOTS datasets like KITTI MOTS and MOTS Challenge smaller in size compared to VOS datasets like YouTube VOS, even though BDD100K MOTS has a comparable number of annotations?"
spiqa_592,1805.01216v3,"Why did Seq2Seq and Mem2Seq models struggle to identify the correct restaurant address in Table 4 of the bAbI Task 5 KA test set in ""Disentangling Language and Knowledge in Task-Oriented Dialogs,"" especially when the knowledge base contained 100% unseen entities?","Seq2Seq and Mem2Seq models performed poorly because they struggled to capture the semantic representations of unseen entities. This means they couldn't understand the meaning and relationships of new restaurants introduced in the KB. As a result, they were unable to accurately identify the correct restaurant and provide its address when faced with unseen entities.",1805.01216v3.pdf,"['1805.01216v3.pdf', '1705.07384v2.pdf', '1611.04684v1.pdf', '1803.06506v3.pdf', '1703.10730v2.pdf', '1803.04572v2.pdf', '1805.02349v2.pdf', '1804.00863v3.pdf', '1709.00139v4.pdf', '1605.07496v3.pdf', '1805.06447v3.pdf']","The table shows an example where the user requests the address of an ""overpriced"" Thai restaurant in Bangkok. While the correct restaurant is ""r\_bangkok\_overpriced\_thai\_8"", both Seq2Seq and Mem2Seq models incorrectly provided the address of ""r\_bangkok\_overpriced\_thai\_4"". This error likely occurred because these models couldn't distinguish between the unseen restaurants in the KB due to their inability to grasp their semantic representations. This example illustrates the general trend observed in Figure 4a and 4b, where the performance of these models declined significantly with increasing unseen entities.",1805.01216v3-Table4-1.png,Disentangling Language and Knowledge in Task-Oriented Dialogs,"The Knowledge Base (KB) used for real-world applications, such as booking a
movie or restaurant reservation, keeps changing over time. End-to-end neural
networks trained for these task-oriented dialogs are expected to be immune to
any changes in the KB. However, existing approaches breakdown when asked to
handle such changes. We propose an encoder-decoder architecture (BoSsNet) with
a novel Bag-of-Sequences (BoSs) memory, which facilitates the disentangled
learning of the response's language model and its knowledge incorporation.
Consequently, the KB can be modified with new knowledge without a drop in
interpretability. We find that BoSsNet outperforms state-of-the-art models,
with considerable improvements (> 10\%) on bAbI OOV test sets and other
human-human datasets. We also systematically modify existing datasets to
measure disentanglement and show BoSsNet to be robust to KB modifications.",Table 4: Example from bAbI Task 5 KA test set with 100% OOV entities. Identifying the address of an unseen restaurant is challenging for all models.,Why did Seq2Seq and Mem2Seq models perform poorly when the percentage of unseen entities in the knowledge base (KB) increased?
spiqa_593,1701.03077v10,"In the figure illustrating the spline approximation of the log partition function in ""A General and Adaptive Robust Loss Function,"" why did the authors apply a nonlinearity to α that increases knot density near α = 2 and decreases knot density beyond α = 4 before fitting the cubic hermite spline?","The authors chose to use a nonlinearity to curve α before fitting the cubic hermite spline because it allows for increased knot density near α = 2 and decreased knot density when α > 4. This helps to better approximate the log partition function, which is difficult to evaluate for arbitrary inputs.",1701.03077v10.pdf,"['1701.03077v10.pdf', '1704.05426v4.pdf', '1703.07015v3.pdf', '1802.07222v1.pdf', '1804.01429v3.pdf', '1804.00863v3.pdf', '1703.10730v2.pdf', '1803.05776v2.pdf', '1708.00160v2.pdf', '1705.10667v4.pdf', '1708.06832v3.pdf', '1809.03149v2.pdf', '1707.00524v2.pdf', '1611.07718v2.pdf']","The top plot of the figure shows the nonlinearity that is used to curve α. The bottom plot shows the cubic hermite spline that is fit to the curved α. The dots in the bottom plot represent the knots that are used by the spline. It can be seen that the knots are more densely spaced near α = 2 and less densely spaced when α > 4. This is because the nonlinearity curves α in such a way that more knots are needed to accurately approximate the log partition function in the region where α is close to 2, and fewer knots are needed when α is greater than 4.",1701.03077v10-Figure8-1.png,A General and Adaptive Robust Loss Function,"We present a generalization of the Cauchy/Lorentzian, Geman-McClure,
Welsch/Leclerc, generalized Charbonnier, Charbonnier/pseudo-Huber/L1-L2, and L2
loss functions. By introducing robustness as a continuous parameter, our loss
function allows algorithms built around robust loss minimization to be
generalized, which improves performance on basic vision tasks such as
registration and clustering. Interpreting our loss as the negative log of a
univariate density yields a general probability distribution that includes
normal and Cauchy distributions as special cases. This probabilistic
interpretation enables the training of neural networks in which the robustness
of the loss automatically adapts itself during training, which improves
performance on learning-based tasks such as generative image synthesis and
unsupervised monocular depth estimation, without requiring any manual parameter
tuning.","Because our distribution’s log partition function log(Z (α)) is difficult to evaluate for arbitrary inputs, we approximate it using cubic hermite spline interpolation in a transformed space: first we curve α by a continuously differentiable nonlinearity that increases knot density near α = 2 and decreases knot density when α > 4 (top) and then we fit an evenly-sampled cubic hermite spline in that curved space (bottom). The dots shown in the bottom plot are a subset of the knots used by our cubic spline, and are presented here to demonstrate how this approach allocates spline knots with respect to α.",Why did the authors choose to use a nonlinearity to curve α before fitting the cubic hermite spline?
spiqa_594,1811.09393v4,"In the figure demonstrating temporal coherence issues, why does flow estimation accuracy decrease specifically near image boundaries, particularly when objects move in and out of view, and how does this relate to challenges in maintaining spatial detail in your GAN-based video generation method?","Flow estimation becomes less accurate near image boundaries because there is less information available to estimate the flow. This is because the pixels at the boundaries are only surrounded by pixels on one side, whereas pixels in the interior of the image are surrounded by pixels on all sides.",1811.09393v4.pdf,"['1811.09393v4.pdf', '1805.07567v2.pdf', '1611.03780v2.pdf', '1901.00056v2.pdf', '1804.05995v2.pdf', '1708.01425v4.pdf', '1809.01989v2.pdf', '1804.07849v4.pdf', '1804.05936v2.pdf', '1703.00899v2.pdf', '1705.07384v2.pdf']","The figure shows that the differences after warping are not all black near the image boundaries, indicating that the flow estimation is less accurate in these regions.",1811.09393v4-Figure22-1.png,Learning Temporal Coherence via Self-Supervision for GAN-based Video Generation,"Our work explores temporal self-supervision for GAN-based video generation
tasks. While adversarial training successfully yields generative models for a
variety of areas, temporal relationships in the generated data are much less
explored. Natural temporal changes are crucial for sequential generation tasks,
e.g. video super-resolution and unpaired video translation. For the former,
state-of-the-art methods often favor simpler norm losses such as $L^2$ over
adversarial training. However, their averaging nature easily leads to
temporally smooth results with an undesirable lack of spatial detail. For
unpaired video translation, existing approaches modify the generator networks
to form spatio-temporal cycle consistencies. In contrast, we focus on improving
learning objectives and propose a temporally self-supervised algorithm. For
both tasks, we show that temporal adversarial learning is key to achieving
temporally coherent solutions without sacrificing spatial detail. We also
propose a novel Ping-Pong loss to improve the long-term temporal consistency.
It effectively prevents recurrent networks from accumulating artifacts
temporally without depressing detailed features. Additionally, we propose a
first set of metrics to quantitatively evaluate the accuracy as well as the
perceptual quality of the temporal evolution. A series of user studies confirm
the rankings computed with these metrics. Code, data, models, and results are
provided at https://github.com/thunil/TecoGAN. The project page
https://ge.in.tum.de/publications/2019-tecogan-chu/ contains supplemental
materials.","Near image boundaries, flow estimation is less accurate and warping often fails to align content. The first two columns show original and warped frames, the third one shows differences after warping (ideally all black). The top row shows that structures moving into the view can cause problems, visible at the bottom of the images. The second row has objects moving out of the view.",Why does flow estimation become less accurate near image boundaries?
spiqa_595,1805.07567v2,"Why does FLoss provide better performance than balanced cross-entropy loss, as demonstrated in Table 4 of the ""Optimizing the F-measure for Threshold-free Salient Object Detection"" paper, due to its ability to automatically adjust to data imbalance using the F-measure criterion?","The FLoss method performs better than the balanced cross-entropy loss because it can automatically adjust to data imbalance using the F-measure criterion, while the balanced cross-entropy loss relies on pre-defined weights for positive and negative samples.",1805.07567v2.pdf,"['1805.07567v2.pdf', '1809.01246v1.pdf', '1802.07459v2.pdf', '1803.04572v2.pdf', '1803.05776v2.pdf', '1811.07073v3.pdf', '1809.03449v3.pdf', '1812.00281v3.pdf', '1809.01989v2.pdf', '1709.08294v3.pdf']","Table 1 shows the performance of three methods: DSS with original cross-entropy loss, DSS with balanced cross-entropy loss, and DSS with FLoss. The FLoss method consistently achieves the highest scores across all datasets in terms of MaxF, MeanF, and MAE. 

The passage explains that both the balanced cross-entropy loss and FLoss attempt to address the data imbalance problem. However, the balanced cross-entropy loss requires manually setting weights for positive and negative samples based on their proportion in a mini-batch. This approach might not be optimal for all cases. 

In contrast, FLoss utilizes the F-measure, which inherently balances precision and recall, allowing it to automatically adapt to the data imbalance without requiring predefined weights. This adaptability likely contributes to FLoss's superior performance compared to the balanced cross-entropy loss.",1805.07567v2-Table4-1.png,Optimizing the F-measure for Threshold-free Salient Object Detection,"Current CNN-based solutions to salient object detection (SOD) mainly rely on
the optimization of cross-entropy loss (CELoss). Then the quality of detected
saliency maps is often evaluated in terms of F-measure. In this paper, we
investigate an interesting issue: can we consistently use the F-measure
formulation in both training and evaluation for SOD? By reformulating the
standard F-measure we propose the relaxed F-measure which is differentiable
w.r.t the posterior and can be easily appended to the back of CNNs as the loss
function. Compared to the conventional cross-entropy loss of which the
gradients decrease dramatically in the saturated area, our loss function, named
FLoss, holds considerable gradients even when the activation approaches the
target. Consequently, the FLoss can continuously force the network to produce
polarized activations. Comprehensive benchmarks on several popular datasets
show that FLoss outperforms the state-of-the-art with a considerable margin.
More specifically, due to the polarized predictions, our method is able to
obtain high-quality saliency maps without carefully tuning the optimal
threshold, showing significant advantages in real-world applications.","Table 4. Performance comparisons across the original cross-entropy loss (Eq. 10), balanced cross-entropy loss (Eq. 15) and our proposed FLoss (Eq. 6). Original cross-entropy learns a biased prior towards the major class (the background). This is evidenced by the low recall: many positive points are mis-predicted as negative because of biased prior. By assigning loss weights on foreground/background samples, the balanced cross-entropy loss can alleviate the unbalancing problem. Our proposed method performs better than the balanced cross-entropy loss, because the F-measure criterion can automatically adjust data unbalance.","Why does the proposed FLoss method perform better than the balanced cross-entropy loss, even though both methods aim to address the data imbalance problem in salient object detection?"
spiqa_596,1805.04687v2,"""According to Table 1 in your paper, why does the BDD100K dataset, despite its significantly higher total pedestrian count (86,047), record substantially fewer persons per image (1.2) compared to Cityscapes (7.0)? What factors, mentioned in the passage and table, explain this disparity?""","The proposed dataset contains non-city scenes like highways, which typically have fewer pedestrians per image compared to cityscapes.",1805.04687v2.pdf,"['1805.04687v2.pdf', '1901.00056v2.pdf', '1708.02153v2.pdf', '1709.08294v3.pdf', '1805.04609v3.pdf', '1603.00286v5.pdf', '1811.07073v3.pdf', '1805.02349v2.pdf', '1603.03833v4.pdf']","Table 1 shows that the ""Ours"" dataset has a significantly higher total number of pedestrians (86,047) compared to Cityscapes (19,654). However, the passage explains that the ""Ours"" dataset also includes non-city scenes like highways. These scenes usually have fewer pedestrians compared to cityscapes, which explains why the number of persons per image is lower for ""Ours"" (1.2) than Cityscapes (7.0).",1805.04687v2-Table10-1.png,BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning,"Datasets drive vision progress, yet existing driving datasets are
impoverished in terms of visual content and supported tasks to study multitask
learning for autonomous driving. Researchers are usually constrained to study a
small set of problems on one dataset, while real-world computer vision
applications require performing tasks of various complexities. We construct
BDD100K, the largest driving video dataset with 100K videos and 10 tasks to
evaluate the exciting progress of image recognition algorithms on autonomous
driving. The dataset possesses geographic, environmental, and weather
diversity, which is useful for training models that are less likely to be
surprised by new conditions. Based on this diverse dataset, we build a
benchmark for heterogeneous multitask learning and study how to solve the tasks
together. Our experiments show that special training strategies are needed for
existing models to perform such heterogeneous tasks. BDD100K opens the door for
future studies in this important venue.",Table 10: Comparisons on number of pedestrians with other datasets. The statistics are based on the training set in each dataset.,"Why does the proposed dataset have a lower number of persons per image compared to the Cityscapes dataset, even though it has a much higher total number of pedestrians?"
spiqa_597,1804.01429v3,"In the context of Figure 8, why does the proposed Layout-Induced Video Representation (LIVR) framework achieve higher predicted confidence for the action ""<person, move toward (home), walkway>"" compared to baseline methods, specifically in modeling the direction of agent movement?","The proposed method outperforms the baselines for the action ""<person, move toward (home), walkway>"" because it is better at modeling moving directions.",1804.01429v3.pdf,"['1804.01429v3.pdf', '1703.02507v3.pdf', '1805.06431v4.pdf', '1706.03847v3.pdf']",The figure shows that the proposed method has a higher predicted probability for the groundtruth action than the baselines. This suggests that the proposed method is better at understanding the direction of movement of the person in the video.,1804.01429v3-Figure8-1.png,Layout-induced Video Representation for Recognizing Agent-in-Place Actions,"We address the recognition of agent-in-place actions, which are associated
with agents who perform them and places where they occur, in the context of
outdoor home surveillance. We introduce a representation of the geometry and
topology of scene layouts so that a network can generalize from the layouts
observed in the training set to unseen layouts in the test set. This
Layout-Induced Video Representation (LIVR) abstracts away low-level appearance
variance and encodes geometric and topological relationships of places in a
specific scene layout. LIVR partitions the semantic features of a video clip
into different places to force the network to learn place-based feature
descriptions; to predict the confidence of each action, LIVR aggregates
features from the place associated with an action and its adjacent places on
the scene layout. We introduce the Agent-in-Place Action dataset to show that
our method allows neural network models to generalize significantly better to
unseen scenes.",Figure 8. Qualitative examples: The predicted confidences of groundtruth actions using different methods. We use 3 frames to visualize a motion and orange ellipses to highlight moving agents.,"Why does the proposed method outperform the baselines for the action ""<person, move toward (home), walkway>""?"
spiqa_598,1811.02721v3,"How does 6LoWPAN fragmentation, as demonstrated by the data in Table 5, enhance bandwidth efficiency by reducing TCP/IP header overhead for subsequent fragments in low-power wireless networks?","Relying on fragmentation is effective because the TCP/IP headers are only included in the first fragment, not in subsequent fragments. This significantly reduces the overhead in later fragments.",1811.02721v3.pdf,"['1811.02721v3.pdf', '1708.02153v2.pdf', '1708.05239v3.pdf', '1706.03847v3.pdf', '1811.02553v4.pdf', '1705.07384v2.pdf', '1811.10673v1.pdf', '1803.05776v2.pdf', '1603.03833v4.pdf', '1706.00827v2.pdf', '1704.00774v3.pdf', '1605.07496v3.pdf', '1811.06635v1.pdf']","Table 1 shows the header overhead for different protocols with 6LoWPAN fragmentation. While the first frame carries the full overhead of all headers (38-107 bytes), subsequent fragments only have the overhead of 6LoWPAN and 802.15.4 headers (16-35 bytes). This reduction in overhead improves efficiency by utilizing less bandwidth for header information.",1811.02721v3-Table5-1.png,Performant TCP for Low-Power Wireless Networks,"Low-power and lossy networks (LLNs) enable diverse applications integrating
many resource-constrained embedded devices, often requiring interconnectivity
with existing TCP/IP networks as part of the Internet of Things. But TCP has
received little attention in LLNs due to concerns about its overhead and
performance, leading to LLN-specific protocols that require specialized
gateways for interoperability. We present a systematic study of a well-designed
TCP stack in IEEE 802.15.4-based LLNs, based on the TCP protocol logic in
FreeBSD. Through careful implementation and extensive experiments, we show that
modern low-power sensor platforms are capable of running full-scale TCP and
that TCP, counter to common belief, performs well despite the lossy nature of
LLNs. By carefully studying the interaction between the transport and link
layers, we identify subtle but important modifications to both, achieving TCP
goodput within 25% of an upper bound (5-40x higher than prior results) and
low-power operation commensurate to CoAP in a practical LLN application
scenario. This suggests that a TCP-based transport layer, seamlessly
interoperable with existing TCP/IP networks, is viable and performant in LLNs.",Table 5: Header overhead with 6LoWPAN fragmentation,Why is relying on fragmentation effective for reducing header overhead?
spiqa_599,1809.04276v2,"Based on Table 1, why does the MLE-based training process penalize the generated response (RSP) for deviating from the ground-truth response (GT), even though the RSP incorporates relevant content from the N-best candidates (C#1 and C#2)?","The model is discouraged because it is trained using the Maximum Likelihood Estimation (MLE) objective, which prioritizes generating responses that are identical to the ground-truth (GT) response. Even though the RSP integrates relevant content from the candidates and seems appropriate in the context, it is penalized because it deviates from the exact wording of the GT.",1809.04276v2.pdf,"['1809.04276v2.pdf', '1906.06589v3.pdf', '1701.03077v10.pdf', '1704.00774v3.pdf', '1703.10730v2.pdf', '1706.04269v2.pdf', '1706.08146v3.pdf', '1805.07567v2.pdf', '1809.01989v2.pdf', '1705.07384v2.pdf', '1811.02553v4.pdf']","Table 1 showcases the training process of a Seq2Seq model with N-best response candidates. The highlighted portions in the RSP, C#1, and C#2 indicate similar content. While the RSP successfully incorporates these relevant aspects, it still differs from the GT. This difference leads to the model being discouraged under the MLE objective, which solely focuses on matching the GT response and doesn't account for the semantic similarity or appropriateness of the generated response.",1809.04276v2-Table1-1.png,Retrieval-Enhanced Adversarial Training for Neural Response Generation,"Dialogue systems are usually built on either generation-based or
retrieval-based approaches, yet they do not benefit from the advantages of
different models. In this paper, we propose a Retrieval-Enhanced Adversarial
Training (REAT) method for neural response generation. Distinct from existing
approaches, the REAT method leverages an encoder-decoder framework in terms of
an adversarial training paradigm, while taking advantage of N-best response
candidates from a retrieval-based system to construct the discriminator. An
empirical study on a large scale public available benchmark dataset shows that
the REAT method significantly outperforms the vanilla Seq2Seq model as well as
the conventional adversarial training approach.","Table 1: An example of a message (MSG), a groundtruth response (GT), a generated response (RSP) and N-best response candidates (C#1 and C#2) during the training process. Similar contents in the response and candidates are in boldface.",Why is the model discouraged even though the generated response (RSP) incorporates relevant content from the N-best response candidates (C#1 and C#2)?
spiqa_600,1603.03833v4,"In Table 1 of this paper, why does the ""Pick and Place"" task feature additional ""Demonstrations after shift,"" while no such demonstrations are listed for the ""Push to Pose"" task?","The passage mentions that additional trajectories were generated for the ""Pick and Place"" task by reducing the frequency of the recorded demonstrations. This process was not applied to the ""Push to Pose"" task, therefore no ""Demonstrations after shift"" are listed for it.",1603.03833v4.pdf,"['1603.03833v4.pdf', '1809.04276v2.pdf', '1812.00108v4.pdf', '1704.00774v3.pdf', '1708.00160v2.pdf', '1812.06589v2.pdf', '1701.06171v4.pdf']","The table shows a significant increase in the number of demonstrations for the ""Pick and Place"" task after applying the frequency reduction technique (""Demonstrations after shift""). This value is absent for the ""Push to Pose"" task, indicating that this specific technique was not used for that task.",1603.03833v4-Table1-1.png,From virtual demonstration to real-world manipulation using LSTM and MDN,"Robots assisting the disabled or elderly must perform complex manipulation
tasks and must adapt to the home environment and preferences of their user.
Learning from demonstration is a promising choice, that would allow the
non-technical user to teach the robot different tasks. However, collecting
demonstrations in the home environment of a disabled user is time consuming,
disruptive to the comfort of the user, and presents safety challenges. It would
be desirable to perform the demonstrations in a virtual environment. In this
paper we describe a solution to the challenging problem of behavior transfer
from virtual demonstration to a physical robot. The virtual demonstrations are
used to train a deep neural network based controller, which is using a Long
Short Term Memory (LSTM) recurrent neural network to generate trajectories. The
training process uses a Mixture Density Network (MDN) to calculate an error
signal suitable for the multimodal nature of demonstrations. The controller
learned in the virtual environment is transferred to a physical robot (a
Rethink Robotics Baxter). An off-the-shelf vision component is used to
substitute for geometric knowledge available in the simulation and an inverse
kinematics module is used to allow the Baxter to enact the trajectory. Our
experimental studies validate the three contributions of the paper: (1) the
controller learned from virtual demonstrations can be used to successfully
perform the manipulation tasks on a physical robot, (2) the LSTM+MDN
architectural choice outperforms other choices, such as the use of feedforward
networks and mean-squared error based training signals and (3) allowing
imperfect demonstrations in the training set also allows the controller to
learn how to correct its manipulation mistakes.",Table 1: The size of the datasets for the two studied tasks,"Why is the number of demonstrations after the shift not available for the ""Push to Pose"" task?"
spiqa_601,1809.03550v3,"Considering Figure 3 in the paper ""Pursuit of Low-Rank Models of Time-Varying Matrices Robust to Sparse and Measurement Noise,"" why is the optimal threshold placed at the right margin of the dense region near the mode in the residuals histogram, and how does this contribute to effectively separating noise from important image features?","The region around the mode of the histogram mostly contains noise. Therefore, the optimal threshold is chosen to be at the right margin of this region to avoid including too much noise in the thresholded image.",1809.03550v3.pdf,"['1809.03550v3.pdf', '1804.04410v2.pdf', '1705.07384v2.pdf', '1705.08016v3.pdf', '1811.02721v3.pdf', '1811.08481v2.pdf', '1803.04383v2.pdf', '1804.01429v3.pdf', '1603.03833v4.pdf', '1812.10735v2.pdf', '1812.00108v4.pdf']","The figure shows that the region around the mode of the histogram has a high count, which indicates that there are many pixels with similar intensity values in this region. Since noise is typically characterized by random variations in intensity, it is likely that this region contains a significant amount of noise. By choosing the optimal threshold at the right margin of this region, we can exclude most of the noise while still preserving the important features of the image.",1809.03550v3-Figure3-1.png,Pursuit of Low-Rank Models of Time-Varying Matrices Robust to Sparse and Measurement Noise,"In tracking of time-varying low-rank models of time-varying matrices, we
present a method robust to both uniformly-distributed measurement noise and
arbitrarily-distributed ``sparse'' noise. In theory, we bound the tracking
error. In practice, our use of randomised coordinate descent is scalable and
allows for encouraging results on changedetection net, a benchmark.","Figure 3: A histogram of residuals. The histogram was truncated from the original 3·255 residuals to allow for some clarity of presentation. In green, there is the middle of the least-width interval representing half of the mass. In yellow, there are the end-points of the interval. In red, the “optimal” threshold we use.",Why is the optimal threshold chosen to be at the right margin of the region around the mode of the histogram?
spiqa_602,1705.10667v4,"Based on the results presented in Table 4, why does CDAN+E demonstrate consistently strong performance across both digits and synthetic-to-real datasets, while UNIT, CyCADA, and GTA excel only in specific domain adaptation tasks like digit and synthetic-to-real transfers?","CDAN+E performs well across all five datasets listed in the table, including both digit and synthetic-to-real datasets, while UNIT, CyCADA, and GTA show strong results only on the digits and synthetic-to-real datasets.",1705.10667v4.pdf,"['1705.10667v4.pdf', '1705.02798v6.pdf', '1705.08016v3.pdf', '1703.02507v3.pdf', '1704.07121v2.pdf', '1811.06635v1.pdf', '1805.04609v3.pdf', '1707.08608v3.pdf', '1703.04887v4.pdf', '1805.00912v4.pdf', '1707.00524v2.pdf', '1804.07849v4.pdf', '1805.06431v4.pdf', '1705.02946v3.pdf']","The passage highlights that UNIT, CyCADA, and GTA are specifically designed for digit and synthetic-to-real adaptation tasks, which explains their high performance in those domains. However, their performance might not generalize well to other types of domain adaptation tasks. On the other hand, CDAN+E, despite being a simpler discriminative model, achieves good performance across all datasets, suggesting it is more adaptable and less reliant on task-specific design choices.",1705.10667v4-Table4-1.png,Conditional Adversarial Domain Adaptation,"Adversarial learning has been embedded into deep networks to learn
disentangled and transferable representations for domain adaptation. Existing
adversarial domain adaptation methods may not effectively align different
domains of multimodal distributions native in classification problems. In this
paper, we present conditional adversarial domain adaptation, a principled
framework that conditions the adversarial adaptation models on discriminative
information conveyed in the classifier predictions. Conditional domain
adversarial networks (CDANs) are designed with two novel conditioning
strategies: multilinear conditioning that captures the cross-covariance between
feature representations and classifier predictions to improve the
discriminability, and entropy conditioning that controls the uncertainty of
classifier predictions to guarantee the transferability. With theoretical
guarantees and a few lines of codes, the approach has exceeded state-of-the-art
results on five datasets.",Table 4: Accuracy (%) on Digits and VisDA-2017 for unsupervised domain adaptation (ResNet-50),"Why might CDAN+E be considered a more versatile method for unsupervised domain adaptation compared to UNIT, CyCADA, and GTA?"
