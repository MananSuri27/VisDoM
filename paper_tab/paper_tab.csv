q_id,doc_id,question,answer,doc_path,documents,evidence,title,abstract
4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94,1809.01202,What baseline approaches using state-of-the-art PDTB taggers were employed for the evaluation of causality prediction in the automatic causal explanation analysis pipeline?,"[{'answer': 'state-of-the-art PDTB taggers', 'type': 'extractive'}, {'answer': 'Linear SVM, RBF SVM, and Random Forest', 'type': 'abstractive'}]",1809.01202.pdf,"['1809.01202.pdf', '1908.06083.pdf', '2002.01984.pdf', '1809.03449.pdf', '2002.06675.pdf', '2002.01207.pdf', '1912.01772.pdf', '1809.02279.pdf', '1904.05584.pdf', '1704.05907.pdf', '1910.14497.pdf', '1903.00172.pdf', '1911.05153.pdf', '1901.04899.pdf']","We first use state-of-the-art PDTB taggers for our baseline BIBREF13 , BIBREF12 for the evaluation of the causality prediction of our models ( BIBREF12 requires sentences extracted from the text as its input, so we used our parser to extract sentences from the message).",Causal Explanation Analysis on Social Media,"Understanding causal explanations - reasons given for happenings in one's
life - has been found to be an important psychological factor linked to
physical and mental health. Causal explanations are often studied through
manual identification of phrases over limited samples of personal writing.
Automatic identification of causal explanations in social media, while
challenging in relying on contextual and sequential cues, offers a larger-scale
alternative to expensive manual ratings and opens the door for new applications
(e.g. studying prevailing beliefs about causes, such as climate change). Here,
we explore automating causal explanation analysis, building on discourse
parsing, and presenting two novel subtasks: causality detection (determining
whether a causal explanation exists at all) and causal explanation
identification (identifying the specific phrase that is the explanation). We
achieve strong accuracies for both tasks but find different approaches best: an
SVM for causality prediction (F1 = 0.791) and a hierarchy of Bidirectional
LSTMs for causal explanation identification (F1 = 0.853). Finally, we explore
applications of our complete pipeline (F1 = 0.868), showing demographic
differences in mentions of causal explanation and that the association between
a word and sentiment can change when it is used within a causal explanation."
ed7a3e7fc1672f85a768613e7d1b419475950ab4,1909.00754,"Based on the joint goal accuracy and inference time complexity, does the model demonstrate superior performance in the single-domain WoZ2.0 test set or the multi-domain MultiWoZ test set?","[{'answer': 'single-domain setting', 'type': 'abstractive'}]",1909.00754.pdf,"['1909.00754.pdf', '1710.09340.pdf', '2004.01980.pdf', '1910.12129.pdf', '1701.06538.pdf', '1811.12254.pdf', '1902.10525.pdf']","FLOAT SELECTED: Table 3: The joint goal accuracy of the DST models on the WoZ2.0 test set and the MultiWoZ test set. We also include the Inference Time Complexity (ITC) for each model as a metric for scalability. The baseline accuracy for the WoZ2.0 dataset is the Delexicalisation-Based (DB) Model (Mrksic et al., 2017), while the baseline for the MultiWoZ dataset is taken from the official website of MultiWoZ (Budzianowski et al., 2018).",Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation,"Existing approaches to dialogue state tracking rely on pre-defined ontologies
consisting of a set of all possible slot types and values. Though such
approaches exhibit promising performance on single-domain benchmarks, they
suffer from computational complexity that increases proportionally to the
number of pre-defined slots that need tracking. This issue becomes more severe
when it comes to multi-domain dialogues which include larger numbers of slots.
In this paper, we investigate how to approach DST using a generation framework
without the pre-defined ontology list. Given each turn of user utterance and
system response, we directly generate a sequence of belief states by applying a
hierarchical encoder-decoder structure. In this way, the computational
complexity of our model will be a constant regardless of the number of
pre-defined slots. Experiments on both the multi-domain and the single domain
dialogue state tracking dataset show that our model not only scales easily with
the increasing number of pre-defined domains and slots but also reaches the
state-of-the-art performance."
664db503509b8236bc4d3dc39cebb74498365750,1912.10011,Which hierarchical model variant achieves a higher BLEU score of 17.5 compared to the Flat scenario (16.7)?,"[{'answer': 'Hierarchical-k', 'type': 'extractive'}]",1912.10011.pdf,"['1912.10011.pdf', '1809.08298.pdf', '1810.12085.pdf', '1905.11901.pdf', '1908.10084.pdf', '2001.08051.pdf', '1909.08041.pdf', '1910.14497.pdf', '1909.08859.pdf', '2004.04721.pdf', '1809.00540.pdf', '1909.11297.pdf', '1610.00879.pdf', '2002.02070.pdf', '2002.01359.pdf', '1811.02906.pdf']","To evaluate the impact of our model components, we first compare scenarios Flat, Hierarchical-k, and Hierarchical-kv. As shown in Table TABREF25, we can see the lower results obtained by the Flat scenario compared to the other scenarios (e.g. BLEU $16.7$ vs. $17.5$ for resp. Flat and Hierarchical-k), suggesting the effectiveness of encoding the data-structure using a hierarchy.",A Hierarchical Model for Data-to-Text Generation,"Transcribing structured data into natural language descriptions has emerged
as a challenging task, referred to as ""data-to-text"". These structures
generally regroup multiple elements, as well as their attributes. Most attempts
rely on translation encoder-decoder methods which linearize elements into a
sequence. This however loses most of the structure contained in the data. In
this work, we propose to overpass this limitation with a hierarchical model
that encodes the data-structure at the element-level and the structure level.
Evaluations on RotoWire show the effectiveness of our model w.r.t. qualitative
and quantitative metrics."
1a8b7d3d126935c09306cacca7ddb4b953ef68ab,1707.08559,"What were the F-scores of the best performing model on the NALCS (English) and LMS (Traditional Chinese) test sets, as reported in the video highlight prediction paper using multimodal and multilingual audience chat reactions?","[{'answer': 'Best model achieved F-score 74.7 on NALCS and F-score of 70.0 on LMS on test set', 'type': 'abstractive'}]",1707.08559.pdf,"['1707.08559.pdf', '1909.03135.pdf', '2003.11563.pdf', '1707.05236.pdf', '1906.05474.pdf', '2002.01664.pdf', '1901.08079.pdf', '2002.08899.pdf', '1904.05584.pdf', '1810.12085.pdf', '1712.00991.pdf', '1901.05280.pdf', '1709.05413.pdf', '1901.02262.pdf', '1701.06538.pdf', '1909.00252.pdf', '1707.03569.pdf', '1912.01772.pdf']",FLOAT SELECTED: Table 3: Test Results on the NALCS (English) and LMS (Traditional Chinese) datasets.,Video Highlight Prediction Using Audience Chat Reactions,"Sports channel video portals offer an exciting domain for research on
multimodal, multilingual analysis. We present methods addressing the problem of
automatic video highlight prediction based on joint visual features and textual
analysis of the real-world audience discourse with complex slang, in both
English and traditional Chinese. We present a novel dataset based on League of
Legends championships recorded from North American and Taiwanese Twitch.tv
channels (will be released for further research), and demonstrate strong
results on these using multimodal, character-level CNN-RNN model architectures."
281cd4e78b27a62713ec43249df5000812522a89,1906.03538,What is the average token count of claims in the PERSPECTRUM dataset?,"[{'answer': 'Average claim length is 8.9 tokens.', 'type': 'abstractive'}]",1906.03538.pdf,"['1906.03538.pdf', '1705.00108.pdf', '1901.09755.pdf', '1908.05828.pdf', '2001.08051.pdf', '1705.01214.pdf', '1909.08089.pdf', '2002.08899.pdf']",FLOAT SELECTED: Table 2: A summary of PERSPECTRUM statistics,Seeing Things from a Different Angle: Discovering Diverse Perspectives about Claims,"One key consequence of the information revolution is a significant increase
and a contamination of our information supply. The practice of fact checking
won't suffice to eliminate the biases in text data we observe, as the degree of
factuality alone does not determine whether biases exist in the spectrum of
opinions visible to us. To better understand controversial issues, one needs to
view them from a diverse yet comprehensive set of perspectives. For example,
there are many ways to respond to a claim such as ""animals should have lawful
rights"", and these responses form a spectrum of perspectives, each with a
stance relative to this claim and, ideally, with evidence supporting it.
Inherently, this is a natural language understanding task, and we propose to
address it as such. Specifically, we propose the task of substantiated
perspective discovery where, given a claim, a system is expected to discover a
diverse set of well-corroborated perspectives that take a stance with respect
to the claim. Each perspective should be substantiated by evidence paragraphs
which summarize pertinent results and facts. We construct PERSPECTRUM, a
dataset of claims, perspectives and evidence, making use of online debate
websites to create the initial data collection, and augmenting it using search
engines in order to expand and diversify our dataset. We use crowd-sourcing to
filter out noise and ensure high-quality data. Our dataset contains 1k claims,
accompanied with pools of 10k and 8k perspective sentences and evidence
paragraphs, respectively. We provide a thorough analysis of the dataset to
highlight key underlying language understanding challenges, and show that human
baselines across multiple subtasks far outperform ma-chine baselines built upon
state-of-the-art NLP techniques. This poses a challenge and opportunity for the
NLP community to address."
f2155dc4aeab86bf31a838c8ff388c85440fce6e,1908.11047,Does incorporating shallow syntax-aware embeddings lead to better performance on the sentiment classification task compared to using standard ELMo embeddings?,"[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'No', 'type': 'boolean'}]",1908.11047.pdf,"['1908.11047.pdf', '1910.03467.pdf', '1703.06492.pdf', '1810.12885.pdf', '1906.11180.pdf', '1809.10644.pdf']","Results are shown in Table TABREF12. Consistent with previous findings, cwrs offer large improvements across all tasks.",Shallow Syntax in Deep Water,"Shallow syntax provides an approximation of phrase-syntactic structure of
sentences; it can be produced with high accuracy, and is computationally cheap
to obtain. We investigate the role of shallow syntax-aware representations for
NLP tasks using two techniques. First, we enhance the ELMo architecture to
allow pretraining on predicted shallow syntactic parses, instead of just raw
text, so that contextual embeddings make use of shallow syntactic context. Our
second method involves shallow syntactic features obtained automatically on
downstream task data. Neither approach leads to a significant gain on any of
the four downstream tasks we considered relative to ELMo-only baselines.
Further analysis using black-box probes confirms that our shallow-syntax-aware
contextual embeddings do not transfer to linguistic tasks any more easily than
ELMo's embeddings. We take these findings as evidence that ELMo-style
pretraining discovers representations which make additional awareness of
shallow syntax redundant."
ed6a15f0f7fa4594e51d5bde21cc0c6c1bedbfdc,1908.11047,"Out of the 9 probe tasks, how many tasks show better performance with the mSynC model compared to baseline ELMo-transformer embeddings in the ""Shallow Syntax in Deep Water"" study?","[{'answer': 'performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks', 'type': 'extractive'}, {'answer': '3', 'type': 'abstractive'}]",1908.11047.pdf,"['1908.11047.pdf', '2003.11645.pdf', '1906.01081.pdf', '1904.10500.pdf', '1911.08673.pdf', '1907.09369.pdf']","Results in Table TABREF13 show ten probes. Again, we see the performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks.",Shallow Syntax in Deep Water,"Shallow syntax provides an approximation of phrase-syntactic structure of
sentences; it can be produced with high accuracy, and is computationally cheap
to obtain. We investigate the role of shallow syntax-aware representations for
NLP tasks using two techniques. First, we enhance the ELMo architecture to
allow pretraining on predicted shallow syntactic parses, instead of just raw
text, so that contextual embeddings make use of shallow syntactic context. Our
second method involves shallow syntactic features obtained automatically on
downstream task data. Neither approach leads to a significant gain on any of
the four downstream tasks we considered relative to ELMo-only baselines.
Further analysis using black-box probes confirms that our shallow-syntax-aware
contextual embeddings do not transfer to linguistic tasks any more easily than
ELMo's embeddings. We take these findings as evidence that ELMo-style
pretraining discovers representations which make additional awareness of
shallow syntax redundant."
4d706ce5bde82caf40241f5b78338ea5ee5eb01e,1908.11047,"What specific set of black-box probes, as described in the evidence section of this paper, were used to analyze the role of shallow syntactic awareness in the enhanced ELMo architecture's embeddings for capturing linguistic properties?","[{'answer': 'CCG Supertagging CCGBank , PTB part-of-speech tagging, EWT part-of-speech tagging,\nChunking, Named Entity Recognition, Semantic Tagging, Grammar Error Detection, Preposition Supersense Role, Preposition Supersense Function, Event Factuality Detection', 'type': 'abstractive'}, {'answer': 'Probes are linear models trained on frozen cwrs to make predictions about linguistic (syntactic and semantic) properties of words and phrases.', 'type': 'extractive'}]",1908.11047.pdf,"['1908.11047.pdf', '2001.10161.pdf', '1911.11951.pdf', '1911.01799.pdf', '1908.07816.pdf', '1811.02906.pdf', '1809.02279.pdf', '1904.10500.pdf', '1910.10288.pdf']"," We further examine the contextual embeddings obtained from the enhanced architecture and a shallow syntactic context, using black-box probes from BIBREF1",Shallow Syntax in Deep Water,"Shallow syntax provides an approximation of phrase-syntactic structure of
sentences; it can be produced with high accuracy, and is computationally cheap
to obtain. We investigate the role of shallow syntax-aware representations for
NLP tasks using two techniques. First, we enhance the ELMo architecture to
allow pretraining on predicted shallow syntactic parses, instead of just raw
text, so that contextual embeddings make use of shallow syntactic context. Our
second method involves shallow syntactic features obtained automatically on
downstream task data. Neither approach leads to a significant gain on any of
the four downstream tasks we considered relative to ELMo-only baselines.
Further analysis using black-box probes confirms that our shallow-syntax-aware
contextual embeddings do not transfer to linguistic tasks any more easily than
ELMo's embeddings. We take these findings as evidence that ELMo-style
pretraining discovers representations which make additional awareness of
shallow syntax redundant."
86bf75245358f17e35fc133e46a92439ac86d472,1908.11047,"Based on the findings in *Shallow Syntax in Deep Water*, how does mSynC's performance on downstream NLP tasks, especially in chunk tag prediction, compare to that of ELMo-transformer, and what does this imply about the utility of shallow syntactic features?","[{'answer': 'only modest gains on three of the four downstream tasks', 'type': 'extractive'}, {'answer': ' the performance differences across all tasks are small enough ', 'type': 'extractive'}]",1908.11047.pdf,"['1908.11047.pdf', '1711.11221.pdf', '1611.03382.pdf', '1709.10367.pdf', '1910.00458.pdf', '1701.02877.pdf', '1911.02086.pdf', '1909.09484.pdf']","Results in Table TABREF13 show ten probes. Again, we see the performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks. As we would expect, on the probe for predicting chunk tags, mSynC achieves 96.9 $F_1$ vs. 92.2 $F_1$ for ELMo-transformer, indicating that mSynC is indeed encoding shallow syntax. Overall, the results further confirm that explicit shallow syntax does not offer any benefits over ELMo-transformer.",Shallow Syntax in Deep Water,"Shallow syntax provides an approximation of phrase-syntactic structure of
sentences; it can be produced with high accuracy, and is computationally cheap
to obtain. We investigate the role of shallow syntax-aware representations for
NLP tasks using two techniques. First, we enhance the ELMo architecture to
allow pretraining on predicted shallow syntactic parses, instead of just raw
text, so that contextual embeddings make use of shallow syntactic context. Our
second method involves shallow syntactic features obtained automatically on
downstream task data. Neither approach leads to a significant gain on any of
the four downstream tasks we considered relative to ELMo-only baselines.
Further analysis using black-box probes confirms that our shallow-syntax-aware
contextual embeddings do not transfer to linguistic tasks any more easily than
ELMo's embeddings. We take these findings as evidence that ELMo-style
pretraining discovers representations which make additional awareness of
shallow syntax redundant."
9ebb2adf92a0f8db99efddcade02a20a219ca7d9,2001.08051,"How were the six predefined proficiency indicators scored in the 2017/2018 recordings of the TLT-school corpus for both spoken and written items, and what values did human experts assign to reflect proficiency levels?","[{'answer': 'They used 6 indicators for proficiency (same for written and spoken) each marked by bad, medium or good by one expert.', 'type': 'abstractive'}]",2001.08051.pdf,"['2001.08051.pdf', '1909.08041.pdf', '1901.02257.pdf', '1912.10435.pdf', '1910.06748.pdf', '1909.00754.pdf', '2001.10161.pdf', '1903.00172.pdf', '1712.03547.pdf', '1904.10503.pdf', '1804.07789.pdf', '2003.04866.pdf', '1901.03866.pdf', '1909.08824.pdf', '1902.00672.pdf', '2002.01664.pdf', '1902.00330.pdf', '1908.05828.pdf']","Each written or spoken item received a total score by human experts, computed by summing up the scores related to 6 indicators in 2017/2018 (from 3 to 6 in the 2016 campaign, according to the proficiency levels and the type of test). Each indicator can assume a value 0, 1, 2, corresponding to bad, medium, good, respectively.",TLT-school: a Corpus of Non Native Children Speech,"This paper describes ""TLT-school"" a corpus of speech utterances collected in
schools of northern Italy for assessing the performance of students learning
both English and German. The corpus was recorded in the years 2017 and 2018
from students aged between nine and sixteen years, attending primary, middle
and high school. All utterances have been scored, in terms of some predefined
proficiency indicators, by human experts. In addition, most of utterances
recorded in 2017 have been manually transcribed carefully. Guidelines and
procedures used for manual transcriptions of utterances will be described in
detail, as well as results achieved by means of an automatic speech recognition
system developed by us. Part of the corpus is going to be freely distributed to
scientific community particularly interested both in non-native speech
recognition and automatic assessment of second language proficiency."
973f6284664675654cc9881745880a0e88f3280e,2001.08051,What are the six linguistic proficiency indicators outlined in the TLT-school corpus that human experts used to assess the English and German utterances of non-native speaking students?,"[{'answer': '6 indicators:\n- lexical richness\n- pronunciation and fluency\n- syntactical correctness\n- fulfillment of delivery\n- coherence and cohesion\n- communicative, descriptive, narrative skills', 'type': 'abstractive'}]",2001.08051.pdf,"['2001.08051.pdf', '1901.05280.pdf', '1712.03547.pdf', '1809.00540.pdf', '1910.12795.pdf', '1603.00968.pdf', '2001.05970.pdf', '1910.04269.pdf', '1806.04330.pdf', '2002.02492.pdf', '1908.08345.pdf', '1901.04899.pdf', '1811.12254.pdf', '1909.13695.pdf', '1804.08050.pdf']",FLOAT SELECTED: Table 4: List of the indicators used by human experts to evaluate specific linguistic competences.,TLT-school: a Corpus of Non Native Children Speech,"This paper describes ""TLT-school"" a corpus of speech utterances collected in
schools of northern Italy for assessing the performance of students learning
both English and German. The corpus was recorded in the years 2017 and 2018
from students aged between nine and sixteen years, attending primary, middle
and high school. All utterances have been scored, in terms of some predefined
proficiency indicators, by human experts. In addition, most of utterances
recorded in 2017 have been manually transcribed carefully. Guidelines and
procedures used for manual transcriptions of utterances will be described in
detail, as well as results achieved by means of an automatic speech recognition
system developed by us. Part of the corpus is going to be freely distributed to
scientific community particularly interested both in non-native speech
recognition and automatic assessment of second language proficiency."
0a3a8d1b0cbac559f7de845d845ebbfefb91135e,2001.08051,What are the Word Error Rates (WER) achieved by the adapted Automatic Speech Recognition (ASR) system on the non-native English and German children's speech evaluation data in the TLT-school corpus?,"[{'answer': 'Accuracy not available: WER results are reported 42.6 German, 35.9 English', 'type': 'abstractive'}]",2001.08051.pdf,"['2001.08051.pdf', '1804.08139.pdf', '1909.08041.pdf', '1809.09194.pdf', '1804.08050.pdf', '1701.00185.pdf']","Table , extracted from BIBREF0, reports WERs obtained on evaluation data sets with a strongly adapted ASR, demonstrating the difficulty of the related speech recognition task for both languages.",TLT-school: a Corpus of Non Native Children Speech,"This paper describes ""TLT-school"" a corpus of speech utterances collected in
schools of northern Italy for assessing the performance of students learning
both English and German. The corpus was recorded in the years 2017 and 2018
from students aged between nine and sixteen years, attending primary, middle
and high school. All utterances have been scored, in terms of some predefined
proficiency indicators, by human experts. In addition, most of utterances
recorded in 2017 have been manually transcribed carefully. Guidelines and
procedures used for manual transcriptions of utterances will be described in
detail, as well as results achieved by means of an automatic speech recognition
system developed by us. Part of the corpus is going to be freely distributed to
scientific community particularly interested both in non-native speech
recognition and automatic assessment of second language proficiency."
ec2b8c43f14227cf74f9b49573cceb137dd336e7,2001.08051,What metric is used to evaluate the performance of the adapted ASR system on non-native children's English and German speech?,"[{'answer': 'Speech recognition system is evaluated using WER metric.', 'type': 'abstractive'}]",2001.08051.pdf,"['2001.08051.pdf', '1910.06036.pdf', '2004.01878.pdf', '2004.01980.pdf', '1908.05434.pdf', '1911.12579.pdf', '1911.08962.pdf', '1901.09755.pdf', '1610.07809.pdf', '1709.10217.pdf', '1910.03814.pdf', '1902.09393.pdf', '1909.01013.pdf', '1812.10479.pdf', '1704.05907.pdf', '1710.06700.pdf', '2001.10161.pdf']","Table , extracted from BIBREF0, reports WERs obtained on evaluation data sets with a strongly adapted ASR, demonstrating the difficulty of the related speech recognition task for both languages.",TLT-school: a Corpus of Non Native Children Speech,"This paper describes ""TLT-school"" a corpus of speech utterances collected in
schools of northern Italy for assessing the performance of students learning
both English and German. The corpus was recorded in the years 2017 and 2018
from students aged between nine and sixteen years, attending primary, middle
and high school. All utterances have been scored, in terms of some predefined
proficiency indicators, by human experts. In addition, most of utterances
recorded in 2017 have been manually transcribed carefully. Guidelines and
procedures used for manual transcriptions of utterances will be described in
detail, as well as results achieved by means of an automatic speech recognition
system developed by us. Part of the corpus is going to be freely distributed to
scientific community particularly interested both in non-native speech
recognition and automatic assessment of second language proficiency."
5e5460ea955d8bce89526647dd7c4f19b173ab34,2001.08051,"How many utterances were manually transcribed in the TLT-school corpus, across both English and German languages, including train and evaluation sets?","[{'answer': 'Total number of transcribed utterances including Train and Test for both Eng and Ger language is 5562 (2188 cleaned)', 'type': 'abstractive'}]",2001.08051.pdf,"['2001.08051.pdf', '1809.00540.pdf', '1809.06537.pdf', '1711.00106.pdf', '1811.12254.pdf', '2002.11402.pdf', '1908.11365.pdf', '1904.03288.pdf', '1808.03430.pdf', '1606.00189.pdf', '1910.11769.pdf', '1809.09194.pdf', '1902.09314.pdf', '1909.06937.pdf', '1909.11687.pdf', '1902.09393.pdf', '2002.06424.pdf', '1809.04960.pdf']","Speakers were assigned either to training or evaluation sets, with proportions of $\frac{2}{3}$ and $\frac{1}{3}$, respectively; then training and evaluation lists were built, accordingly. Table reports statistics from the spoken data set. The id All identifies the whole data set, while Clean defines the subset in which sentences containing background voices, incomprehensible speech and word fragments were excluded.",TLT-school: a Corpus of Non Native Children Speech,"This paper describes ""TLT-school"" a corpus of speech utterances collected in
schools of northern Italy for assessing the performance of students learning
both English and German. The corpus was recorded in the years 2017 and 2018
from students aged between nine and sixteen years, attending primary, middle
and high school. All utterances have been scored, in terms of some predefined
proficiency indicators, by human experts. In addition, most of utterances
recorded in 2017 have been manually transcribed carefully. Guidelines and
procedures used for manual transcriptions of utterances will be described in
detail, as well as results achieved by means of an automatic speech recognition
system developed by us. Part of the corpus is going to be freely distributed to
scientific community particularly interested both in non-native speech
recognition and automatic assessment of second language proficiency."
73bbe0b6457423f08d9297a0951381098bd89a2b,1901.0528,"What are the baseline models compared in this paper, which summarizes prior work on span and dependency SRL, including models like Fitzgerald2015?","[{'answer': '2008 Punyakanok et al. \n2009 Zhao et al. + ME \n2008 Toutanova et al. \n2010 Bjorkelund et al.  \n2015 FitzGerald et al. \n2015 Zhou and Xu \n2016 Roth and Lapata \n2017 He et al. \n2017 Marcheggiani et al.\n2017 Marcheggiani and Titov \n2018 Tan et al. \n2018 He et al. \n2018 Strubell et al. \n2018 Cai et al. \n2018 He et al. \n2018 Li et al. \n', 'type': 'abstractive'}]",1901.05280.pdf,"['1901.05280.pdf', '1910.08987.pdf', '1911.10049.pdf', '1706.08032.pdf', '1806.07711.pdf', '2002.04181.pdf', '1908.06264.pdf', '1910.06036.pdf', '1904.05584.pdf', '2003.12738.pdf', '1909.06162.pdf', '1704.06194.pdf', '1809.04960.pdf', '1911.00069.pdf', '2001.05970.pdf']","Generally, the above work is summarized in Table TABREF2 . Considering motivation, our work is most closely related to the work of BIBREF14 Fitzgerald2015, which also tackles span and dependency SRL in a uniform fashion. The essential difference is that their model employs the syntactic features and takes pre-identified predicates as inputs, while our model puts syntax aside and jointly learns and predicts predicates and arguments.",,
1d3e914d0890fc09311a70de0b20974bf7f0c9fe,2004.03354,What are the eight biomedical NER tasks that were used to compare the performance of GreenBioBERT and BioBERT in the context of inexpensive domain adaptation?,"[{'answer': 'BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800', 'type': 'abstractive'}, {'answer': 'BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800', 'type': 'abstractive'}]",2004.03354.pdf,"['2004.03354.pdf', '1903.09722.pdf', '2003.11645.pdf', '2002.01359.pdf', '1712.03547.pdf', '1908.05434.pdf', '1906.03538.pdf', '1908.10084.pdf', '1809.09795.pdf', '2002.04181.pdf', '2003.03106.pdf', '1908.06267.pdf', '1702.03342.pdf', '1711.00106.pdf']",FLOAT SELECTED: Table 2: Top: Examples of within-space and cross-space nearest neighbors (NNs) by cosine similarity in GreenBioBERT’s wordpiece embedding layer. Blue: Original wordpiece space. Green: Aligned Word2Vec space. Bottom: Biomedical NER test set precision / recall / F1 (%) measured with the CoNLL NER scorer. Boldface: Best model in row. Underlined: Best inexpensive model (without target-domain pretraining) in row.,Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA,"Domain adaptation of Pretrained Language Models (PTLMs) is typically achieved
by unsupervised pretraining on target-domain text. While successful, this
approach is expensive in terms of hardware, runtime and CO_2 emissions. Here,
we propose a cheaper alternative: We train Word2Vec on target-domain text and
align the resulting word vectors with the wordpiece vectors of a general-domain
PTLM. We evaluate on eight biomedical Named Entity Recognition (NER) tasks and
compare against the recently proposed BioBERT model. We cover over 60% of the
BioBERT-BERT F1 delta, at 5% of BioBERT's CO_2 footprint and 2% of its cloud
compute cost. We also show how to quickly adapt an existing general-domain
Question Answering (QA) model to an emerging domain: the Covid-19 pandemic."
e9ccc74b1f1b172224cf9f01e66b1fa9e34d2593,1909.03242,"What specific metadata fields are provided, in addition to the claim and its label, in the example claim instance from the MultiFC dataset?","[{'answer': 'besides claim, label and claim url, it also includes a claim ID, reason, category, speaker, checker, tags, claim entities, article title, publish data and claim date', 'type': 'abstractive'}]",1909.03242.pdf,"['1909.03242.pdf', '1912.01772.pdf', '1905.10810.pdf', '2002.01207.pdf', '1611.04642.pdf', '1811.01088.pdf', '1906.01081.pdf', '1809.05752.pdf', '1704.08960.pdf']","FLOAT SELECTED: Table 1: An example of a claim instance. Entities are obtained via entity linking. Article and outlink texts, evidence search snippets and pages are not shown.",MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims,"We contribute the largest publicly available dataset of naturally occurring
factual claims for the purpose of automatic claim verification. It is collected
from 26 fact checking websites in English, paired with textual sources and rich
metadata, and labelled for veracity by human expert journalists. We present an
in-depth analysis of the dataset, highlighting characteristics and challenges.
Further, we present results for automatic veracity prediction, both with
established baselines and with a novel method for joint ranking of evidence
pages and predicting veracity that outperforms all baselines. Significant
performance increases are achieved by encoding evidence, and by modelling
metadata. Our best-performing model achieves a Macro F1 of 49.2%, showing that
this is a challenging testbed for claim veracity prediction."
f1831b2e96ff8ef65b8fde8b4c2ee3e04b7ac4bf,1910.08987,"What are the NMI values between the clustered tone contours and the ground truth tone categories for all syllables in both Mandarin and Cantonese, as reported in your phonemic tone clustering analysis?","[{'answer': 'NMI between cluster assignments and ground truth tones for all sylables is:\nMandarin: 0.641\nCantonese: 0.464', 'type': 'abstractive'}]",1910.08987.pdf,"['1910.08987.pdf', '1903.09588.pdf', '1705.01265.pdf', '1910.00912.pdf', '1701.06538.pdf', '1912.10806.pdf', '1911.03597.pdf', '2003.11563.pdf', '1904.05584.pdf', '1910.12795.pdf', '2004.04721.pdf']","To test this hypothesis, we evaluate the model on only the first syllable of every word, which eliminates carry-over and declination effects (Table TABREF14). In both Mandarin and Cantonese, the clustering is more accurate when using only the first syllables, compared to using all of the syllables.",Representation Learning for Discovering Phonemic Tone Contours,"Tone is a prosodic feature used to distinguish words in many languages, some
of which are endangered and scarcely documented. In this work, we use
unsupervised representation learning to identify probable clusters of syllables
that share the same phonemic tone. Our method extracts the pitch for each
syllable, then trains a convolutional autoencoder to learn a low dimensional
representation for each contour. We then apply the mean shift algorithm to
cluster tones in high-density regions of the latent space. Furthermore, by
feeding the centers of each cluster into the decoder, we produce a prototypical
contour that represents each cluster. We apply this method to spoken
multi-syllable words in Mandarin Chinese and Cantonese and evaluate how closely
our clusters match the ground truth tone categories. Finally, we discuss some
difficulties with our approach, including contextual tone variation and
allophony effects."
7f5ab9a53aef7ea1a1c2221967057ee71abb27cb,1911.02086,Does the paper compare the execution time of their GDSConv-based model with other state-of-the-art keyword spotting models that use traditional acoustic feature extraction techniques?,"[{'answer': 'No', 'type': 'boolean'}]",1911.02086.pdf,"['1911.02086.pdf', '1806.04330.pdf', '2003.03014.pdf', '2002.02492.pdf', '1911.08976.pdf', '1910.06592.pdf', '1811.02906.pdf', '1911.01680.pdf', '1912.01214.pdf', '1908.06264.pdf', '2002.06644.pdf', '1904.10500.pdf', '1710.09340.pdf']",The base model composed of DSConv layers without grouping achieves the state-of-the-art accuracy of 96.6% on the Speech Commands test set. The low-parameter model with GDSConv achieves almost the same accuracy of 96.4% with only about half the parameters. ,Small-Footprint Keyword Spotting on Raw Audio Data with Sinc-Convolutions,"Keyword Spotting (KWS) enables speech-based user interaction on smart
devices. Always-on and battery-powered application scenarios for smart devices
put constraints on hardware resources and power consumption, while also
demanding high accuracy as well as real-time capability. Previous architectures
first extracted acoustic features and then applied a neural network to classify
keyword probabilities, optimizing towards memory footprint and execution time.
Compared to previous publications, we took additional steps to reduce power and
memory consumption without reducing classification accuracy. Power-consuming
audio preprocessing and data transfer steps are eliminated by directly
classifying from raw audio. For this, our end-to-end architecture extracts
spectral features using parametrized Sinc-convolutions. Its memory footprint is
further reduced by grouping depthwise separable convolutions. Our network
achieves the competitive accuracy of 96.4% on Google's Speech Commands test set
with only 62k parameters."
9776156fc93daa36f4613df591e2b49827d25ad2,1803.0923,How much do the F1 scores improve when using the hybrid attention method and DCA compared to BiDAF and DCN on the SQuAD dataset?,"[{'answer': 'In terms of F1 score, the Hybrid approach improved by 23.47% and 1.39% on BiDAF and DCN respectively. The DCA approach improved by 23.2% and 1.12% on BiDAF and DCN respectively.', 'type': 'abstractive'}]",1803.09230.pdf,"['1803.09230.pdf', '1809.00530.pdf', '1712.05999.pdf', '1910.00912.pdf', '1904.10503.pdf', '2002.06424.pdf', '1911.05153.pdf']",FLOAT SELECTED: Table 1: Effect of Character Embedding,,
160e6d2fc6e04bb0b4ee8d59c06715355dec4a17,1911.13066,What is the accuracy score of the highest-performing McM model on the 12-class bilingual SMS dataset?,"[{'answer': 'the best performing model obtained an accuracy of 0.86', 'type': 'abstractive'}]",1911.13066.pdf,"['1911.13066.pdf', '1902.09666.pdf', '1812.06864.pdf', '1810.03459.pdf', '1701.09123.pdf', '1902.00330.pdf', '1910.08210.pdf', '1810.10254.pdf', '1804.08139.pdf', '1911.08962.pdf', '1908.08345.pdf']",FLOAT SELECTED: Table 3. Performance evaluation of variations of the proposed model and baseline. Showing highest scores in boldface.,A Multi-cascaded Deep Model for Bilingual SMS Classification,"Most studies on text classification are focused on the English language.
However, short texts such as SMS are influenced by regional languages. This
makes the automatic text classification task challenging due to the
multilingual, informal, and noisy nature of language in the text. In this work,
we propose a novel multi-cascaded deep learning model called McM for bilingual
SMS classification. McM exploits $n$-gram level information as well as
long-term dependencies of text for learning. Our approach aims to learn a model
without any code-switching indication, lexical normalization, language
translation, or language transliteration. The model relies entirely upon the
text as no external knowledge base is utilized for learning. For this purpose,
a 12 class bilingual text dataset is developed from SMS feedbacks of citizens
on public services containing mixed Roman Urdu and English languages. Our model
achieves high accuracy for classification on this dataset and outperforms the
previous model for multilingual text classification, highlighting language
independence of McM."
30dad5d9b4a03e56fa31f932c879aa56e11ed15b,1911.13066,What are the 12 class labels and their distribution (as given in %) for the bilingual Roman Urdu-English SMS feedback dataset discussed in the study on McM deep learning model for SMS classification?,"[{'answer': 'Appreciation, Satisfied, Peripheral complaint, Demanded inquiry, Corruption, Lagged response, Unresponsive, Medicine payment, Adverse behavior, Grievance ascribed and Obnoxious/irrelevant', 'type': 'abstractive'}]",1911.13066.pdf,"['1911.13066.pdf', '2001.10161.pdf', '2003.12218.pdf', '1809.09795.pdf', '1810.03459.pdf', '2002.02070.pdf', '1910.08210.pdf', '1906.10551.pdf', '1905.10810.pdf', '1804.05918.pdf', '1812.06705.pdf', '1912.03457.pdf', '2003.08385.pdf', '2001.08868.pdf', '1909.00361.pdf', '1911.08962.pdf']",FLOAT SELECTED: Table 1. Description of class label along with distribution of each class (in %) in the acquired dataset,A Multi-cascaded Deep Model for Bilingual SMS Classification,"Most studies on text classification are focused on the English language.
However, short texts such as SMS are influenced by regional languages. This
makes the automatic text classification task challenging due to the
multilingual, informal, and noisy nature of language in the text. In this work,
we propose a novel multi-cascaded deep learning model called McM for bilingual
SMS classification. McM exploits $n$-gram level information as well as
long-term dependencies of text for learning. Our approach aims to learn a model
without any code-switching indication, lexical normalization, language
translation, or language transliteration. The model relies entirely upon the
text as no external knowledge base is utilized for learning. For this purpose,
a 12 class bilingual text dataset is developed from SMS feedbacks of citizens
on public services containing mixed Roman Urdu and English languages. Our model
achieves high accuracy for classification on this dataset and outperforms the
previous model for multilingual text classification, highlighting language
independence of McM."
20ec88c45c1d633adfd7bff7bbf3336d01fb6f37,1701.09123,"What metrics (precision, recall, F1) are reported for evaluating the NER system's performance in the CoNLL 2003 English results?","[{'answer': 'Precision, Recall, F1', 'type': 'abstractive'}]",1701.09123.pdf,"['1701.09123.pdf', '2003.01769.pdf', '1805.04033.pdf', '1711.11221.pdf']",FLOAT SELECTED: Table 5: CoNLL 2003 English results.,Robust Multilingual Named Entity Recognition with Shallow Semi-Supervised Features,"We present a multilingual Named Entity Recognition approach based on a robust
and general set of features across languages and datasets. Our system combines
shallow local information with clustering semi-supervised features induced on
large amounts of unlabeled text. Understanding via empirical experimentation
how to effectively combine various types of clustering features allows us to
seamlessly export our system to other datasets and languages. The result is a
simple but highly competitive system which obtains state of the art results
across five languages and twelve datasets. The results are reported on standard
shared task evaluation data such as CoNLL for English, Spanish and Dutch.
Furthermore, and despite the lack of linguistically motivated features, we also
report best results for languages such as Basque and German. In addition, we
demonstrate that our method also obtains very competitive results even when the
amount of supervised data is cut by half, alleviating the dependency on
manually annotated data. Finally, the results show that our emphasis on
clustering features is crucial to develop robust out-of-domain models. The
system and models are freely available to facilitate its use and guarantee the
reproducibility of results."
a4fe5d182ddee24e5bbf222d6d6996b3925060c8,1701.09123,"What are the datasets that were used for training, development, and out-of-domain evaluation in the multilingual named entity recognition experiments, including corpora such as MUC7, SONAR-1, and Ancora 2.0?","[{'answer': 'CoNLL 2003, GermEval 2014, CoNLL 2002, Egunkaria, MUC7, Wikigold, MEANTIME, SONAR-1, Ancora 2.0', 'type': 'abstractive'}]",1701.09123.pdf,"['1701.09123.pdf', '1809.06537.pdf', '1707.05236.pdf', '1905.10810.pdf', '1904.05584.pdf', '2004.04721.pdf', '1812.10479.pdf', '1910.08210.pdf', '1911.02086.pdf', '1809.00540.pdf', '1803.09230.pdf', '1810.09774.pdf', '1701.05574.pdf', '1603.07044.pdf', '1808.09920.pdf']","FLOAT SELECTED: Table 1: Datasets used for training, development and evaluation. MUC7: only three classes (LOC, ORG, PER) of the formal run are used for out-of-domain evaluation. As there are not standard partitions of SONAR-1 and Ancora 2.0, the full corpus was used for training and later evaluated in-out-of-domain settings.",Robust Multilingual Named Entity Recognition with Shallow Semi-Supervised Features,"We present a multilingual Named Entity Recognition approach based on a robust
and general set of features across languages and datasets. Our system combines
shallow local information with clustering semi-supervised features induced on
large amounts of unlabeled text. Understanding via empirical experimentation
how to effectively combine various types of clustering features allows us to
seamlessly export our system to other datasets and languages. The result is a
simple but highly competitive system which obtains state of the art results
across five languages and twelve datasets. The results are reported on standard
shared task evaluation data such as CoNLL for English, Spanish and Dutch.
Furthermore, and despite the lack of linguistically motivated features, we also
report best results for languages such as Basque and German. In addition, we
demonstrate that our method also obtains very competitive results even when the
amount of supervised data is cut by half, alleviating the dependency on
manually annotated data. Finally, the results show that our emphasis on
clustering features is crucial to develop robust out-of-domain models. The
system and models are freely available to facilitate its use and guarantee the
reproducibility of results."
5a23f436a7e0c33e4842425cf86d5fd8ba78ac92,2004.01878,What is the total number of documents in the dataset in the paper on modeling stock movements using a recurrent state transition model influenced by news events?,"[{'answer': '553,451 documents', 'type': 'abstractive'}]",2004.01878.pdf,"['2004.01878.pdf', '1909.00754.pdf', '1902.10525.pdf', '1707.05236.pdf', '1911.08962.pdf', '1911.02086.pdf', '1909.11687.pdf', '1605.07683.pdf', '1909.06162.pdf', '1809.06537.pdf', '2001.00137.pdf', '1812.10479.pdf', '2001.06286.pdf', '1909.03242.pdf', '1701.03214.pdf']",FLOAT SELECTED: Table 1: Statistics of the datasets.,News-Driven Stock Prediction With Attention-Based Noisy Recurrent State Transition,"We consider direct modeling of underlying stock value movement sequences over
time in the news-driven stock movement prediction. A recurrent state transition
model is constructed, which better captures a gradual process of stock movement
continuously by modeling the correlation between past and future price
movements. By separating the effects of news and noise, a noisy random factor
is also explicitly fitted based on the recurrent states. Results show that the
proposed model outperforms strong baselines. Thanks to the use of attention
over news events, our model is also more explainable. To our knowledge, we are
the first to explicitly model both events and noise over a fundamental stock
value state for news-driven stock movement prediction."
a616a3f0d244368ec588f04dfbc37d77fda01b4c,2003.04866,"Which 12 languages, including both major and less-resourced ones, are listed summarizing the language datasets in the Multi-SimLex lexical resource?","[{'answer': 'Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese', 'type': 'abstractive'}, {'answer': 'Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese', 'type': 'abstractive'}]",2003.04866.pdf,"['2003.04866.pdf', '1911.01680.pdf', '1901.01010.pdf', '1903.00172.pdf', '1809.06537.pdf', '1603.07044.pdf', '1901.08079.pdf', '2001.05970.pdf']",Table TABREF10 summarizes key information about the languages currently included in Multi-SimLex.,Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual Lexical Semantic Similarity,"We introduce Multi-SimLex, a large-scale lexical resource and evaluation
benchmark covering datasets for 12 typologically diverse languages, including
major languages (e.g., Mandarin Chinese, Spanish, Russian) as well as
less-resourced ones (e.g., Welsh, Kiswahili). Each language dataset is
annotated for the lexical relation of semantic similarity and contains 1,888
semantically aligned concept pairs, providing a representative coverage of word
classes (nouns, verbs, adjectives, adverbs), frequency ranks, similarity
intervals, lexical fields, and concreteness levels. Additionally, owing to the
alignment of concepts across languages, we provide a suite of 66 cross-lingual
semantic similarity datasets. Due to its extensive size and language coverage,
Multi-SimLex provides entirely novel opportunities for experimental evaluation
and analysis. On its monolingual and cross-lingual benchmarks, we evaluate and
analyze a wide array of recent state-of-the-art monolingual and cross-lingual
representation models, including static and contextualized word embeddings
(such as fastText, M-BERT and XLM), externally informed lexical
representations, as well as fully unsupervised and (weakly) supervised
cross-lingual word embeddings. We also present a step-by-step dataset creation
protocol for creating consistent, Multi-Simlex-style resources for additional
languages. We make these contributions -- the public release of Multi-SimLex
datasets, their creation protocol, strong baseline results, and in-depth
analyses which can be be helpful in guiding future developments in multilingual
lexical semantics and representation learning -- available via a website which
will encourage community effort in further expansion of Multi-Simlex to many
more languages. Such a large-scale semantic resource could inspire significant
further advances in NLP across languages."
38f58f13c7f23442d5952c8caf126073a477bac0,1904.07904,"What is the exact EM and F1 score achieved by the proposed adversarial domain adaptation model to address ASR errors, as reported in the study on mitigating ASR errors in spoken question answering?","[{'answer': 'Best results authors obtain is EM 51.10 and F1 63.11', 'type': 'abstractive'}, {'answer': 'EM Score of 51.10', 'type': 'abstractive'}]",1904.07904.pdf,"['1904.07904.pdf', '2004.04721.pdf', '1910.06036.pdf', '2003.03106.pdf', '1704.05907.pdf', '2001.08868.pdf', '1603.00968.pdf', '1902.00672.pdf', '1903.09722.pdf', '2002.01664.pdf', '1804.11346.pdf']","To better demonstrate the effectiveness of the proposed model, we compare with baselines and show the results in Table TABREF12 .",Mitigating the Impact of Speech Recognition Errors on Spoken Question Answering by Adversarial Domain Adaptation,"Spoken question answering (SQA) is challenging due to complex reasoning on
top of the spoken documents. The recent studies have also shown the
catastrophic impact of automatic speech recognition (ASR) errors on SQA.
Therefore, this work proposes to mitigate the ASR errors by aligning the
mismatch between ASR hypotheses and their corresponding reference
transcriptions. An adversarial model is applied to this domain adaptation task,
which forces the model to learn domain-invariant features the QA model can
effectively utilize in order to improve the SQA results. The experiments
successfully demonstrate the effectiveness of our proposed model, and the
results are better than the previous best model by 2% EM score."
67b66fe67a3cb2ce043070513664203e564bdcbd,1909.00175,"What baseline models are compared for pun detection and location in terms of precision, recall, and F1-score against the proposed approach?","[{'answer': 'They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF.', 'type': 'abstractive'}]",1909.00175.pdf,"['1909.00175.pdf', '1912.01772.pdf', '1805.03710.pdf', '1908.11365.pdf', '2002.01359.pdf', '1703.02507.pdf']","FLOAT SELECTED: Table 1: Comparison results on two benchmark datasets. (P.: Precision, R.: Recall, F1: F1 score.)",Joint Detection and Location of English Puns,"A pun is a form of wordplay for an intended humorous or rhetorical effect,
where a word suggests two or more meanings by exploiting polysemy (homographic
pun) or phonological similarity to another word (heterographic pun). This paper
presents an approach that addresses pun detection and pun location jointly from
a sequence labeling perspective. We employ a new tagging scheme such that the
model is capable of performing such a joint task, where useful structural
information can be properly captured. We show that our proposed model is
effective in handling both homographic and heterographic puns. Empirical
results on the benchmark datasets demonstrate that our approach can achieve new
state-of-the-art results."
d509081673f5667060400eb325a8050fa5db7cc8,1909.03135,What were the token counts for the English and Russian training corpora in the experiments comparing lemmatized and non-lemmatized inputs for word sense disambiguation?,"[{'answer': '2174000000, 989000000', 'type': 'abstractive'}, {'answer': '2174 million tokens for English and 989 million tokens for Russian', 'type': 'abstractive'}]",1909.03135.pdf,"['1909.03135.pdf', '1908.11365.pdf', '1709.10217.pdf', '1908.05434.pdf', '1808.09029.pdf', '1809.06537.pdf', '1611.04798.pdf', '1805.03710.pdf', '1912.01214.pdf']",FLOAT SELECTED: Table 1: Training corpora,To lemmatize or not to lemmatize: how word normalisation affects ELMo performance in word sense disambiguation,"We critically evaluate the widespread assumption that deep learning NLP
models do not require lemmatized input. To test this, we trained versions of
contextualised word embedding ELMo models on raw tokenized corpora and on the
corpora with word tokens replaced by their lemmas. Then, these models were
evaluated on the word sense disambiguation task. This was done for the English
and Russian languages.
  The experiments showed that while lemmatization is indeed not necessary for
English, the situation is different for Russian. It seems that for
rich-morphology languages, using lemmatized training and testing data yields
small but consistent improvements: at least for word sense disambiguation. This
means that the decisions about text pre-processing before training ELMo should
consider the linguistic nature of the language in question."
4b8a0e99bf3f2f6c80c57c0e474c47a5ee842b2c,2001.05467,"What baseline models, including LSTM, HRED, VHRED (with and without attention), and others reported in prior work, are compared to the AvgOut-based models in terms of F1 score and dialogue diversity/relevance?","[{'answer': 'LSTMs with and without attention, HRED, VHRED with and without attention, MMI and Reranking-RL', 'type': 'abstractive'}]",2001.05467.pdf,"['2001.05467.pdf', '1610.00879.pdf', '1605.07683.pdf', '2004.01980.pdf', '1909.01013.pdf', '1603.04513.pdf', '1911.03597.pdf', '2001.00137.pdf', '1910.11235.pdf', '1907.09369.pdf', '1704.08960.pdf', '1605.08675.pdf', '1910.12129.pdf', '2002.00652.pdf', '2002.06424.pdf', '1912.01673.pdf']","FLOAT SELECTED: Table 1: Automatic Evaluation Activity/Entity F1 results for baselines and our 3 models (attn means “with attention”). LSTM, HRED and VHRED are reported in Serban et al. (2017a), VHRED (attn) and Reranking-RL in Niu and Bansal (2018a), and the rest are produced by our work. All our four models have statistically significantly higher F1 values (p < 0.001) against VHRED (attn) and MMI.",AvgOut: A Simple Output-Probability Measure to Eliminate Dull Responses,"Many sequence-to-sequence dialogue models tend to generate safe,
uninformative responses. There have been various useful efforts on trying to
eliminate them. However, these approaches either improve decoding algorithms
during inference, rely on hand-crafted features, or employ complex models. In
our work, we build dialogue models that are dynamically aware of what
utterances or tokens are dull without any feature-engineering. Specifically, we
start with a simple yet effective automatic metric, AvgOut, which calculates
the average output probability distribution of all time steps on the decoder
side during training. This metric directly estimates which tokens are more
likely to be generated, thus making it a faithful evaluation of the model
diversity (i.e., for diverse models, the token probabilities should be more
evenly distributed rather than peaked at a few dull tokens). We then leverage
this novel metric to propose three models that promote diversity without losing
relevance. The first model, MinAvgOut, directly maximizes the diversity score
through the output distributions of each batch; the second model, Label
Fine-Tuning (LFT), prepends to the source sequence a label continuously scaled
by the diversity score to control the diversity level; the third model, RL,
adopts Reinforcement Learning and treats the diversity score as a reward
signal. Moreover, we experiment with a hybrid model by combining the loss terms
of MinAvgOut and RL. All four models outperform their base LSTM-RNN model on
both diversity and relevance by a large margin, and are comparable to or better
than competitive baselines (also verified via human evaluation). Moreover, our
approaches are orthogonal to the base model, making them applicable as an
add-on to other emerging better dialogue models in the future."
5e9732ff8595b31f81740082333b241d0a5f7c9a,2001.05467,"What is the reported improvement in both diversity and relevance scores for the MinAvgOut, LFT, RL, and hybrid models compared to the base LSTM-RNN?","[{'answer': 'on diversity 6.87 and on relevance 4.6 points higher', 'type': 'abstractive'}]",2001.05467.pdf,"['2001.05467.pdf', '2002.04181.pdf', '1704.06194.pdf', '1901.05280.pdf']","FLOAT SELECTED: Table 1: Automatic Evaluation Activity/Entity F1 results for baselines and our 3 models (attn means “with attention”). LSTM, HRED and VHRED are reported in Serban et al. (2017a), VHRED (attn) and Reranking-RL in Niu and Bansal (2018a), and the rest are produced by our work. All our four models have statistically significantly higher F1 values (p < 0.001) against VHRED (attn) and MMI.",AvgOut: A Simple Output-Probability Measure to Eliminate Dull Responses,"Many sequence-to-sequence dialogue models tend to generate safe,
uninformative responses. There have been various useful efforts on trying to
eliminate them. However, these approaches either improve decoding algorithms
during inference, rely on hand-crafted features, or employ complex models. In
our work, we build dialogue models that are dynamically aware of what
utterances or tokens are dull without any feature-engineering. Specifically, we
start with a simple yet effective automatic metric, AvgOut, which calculates
the average output probability distribution of all time steps on the decoder
side during training. This metric directly estimates which tokens are more
likely to be generated, thus making it a faithful evaluation of the model
diversity (i.e., for diverse models, the token probabilities should be more
evenly distributed rather than peaked at a few dull tokens). We then leverage
this novel metric to propose three models that promote diversity without losing
relevance. The first model, MinAvgOut, directly maximizes the diversity score
through the output distributions of each batch; the second model, Label
Fine-Tuning (LFT), prepends to the source sequence a label continuously scaled
by the diversity score to control the diversity level; the third model, RL,
adopts Reinforcement Learning and treats the diversity score as a reward
signal. Moreover, we experiment with a hybrid model by combining the loss terms
of MinAvgOut and RL. All four models outperform their base LSTM-RNN model on
both diversity and relevance by a large margin, and are comparable to or better
than competitive baselines (also verified via human evaluation). Moreover, our
approaches are orthogonal to the base model, making them applicable as an
add-on to other emerging better dialogue models in the future."
c17b609b0b090d7e8f99de1445be04f8f66367d4,1908.08345,"What are the ROUGE-1, ROUGE-2, and ROUGE-L scores achieved by the best-performing abstractive model on the CNN/DailyMail, XSum, and NYT datasets, as reported in the ""Text Summarization with Pretrained Encoders"" paper?","[{'answer': 'Best results on unigram:\nCNN/Daily Mail: Rogue F1 43.85\nNYT: Rogue Recall 49.02\nXSum: Rogue F1 38.81', 'type': 'abstractive'}, {'answer': 'Highest scores for ROUGE-1, ROUGE-2 and ROUGE-L on CNN/DailyMail test set are 43.85, 20.34 and 39.90 respectively; on the XSum test set 38.81, 16.50 and 31.27 and on the NYT test set 49.02, 31.02 and 45.55', 'type': 'abstractive'}]",1908.08345.pdf,"['1908.08345.pdf', '1908.11047.pdf', '1909.01247.pdf', '1809.02279.pdf', '2003.05377.pdf', '1911.01680.pdf', '1910.08987.pdf', '1910.00912.pdf', '1910.03467.pdf', '1708.09609.pdf', '2002.00652.pdf', '1909.03405.pdf', '1910.14537.pdf', '2001.06888.pdf', '2002.01359.pdf', '2004.03744.pdf', '1603.04513.pdf']",The third block in Table TABREF23 highlights the performance of several abstractive models on the CNN/DailyMail dataset (see Section SECREF6 for an overview).,Text Summarization with Pretrained Encoders,"Bidirectional Encoder Representations from Transformers (BERT) represents the
latest incarnation of pretrained language models which have recently advanced a
wide range of natural language processing tasks. In this paper, we showcase how
BERT can be usefully applied in text summarization and propose a general
framework for both extractive and abstractive models. We introduce a novel
document-level encoder based on BERT which is able to express the semantics of
a document and obtain representations for its sentences. Our extractive model
is built on top of this encoder by stacking several inter-sentence Transformer
layers. For abstractive summarization, we propose a new fine-tuning schedule
which adopts different optimizers for the encoder and the decoder as a means of
alleviating the mismatch between the two (the former is pretrained while the
latter is not). We also demonstrate that a two-staged fine-tuning approach can
further boost the quality of the generated summaries. Experiments on three
datasets show that our model achieves state-of-the-art results across the board
in both extractive and abstractive settings. Our code is available at
https://github.com/nlpyang/PreSumm"
a49832c89a2d7f95c1fe6132902d74e4e7a3f2d0,1606.00189,"Which dataset, consisting of 1,312 error-annotated sentences with 30,144 tokens and using the M2 Scorer v3.2 for evaluation, did the authors employ for grammatical error correction in their neural network translation models study?","[{'answer': 'CoNLL 2014', 'type': 'extractive'}]",1606.00189.pdf,"['1606.00189.pdf', '1801.05147.pdf', '1904.10500.pdf', '1905.10810.pdf', '1809.01202.pdf', '1910.14497.pdf', '2001.08051.pdf', '1909.00512.pdf', '1910.08987.pdf']",". The evaluation is performed similar to the CoNLL 2014 shared task setting using the the official test data of the CoNLL 2014 shared task with annotations from two annotators (without considering alternative annotations suggested by the participating teams). The test dataset consists of 1,312 error-annotated sentences with 30,144 tokens on the source side. We make use of the official scorer for the shared task, M INLINEFORM0 Scorer v3.2 BIBREF19 , for evaluation.",Neural Network Translation Models for Grammatical Error Correction,"Phrase-based statistical machine translation (SMT) systems have previously
been used for the task of grammatical error correction (GEC) to achieve
state-of-the-art accuracy. The superiority of SMT systems comes from their
ability to learn text transformations from erroneous to corrected text, without
explicitly modeling error types. However, phrase-based SMT systems suffer from
limitations of discrete word representation, linear mapping, and lack of global
context. In this paper, we address these limitations by using two different yet
complementary neural network models, namely a neural network global lexicon
model and a neural network joint model. These neural networks can generalize
better by using continuous space representation of words and learn non-linear
mappings. Moreover, they can leverage contextual information from the source
sentence more effectively. By adding these two components, we achieve
statistically significant improvement in accuracy for grammatical error
correction over a state-of-the-art GEC system."
1097768b89f8bd28d6ef6443c94feb04c1a1318e,1901.0101,"Does the joint model combining text and visual features, as described in the paper's experimental results, outperform text-only and visual-only models on both Wikipedia articles and arXiv subsets?","[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'No', 'type': 'boolean'}]",1901.01010.pdf,"['1901.01010.pdf', '2002.06424.pdf', '1903.09722.pdf', '1809.01541.pdf', '2001.10161.pdf', '1909.01383.pdf', '1804.11346.pdf', '1812.06864.pdf', '1612.08205.pdf', '1606.00189.pdf']","Experimental results show that our joint model outperforms the visual-only model in all cases, and the text-only model on Wikipedia and two subsets of arXiv.",,
fc1679c714eab822431bbe96f0e9cf4079cd8b8d,1901.0101,"What are the exact accuracy results on the Wikipedia dataset, as well as the peer-reviewed Archive AI, Archive Computation and Language, and Archive Machine Learning papers, as reported in the document quality assessment paper?","[{'answer': '59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers', 'type': 'abstractive'}]",1901.01010.pdf,"['1901.01010.pdf', '1901.03866.pdf', '1909.00361.pdf', '1912.01214.pdf', '1705.08142.pdf', '1811.01088.pdf', '1911.07228.pdf', '1911.01680.pdf', '1603.00968.pdf', '1906.03538.pdf', '2003.03014.pdf', '1612.08205.pdf']","FLOAT SELECTED: Table 1: Experimental results. The best result for each dataset is indicated in bold, and marked with “†” if it is significantly higher than the second best result (based on a one-tailed Wilcoxon signed-rank test; p < 0.05). The results of Benchmark on Peer Review are from the original paper, where the standard deviation values were not reported.",,
b13d0e463d5eb6028cdaa0c36ac7de3b76b5e933,1611.04642,"Which datasets are used to report the link prediction results for the Embedded Knowledge Graph Network (EKGN) model in the ""Link Prediction using Embedded Knowledge Graphs"" paper?","[{'answer': 'WN18 and FB15k', 'type': 'abstractive'}]",1611.04642.pdf,"['1611.04642.pdf', '2004.01980.pdf', '1603.00968.pdf', '1911.07555.pdf', '1910.04269.pdf', '1611.00514.pdf', '2003.11645.pdf', '1910.03467.pdf', '1910.11235.pdf', '1910.11204.pdf', '1808.03430.pdf']",FLOAT SELECTED: Table 1: The knowledge base completion (link prediction) results on WN18 and FB15k.,Link Prediction using Embedded Knowledge Graphs,"Since large knowledge bases are typically incomplete, missing facts need to
be inferred from observed facts in a task called knowledge base completion. The
most successful approaches to this task have typically explored explicit paths
through sequences of triples. These approaches have usually resorted to
human-designed sampling procedures, since large knowledge graphs produce
prohibitively large numbers of possible paths, most of which are uninformative.
As an alternative approach, we propose performing a single, short sequence of
interactive lookup operations on an embedded knowledge graph which has been
trained through end-to-end backpropagation to be an optimized and compressed
version of the initial knowledge base. Our proposed model, called Embedded
Knowledge Graph Network (EKGN), achieves new state-of-the-art results on
popular knowledge base completion benchmarks."
0c09a0e8f9c5bdb678563be49f912ab6e3f97619,1806.07711,"Based on the analysis of syntactic patterns in WordNet's glosses, how many distinct semantic roles are introduced?","[{'answer': '12', 'type': 'abstractive'}]",1806.07711.pdf,"['1806.07711.pdf', '1901.09755.pdf', '2003.07996.pdf', '1809.08298.pdf', '1911.01680.pdf', '1702.03342.pdf', '1909.00694.pdf', '1810.05241.pdf', '1909.06162.pdf', '1804.05918.pdf', '1910.06036.pdf', '2003.11563.pdf', '1912.00864.pdf', '1810.12085.pdf', '1810.12196.pdf', '1911.00069.pdf', '1909.05855.pdf']",FLOAT SELECTED: Table 2: Most common syntactic patterns for each semantic role.,Categorization of Semantic Roles for Dictionary Definitions,"Understanding the semantic relationships between terms is a fundamental task
in natural language processing applications. While structured resources that
can express those relationships in a formal way, such as ontologies, are still
scarce, a large number of linguistic resources gathering dictionary definitions
is becoming available, but understanding the semantic structure of natural
language definitions is fundamental to make them useful in semantic
interpretation tasks. Based on an analysis of a subset of WordNet's glosses, we
propose a set of semantic roles that compose the semantic structure of a
dictionary definition, and show how they are related to the definition's
syntactic configuration, identifying patterns that can be used in the
development of information extraction frameworks and semantic models."
ceb767e33fde4b927e730f893db5ece947ffb0d8,1810.12085,What specific HPI categories are outlined under the Annotation Instructions for the LSTM model's task of extractive summarization in this study?,"[{'answer': 'Demographics Age, DiagnosisHistory, MedicationHistory, ProcedureHistory, Symptoms/Signs, Vitals/Labs, Procedures/Results, Meds/Treatments, Movement, Other.', 'type': 'abstractive'}, {'answer': 'Demographics, Diagnosis History, Medication History, Procedure History, Symptoms, Labs, Procedures, Treatments, Hospital movements, and others', 'type': 'abstractive'}]",1810.12085.pdf,"['1810.12085.pdf', '1901.03866.pdf', '1909.08041.pdf', '1902.09314.pdf', '1909.01383.pdf', '1902.00672.pdf', '1912.00864.pdf', '2003.04642.pdf', '1608.06757.pdf', '1804.08139.pdf']",FLOAT SELECTED: Table 1. HPI Categories and Annotation Instructions,Extractive Summarization of EHR Discharge Notes,"Patient summarization is essential for clinicians to provide coordinated care
and practice effective communication. Automated summarization has the potential
to save time, standardize notes, aid clinical decision making, and reduce
medical errors. Here we provide an upper bound on extractive summarization of
discharge notes and develop an LSTM model to sequentially label topics of
history of present illness notes. We achieve an F1 score of 0.876, which
indicates that this model can be employed to create a dataset for evaluation of
extractive summarization methods."
c2cb6c4500d9e02fc9a1bdffd22c3df69655189f,1810.12085,Did the evaluation of the LSTM model on the 515 annotated history of present illness notes include a comparison with other existing extractive summarization techniques?,"[{'answer': 'No', 'type': 'boolean'}]",1810.12085.pdf,"['1810.12085.pdf', '2001.08868.pdf', '1709.10217.pdf', '1909.00105.pdf', '1909.00175.pdf', '1909.01013.pdf', '2003.04866.pdf', '1901.02257.pdf', '1603.07044.pdf', '1707.08559.pdf', '1911.01680.pdf', '1811.12254.pdf', '1910.14497.pdf', '1901.04899.pdf']","We evaluated our model on the 515 annotated history of present illness notes, which were split in a 70% train set, 15% development set, and a 15% test set.",Extractive Summarization of EHR Discharge Notes,"Patient summarization is essential for clinicians to provide coordinated care
and practice effective communication. Automated summarization has the potential
to save time, standardize notes, aid clinical decision making, and reduce
medical errors. Here we provide an upper bound on extractive summarization of
discharge notes and develop an LSTM model to sequentially label topics of
history of present illness notes. We achieve an F1 score of 0.876, which
indicates that this model can be employed to create a dataset for evaluation of
extractive summarization methods."
7772cb23b7609f1d4cfd6511ac3fcdc20f8481ba,1909.03544,"According to previous methods for POS tagging and lemmatization in Czech text processing, which were outperformed by the BERT and Flair-based approaches?","[{'answer': 'Table TABREF44, Table TABREF44, Table TABREF47, Table TABREF47', 'type': 'extractive'}]",1909.03544.pdf,"['1909.03544.pdf', '1910.06036.pdf', '1909.00105.pdf', '1912.10435.pdf', '1809.04960.pdf', '1909.11467.pdf']",The POS tagging and lemmatization results are presented in Table TABREF44.,"Czech Text Processing with Contextual Embeddings: POS Tagging, Lemmatization, Parsing and NER","Contextualized embeddings, which capture appropriate word meaning depending
on context, have recently been proposed. We evaluate two meth ods for
precomputing such embeddings, BERT and Flair, on four Czech text processing
tasks: part-of-speech (POS) tagging, lemmatization, dependency pars ing and
named entity recognition (NER). The first three tasks, POS tagging,
lemmatization and dependency parsing, are evaluated on two corpora: the Prague
Dependency Treebank 3.5 and the Universal Dependencies 2.3. The named entity
recognition (NER) is evaluated on the Czech Named Entity Corpus 1.1 and 2.0. We
report state-of-the-art results for the above mentioned tasks and corpora."
bd99aba3309da96e96eab3e0f4c4c8c70b51980a,1805.04033,"According to the ROUGE metrics, which baseline models are outperformed by the proposed regularization approach in the context of abstractive Chinese social media text summarization?","[{'answer': 'RNN-context, SRB, CopyNet, RNN-distract, DRGD', 'type': 'abstractive'}]",1805.04033.pdf,"['1805.04033.pdf', '1603.07044.pdf', '1910.12795.pdf', '1902.10525.pdf', '1904.05584.pdf', '1909.06162.pdf', '1711.11221.pdf', '1809.10644.pdf', '2003.06044.pdf', '1909.03242.pdf', '1912.01673.pdf', '1909.09484.pdf', '1606.00189.pdf', '1809.01541.pdf']",FLOAT SELECTED: Table 3. Comparisons with the Existing Models in Terms of ROUGE Metrics,Regularizing Output Distribution of Abstractive Chinese Social Media Text Summarization for Improved Semantic Consistency,"Abstractive text summarization is a highly difficult problem, and the
sequence-to-sequence model has shown success in improving the performance on
the task. However, the generated summaries are often inconsistent with the
source content in semantics. In such cases, when generating summaries, the
model selects semantically unrelated words with respect to the source content
as the most probable output. The problem can be attributed to heuristically
constructed training data, where summaries can be unrelated to the source
content, thus containing semantically unrelated words and spurious word
correspondence. In this paper, we propose a regularization approach for the
sequence-to-sequence model and make use of what the model has learned to
regularize the learning objective to alleviate the effect of the problem. In
addition, we propose a practical human evaluation method to address the problem
that the existing automatic evaluation method does not evaluate the semantic
consistency with the source content properly. Experimental results demonstrate
the effectiveness of the proposed approach, which outperforms almost all the
existing models. Especially, the proposed approach improves the semantic
consistency by 4\% in terms of human evaluation."
c180f44667505ec03214d44f4970c0db487a8bae,2001.10161,"How did participants in the evaluations on interactive fiction generation rate the neural model's performance in terms of coherence, genre resemblance, and overall quality compared to rule-based methods and human-made games across both mystery and fairy-tale genres?","[{'answer': 'the neural approach is generally preferred by a greater percentage of participants than the rules or random, human-made game outperforms them all', 'type': 'extractive'}]",2001.10161.pdf,"['2001.10161.pdf', '1801.05147.pdf', '1611.02550.pdf', '1909.00578.pdf', '1810.03459.pdf', '1604.00400.pdf', '1709.10367.pdf', '2004.01980.pdf', '1911.00069.pdf', '2002.11402.pdf', '1707.03569.pdf', '2002.02070.pdf']","We conducted two sets of human participant evaluations by recruiting participants over Amazon Mechanical Turk. The first evaluation tests the knowledge graph construction phase, in which we measure perceived coherence and genre or theme resemblance of graphs extracted by different models. The second study compares full games—including description generation and game assembly, which can't easily be isolated from graph construction—generated by different methods. This study looks at how interesting the games were to the players in addition to overall coherence and genre resemblance. Both studies are performed across two genres: mystery and fairy-tales.",Bringing Stories Alive: Generating Interactive Fiction Worlds,"World building forms the foundation of any task that requires narrative
intelligence. In this work, we focus on procedurally generating interactive
fiction worlds---text-based worlds that players ""see"" and ""talk to"" using
natural language. Generating these worlds requires referencing everyday and
thematic commonsense priors in addition to being semantically consistent,
interesting, and coherent throughout. Using existing story plots as
inspiration, we present a method that first extracts a partial knowledge graph
encoding basic information regarding world structure such as locations and
objects. This knowledge graph is then automatically completed utilizing
thematic knowledge and used to guide a neural language generation model that
fleshes out the rest of the world. We perform human participant-based
evaluations, testing our neural model's ability to extract and fill-in a
knowledge graph and to generate language conditioned on it against rule-based
and human-made baselines. Our code is available at
https://github.com/rajammanabrolu/WorldGeneration."
01f4a0a19467947a8f3bdd7ec9fac75b5222d710,1906.10225,"What specific evaluation metrics, including unlabeled INLINEFORM0 scores, are reported for assessing the performance of your compound probabilistic context-free grammar model compared to baseline models in unsupervised parsing experiments on English and Chinese?","[{'answer': 'INLINEFORM0 scores', 'type': 'extractive'}, {'answer': 'Unlabeled sentence-level F1, perplexity, grammatically judgment performance', 'type': 'abstractive'}]",1906.10225.pdf,"['1906.10225.pdf', '1908.05828.pdf', '1704.00939.pdf', '1912.13109.pdf', '1909.13714.pdf', '1611.00514.pdf', '1912.01772.pdf', '1910.06592.pdf', '1810.12085.pdf', '1804.08050.pdf', '2001.10161.pdf', '1911.03597.pdf', '2004.01980.pdf', '1805.04033.pdf', '1909.00694.pdf', '1904.09678.pdf']",Table TABREF23 shows the unlabeled INLINEFORM0 scores for our models and various baselines.,Compound Probabilistic Context-Free Grammars for Grammar Induction,"We study a formalization of the grammar induction problem that models
sentences as being generated by a compound probabilistic context-free grammar.
In contrast to traditional formulations which learn a single stochastic
grammar, our grammar's rule probabilities are modulated by a per-sentence
continuous latent variable, which induces marginal dependencies beyond the
traditional context-free assumptions. Inference in this grammar is performed by
collapsed variational inference, in which an amortized variational posterior is
placed on the continuous variable, and the latent trees are marginalized out
with dynamic programming. Experiments on English and Chinese show the
effectiveness of our approach compared to recent state-of-the-art methods when
evaluated on unsupervised parsing."
a996b6aee9be88a3db3f4127f9f77a18ed10caba,1906.1118,"What are the precision scores for semantic typing and entity matching as reported in the paper, specifically on the S-Lite and R-Lite datasets compared to the baselines?","[{'answer': '0.8320 on semantic typing, 0.7194 on entity matching', 'type': 'abstractive'}]",1906.11180.pdf,"['1906.11180.pdf', '2004.01980.pdf', '1909.08824.pdf', '2003.04866.pdf', '1910.08210.pdf']",FLOAT SELECTED: Table 3. Overall typing performance of our method and the baselines on S-Lite and R-Lite.,,
8bf7f1f93d0a2816234d36395ab40c481be9a0e0,1806.0433,Does the paper summarize any transformer-based models such as BERT or GPT for sentence pair modeling?,"[{'answer': 'No', 'type': 'boolean'}, {'answer': 'No', 'type': 'boolean'}]",1806.04330.pdf,"['1806.04330.pdf', '1902.10525.pdf', '1901.02262.pdf', '1910.04269.pdf', '2003.07996.pdf', '2003.06044.pdf', '1706.08032.pdf']","FLOAT SELECTED: Table 1: Summary of representative neural models for sentence pair modeling. The upper half contains sentence encoding models, and the lower half contains sentence pair interaction models.",,
f9bf6bef946012dd42835bf0c547c0de9c1d229f,1912.01772,How are non-standard pronunciations and mispronunciations annotated in the transcriptions of the 142-hour Mapudungun medical conversations corpus provided for computational linguistic experiments?,"[{'answer': 'Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation.', 'type': 'abstractive'}]",1912.01772.pdf,"['1912.01772.pdf', '1812.06705.pdf', '1909.00754.pdf', '1810.12885.pdf', '2002.05829.pdf', '1701.02877.pdf', '1904.10503.pdf', '1912.03457.pdf', '1603.04513.pdf', '1901.05280.pdf', '1905.00563.pdf', '1912.08960.pdf', '2002.06675.pdf', '2004.03788.pdf']","In addition, the transcription includes annotations for noises and disfluencies including aborted words, mispronunciations, poor intelligibility, repeated and corrected words, false starts, hesitations, undefined sound or pronunciations, non-verbal articulations, and pauses. Foreign words, in this case Spanish words, are also labelled as such.",A Resource for Computational Experiments on Mapudungun,"We present a resource for computational experiments on Mapudungun, a
polysynthetic indigenous language spoken in Chile with upwards of 200 thousand
speakers. We provide 142 hours of culturally significant conversations in the
domain of medical treatment. The conversations are fully transcribed and
translated into Spanish. The transcriptions also include annotations for
code-switching and non-standard pronunciations. We also provide baseline
results on three core NLP tasks: speech recognition, speech synthesis, and
machine translation between Spanish and Mapudungun. We further explore other
applications for which the corpus will be suitable, including the study of
code-switching, historical orthography change, linguistic structure, and
sociological and anthropological studies."
c2b8ee872b99f698b3d2082d57f9408a91e1b4c1,1608.06757,"Which NER systems were evaluated with micro-precision, recall, and F1 scores on datasets like CoNLL2003, KORE50, ACE2004, and MSNBC in the 'Robust Named Entity Recognition in Idiosyncratic Domains' paper?","[{'answer': 'Babelfy, DBpedia Spotlight, Entityclassifier.eu, FOX, LingPipe MUC-7, NERD-ML, Stanford NER, TagMe 2', 'type': 'abstractive'}]",1608.06757.pdf,"['1608.06757.pdf', '1908.07816.pdf', '1704.06194.pdf', '2004.01980.pdf', '1903.09588.pdf', '1704.05907.pdf', '1910.12129.pdf', '1701.02877.pdf', '1812.06864.pdf']","FLOAT SELECTED: Table 2: Comparison of annotators trained for common English news texts (micro-averaged scores on match per annotation span). The table shows micro-precision, recall and NER-style F1 for CoNLL2003, KORE50, ACE2004 and MSNBC datasets.",Robust Named Entity Recognition in Idiosyncratic Domains,"Named entity recognition often fails in idiosyncratic domains. That causes a
problem for depending tasks, such as entity linking and relation extraction. We
propose a generic and robust approach for high-recall named entity recognition.
Our approach is easy to train and offers strong generalization over diverse
domain-specific language, such as news documents (e.g. Reuters) or biomedical
text (e.g. Medline). Our approach is based on deep contextual sequence learning
and utilizes stacked bidirectional LSTM networks. Our model is trained with
only few hundred labeled sentences and does not rely on further external
knowledge. We report from our results F1 scores in the range of 84-94% on
standard datasets."
6aaf12505add25dd133c7b0dafe8f4fe966d1f1d,1808.09029,What recurrent neural network models are benchmarked against the Pyramidal Recurrent Unit to evaluate its performance on word-level language modeling?,"[{'answer': 'Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM', 'type': 'abstractive'}]",1808.09029.pdf,"['1808.09029.pdf', '1908.11546.pdf', '1905.12260.pdf', '1906.05474.pdf', '2001.05493.pdf', '1912.01772.pdf', '1704.06194.pdf', '1604.00400.pdf', '1910.07481.pdf', '1909.00754.pdf', '1909.08859.pdf']",Table TABREF23 compares the performance of the PRU with state-of-the-art methods. ,Pyramidal Recurrent Unit for Language Modeling,"LSTMs are powerful tools for modeling contextual information, as evidenced by
their success at the task of language modeling. However, modeling contexts in
very high dimensional space can lead to poor generalizability. We introduce the
Pyramidal Recurrent Unit (PRU), which enables learning representations in high
dimensional space with more generalization power and fewer parameters. PRUs
replace the linear transformation in LSTMs with more sophisticated interactions
including pyramidal and grouped linear transformations. This architecture gives
strong results on word-level language modeling while reducing the number of
parameters significantly. In particular, PRU improves the perplexity of a
recent state-of-the-art language model Merity et al. (2018) by up to 1.3 points
while learning 15-20% fewer parameters. For similar number of model parameters,
PRU outperforms all previous RNN models that exploit different gating
mechanisms and transformations. We provide a detailed examination of the PRU
and its behavior on the language modeling tasks. Our code is open-source and
available at https://sacmehta.github.io/PRU/"
14e259a312e653f8fc0d52ca5325b43c3bdfb968,1910.12129,"Was the baseline Transformer-based sequence-to-sequence model evaluated on the newly introduced video game data-to-text corpus, and what were its performance metrics, including BLEU, METEOR, ROUGE, CIDEr, and SER?","[{'answer': 'Yes, Transformer based seq2seq is evaluated with average BLEU 0.519, METEOR 0.388, ROUGE 0.631 CIDEr 2.531 and SER 2.55%.', 'type': 'abstractive'}]",1910.12129.pdf,"['1910.12129.pdf', '1908.06264.pdf', '1909.00361.pdf', '2002.04181.pdf', '2002.06644.pdf', '1812.10479.pdf', '1603.00968.pdf', '1911.04952.pdf', '1910.10288.pdf', '1804.07789.pdf', '1912.01772.pdf', '1809.02279.pdf', '1904.09678.pdf']",The NLG model we use to establish a baseline for this dataset is a standard Transformer-based BIBREF19 sequence-to-sequence model.,ViGGO: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation,"The uptake of deep learning in natural language generation (NLG) led to the
release of both small and relatively large parallel corpora for training neural
models. The existing data-to-text datasets are, however, aimed at task-oriented
dialogue systems, and often thus limited in diversity and versatility. They are
typically crowdsourced, with much of the noise left in them. Moreover, current
neural NLG models do not take full advantage of large training data, and due to
their strong generalizing properties produce sentences that look template-like
regardless. We therefore present a new corpus of 7K samples, which (1) is clean
despite being crowdsourced, (2) has utterances of 9 generalizable and
conversational dialogue act types, making it more suitable for open-domain
dialogue systems, and (3) explores the domain of video games, which is new to
dialogue systems despite having excellent potential for supporting rich
conversations."
76ed74788e3eb3321e646c48ae8bf6cdfe46dca1,2002.01207,What linguistic features are specified in the biLSTM-based Arabic diacritic recovery model for case ending reconstruction?,"[{'answer': 'POS, gender/number and stem POS', 'type': 'abstractive'}]",2002.01207.pdf,"['2002.01207.pdf', '1909.06162.pdf', '1911.10049.pdf', '1911.07228.pdf', '1810.05241.pdf']",Table TABREF17 lists the features that we used for CE recovery.,Arabic Diacritic Recovery Using a Feature-Rich biLSTM Model,"Diacritics (short vowels) are typically omitted when writing Arabic text, and
readers have to reintroduce them to correctly pronounce words. There are two
types of Arabic diacritics: the first are core-word diacritics (CW), which
specify the lexical selection, and the second are case endings (CE), which
typically appear at the end of the word stem and generally specify their
syntactic roles. Recovering CEs is relatively harder than recovering core-word
diacritics due to inter-word dependencies, which are often distant. In this
paper, we use a feature-rich recurrent neural network model that uses a variety
of linguistic and surface-level features to recover both core word diacritics
and case endings. Our model surpasses all previous state-of-the-art systems
with a CW error rate (CWER) of 2.86\% and a CE error rate (CEER) of 3.7% for
Modern Standard Arabic (MSA) and CWER of 2.2% and CEER of 2.5% for Classical
Arabic (CA). When combining diacritized word cores with case endings, the
resultant word error rate is 6.0% and 4.3% for MSA and CA respectively. This
highlights the effectiveness of feature engineering for such deep neural
models."
6ea63327ffbab2fc734dd5c2414e59d3acc56ea5,1606.0532,"How much lower is the log likelihood for LSTMs compared to HMMs across datasets, both when they have similar numbers of parameters and when the LSTM has an increased number of parameters?","[{'answer': 'With similar number of parameters, the log likelihood is about 0.1 lower for LSTMs across datasets. When the number of parameters in LSTMs is increased, their log likelihood is up to 0.7 lower.', 'type': 'abstractive'}]",1606.05320.pdf,"['1606.05320.pdf', '2004.03744.pdf', '1810.12085.pdf', '1911.04952.pdf', '1909.00279.pdf', '1908.05434.pdf', '1903.09588.pdf', '2004.03354.pdf']","FLOAT SELECTED: Table 1: Predictive loglikelihood (LL) comparison, sorted by validation set performance.",,
d484a71e23d128f146182dccc30001df35cdf93f,1909.00279,"How do the perplexity and BLEU scores of the best-performing model, using +Anti OT and +Anti UT modifications, compare to the baseline unsupervised machine translation (UMT) model?","[{'answer': 'Perplexity of the best model is 65.58 compared to best baseline 105.79.\nBleu of the best model is 6.57 compared to best baseline 5.50.', 'type': 'abstractive'}]",1909.00279.pdf,"['1909.00279.pdf', '1904.05584.pdf', '1911.10049.pdf', '1809.00540.pdf', '1909.03135.pdf', '1709.10367.pdf', '1701.09123.pdf', '1910.08210.pdf', '1806.07711.pdf', '1908.06267.pdf', '1909.00430.pdf', '1910.12795.pdf', '1910.04269.pdf', '1904.09678.pdf', '1909.08041.pdf', '1712.03556.pdf', '1912.01673.pdf', '1909.04002.pdf']","We report mean perplexity and BLEU scores in Table TABREF19 (Where +Anti OT refers to adding the reinforcement loss to mitigate over-fitting and +Anti UT refers to adding phrase segmentation-based padding to mitigate under-translation), human evaluation results in Table TABREF20.",Generating Classical Chinese Poems from Vernacular Chinese,"Classical Chinese poetry is a jewel in the treasure house of Chinese culture.
Previous poem generation models only allow users to employ keywords to
interfere the meaning of generated poems, leaving the dominion of generation to
the model. In this paper, we propose a novel task of generating classical
Chinese poems from vernacular, which allows users to have more control over the
semantic of generated poems. We adapt the approach of unsupervised machine
translation (UMT) to our task. We use segmentation-based padding and
reinforcement learning to address under-translation and over-translation
respectively. According to experiments, our approach significantly improve the
perplexity and BLEU compared with typical UMT models. Furthermore, we explored
guidelines on how to write the input vernacular to generate better poems. Human
evaluation showed our approach can generate high-quality poems which are
comparable to amateur poems."
57f23dfc264feb62f45d9a9e24c60bd73d7fe563,1811.12254,"What is the total number of samples in the augmented dataset combining AD-specific picture descriptions with multi-task healthy speech data, as detailed in the study on heterogeneous data for Alzheimer's disease detection?","[{'answer': '609', 'type': 'abstractive'}]",1811.12254.pdf,"['1811.12254.pdf', '1911.05153.pdf', '1909.02480.pdf', '1910.11204.pdf', '1909.04002.pdf', '2001.10161.pdf', '1906.03538.pdf', '1905.11901.pdf']","FLOAT SELECTED: Table 1: Speech datasets used. Note that HAPD, HAFP and FP only have samples from healthy subjects. Detailed description in App. 2.",The Effect of Heterogeneous Data for Alzheimer's Disease Detection from Speech,"Speech datasets for identifying Alzheimer's disease (AD) are generally
restricted to participants performing a single task, e.g. describing an image
shown to them. As a result, models trained on linguistic features derived from
such datasets may not be generalizable across tasks. Building on prior work
demonstrating that same-task data of healthy participants helps improve AD
detection on a single-task dataset of pathological speech, we augment an
AD-specific dataset consisting of subjects describing a picture with multi-task
healthy data. We demonstrate that normative data from multiple speech-based
tasks helps improve AD detection by up to 9%. Visualization of decision
boundaries reveals that models trained on a combination of structured picture
descriptions and unstructured conversational speech have the least out-of-task
error and show the most potential to generalize to multiple tasks. We analyze
the impact of age of the added samples and if they affect fairness in
classification. We also provide explanations for a possible inductive bias
effect across tasks using model-agnostic feature anchors. This work highlights
the need for heterogeneous datasets for encoding changes in multiple facets of
cognition and for developing a task-independent AD detection model."
307e8ab37b67202fe22aedd9a98d9d06aaa169c5,1911.07555,"Does the table, which presents LID accuracy results for South African languages, include the performance of a baseline model?","[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]",1911.07555.pdf,"['1911.07555.pdf', '1809.09795.pdf', '2002.10361.pdf', '1909.01247.pdf', '1910.02339.pdf', '1809.02286.pdf', '2004.01980.pdf', '1901.04899.pdf']",FLOAT SELECTED: Table 2: LID Accuracy Results. The models we executed ourselves are marked with *. The results that are not available from our own tests or the literature are indicated with ’—’.,Short Text Language Identification for Under Resourced Languages,"The paper presents a hierarchical naive Bayesian and lexicon based classifier
for short text language identification (LID) useful for under resourced
languages. The algorithm is evaluated on short pieces of text for the 11
official South African languages some of which are similar languages. The
algorithm is compared to recent approaches using test sets from previous works
on South African languages as well as the Discriminating between Similar
Languages (DSL) shared tasks' datasets. Remaining research opportunities and
pressing concerns in evaluating and comparing LID approaches are also
discussed."
e5c8e9e54e77960c8c26e8e238168a603fcdfcc6,1911.07555,"Did the results demonstrate that your hierarchical naive Bayesian and lexicon-based classifier achieved the highest accuracy across the South African and DSL datasets, outperforming other reported methods?","[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'From all reported results proposed method (NB+Lex) shows best accuracy on all 3 datasets - some models are not evaluated and not available in literature.', 'type': 'abstractive'}]",1911.07555.pdf,"['1911.07555.pdf', '1912.01772.pdf', '1703.06492.pdf', '2001.05970.pdf', '1908.11047.pdf', '1706.08032.pdf', '1910.06036.pdf', '1806.04330.pdf', '2002.06644.pdf', '1909.11467.pdf', '2003.11645.pdf', '1909.00578.pdf', '1903.09722.pdf', '1905.11901.pdf', '1710.09340.pdf']",FLOAT SELECTED: Table 2: LID Accuracy Results. The models we executed ourselves are marked with *. The results that are not available from our own tests or the literature are indicated with ’—’.,Short Text Language Identification for Under Resourced Languages,"The paper presents a hierarchical naive Bayesian and lexicon based classifier
for short text language identification (LID) useful for under resourced
languages. The algorithm is evaluated on short pieces of text for the 11
official South African languages some of which are similar languages. The
algorithm is compared to recent approaches using test sets from previous works
on South African languages as well as the Discriminating between Similar
Languages (DSL) shared tasks' datasets. Remaining research opportunities and
pressing concerns in evaluating and comparing LID approaches are also
discussed."
5c4c8e91d28935e1655a582568cc9d94149da2b2,1910.10288,"How do the MOS naturalness scores, with 95% confidence intervals, compare for DCA and GMM-based attention mechanisms across the Lessac and LJ datasets, and do these results indicate a performance difference?","[{'answer': 'About the same performance', 'type': 'abstractive'}]",1910.10288.pdf,"['1910.10288.pdf', '2002.04181.pdf', '2003.04642.pdf', '2002.11402.pdf', '2003.12218.pdf', '1809.10644.pdf', '1910.05154.pdf', '1903.09722.pdf', '1909.08859.pdf', '1809.00540.pdf', '1611.03382.pdf', '1908.05434.pdf', '1909.01013.pdf', '2002.10361.pdf']",FLOAT SELECTED: Table 3. MOS naturalness results along with 95% confidence intervals for the Lessac and LJ datasets.,Location-Relative Attention Mechanisms For Robust Long-Form Speech Synthesis,"Despite the ability to produce human-level speech for in-domain text,
attention-based end-to-end text-to-speech (TTS) systems suffer from text
alignment failures that increase in frequency for out-of-domain text. We show
that these failures can be addressed using simple location-relative attention
mechanisms that do away with content-based query/key comparisons. We compare
two families of attention mechanisms: location-relative GMM-based mechanisms
and additive energy-based mechanisms. We suggest simple modifications to
GMM-based attention that allow it to align quickly and consistently during
training, and introduce a new location-relative attention mechanism to the
additive energy-based family, called Dynamic Convolution Attention (DCA). We
compare the various mechanisms in terms of alignment speed and consistency
during training, naturalness, and ability to generalize to long utterances, and
conclude that GMM attention and DCA can generalize to very long utterances,
while preserving naturalness for shorter, in-domain utterances."
ab9b0bde6113ffef8eb1c39919d21e5913a05081,1707.05236,What are the F0.5 scores on the FCE dataset and both annotations of CoNLL-14 when combining machine translation-based and syntactic pattern-based error generation?,"[{'answer': 'Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. ', 'type': 'abstractive'}]",1707.05236.pdf,"['1707.05236.pdf', '1910.06748.pdf', '1908.11365.pdf', '1809.10644.pdf', '1805.03710.pdf', '1611.00514.pdf', '1911.02821.pdf', '1906.01081.pdf', '1909.11297.pdf', '1904.01608.pdf', '1908.11047.pdf', '2001.08051.pdf', '1701.09123.pdf']","The error detection results can be seen in Table TABREF4 . We use INLINEFORM0 as the main evaluation measure, which was established as the preferred measure for error correction and detection by the CoNLL-14 shared task BIBREF3 .",Artificial Error Generation with Machine Translation and Syntactic Patterns,"Shortage of available training data is holding back progress in the area of
automated error detection. This paper investigates two alternative methods for
artificially generating writing errors, in order to create additional
resources. We propose treating error generation as a machine translation task,
where grammatically correct text is translated to contain errors. In addition,
we explore a system for extracting textual patterns from an annotated corpus,
which can then be used to insert errors into grammatically correct sentences.
Our experiments show that the inclusion of artificially generated errors
significantly improves error detection accuracy on both FCE and CoNLL 2014
datasets."
fa3312ae4bbed11a5bebd77caf15d651962e0b26,1909.06937,What F1 scores did the CM-Net achieve for intent detection and slot filling on the CAIS dataset?,"[{'answer': 'F1 scores of 86.16 on slot filling and 94.56 on intent detection', 'type': 'abstractive'}]",1909.06937.pdf,"['1909.06937.pdf', '1808.03430.pdf', '1606.05320.pdf', '1909.08089.pdf', '1911.01680.pdf', '1909.02480.pdf', '1603.00968.pdf', '1904.10503.pdf', '2003.07996.pdf', '1912.01214.pdf', '1910.03814.pdf', '1701.05574.pdf']","FLOAT SELECTED: Table 6: Results on our CAIS dataset, where “†” indicates our implementation of the S-LSTM.",CM-Net: A Novel Collaborative Memory Network for Spoken Language Understanding,"Spoken Language Understanding (SLU) mainly involves two tasks, intent
detection and slot filling, which are generally modeled jointly in existing
works. However, most existing models fail to fully utilize co-occurrence
relations between slots and intents, which restricts their potential
performance. To address this issue, in this paper we propose a novel
Collaborative Memory Network (CM-Net) based on the well-designed block, named
CM-block. The CM-block firstly captures slot-specific and intent-specific
features from memories in a collaborative manner, and then uses these enriched
features to enhance local context representations, based on which the
sequential information flow leads to more specific (slot and intent) global
utterance representations. Through stacking multiple CM-blocks, our CM-Net is
able to alternately perform information exchange among specific memories, local
contexts and the global utterance, and thus incrementally enriches each other.
We evaluate the CM-Net on two standard benchmarks (ATIS and SNIPS) and a
self-collected corpus (CAIS). Experimental results show that the CM-Net
achieves the state-of-the-art results on the ATIS and SNIPS in most of
criteria, and significantly outperforms the baseline models on the CAIS.
Additionally, we make the CAIS dataset publicly available for the research
community."
26c290584c97e22b25035f5458625944db181552,1909.06937,How many utterances are present in the CAIS dataset as specified in the study on Collaborative Memory Networks (CM-Net)?,"[{'answer': '10,001 utterances', 'type': 'abstractive'}]",1909.06937.pdf,"['1909.06937.pdf', '1911.11951.pdf', '1707.00110.pdf', '1912.03457.pdf', '1902.09393.pdf']",FLOAT SELECTED: Table 2: Dataset statistics.,CM-Net: A Novel Collaborative Memory Network for Spoken Language Understanding,"Spoken Language Understanding (SLU) mainly involves two tasks, intent
detection and slot filling, which are generally modeled jointly in existing
works. However, most existing models fail to fully utilize co-occurrence
relations between slots and intents, which restricts their potential
performance. To address this issue, in this paper we propose a novel
Collaborative Memory Network (CM-Net) based on the well-designed block, named
CM-block. The CM-block firstly captures slot-specific and intent-specific
features from memories in a collaborative manner, and then uses these enriched
features to enhance local context representations, based on which the
sequential information flow leads to more specific (slot and intent) global
utterance representations. Through stacking multiple CM-blocks, our CM-Net is
able to alternately perform information exchange among specific memories, local
contexts and the global utterance, and thus incrementally enriches each other.
We evaluate the CM-Net on two standard benchmarks (ATIS and SNIPS) and a
self-collected corpus (CAIS). Experimental results show that the CM-Net
achieves the state-of-the-art results on the ATIS and SNIPS in most of
criteria, and significantly outperforms the baseline models on the CAIS.
Additionally, we make the CAIS dataset publicly available for the research
community."
b6b5f92a1d9fa623b25c70c1ac67d59d84d9eec8,1611.0255,What is the improvement in average precision of their best-performing model over previous results on the word discrimination task?,"[{'answer': 'Their best average precision tops previous best result by 0.202', 'type': 'abstractive'}]",1611.02550.pdf,"['1611.02550.pdf', '1804.08139.pdf', '1908.11546.pdf', '1804.07789.pdf', '2002.06424.pdf', '1702.03342.pdf', '2003.07723.pdf', '2002.01664.pdf', '1809.01202.pdf', '2001.06888.pdf', '1901.01010.pdf', '1910.06592.pdf', '1810.00663.pdf']","FLOAT SELECTED: Table 1: Final test set results in terms of average precision (AP). Dimensionalities marked with * refer to dimensionality per frame for DTW-based approaches. For CNN and LSTM models, results are given as means over several training runs (5 and 10, respectively) along with their standard deviations.",,
9aa52b898d029af615b95b18b79078e9bed3d766,1911.08673,What is the comparative performance of the proposed global greedy parser versus BIAF and STACKPTR in terms of both training convergence time and decoding speed?,"[{'answer': 'Proposed vs best baseline:\nDecoding: 8541 vs 8532 tokens/sec\nTraining: 8h vs 8h', 'type': 'abstractive'}]",1911.08673.pdf,"['1911.08673.pdf', '1909.00578.pdf', '1902.09314.pdf', '2004.03354.pdf', '1912.10011.pdf', '1904.07904.pdf', '1904.05584.pdf', '1901.08079.pdf', '1709.10217.pdf', '1902.09393.pdf', '1707.00110.pdf', '2004.03788.pdf']","The comparison in Table TABREF24 shows that in terms of convergence time, our model is basically the same speed as BIAF, while STACKPTR is much slower. For decoding, our model is the fastest, followed by BIAF. STACKPTR is unexpectedly the slowest.",Global Greedy Dependency Parsing,"Most syntactic dependency parsing models may fall into one of two categories:
transition- and graph-based models. The former models enjoy high inference
efficiency with linear time complexity, but they rely on the stacking or
re-ranking of partially-built parse trees to build a complete parse tree and
are stuck with slower training for the necessity of dynamic oracle training.
The latter, graph-based models, may boast better performance but are
unfortunately marred by polynomial time inference. In this paper, we propose a
novel parsing order objective, resulting in a novel dependency parsing model
capable of both global (in sentence scope) feature extraction as in graph
models and linear time inference as in transitional models. The proposed global
greedy parser only uses two arc-building actions, left and right arcs, for
projective parsing. When equipped with two extra non-projective arc-building
actions, the proposed parser may also smoothly support non-projective parsing.
Using multiple benchmark treebanks, including the Penn Treebank (PTB), the
CoNLL-X treebanks, and the Universal Dependency Treebanks, we evaluate our
parser and demonstrate that the proposed novel parser achieves good performance
with faster training and decoding."
25e4dbc7e211a1ebe02ee8dff675b846fb18fdc5,1704.0896,What external datasets are provided for pretraining the neural word segmentation model in this study?,"[{'answer': ""Raw data from Gigaword, Automatically segmented text from Gigaword, Heterogenous training data from People's Daily, POS data from People's Daily"", 'type': 'abstractive'}]",1704.08960.pdf,"['1704.08960.pdf', '1910.13215.pdf', '2003.11645.pdf', '2002.06424.pdf', '2001.06286.pdf', '1804.07789.pdf', '1608.06757.pdf', '2003.04866.pdf', '2004.04721.pdf', '1910.06592.pdf', '1904.10500.pdf', '1910.06748.pdf', '1908.06083.pdf']",FLOAT SELECTED: Table 3: Statistics of external data.,,
c9305e5794b65b33399c22ac8e4e024f6b757a30,1909.06162,"What are the precision, recall, and F1-score values of the best-performing teams in the sentence-level classification (SLC) and fragment-level classification (FLC) tasks as reported in the MIC-CIS system's performance comparison?","[{'answer': 'For SLC task, the ""ltuorp"" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the ""newspeak"" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively).', 'type': 'abstractive'}]",1909.06162.pdf,"['1909.06162.pdf', '1612.08205.pdf', '1912.08960.pdf', '1809.09795.pdf', '1812.10479.pdf', '1901.08079.pdf', '1901.01010.pdf', '2002.11910.pdf', '1902.00672.pdf', '1910.14537.pdf', '1909.08824.pdf']",FLOAT SELECTED: Table 2: Comparison of our system (MIC-CIS) with top-5 participants: Scores on Test set for SLC and FLC,Neural Architectures for Fine-Grained Propaganda Detection in News,"This paper describes our system (MIC-CIS) details and results of
participation in the fine-grained propaganda detection shared task 2019. To
address the tasks of sentence (SLC) and fragment level (FLC) propaganda
detection, we explore different neural architectures (e.g., CNN, LSTM-CRF and
BERT) and extract linguistic (e.g., part-of-speech, named entity, readability,
sentiment, emotion, etc.), layout and topical features. Specifically, we have
designed multi-granularity and multi-tasking neural architectures to jointly
perform both the sentence and fragment level propaganda detection.
Additionally, we investigate different ensemble schemes such as
majority-voting, relax-voting, etc. to boost overall system performance.
Compared to the other participating systems, our submissions are ranked 3rd and
4th in FLC and SLC tasks, respectively."
56b7319be68197727baa7d498fa38af0a8440fe4,1909.06162,"What extracted linguistic features, combined with BERT, had the most significant influence on performance in the sentence-level classification (SLC) task in the MIC-CIS system's evaluation for fine-grained propaganda detection?","[{'answer': 'Linguistic', 'type': 'abstractive'}, {'answer': 'BERT', 'type': 'extractive'}]",1909.06162.pdf,"['1909.06162.pdf', '1901.02257.pdf', '1807.07961.pdf', '1902.09393.pdf', '1809.05752.pdf', '1912.03457.pdf', '1705.08142.pdf']",FLOAT SELECTED: Table 3: SLC: Scores on Dev (internal) of Fold1 and Dev (external) using different classifiers and features.,Neural Architectures for Fine-Grained Propaganda Detection in News,"This paper describes our system (MIC-CIS) details and results of
participation in the fine-grained propaganda detection shared task 2019. To
address the tasks of sentence (SLC) and fragment level (FLC) propaganda
detection, we explore different neural architectures (e.g., CNN, LSTM-CRF and
BERT) and extract linguistic (e.g., part-of-speech, named entity, readability,
sentiment, emotion, etc.), layout and topical features. Specifically, we have
designed multi-granularity and multi-tasking neural architectures to jointly
perform both the sentence and fragment level propaganda detection.
Additionally, we investigate different ensemble schemes such as
majority-voting, relax-voting, etc. to boost overall system performance.
Compared to the other participating systems, our submissions are ranked 3rd and
4th in FLC and SLC tasks, respectively."
2268c9044e868ba0a16e92d2063ada87f68b5d03,1909.06162,"What was the F1 score difference between the best performing ensemble and the top individual classifier for Sentence and Fragment Level Classification (SLC and FLC) in the MIC-CIS system, according to the reported results?","[{'answer': 'The best ensemble topped the best single model by 0.029 in F1 score on dev (external).', 'type': 'abstractive'}, {'answer': 'They increased F1 Score by 0.029 in Sentence Level Classification, and by 0.044 in Fragment-Level classification', 'type': 'abstractive'}]",1909.06162.pdf,"['1909.06162.pdf', '2003.03014.pdf', '1809.00540.pdf', '2002.01207.pdf', '1911.08976.pdf', '1810.05241.pdf', '1804.05918.pdf', '1804.00079.pdf', '1902.09393.pdf', '1908.06083.pdf', '1811.12254.pdf', '1703.02507.pdf', '1909.11297.pdf', '1908.07195.pdf']",FLOAT SELECTED: Table 3: SLC: Scores on Dev (internal) of Fold1 and Dev (external) using different classifiers and features.,Neural Architectures for Fine-Grained Propaganda Detection in News,"This paper describes our system (MIC-CIS) details and results of
participation in the fine-grained propaganda detection shared task 2019. To
address the tasks of sentence (SLC) and fragment level (FLC) propaganda
detection, we explore different neural architectures (e.g., CNN, LSTM-CRF and
BERT) and extract linguistic (e.g., part-of-speech, named entity, readability,
sentiment, emotion, etc.), layout and topical features. Specifically, we have
designed multi-granularity and multi-tasking neural architectures to jointly
perform both the sentence and fragment level propaganda detection.
Additionally, we investigate different ensemble schemes such as
majority-voting, relax-voting, etc. to boost overall system performance.
Compared to the other participating systems, our submissions are ranked 3rd and
4th in FLC and SLC tasks, respectively."
6b7354d7d715bad83183296ce2f3ddf2357cb449,1909.06162,Which neural architecture achieved the highest performance for sentence-level (SLC) propaganda detection on the internal Fold1 and external Dev sets?,"[{'answer': 'BERT', 'type': 'abstractive'}]",1909.06162.pdf,"['1909.06162.pdf', '1809.06537.pdf', '2002.00652.pdf', '1910.02339.pdf', '1909.00578.pdf', '2003.08385.pdf', '1812.01704.pdf', '2001.06888.pdf', '1910.12795.pdf']",FLOAT SELECTED: Table 3: SLC: Scores on Dev (internal) of Fold1 and Dev (external) using different classifiers and features.,Neural Architectures for Fine-Grained Propaganda Detection in News,"This paper describes our system (MIC-CIS) details and results of
participation in the fine-grained propaganda detection shared task 2019. To
address the tasks of sentence (SLC) and fragment level (FLC) propaganda
detection, we explore different neural architectures (e.g., CNN, LSTM-CRF and
BERT) and extract linguistic (e.g., part-of-speech, named entity, readability,
sentiment, emotion, etc.), layout and topical features. Specifically, we have
designed multi-granularity and multi-tasking neural architectures to jointly
perform both the sentence and fragment level propaganda detection.
Additionally, we investigate different ensemble schemes such as
majority-voting, relax-voting, etc. to boost overall system performance.
Compared to the other participating systems, our submissions are ranked 3rd and
4th in FLC and SLC tasks, respectively."
e949b28f6d1f20e18e82742e04d68158415dc61e,1909.06162,Which participating teams achieved higher scores than the MIC-CIS system in both the SLC and FLC tasks?,"[{'answer': 'For SLC task : Ituorp, ProperGander and YMJA  teams had better results.\nFor FLC task: newspeak and Antiganda teams had better results.', 'type': 'abstractive'}]",1909.06162.pdf,"['1909.06162.pdf', '2002.06675.pdf', '1909.09484.pdf', '2002.11402.pdf', '2003.03044.pdf', '1701.05574.pdf', '1807.07961.pdf', '1908.05828.pdf', '2004.01980.pdf', '1911.02711.pdf', '1906.10225.pdf', '1809.00530.pdf', '1812.06705.pdf']",FLOAT SELECTED: Table 2: Comparison of our system (MIC-CIS) with top-5 participants: Scores on Test set for SLC and FLC,Neural Architectures for Fine-Grained Propaganda Detection in News,"This paper describes our system (MIC-CIS) details and results of
participation in the fine-grained propaganda detection shared task 2019. To
address the tasks of sentence (SLC) and fragment level (FLC) propaganda
detection, we explore different neural architectures (e.g., CNN, LSTM-CRF and
BERT) and extract linguistic (e.g., part-of-speech, named entity, readability,
sentiment, emotion, etc.), layout and topical features. Specifically, we have
designed multi-granularity and multi-tasking neural architectures to jointly
perform both the sentence and fragment level propaganda detection.
Additionally, we investigate different ensemble schemes such as
majority-voting, relax-voting, etc. to boost overall system performance.
Compared to the other participating systems, our submissions are ranked 3rd and
4th in FLC and SLC tasks, respectively."
6cd25c637c6b772ce29e8ee81571e8694549c5ab,1804.07789,"Which datasets, including any non-English ones, are discussed in the performance comparison of models for generating one-line biographical descriptions from structured tables?","[{'answer': 'English WIKIBIO, French WIKIBIO , German WIKIBIO ', 'type': 'abstractive'}, {'answer': 'WikiBio dataset,  introduce two new biography datasets, one in French and one in German', 'type': 'extractive'}]",1804.07789.pdf,"['1804.07789.pdf', '2002.04181.pdf', '2004.04721.pdf', '1908.07245.pdf', '1809.02286.pdf', '1909.00694.pdf']",FLOAT SELECTED: Table 1: Comparison of different models on the English WIKIBIO dataset,Generating Descriptions from Structured Data Using a Bifocal Attention Mechanism and Gated Orthogonalization,"In this work, we focus on the task of generating natural language
descriptions from a structured table of facts containing fields (such as
nationality, occupation, etc) and values (such as Indian, actor, director,
etc). One simple choice is to treat the table as a sequence of fields and
values and then use a standard seq2seq model for this task. However, such a
model is too generic and does not exploit task-specific characteristics. For
example, while generating descriptions from a table, a human would attend to
information at two levels: (i) the fields (macro level) and (ii) the values
within the field (micro level). Further, a human would continue attending to a
field for a few timesteps till all the information from that field has been
rendered and then never return back to this field (because there is nothing
left to say about it). To capture this behavior we use (i) a fused bifocal
attention mechanism which exploits and combines this micro and macro level
information and (ii) a gated orthogonalization mechanism which tries to ensure
that a field is remembered for a few time steps and then forgotten. We
experiment with a recently released dataset which contains fact tables about
people and their corresponding one line biographical descriptions in English.
In addition, we also introduce two similar datasets for French and German. Our
experiments show that the proposed model gives 21% relative improvement over a
recently proposed state of the art method and 10% relative improvement over
basic seq2seq models. The code and the datasets developed as a part of this
work are publicly available."
d53299fac8c94bd0179968eb868506124af407d1,2002.0207,"Which classifier achieved the highest F1 micro and F1 macro scores during 4-fold cross-validation on the shuffled data set, and what explanation is given for its superior performance?","[{'answer': 'Table TABREF10,  The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set,  While these classifiers did not perform particularly well, they provide a good starting point for future work on this subject', 'type': 'extractive'}, {'answer': 'Using F1 Micro measure, the KNN classifier perform 0.6762, the RF 0.6687, SVM 0.6712 and MLP 0.6778.', 'type': 'abstractive'}]",2002.02070.pdf,"['2002.02070.pdf', '1701.03214.pdf', '1805.04033.pdf', '1912.00864.pdf', '1709.05413.pdf', '1910.00912.pdf', '1904.10503.pdf', '1908.07245.pdf', '1912.01673.pdf', '1809.01202.pdf', '1707.05236.pdf', '1809.04960.pdf', '1911.03310.pdf', '1901.02262.pdf', '1809.06537.pdf', '1804.08050.pdf', '1906.11180.pdf', '1701.00185.pdf']","In order to evaluate our classifiers, we perform 4-fold cross validation on a shuffled data set. Table TABREF10 shows the F1 micro and F1 macro scores for all the classifiers. The KNN classifier seem to perform the best across all four metrics. This is probably due to the multi-class nature of the data set.",,
29f2954098f055fb19d9502572f085862d75bf61,2002.0207,"Based on the study, which machine learning classifiers were evaluated on classifying dealership interaction data?","[{'answer': 'KNN\nRF\nSVM\nMLP', 'type': 'abstractive'}, {'answer': ' K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Multi-layer Perceptron (MLP)', 'type': 'extractive'}]",2002.02070.pdf,"['2002.02070.pdf', '1909.00430.pdf', '1707.08559.pdf', '1711.11221.pdf', '1711.00106.pdf', '1608.06757.pdf']",FLOAT SELECTED: Table 2: Evaluation metrics for all classifiers.,,
63496705fff20c55d4b3d8cdf4786f93e742dd3d,1605.08675,"Does the RAFAEL system paper provide a comparative analysis of question-answering accuracy between DeepER, traditional NER methods (like Nerf and Liner2), and hybrid entity recognition strategies?","[{'answer': 'Yes', 'type': 'boolean'}]",1605.08675.pdf,"['1605.08675.pdf', '1908.05434.pdf', '1910.14537.pdf', '1705.08142.pdf']","FLOAT SELECTED: Table 3. Question answering accuracy of RAFAEL with different entity recognition strategies: quantities only (Quant), traditional NER (Nerf, Liner2 ), deep entity recognition (DeepER) and their combination (Hybrid).",Boosting Question Answering by Deep Entity Recognition,"In this paper an open-domain factoid question answering system for Polish,
RAFAEL, is presented. The system goes beyond finding an answering sentence; it
also extracts a single string, corresponding to the required entity. Herein the
focus is placed on different approaches to entity recognition, essential for
retrieving information matching question constraints. Apart from traditional
approach, including named entity recognition (NER) solutions, a novel
technique, called Deep Entity Recognition (DeepER), is introduced and
implemented. It allows a comprehensive search of all forms of entity references
matching a given WordNet synset (e.g. an impressionist), based on a previously
assembled entity library. It has been created by analysing the first sentences
of encyclopaedia entries and disambiguation and redirect pages. DeepER also
provides automatic evaluation, which makes possible numerous experiments,
including over a thousand questions from a quiz TV show answered on the grounds
of Polish Wikipedia. The final results of a manual evaluation on a separate
question set show that the strength of DeepER approach lies in its ability to
answer questions that demand answers beyond the traditional categories of named
entities."
d3dbb5c22ef204d85707d2d24284cc77fa816b6c,1809.09194,What models are evaluated in the paper's experimental results when comparing the extended Stochastic Answer Network (SAN) to baseline and state-of-the-art methods on SQuAD 2.0?,"[{'answer': 'SAN Baseline, BNA, DocQA, R.M-Reader, R.M-Reader+Verifier and DocQA+ELMo', 'type': 'abstractive'}, {'answer': 'BNA, DocQA, R.M-Reader, R.M-Reader + Verifier, DocQA + ELMo, R.M-Reader+Verifier+ELMo', 'type': 'abstractive'}]",1809.09194.pdf,"['1809.09194.pdf', '1901.03866.pdf', '1704.06194.pdf', '1711.11221.pdf', '1909.09484.pdf', '1608.06757.pdf', '1909.03405.pdf', '1910.13215.pdf', '1911.07228.pdf', '1904.07904.pdf', '1901.01010.pdf', '1911.13066.pdf', '1910.07481.pdf', '1908.07195.pdf', '1809.10644.pdf', '2002.06644.pdf', '1701.05574.pdf']",Table TABREF21 reports comparison results in literature published .,Stochastic Answer Networks for SQuAD 2.0,"This paper presents an extension of the Stochastic Answer Network (SAN), one
of the state-of-the-art machine reading comprehension models, to be able to
judge whether a question is unanswerable or not. The extended SAN contains two
components: a span detector and a binary classifier for judging whether the
question is unanswerable, and both components are jointly optimized.
Experiments show that SAN achieves the results competitive to the
state-of-the-art on Stanford Question Answering Dataset (SQuAD) 2.0. To
facilitate the research on this field, we release our code:
https://github.com/kevinduh/san_mrc."
e9d9bb87a5c4faa965ceddd98d8b80d4b99e339e,1903.09588,"What is the F1 score improvement of the BERT-pair models, specifically BERT-pair-NLI-B for aspect category detection and BERT-pair-QA-B for aspect category polarity (in 4-way, 3-way, and binary settings) on the SemEval-2014 dataset, compared to prior state-of-the-art results?","[{'answer': 'On subtask 3 best proposed model has F1 score of 92.18 compared to best previous F1 score of 88.58.\nOn subtask 4 best proposed model has 85.9, 89.9 and 95.6 compared to best previous results of 82.9, 84.0 and 89.9 on 4-way, 3-way and binary aspect polarity.', 'type': 'abstractive'}]",1903.09588.pdf,"['1903.09588.pdf', '1609.00559.pdf', '1701.02877.pdf', '1909.08824.pdf']","Results on SemEval-2014 are presented in Table TABREF35 and Table TABREF36 . We find that BERT-single has achieved better results on these two subtasks, and BERT-pair has achieved further improvements over BERT-single. The BERT-pair-NLI-B model achieves the best performance for aspect category detection. For aspect category polarity, BERT-pair-QA-B performs best on all 4-way, 3-way, and binary settings.",Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence,"Aspect-based sentiment analysis (ABSA), which aims to identify fine-grained
opinion polarity towards a specific aspect, is a challenging subtask of
sentiment analysis (SA). In this paper, we construct an auxiliary sentence from
the aspect and convert ABSA to a sentence-pair classification task, such as
question answering (QA) and natural language inference (NLI). We fine-tune the
pre-trained model from BERT and achieve new state-of-the-art results on
SentiHood and SemEval-2014 Task 4 datasets."
f8c1b17d265a61502347c9a937269b38fc3fcab1,1909.00015,"What are the BLEU score differences between the baseline Transformer, 1.5-entmax, and α-entmax methods on the DE-EN, JA-EN, RO-EN, and EN-DE datasets as reported in the adaptively sparse Transformer paper?","[{'answer': 'On the datasets DE-EN, JA-EN, RO-EN, and EN-DE, the baseline achieves 29.79, 21.57, 32.70, and 26.02  BLEU score, respectively. The 1.5-entmax achieves  29.83, 22.13, 33.10, and 25.89 BLEU score, which is a difference of +0.04, +0.56, +0.40, and -0.13 BLEU score versus the baseline. The α-entmax achieves 29.90, 21.74, 32.89, and 26.93 BLEU score, which is a difference of +0.11, +0.17, +0.19, +0.91 BLEU score versus the baseline.', 'type': 'abstractive'}]",1909.00015.pdf,"['1909.00015.pdf', '1901.05280.pdf', '1810.12196.pdf', '1909.00252.pdf', '1909.00361.pdf', '1609.00559.pdf', '1802.06024.pdf', '1909.00694.pdf', '1910.00458.pdf', '1910.13215.pdf', '1909.00105.pdf', '1904.09678.pdf', '1807.07961.pdf', '2002.06424.pdf', '1908.06083.pdf', '1910.10288.pdf']","FLOAT SELECTED: Table 1: Machine translation tokenized BLEU test results on IWSLT 2017 DE EN, KFTT JA EN, WMT 2016 RO EN and WMT 2014 EN DE, respectively.",Adaptively Sparse Transformers,"Attention mechanisms have become ubiquitous in NLP. Recent architectures,
notably the Transformer, learn powerful context-aware word representations
through layered, multi-headed attention. The multiple heads learn diverse types
of word relationships. However, with standard softmax attention, all attention
heads are dense, assigning a non-zero weight to all context words. In this
work, we introduce the adaptively sparse Transformer, wherein attention heads
have flexible, context-dependent sparsity patterns. This sparsity is
accomplished by replacing softmax with $\alpha$-entmax: a differentiable
generalization of softmax that allows low-scoring words to receive precisely
zero weight. Moreover, we derive a method to automatically learn the $\alpha$
parameter -- which controls the shape and sparsity of $\alpha$-entmax --
allowing attention heads to choose between focused or spread-out behavior. Our
adaptively sparse Transformer improves interpretability and head diversity when
compared to softmax Transformers on machine translation datasets. Findings of
the quantitative and qualitative analysis of our approach include that heads in
different layers learn different sparsity preferences and tend to be more
diverse in their attention distributions than softmax Transformers.
Furthermore, at no cost in accuracy, sparsity in attention heads helps to
uncover different head specializations."
907b3af3cfaf68fe188de9467ed1260e52ec6cf1,1712.05999,"What were the meta-data features, such as Followers, Friends, and URLs, that showed significant differences in distribution between fake news and non-fake news tweets, according to the Kolmogorov-Smirnov test results?","[{'answer': 'Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different', 'type': 'abstractive'}]",1712.05999.pdf,"['1712.05999.pdf', '1909.00105.pdf', '1810.09774.pdf', '1707.08559.pdf', '1704.05907.pdf', '1912.10435.pdf', '1909.00754.pdf', '1910.11204.pdf', '1909.13714.pdf']","FLOAT SELECTED: Table 1: For each one of the selected features, the table shows the difference between the set of tweets containing fake news and those non containing them, and the associated p-value (applying a KolmogorovSmirnov test). The null hypothesis is that both distributions are equal (two sided). Results are ordered by decreasing p-value.",Characterizing Political Fake News in Twitter by its Meta-Data,"This article presents a preliminary approach towards characterizing political
fake news on Twitter through the analysis of their meta-data. In particular, we
focus on more than 1.5M tweets collected on the day of the election of Donald
Trump as 45th president of the United States of America. We use the meta-data
embedded within those tweets in order to look for differences between tweets
containing fake news and tweets not containing them. Specifically, we perform
our analysis only on tweets that went viral, by studying proxies for users'
exposure to the tweets, by characterizing accounts spreading fake news, and by
looking at their polarization. We found significant differences on the
distribution of followers, the number of URLs on tweets, and the verification
of the users."
37e8f5851133a748c4e3e0beeef0d83883117a98,1910.0821,"What are the final win rates of the proposed txt2$\pi$, CNN, and FiLM models on the evaluation set with new environment dynamics for the simplest RTFM variant?","[{'answer': 'Proposed model achive 66+-22 win rate, baseline CNN 13+-1  and baseline FiLM 32+-3 .', 'type': 'abstractive'}]",1910.08210.pdf,"['1910.08210.pdf', '1909.13375.pdf', '1705.01214.pdf', '2001.08868.pdf', '2001.10161.pdf', '2002.06424.pdf', '1911.02821.pdf', '1910.11769.pdf', '1808.03430.pdf', '1911.08673.pdf', '2004.03788.pdf', '1910.06748.pdf', '1912.01772.pdf', '1903.09588.pdf', '1909.09484.pdf', '1909.08859.pdf']",FLOAT SELECTED: Table 1: Final win rate on simplest variant of RTFM. The models are trained on one set of dynamics (e.g. training set) and evaluated on another set of dynamics (e.g. evaluation set). “Train” and “Eval” show final win rates on training and eval environments.,,
52f8a3e3cd5d42126b5307adc740b71510a6bdf5,1810.12196,"What tasks were the models evaluated on in ReviewQA's test set, specifically relating to aspect detection, rating predictions, and opinion analysis?","[{'answer': ""ReviewQA's test set"", 'type': 'extractive'}, {'answer': 'Detection of an aspect in a review, Prediction of the customer general satisfaction, Prediction of the global trend of an aspect in a given review, Prediction of whether the rating of a given aspect is above or under a given value, Prediction of the exact rating of an aspect in a review, Prediction of the list of all the positive/negative aspects mentioned in the review, Comparison between aspects, Prediction of the strengths and weaknesses in a review', 'type': 'abstractive'}]",1810.12196.pdf,"['1810.12196.pdf', '1901.05280.pdf', '1909.01247.pdf', '1908.06264.pdf', '1909.00430.pdf', '1910.03814.pdf', '1603.07044.pdf', '1910.02339.pdf', '2002.06424.pdf']",Table TABREF19 displays the performance of the 4 baselines on the ReviewQA's test set. These results are the performance achieved by our own implementation of these 4 models.,ReviewQA: a relational aspect-based opinion reading dataset,"Deep reading models for question-answering have demonstrated promising
performance over the last couple of years. However current systems tend to
learn how to cleverly extract a span of the source document, based on its
similarity with the question, instead of seeking for the appropriate answer.
Indeed, a reading machine should be able to detect relevant passages in a
document regarding a question, but more importantly, it should be able to
reason over the important pieces of the document in order to produce an answer
when it is required. To motivate this purpose, we present ReviewQA, a
question-answering dataset based on hotel reviews. The questions of this
dataset are linked to a set of relational understanding competencies that we
expect a model to master. Indeed, each question comes with an associated type
that characterizes the required competency. With this framework, it is possible
to benchmark the main families of models and to get an overview of what are the
strengths and the weaknesses of a given model on the set of tasks evaluated in
this dataset. Our corpus contains more than 500.000 questions in natural
language over 100.000 hotel reviews. Our setup is projective, the answer of a
question does not need to be extracted from a document, like in most of the
recent datasets, but selected among a set of candidates that contains all the
possible answers to the questions of the dataset. Finally, we present several
baselines over this dataset."
c034f38a570d40360c3551a6469486044585c63c,1908.07816,"What is the test set perplexity of the proposed MEED model, and how does it compare to the next-best baseline?","[{'answer': 'Perplexity of proposed MEED model is 19.795 vs 19.913 of next best result on test set.', 'type': 'abstractive'}]",1908.07816.pdf,"['1908.07816.pdf', '1811.01088.pdf', '1809.02286.pdf', '1605.08675.pdf', '2002.01359.pdf', '1810.06743.pdf', '1909.00252.pdf', '1611.03382.pdf', '1701.03214.pdf', '1912.10011.pdf', '1804.08050.pdf', '1811.12254.pdf', '1704.05907.pdf', '1705.01214.pdf']","Table TABREF34 gives the perplexity scores obtained by the three models on the two validation sets and the test set. As shown in the table, MEED achieves the lowest perplexity score on all three sets.",A Multi-Turn Emotionally Engaging Dialog Model,"Open-domain dialog systems (also known as chatbots) have increasingly drawn
attention in natural language processing. Some of the recent work aims at
incorporating affect information into sequence-to-sequence neural dialog
modeling, making the response emotionally richer, while others use hand-crafted
rules to determine the desired emotion response. However, they do not
explicitly learn the subtle emotional interactions captured in human dialogs.
In this paper, we propose a multi-turn dialog system aimed at learning and
generating emotional responses that so far only humans know how to do. Compared
with two baseline models, offline experiments show that our method performs the
best in perplexity scores. Further human evaluations confirm that our chatbot
can keep track of the conversation context and generate emotionally more
appropriate responses while performing equally well on grammar."
0ad4359e3e7e5e5f261c2668fe84c12bc762b3b8,1809.02286,"Which baseline models, including tree-structured models and non-tree models, were used for comparison when evaluating the performance of the SATA Tree-LSTM on sentence classification tasks?","[{'answer': 'Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks', 'type': 'abstractive'}, {'answer': 'Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018).', 'type': 'abstractive'}]",1809.02286.pdf,"['1809.02286.pdf', '1911.08962.pdf', '1910.11769.pdf', '1904.09678.pdf', '1901.01010.pdf', '1901.05280.pdf', '1603.07044.pdf', '1910.11204.pdf', '1808.09029.pdf', '1608.06757.pdf', '1904.05584.pdf', '1605.08675.pdf']","FLOAT SELECTED: Table 1: The comparison of various models on different sentence classification tasks. We report the test accuracy of each model in percentage. Our SATA Tree-LSTM shows superior or competitive performance on all tasks, compared to previous treestructured models as well as other sophisticated models. ?: Latent tree-structured models. †: Models which are pre-trained with large external corpora.",Dynamic Compositionality in Recursive Neural Networks with Structure-aware Tag Representations,"Most existing recursive neural network (RvNN) architectures utilize only the
structure of parse trees, ignoring syntactic tags which are provided as
by-products of parsing. We present a novel RvNN architecture that can provide
dynamic compositionality by considering comprehensive syntactic information
derived from both the structure and linguistic tags. Specifically, we introduce
a structure-aware tag representation constructed by a separate tag-level
tree-LSTM. With this, we can control the composition function of the existing
word-level tree-LSTM by augmenting the representation as a supplementary input
to the gate functions of the tree-LSTM. In extensive experiments, we show that
models built upon the proposed architecture obtain superior or competitive
performance on several sentence-level tasks such as sentiment analysis and
natural language inference when compared against previous tree-structured
models and other sophisticated neural models."
94e0cf44345800ef46a8c7d52902f074a1139e1a,1701.02877,"Which NER datasets are used to analyze model generalization in diverse domains such as newswire, broadcast conversation, Web content, and social media?","[{'answer': 'MUC, CoNLL, ACE, OntoNotes, MSM, Ritter, UMBC', 'type': 'abstractive'}]",1701.02877.pdf,"['1701.02877.pdf', '1909.03135.pdf', '1910.10288.pdf', '2003.11563.pdf', '1910.06748.pdf', '1909.00361.pdf', '1911.12579.pdf', '2002.01359.pdf', '1712.00991.pdf', '1909.01247.pdf', '1703.02507.pdf', '1909.13714.pdf', '2002.06424.pdf', '1701.00185.pdf', '1604.00400.pdf', '1612.08205.pdf']","Since the goal of this study is to compare NER performance on corpora from diverse domains and genres, seven benchmark NER corpora are included, spanning newswire, broadcast conversation, Web content, and social media (see Table 1 for details).",Generalisation in Named Entity Recognition: A Quantitative Analysis,"Named Entity Recognition (NER) is a key NLP task, which is all the more
challenging on Web and user-generated content with their diverse and
continuously changing language. This paper aims to quantify how this diversity
impacts state-of-the-art NER methods, by measuring named entity (NE) and
context variability, feature sparsity, and their effects on precision and
recall. In particular, our findings indicate that NER approaches struggle to
generalise in diverse genres with limited training data. Unseen NEs, in
particular, play an important role, which have a higher incidence in diverse
genres such as social media than in more regular genres such as newswire.
Coupled with a higher incidence of unseen features more generally and the lack
of large training corpora, this leads to significantly lower F1 scores for
diverse genres as compared to more regular ones. We also find that leading
systems rely heavily on surface forms found in training data, having problems
generalising beyond these, and offer explanations for this observation."
8ad815b29cc32c1861b77de938c7269c9259a064,1910.06748,What are the language labels included in the Twitter corpus distribution in the paper's language identification system evaluation?,"[{'answer': 'EN, JA, ES, AR, PT, KO, TH, FR, TR, RU, IT, DE, PL, NL, EL, SV, FA, VI, FI, CS, UK, HI, DA, HU, NO, RO, SR, LV, BG, UR, TA, MR, BN, IN, KN, ET, SL, GU, CY, ZH, CKB, IS, LT, ML, SI, IW, NE, KM, MY, TL, KA, BO', 'type': 'abstractive'}]",1910.06748.pdf,"['1910.06748.pdf', '1911.01680.pdf', '2003.08385.pdf', '1911.02711.pdf']",FLOAT SELECTED: Table 2. Twitter corpus distribution by language label.,Language Identification on Massive Datasets of Short Message using an Attention Mechanism CNN,"Language Identification (LID) is a challenging task, especially when the
input texts are short and noisy such as posts and statuses on social media or
chat logs on gaming forums. The task has been tackled by either designing a
feature set for a traditional classifier (e.g. Naive Bayes) or applying a deep
neural network classifier (e.g. Bi-directional Gated Recurrent Unit,
Encoder-Decoder). These methods are usually trained and tested on a huge amount
of private data, then used and evaluated as off-the-shelf packages by other
researchers using their own datasets, and consequently the various results
published are not directly comparable. In this paper, we first create a new
massive labelled dataset based on one year of Twitter data. We use this dataset
to test several existing language identification systems, in order to obtain a
set of coherent benchmarks, and we make our dataset publicly available so that
others can add to this set of benchmarks. Finally, we propose a shallow but
efficient neural LID system, which is a ngram-regional convolution neural
network enhanced with an attention mechanism. Experimental results show that
our architecture is able to predict tens of thousands of samples per second and
surpasses all state-of-the-art systems with an improvement of 5%."
ce807a42370bfca10fa322d6fa772e4a58a8dca1,1708.09609,"According to the study on domain adaptation for identifying products in online cybercrime marketplaces, which four cybercrime forums are analyzed to demonstrate within-forum and cross-forum performance?","[{'answer': 'Darkode,  Hack Forums, Blackhat and Nulled.', 'type': 'abstractive'}]",1708.09609.pdf,"['1708.09609.pdf', '1802.06024.pdf', '1911.08976.pdf', '1909.00694.pdf', '2002.10361.pdf', '1905.00563.pdf', '2003.05377.pdf', '1909.00512.pdf', '1909.13375.pdf', '2002.11402.pdf', '1905.11901.pdf', '1908.06083.pdf', '2003.03106.pdf']",FLOAT SELECTED: Table 3: Test set results at the NP level in within-forum and cross-forum settings for a variety of different systems. Using either Brown clusters or gazetteers gives mixed results on cross-forum performance: only one of the improvements (†) is statistically significant with p < 0.05 according to a bootstrap resampling test. Gazetteers are unavailable for Blackhat and Nulled since we have no training data for those forums.,Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation,"One weakness of machine-learned NLP models is that they typically perform
poorly on out-of-domain data. In this work, we study the task of identifying
products being bought and sold in online cybercrime forums, which exhibits
particularly challenging cross-domain effects. We formulate a task that
represents a hybrid of slot-filling information extraction and named entity
recognition and annotate data from four different forums. Each of these forums
constitutes its own ""fine-grained domain"" in that the forums cover different
market sectors with different properties, even though all forums are in the
broad domain of cybercrime. We characterize these domain differences in the
context of a learning-based system: supervised models see decreased accuracy
when applied to new forums, and standard techniques for semi-supervised
learning and domain adaptation have limited effectiveness on this data, which
suggests the need to improve these techniques. We release a dataset of 1,938
annotated posts from across the four forums."
e3c9e4bc7bb93461856e1f4354f33010bc7d28d5,1809.06537,"Which baseline models, including both neural classifiers and off-the-shelf RC models, are re-implemented and fine-tuned for comparison in the experiments conducted on the real-world civil case dataset in this paper?","[{'answer': 'SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ', 'type': 'extractive'}, {'answer': 'SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4, attention-based method BIBREF3 and other methods we deem important, some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard', 'type': 'extractive'}]",1809.06537.pdf,"['1809.06537.pdf', '1909.08089.pdf', '1908.06267.pdf', '1812.06864.pdf', '1703.06492.pdf', '1806.11432.pdf', '1902.09393.pdf', '1810.00663.pdf', '2003.04642.pdf', '2003.03014.pdf']","For comparison, we adopt and re-implement three kinds of baselines as follows:

We implement an SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4 and select the best feature set on the development set.

We implement and fine-tune a series of neural text classifiers, including attention-based method BIBREF3 and other methods we deem important. CNN BIBREF18 and GRU BIBREF27 , BIBREF21 take as input the concatenation of fact description and plea. Similarly, CNN/GRU+law refers to using the concatenation of fact description, plea and law articles as inputs.

We implement and train some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard. In our initial experiments, these models take fact description as passage and plea as query. Further, Law articles are added to the fact description as a part of the reading materials, which is a simple way to consider them as well.",Automatic Judgment Prediction via Legal Reading Comprehension,"Automatic judgment prediction aims to predict the judicial results based on
case materials. It has been studied for several decades mainly by lawyers and
judges, considered as a novel and prospective application of artificial
intelligence techniques in the legal field. Most existing methods follow the
text classification framework, which fails to model the complex interactions
among complementary case materials. To address this issue, we formalize the
task as Legal Reading Comprehension according to the legal scenario. Following
the working protocol of human judges, LRC predicts the final judgment results
based on three types of information, including fact description, plaintiffs'
pleas, and law articles. Moreover, we propose a novel LRC model, AutoJudge,
which captures the complex semantic interactions among facts, pleas, and laws.
In experiments, we construct a real-world civil case dataset for LRC.
Experimental results on this dataset demonstrate that our model achieves
significant improvement over state-of-the-art models. We will publish all
source codes and datasets of this work on \urlgithub.com for further research."
a4e66e842be1438e5cd8d7cb2a2c589f494aee27,1910.11769,"Based on the benchmark results of the DENS paper, which algorithm and model combination achieved the lowest average micro-F1 score across the 5-fold cross-validation for the emotion analysis task?","[{'answer': 'Depeche + SVM', 'type': 'extractive'}]",1910.11769.pdf,"['1910.11769.pdf', '1905.00563.pdf', '2003.04642.pdf', '1707.05236.pdf', '2003.06044.pdf']",FLOAT SELECTED: Table 4: Benchmark results (averaged 5-fold cross validation),DENS: A Dataset for Multi-class Emotion Analysis,"We introduce a new dataset for multi-class emotion analysis from long-form
narratives in English. The Dataset for Emotions of Narrative Sequences (DENS)
was collected from both classic literature available on Project Gutenberg and
modern online narratives available on Wattpad, annotated using Amazon
Mechanical Turk. A number of statistics and baseline benchmarks are provided
for the dataset. Of the tested techniques, we find that the fine-tuning of a
pre-trained BERT model achieves the best results, with an average micro-F1
score of 60.4%. Our results show that the dataset provides a novel opportunity
in emotion analysis that requires moving beyond existing sentence-level
techniques."
a1c5b95e407127c6bb2f9a19b7d9b1f1bcd4a7a5,1705.08142,Do Sluice networks outperform single-task models and hard parameter sharing in both in-domain and out-of-domain settings for chunking with POS tagging as an auxiliary task in the OntoNotes 5.0 experiments?,"[{'answer': 'Yes', 'type': 'boolean'}]",1705.08142.pdf,"['1705.08142.pdf', '1911.02086.pdf', '1909.00279.pdf', '1911.01799.pdf', '1901.02262.pdf', '1903.00172.pdf', '1809.00540.pdf', '1704.05907.pdf', '1710.09340.pdf', '1908.10084.pdf', '1805.04033.pdf', '1608.06757.pdf', '1901.02257.pdf', '1706.08032.pdf']",FLOAT SELECTED: Table 3: Accuracy scores on in-domain and out-of-domain test sets for chunking (main task) with POS tagging as auxiliary task for different target domains for baselines and Sluice networks. Out-of-domain results for each target domain are averages across the 6 remaining source domains. Average error reduction over single-task performance is 12.8% for in-domain; 8.9% for out-of-domain. In-domain error reduction over hard parameter sharing is 14.8%.,Latent Multi-task Architecture Learning,"Multi-task learning (MTL) allows deep neural networks to learn from related
tasks by sharing parameters with other networks. In practice, however, MTL
involves searching an enormous space of possible parameter sharing
architectures to find (a) the layers or subspaces that benefit from sharing,
(b) the appropriate amount of sharing, and (c) the appropriate relative weights
of the different task losses. Recent work has addressed each of the above
problems in isolation. In this work we present an approach that learns a latent
multi-task architecture that jointly addresses (a)--(c). We present experiments
on synthetic data and data from OntoNotes 5.0, including four different tasks
and seven different domains. Our extension consistently outperforms previous
approaches to learning latent architectures for multi-task problems and
achieves up to 15% average error reductions over common approaches to MTL."
6992f8e5a33f0af0f2206769484c72fecc14700b,1811.01088,Does the paper compare the performance of models trained with STILTs to BERT and ELMo on the same tasks from the GLUE benchmark?,"[{'answer': 'Yes', 'type': 'boolean'}]",1811.01088.pdf,"['1811.01088.pdf', '1904.10503.pdf', '1912.00864.pdf', '1909.00754.pdf']","FLOAT SELECTED: Table 1: GLUE results with and without STILTs, fine-tuning on full training data of each target task. Bold marks the best within each section. Strikethrough indicates cases where the intermediate task is the same as the target task—we substitute the baseline result for that cell. A.Ex is the average excluding MNLI and QQP because of the overlap with intermediate tasks. See text for discussion of WNLI results. Test results on STILTs uses the supplementary training regime for each task based on the performance on the development set, corresponding to the numbers shown in Best of Each. The aggregated GLUE scores differ from the public leaderboard because we report performance on QNLIv1.",Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks,"Pretraining sentence encoders with language modeling and related unsupervised
tasks has recently been shown to be very effective for language understanding
tasks. By supplementing language model-style pretraining with further training
on data-rich supervised tasks, such as natural language inference, we obtain
additional performance improvements on the GLUE benchmark. Applying
supplementary training on BERT (Devlin et al., 2018), we attain a GLUE score of
81.8---the state of the art (as of 02/24/2019) and a 1.4 point improvement over
BERT. We also observe reduced variance across random restarts in this setting.
Our approach yields similar improvements when applied to ELMo (Peters et al.,
2018a) and Radford et al. (2018)'s model. In addition, the benefits of
supplementary training are particularly pronounced in data-constrained regimes,
as we show in experiments with artificially limited training data."
38c74ab8292a94fc5a82999400ee9c06be19f791,2002.10361,"What is the total number of documents in the multilingual Twitter corpus for hate speech detection, covering English, Italian, Polish, Portuguese, and Spanish datasets?","[{'answer': 'It contains 106,350 documents', 'type': 'abstractive'}]",2002.10361.pdf,"['2002.10361.pdf', '1810.09774.pdf', '1908.11047.pdf', '1902.09393.pdf']","FLOAT SELECTED: Table 1: Statistical summary of multilingual corpora across English, Italian, Polish, Portuguese and Spanish. We present number of users (Users), documents (Docs), and average tokens per document (Tokens) in the corpus, plus the label distribution (HS Ratio, percent of documents labeled positive for hate speech).",Multilingual Twitter Corpus and Baselines for Evaluating Demographic Bias in Hate Speech Recognition,"Existing research on fairness evaluation of document classification models
mainly uses synthetic monolingual data without ground truth for author
demographic attributes. In this work, we assemble and publish a multilingual
Twitter corpus for the task of hate speech detection with inferred four author
demographic factors: age, country, gender and race/ethnicity. The corpus covers
five languages: English, Italian, Polish, Portuguese and Spanish. We evaluate
the inferred demographic labels with a crowdsourcing platform, Figure Eight. To
examine factors that can cause biases, we take an empirical analysis of
demographic predictability on the English corpus. We measure the performance of
four popular document classifiers and evaluate the fairness and bias of the
baseline classifiers on the author-level demographic attributes."
12159f04e0427fe33fa05af6ba8c950f1a5ce5ea,1705.01265,"Which hyperparameters, including the number of clusters, word embedding dimensions, and others, were adjusted in the systematic evaluation of clustered word embeddings across named entity recognition and sentiment classification tasks, as described in the paper?","[{'answer': 'number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding', 'type': 'abstractive'}, {'answer': 'different number of clusters, different embeddings', 'type': 'extractive'}]",1705.01265.pdf,"['1705.01265.pdf', '1809.08298.pdf', '2003.04642.pdf', '1911.02821.pdf', '1906.11180.pdf', '1603.00968.pdf', '1705.00108.pdf']",We cluster the embeddings with INLINEFORM0 -Means.,On the effectiveness of feature set augmentation using clusters of word embeddings,"Word clusters have been empirically shown to offer important performance
improvements on various tasks. Despite their importance, their incorporation in
the standard pipeline of feature engineering relies more on a trial-and-error
procedure where one evaluates several hyper-parameters, like the number of
clusters to be used. In order to better understand the role of such features we
systematically evaluate their effect on four tasks, those of named entity
segmentation and classification as well as, those of five-point sentiment
classification and quantification. Our results strongly suggest that cluster
membership features improve the performance."
e829f008d62312357e0354a9ed3b0827c91c9401,2001.05493,"What specific psycholinguistic and basic linguistic features, including punctuation and emoticons, are utilized as meta-data to support aggression detection in both code-mixed and uni-lingual English texts as described in this paper's unified system?","[{'answer': 'Emotion Sensor Feature, Part of Speech, Punctuation, Sentiment Analysis, Empath, TF-IDF Emoticon features', 'type': 'abstractive'}]",2001.05493.pdf,"['2001.05493.pdf', '1904.10500.pdf', '1908.10084.pdf', '1910.14497.pdf', '1809.01202.pdf', '1801.05147.pdf']","Exploiting psycho-linguistic features with basic linguistic features as meta-data. The main aim is to minimize the direct dependencies on in-depth grammatical structure of the language (i.e., to support code-mixed data). We have also included emoticons, and punctuation features with it. We use the term ""NLP Features"" to represent it in the entire paper.",A Unified System for Aggression Identification in English Code-Mixed and Uni-Lingual Texts,"Wide usage of social media platforms has increased the risk of aggression,
which results in mental stress and affects the lives of people negatively like
psychological agony, fighting behavior, and disrespect to others. Majority of
such conversations contains code-mixed languages[28]. Additionally, the way
used to express thought or communication style also changes from one social
media plat-form to another platform (e.g., communication styles are different
in twitter and Facebook). These all have increased the complexity of the
problem. To solve these problems, we have introduced a unified and robust
multi-modal deep learning architecture which works for English code-mixed
dataset and uni-lingual English dataset both.The devised system, uses
psycho-linguistic features and very ba-sic linguistic features. Our multi-modal
deep learning architecture contains, Deep Pyramid CNN, Pooled BiLSTM, and
Disconnected RNN(with Glove and FastText embedding, both). Finally, the system
takes the decision based on model averaging. We evaluated our system on English
Code-Mixed TRAC 2018 dataset and uni-lingual English dataset obtained from
Kaggle. Experimental results show that our proposed system outperforms all the
previous approaches on English code-mixed dataset and uni-lingual English
dataset."
171ebfdc9b3a98e4cdee8f8715003285caeb2f39,1909.08859,How does the Procedural Reasoning Network (PRN) model perform in terms of accuracy in single-task and multi-task training on the visual reasoning tasks in the RecipeQA dataset compared to the previous best models?,"[{'answer': 'Average accuracy of proposed model vs best prevous result:\nSingle-task Training: 57.57 vs 55.06\nMulti-task Training: 50.17 vs 50.59', 'type': 'abstractive'}]",1909.08859.pdf,"['1909.08859.pdf', '1906.05474.pdf', '2002.01207.pdf', '1805.04033.pdf', '1909.08824.pdf', '2002.06644.pdf', '1806.04511.pdf', '1809.04960.pdf', '1909.05855.pdf', '1909.02480.pdf']","Table TABREF29 presents the quantitative results for the visual reasoning tasks in RecipeQA. In single-task training setting, PRN gives state-of-the-art results compared to other neural models.",Procedural Reasoning Networks for Understanding Multimodal Procedures,"This paper addresses the problem of comprehending procedural commonsense
knowledge. This is a challenging task as it requires identifying key entities,
keeping track of their state changes, and understanding temporal and causal
relations. Contrary to most of the previous work, in this study, we do not rely
on strong inductive bias and explore the question of how multimodality can be
exploited to provide a complementary semantic signal. Towards this end, we
introduce a new entity-aware neural comprehension model augmented with external
relational memory units. Our model learns to dynamically update entity states
in relation to each other while reading the text instructions. Our experimental
analysis on the visual reasoning tasks in the recently proposed RecipeQA
dataset reveals that our approach improves the accuracy of the previously
reported models by a large margin. Moreover, we find that our model learns
effective dynamic representations of entities even though we do not use any
supervision at the level of entity states."
897ba53ef44f658c128125edd26abf605060fb13,1611.04798,Does the paper report results for English-to-German translation in a simulated under-resourced scenario using their proposed multilingual NMT framework?,"[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]",1611.04798.pdf,"['1611.04798.pdf', '1603.00968.pdf', '1908.05434.pdf', '1706.08032.pdf', '1904.01608.pdf', '2003.12218.pdf', '1910.07481.pdf', '1808.09029.pdf', '1909.01013.pdf', '2003.01769.pdf', '1909.08824.pdf']",FLOAT SELECTED: Table 1: Results of the English→German systems in a simulated under-resourced scenario.,Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder,"In this paper, we present our first attempts in building a multilingual
Neural Machine Translation framework under a unified approach. We are then able
to employ attention-based NMT for many-to-many multilingual translation tasks.
Our approach does not require any special treatment on the network architecture
and it allows us to learn minimal number of free parameters in a standard way
of training. Our approach has shown its effectiveness in an under-resourced
translation scenario with considerable improvements up to 2.6 BLEU points. In
addition, the approach has achieved interesting and promising results when
applied in the translation task that there is no direct parallel corpus between
source and target languages."
6412e97373e8e9ae3aa20aa17abef8326dc05450,1610.00879,What is the baseline human evaluation method used to compare against the classifier performance on Dataset-H?,"[{'answer': 'Human evaluators', 'type': 'abstractive'}]",1610.00879.pdf,"['1610.00879.pdf', '2001.05493.pdf', '1910.11235.pdf', '1611.02550.pdf', '1909.00252.pdf', '1909.05855.pdf', '1904.09678.pdf', '1711.11221.pdf', '1909.13695.pdf', '1909.06162.pdf', '2003.12218.pdf']","FLOAT SELECTED: Table 5: Performance of human evaluators and our classifiers (trained on all features), for Dataset-H as the test set",A Computational Approach to Automatic Prediction of Drunk Texting,"Alcohol abuse may lead to unsociable behavior such as crime, drunk driving,
or privacy leaks. We introduce automatic drunk-texting prediction as the task
of identifying whether a text was written when under the influence of alcohol.
We experiment with tweets labeled using hashtags as distant supervision. Our
classifiers use a set of N-gram and stylistic features to detect drunk tweets.
Our observations present the first quantitative evidence that text contains
signals that can be exploited to detect drunk-texting."
957bda6b421ef7d2839c3cec083404ac77721f14,1610.00879,What stylistic and structural features are outlined in the paper for detecting drunk-texting behavior?,"[{'answer': 'LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors,  Spelling errors, Repeated characters, Capitalisation, Length,  Emoticon (Presence/Count ) \n and Sentiment Ratio', 'type': 'abstractive'}, {'answer': 'LDA unigrams (Presence/Count), POS Ratio, #Named Entity Mentions, #Discourse Connectors, Spelling errors, Repeated characters, Capitalization, Length, Emoticon (Presence/Count), Sentiment Ratio.', 'type': 'abstractive'}]",1610.00879.pdf,"['1610.00879.pdf', '1911.02821.pdf', '2004.04721.pdf', '1909.01247.pdf', '1909.00279.pdf', '1905.00563.pdf', '1909.03544.pdf', '1909.11467.pdf', '1902.00672.pdf', '1911.02711.pdf', '1611.00514.pdf', '1911.04952.pdf', '1611.04798.pdf', '1703.02507.pdf', '1911.07228.pdf', '2001.05493.pdf', '2002.06644.pdf']",FLOAT SELECTED: Table 1: Our Feature Set for Drunk-texting Prediction,A Computational Approach to Automatic Prediction of Drunk Texting,"Alcohol abuse may lead to unsociable behavior such as crime, drunk driving,
or privacy leaks. We introduce automatic drunk-texting prediction as the task
of identifying whether a text was written when under the influence of alcohol.
We experiment with tweets labeled using hashtags as distant supervision. Our
classifiers use a set of N-gram and stylistic features to detect drunk tweets.
Our observations present the first quantitative evidence that text contains
signals that can be exploited to detect drunk-texting."
8f87215f4709ee1eb9ddcc7900c6c054c970160b,1904.09678,"What metrics are used to evaluate the performance of UniSent in comparison to manually crafted lexica in Czech, German, French, Macedonian, and Spanish?","[{'answer': 'Accuracy and the macro-F1 (averaged F1 over positive and negative classes) are used as a measure of quality.', 'type': 'abstractive'}]",1904.09678.pdf,"['1904.09678.pdf', '1810.12196.pdf', '1901.02262.pdf', '1912.10011.pdf', '1804.08139.pdf', '1909.01013.pdf', '1909.13695.pdf']","FLOAT SELECTED: Table 1: Comparison of manually created lexicon performance with UniSent in Czech, German, French, Macedonians, and Spanish. We report accuracy and the macro-F1 (averaged F1 over positive and negative classes). The baseline is constantly considering the majority label. The last two columns indicate the performance of UniSent after drift weighting.",UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages,"In this paper, we introduce UniSent universal sentiment lexica for $1000+$
languages. Sentiment lexica are vital for sentiment analysis in absence of
document-level annotations, a very common scenario for low-resource languages.
To the best of our knowledge, UniSent is the largest sentiment resource to date
in terms of the number of covered languages, including many low resource ones.
In this work, we use a massively parallel Bible corpus to project sentiment
information from English to other languages for sentiment analysis on Twitter
data. We introduce a method called DomDrift to mitigate the huge domain
mismatch between Bible and Twitter by a confidence weighting scheme that uses
domain-specific embeddings to compare the nearest neighbors for a candidate
sentiment word in the source (Bible) and target (Twitter) domain. We evaluate
the quality of UniSent in a subset of languages for which manually created
ground truth was available, Macedonian, Czech, German, Spanish, and French. We
show that the quality of UniSent is comparable to manually created sentiment
resources when it is used as the sentiment seed for the task of word sentiment
prediction on top of embedding representations. In addition, we show that
emoticon sentiments could be reliably predicted in the Twitter domain using
only UniSent and monolingual embeddings in German, Spanish, French, and
Italian. With the publication of this paper, we release the UniSent sentiment
lexica."
572458399a45fd392c3a4e07ce26dcff2ad5a07d,1912.00864,"By how much do the BLEU-4 and ROUGE-L scores of the NAGM model outperform the best conventional models, Trans and CLSTM, on both the Oshiete-goo and nfL6 datasets, specifically in terms of exact numerical differences?","[{'answer': ""For the Oshiete-goo dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, Trans, by 0.021, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.037.  For the nfL6 dataset, the NAGM model's ROUGE-L score is higher than the highest performing conventional model, CLSTM, by 0.028, and its BLEU-4 score is higher than the highest performing model CLSTM by 0.040. Human evaluation of the NAGM's generated outputs for the Oshiete-goo dataset had 47% ratings of (1), the highest rating, while CLSTM only received 21% ratings of (1). For the nfL6 dataset, the comparison of (1)'s was NAGM's 50% to CLSTM's 30%. "", 'type': 'abstractive'}]",1912.00864.pdf,"['1912.00864.pdf', '1811.02906.pdf', '1810.10254.pdf', '1804.07789.pdf', '1909.13375.pdf', '1910.11769.pdf', '1909.03544.pdf', '2001.06888.pdf', '1709.05413.pdf']","Finally, NAGM is consistently superior to the conventional attentive encoder-decoders regardless of the metric. Its ROUGE-L and BLEU-4 scores are much higher than those of CLSTM. ",Conclusion-Supplement Answer Generation for Non-Factoid Questions,"This paper tackles the goal of conclusion-supplement answer generation for
non-factoid questions, which is a critical issue in the field of Natural
Language Processing (NLP) and Artificial Intelligence (AI), as users often
require supplementary information before accepting a conclusion. The current
encoder-decoder framework, however, has difficulty generating such answers,
since it may become confused when it tries to learn several different long
answers to the same non-factoid question. Our solution, called an ensemble
network, goes beyond single short sentences and fuses logically connected
conclusion statements and supplementary statements. It extracts the context
from the conclusion decoder's output sequence and uses it to create
supplementary decoder states on the basis of an attention mechanism. It also
assesses the closeness of the question encoder's output sequence and the
separate outputs of the conclusion and supplement decoders as well as their
combination. As a result, it generates answers that match the questions and
have natural-sounding supplementary sequences in line with the context
expressed by the conclusion sequence. Evaluations conducted on datasets
including ""Love Advice"" and ""Arts & Humanities"" categories indicate that our
model outputs much more accurate results than the tested baseline models do."
9349acbfce95cb5d6b4d09ac626b55a9cb90e55e,1904.01608,"What are the distinct citation intent labels identified in both the ACL-ARC and SciCite datasets, and how do they differ in terms of their application and coverage?","[{'answer': 'Background, extends, uses, motivation, compare/contrast, and future work for the ACL-ARC dataset. Background, method, result comparison for the SciCite dataset.', 'type': 'abstractive'}]",1904.01608.pdf,"['1904.01608.pdf', '2003.05377.pdf', '1908.11365.pdf', '1912.13109.pdf', '1909.08859.pdf', '1810.12196.pdf', '1909.03544.pdf', '2002.06424.pdf', '1705.01214.pdf', '1809.00530.pdf', '1901.02262.pdf', '1911.07555.pdf', '2002.06675.pdf', '1603.04513.pdf', '1711.00106.pdf', '2001.00137.pdf']",FLOAT SELECTED: Table 2: Characteristics of SciCite compared with ACL-ARC dataset by Jurgens et al. (2018),Structural Scaffolds for Citation Intent Classification in Scientific Publications,"Identifying the intent of a citation in scientific papers (e.g., background
information, use of methods, comparing results) is critical for machine reading
of individual publications and automated analysis of the scientific literature.
We propose structural scaffolds, a multitask model to incorporate structural
information of scientific papers into citations for effective classification of
citation intents. Our model achieves a new state-of-the-art on an existing ACL
anthology dataset (ACL-ARC) with a 13.3% absolute increase in F1 score, without
relying on external linguistic resources or hand-engineered features as done in
existing methods. In addition, we introduce a new dataset of citation intents
(SciCite) which is more than five times larger and covers multiple scientific
domains compared with existing datasets. Our code and data are available at:
https://github.com/allenai/scicite."
b634ff1607ce5756655e61b9a6f18bc736f84c83,1812.10479,"According to the results of the study investigating the effects of combining news and price data for short-term stock volatility prediction, which stock market sector achieved the highest accuracy in volatility forecasting?","[{'answer': 'Energy with accuracy of 0.538', 'type': 'abstractive'}, {'answer': 'Energy', 'type': 'abstractive'}]",1812.10479.pdf,"['1812.10479.pdf', '1909.13695.pdf', '2003.12218.pdf', '1904.09678.pdf', '1901.09755.pdf', '1909.00015.pdf', '1806.07711.pdf', '1909.09587.pdf', '1909.03544.pdf', '2003.07723.pdf', '1909.05855.pdf', '1911.02821.pdf', '1810.05241.pdf', '1901.08079.pdf', '1910.14497.pdf', '2001.00137.pdf', '1911.08673.pdf']",FLOAT SELECTED: Table 8: Sector-level performance comparison.,Multimodal deep learning for short-term stock volatility prediction,"Stock market volatility forecasting is a task relevant to assessing market
risk. We investigate the interaction between news and prices for the
one-day-ahead volatility prediction using state-of-the-art deep learning
approaches. The proposed models are trained either end-to-end or using sentence
encoders transfered from other tasks. We evaluate a broad range of stock market
sectors, namely Consumer Staples, Energy, Utilities, Heathcare, and Financials.
Our experimental results show that adding news improves the volatility
forecasting as compared to the mainstream models that rely only on price data.
In particular, our model outperforms the widely-recognized GARCH(1,1) model for
all sectors in terms of coefficient of determination $R^2$, $MSE$ and $MAE$,
achieving the best performance when training from both news and price data."
5fa36dc8f7c4e65acb962fc484989d20b8fdaeec,1901.08079,"Does the paper focus on datasets from the medical domain, without including general-purpose English datasets?","[{'answer': 'Yes', 'type': 'boolean'}]",1901.08079.pdf,"['1901.08079.pdf', '1910.00458.pdf', '1810.06743.pdf', '1707.03569.pdf', '1909.00512.pdf', '1907.03060.pdf', '2004.04721.pdf', '1908.06267.pdf', '1801.05147.pdf', '1909.02480.pdf', '2002.06675.pdf']",FLOAT SELECTED: Table 1: Description of training and test datasets.,A Question-Entailment Approach to Question Answering,"One of the challenges in large-scale information retrieval (IR) is to develop
fine-grained and domain-specific methods to answer natural language questions.
Despite the availability of numerous sources and datasets for answer retrieval,
Question Answering (QA) remains a challenging problem due to the difficulty of
the question understanding and answer extraction tasks. One of the promising
tracks investigated in QA is to map new questions to formerly answered
questions that are `similar'. In this paper, we propose a novel QA approach
based on Recognizing Question Entailment (RQE) and we describe the QA system
and resources that we built and evaluated on real medical questions. First, we
compare machine learning and deep learning methods for RQE using different
kinds of datasets, including textual inference, question similarity and
entailment in both the open and clinical domains. Second, we combine IR models
with the best RQE method to select entailed questions and rank the retrieved
answers. To study the end-to-end QA approach, we built the MedQuAD collection
of 47,457 question-answer pairs from trusted medical sources, that we introduce
and share in the scope of this paper. Following the evaluation process used in
TREC 2017 LiveQA, we find that our approach exceeds the best results of the
medical task with a 29.8% increase over the best official score. The evaluation
results also support the relevance of question entailment for QA and highlight
the effectiveness of combining IR and RQE for future QA efforts. Our findings
also show that relying on a restricted set of reliable answer sources can bring
a substantial improvement in medical QA."
863d5c6305e5bb4b14882b85b6216fa11bcbf053,1906.10551,"Which 12 authorship verification methods were evaluated and classified based on the AV characteristics proposed in Section SECREF3, particularly in relation to their performance on challenging cases like informal chat conversations and cross-topic verification?","[{'answer': 'MOCC, OCCAV, COAV, AVeer, GLAD, DistAV, Unmasking, Caravel, GenIM, ImpGI, SPATIUM and NNCD', 'type': 'abstractive'}]",1906.10551.pdf,"['1906.10551.pdf', '1905.07464.pdf', '1909.00105.pdf', '1809.09795.pdf', '1804.00079.pdf', '1908.06379.pdf', '1910.06592.pdf', '1901.05280.pdf', '2002.08899.pdf', '1904.01608.pdf']","The methods are listed in Table TABREF33 together with their classifications regarding the AV characteristics, which we proposed in Section SECREF3 .",Assessing the Applicability of Authorship Verification Methods,"Authorship verification (AV) is a research subject in the field of digital
text forensics that concerns itself with the question, whether two documents
have been written by the same person. During the past two decades, an
increasing number of proposed AV approaches can be observed. However, a closer
look at the respective studies reveals that the underlying characteristics of
these methods are rarely addressed, which raises doubts regarding their
applicability in real forensic settings. The objective of this paper is to fill
this gap by proposing clear criteria and properties that aim to improve the
characterization of existing and future AV approaches. Based on these
properties, we conduct three experiments using 12 existing AV approaches,
including the current state of the art. The examined methods were trained,
optimized and evaluated on three self-compiled corpora, where each corpus
focuses on a different aspect of applicability. Our results indicate that part
of the methods are able to cope with very challenging verification cases such
as 250 characters long informal chat conversations (72.7% accuracy) or cases in
which two scientific documents were written at different times with an average
difference of 15.6 years (> 75% accuracy). However, we also identified that all
involved methods are prone to cross-topic verification cases."
f398587b9a0008628278a5ea858e01d3f5559f65,1910.00825,How does SPNet's performance on ROUGE-1 and CIC compare to the best baseline method when evaluated on the MultiWOZ-based abstractive dialog summarization task?,"[{'answer': 'SPNet vs best baseline:\nROUGE-1: 90.97 vs 90.68\nCIC: 70.45 vs 70.25', 'type': 'abstractive'}]",1910.00825.pdf,"['1910.00825.pdf', '1809.00540.pdf', '1901.02257.pdf', '1707.05236.pdf', '1712.00991.pdf', '2002.01359.pdf', '1904.09678.pdf', '1809.10644.pdf', '1912.10011.pdf']",We show all the models' results in Table TABREF24,Abstractive Dialog Summarization with Semantic Scaffolds,"The demand for abstractive dialog summary is growing in real-world
applications. For example, customer service center or hospitals would like to
summarize customer service interaction and doctor-patient interaction. However,
few researchers explored abstractive summarization on dialogs due to the lack
of suitable datasets. We propose an abstractive dialog summarization dataset
based on MultiWOZ. If we directly apply previous state-of-the-art document
summarization methods on dialogs, there are two significant drawbacks: the
informative entities such as restaurant names are difficult to preserve, and
the contents from different dialog domains are sometimes mismatched. To address
these two drawbacks, we propose Scaffold Pointer Network (SPNet)to utilize the
existing annotation on speaker role, semantic slot and dialog domain. SPNet
incorporates these semantic scaffolds for dialog summarization. Since ROUGE
cannot capture the two drawbacks mentioned, we also propose a new evaluation
metric that considers critical informative entities in the text. On MultiWOZ,
our proposed SPNet outperforms state-of-the-art abstractive summarization
methods on all the automatic and human evaluation metrics."
de5b6c25e35b3a6c5e40e350fc5e52c160b33490,1909.08089,"What is the performance difference in ROUGE-1, ROUGE-L, and METEOR scores between the best proposed model and the previous top-performing model on both the arXiv and PubMed datasets in the paper ""Extractive Summarization of Long Documents by Combining Global and Local Context""?","[{'answer': 'Best proposed model result vs best previous result:\nArxiv dataset: Rouge 1 (43.62 vs 42.81), Rouge L (29.30 vs 31.80), Meteor (21.78 vs 21.35)\nPubmed dataset: Rouge 1 (44.85 vs 44.29), Rouge L (31.48 vs 35.21), Meteor (20.83 vs 20.56)', 'type': 'abstractive'}, {'answer': 'On arXiv dataset, the proposed model outperforms baselie model by (ROUGE-1,2,L)  0.67 0.72 0.77 respectively and by Meteor 0.31.\n', 'type': 'abstractive'}]",1909.08089.pdf,"['1909.08089.pdf', '1906.05474.pdf', '2001.05493.pdf', '1703.07090.pdf', '1610.07809.pdf', '1812.06705.pdf', '2004.03354.pdf']","The performance of all models on arXiv and Pubmed is shown in Table TABREF28 and Table TABREF29 , respectively.",Extractive Summarization of Long Documents by Combining Global and Local Context,"In this paper, we propose a novel neural single document extractive
summarization model for long documents, incorporating both the global context
of the whole document and the local context within the current topic. We
evaluate the model on two datasets of scientific papers, Pubmed and arXiv,
where it outperforms previous work, both extractive and abstractive models, on
ROUGE-1, ROUGE-2 and METEOR scores. We also show that, consistently with our
goal, the benefits of our method become stronger as we apply it to longer
documents. Rather surprisingly, an ablation study indicates that the benefits
of our model seem to come exclusively from modeling the local context, even for
the longest documents."
30803eefd7cdeb721f47c9ca72a5b1d750b8e03b,1611.00514,"What are the reported EER, Cmindet, and Cdet values for the Intelligent Voice system on the NIST 2016 Speaker Recognition Evaluation development set?","[{'answer': 'EER 16.04, Cmindet 0.6012, Cdet 0.6107', 'type': 'abstractive'}]",1611.00514.pdf,"['1611.00514.pdf', '1805.03710.pdf', '1707.00110.pdf', '1910.13215.pdf', '1908.06267.pdf', '1911.12579.pdf', '1603.00968.pdf', '1810.12196.pdf', '1909.00361.pdf', '1809.06537.pdf', '1906.11180.pdf', '1912.00864.pdf', '1701.06538.pdf', '2002.05829.pdf', '1909.00578.pdf', '1911.08673.pdf', '1910.07481.pdf', '1811.02906.pdf']",In this section we present the results obtained on the protocol provided by NIST on the development set which is supposed to mirror that of evaluation set. The results are shown in Table TABREF26 .,The Intelligent Voice 2016 Speaker Recognition System,"This paper presents the Intelligent Voice (IV) system submitted to the NIST
2016 Speaker Recognition Evaluation (SRE). The primary emphasis of SRE this
year was on developing speaker recognition technology which is robust for novel
languages that are much more heterogeneous than those used in the current
state-of-the-art, using significantly less training data, that does not contain
meta-data from those languages. The system is based on the state-of-the-art
i-vector/PLDA which is developed on the fixed training condition, and the
results are reported on the protocol defined on the development set of the
challenge."
2d4d0735c50749aa8087d1502ab7499faa2f0dd8,1908.05434,"What are the Mean Absolute Error (MAE), macro-averaged MAE (MAEM), binary classification accuracy, and weighted binary classification accuracy values for the proposed Ordinal Regression Neural Network (ORNN) compared to the previous best state-of-the-art model for the Trafficking-10K dataset, for the task of detecting sex trafficking in escort ads?","[{'answer': 'Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)', 'type': 'abstractive'}]",1908.05434.pdf,"['1908.05434.pdf', '1911.02711.pdf', '1703.02507.pdf', '1909.00252.pdf', '1908.07816.pdf', '1908.07245.pdf', '1909.00361.pdf', '1802.06024.pdf', '1911.00069.pdf', '1810.09774.pdf', '1909.00512.pdf', '2003.08385.pdf']",We report the mean metrics from the CV in Table TABREF14 .,Sex Trafficking Detection with Ordinal Regression Neural Networks,"Sex trafficking is a global epidemic. Escort websites are a primary vehicle
for selling the services of such trafficking victims and thus a major driver of
trafficker revenue. Many law enforcement agencies do not have the resources to
manually identify leads from the millions of escort ads posted across dozens of
public websites. We propose an ordinal regression neural network to identify
escort ads that are likely linked to sex trafficking. Our model uses a modified
cost function to mitigate inconsistencies in predictions often associated with
nonparametric ordinal regression and leverages recent advancements in deep
learning to improve prediction accuracy. The proposed method significantly
improves on the previous state-of-the-art on Trafficking-10K, an
expert-annotated dataset of escort ads. Additionally, because traffickers use
acronyms, deliberate typographical errors, and emojis to replace explicit
keywords, we demonstrate how to expand the lexicon of trafficking flags through
word embeddings and t-SNE."
68e3f3908687505cb63b538e521756390c321a1c,1911.02711,What is the accuracy difference between using a user-written (golden) summary and an auto-generated summary in the hierarchically-refined review-centric attention model?,"[{'answer': '2.7 accuracy points', 'type': 'abstractive'}]",1911.02711.pdf,"['1911.02711.pdf', '2003.04866.pdf', '2003.06044.pdf', '1712.03556.pdf', '1804.00079.pdf', '1910.06592.pdf', '1807.07961.pdf', '1906.01081.pdf', '1706.08032.pdf', '1904.07904.pdf', '1909.06162.pdf', '2003.03044.pdf', '1912.08960.pdf']","Table TABREF34 and Table TABREF35 show the final results. Our model outperforms all the baseline models and the top-performing models with both generated summary and golden summary, for all the three datasets.",Making the Best Use of Review Summary for Sentiment Analysis,"Sentiment analysis provides a useful overview of customer review contents.
Many review websites allow a user to enter a summary in addition to a full
review. Intuitively, summary information may give additional benefit for review
sentiment analysis. In this paper, we conduct a study to exploit methods for
better use of summary information. We start by finding out that the sentimental
signal distribution of a review and that of its corresponding summary are in
fact complementary to each other. We thus explore various architectures to
better guide the interactions between the two and propose a
hierarchically-refined review-centric attention model. Empirical results show
that our review-centric model can make better use of user-written summaries for
review sentiment analysis, and is also more effective compared to existing
methods when the user summary is replaced with summary generated by an
automatic summarization system."
58f50397a075f128b45c6b824edb7a955ee8cba1,2002.06424,"""How many shared layers were utilized in the final model configuration for both NER and RE tasks in the ADE dataset?""","[{'answer': '1', 'type': 'abstractive'}]",2002.06424.pdf,"['2002.06424.pdf', '1909.09484.pdf', '2001.06888.pdf', '1909.08041.pdf', '1703.07090.pdf', '1709.10217.pdf', '2002.00652.pdf', '2002.01664.pdf', '1812.01704.pdf', '1702.03342.pdf', '2002.02492.pdf', '1704.05907.pdf', '1809.00540.pdf', '2003.03106.pdf', '1809.09194.pdf', '1603.04513.pdf', '1906.10551.pdf', '1910.06036.pdf']",FLOAT SELECTED: Table 1: Optimal hyperparameters used for final training on the ADE and CoNLL04 datasets.,Deeper Task-Specificity Improves Joint Entity and Relation Extraction,"Multi-task learning (MTL) is an effective method for learning related tasks,
but designing MTL models necessitates deciding which and how many parameters
should be task-specific, as opposed to shared between tasks. We investigate
this issue for the problem of jointly learning named entity recognition (NER)
and relation extraction (RE) and propose a novel neural architecture that
allows for deeper task-specificity than does prior work. In particular, we
introduce additional task-specific bidirectional RNN layers for both the NER
and RE tasks and tune the number of shared and task-specific layers separately
for different datasets. We achieve state-of-the-art (SOTA) results for both
tasks on the ADE dataset; on the CoNLL04 dataset, we achieve SOTA results on
the NER task and competitive results on the RE task while using an order of
magnitude fewer trainable parameters than the current SOTA architecture. An
ablation study confirms the importance of the additional task-specific layers
for achieving these results. Our work suggests that previous solutions to joint
NER and RE undervalue task-specificity and demonstrates the importance of
correctly balancing the number of shared and task-specific parameters for MTL
approaches in general."
9adcc8c4a10fa0d58f235b740d8d495ee622d596,2002.06424,How many additional task-specific RNN layers are introduced for Named Entity Recognition (NER) and Relation Extraction (RE) in both the ADE and CoNLL04 datasets in the proposed architecture?,"[{'answer': '2 for the ADE dataset and 3 for the CoNLL04 dataset', 'type': 'abstractive'}]",2002.06424.pdf,"['2002.06424.pdf', '1909.04002.pdf', '1912.08960.pdf', '1911.02821.pdf', '1912.01673.pdf', '1909.00252.pdf', '1704.06194.pdf', '1901.09755.pdf', '1801.05147.pdf', '1810.03459.pdf', '1911.11951.pdf', '1912.10011.pdf', '1904.10503.pdf', '1909.08824.pdf', '2002.10361.pdf', '2003.12218.pdf', '2001.08051.pdf', '1910.14497.pdf']",FLOAT SELECTED: Table 1: Optimal hyperparameters used for final training on the ADE and CoNLL04 datasets.,Deeper Task-Specificity Improves Joint Entity and Relation Extraction,"Multi-task learning (MTL) is an effective method for learning related tasks,
but designing MTL models necessitates deciding which and how many parameters
should be task-specific, as opposed to shared between tasks. We investigate
this issue for the problem of jointly learning named entity recognition (NER)
and relation extraction (RE) and propose a novel neural architecture that
allows for deeper task-specificity than does prior work. In particular, we
introduce additional task-specific bidirectional RNN layers for both the NER
and RE tasks and tune the number of shared and task-specific layers separately
for different datasets. We achieve state-of-the-art (SOTA) results for both
tasks on the ADE dataset; on the CoNLL04 dataset, we achieve SOTA results on
the NER task and competitive results on the RE task while using an order of
magnitude fewer trainable parameters than the current SOTA architecture. An
ablation study confirms the importance of the additional task-specific layers
for achieving these results. Our work suggests that previous solutions to joint
NER and RE undervalue task-specificity and demonstrates the importance of
correctly balancing the number of shared and task-specific parameters for MTL
approaches in general."
f62c78be58983ef1d77049738785ec7ab9f2a3ee,1812.01704,"What were the three real-world datasets, including the subverted versions, that were used to evaluate the impact of sentiment detection on toxicity detection in this study?","[{'answer': 'Kaggle\nSubversive Kaggle\nWikipedia\nSubversive Wikipedia\nReddit\nSubversive Reddit ', 'type': 'abstractive'}]",1812.01704.pdf,"['1812.01704.pdf', '1909.11467.pdf', '1912.01673.pdf', '1912.10806.pdf', '2002.01984.pdf', '1703.02507.pdf']","n every experiment, we used a random 70% of messages in the corpus as training data, another 20% as validation data, and the final 10% as testing data. The average results of the three tests are given in Table TABREF40 .",Impact of Sentiment Detection to Recognize Toxic and Subversive Online Comments,"The presence of toxic content has become a major problem for many online
communities. Moderators try to limit this problem by implementing more and more
refined comment filters, but toxic users are constantly finding new ways to
circumvent them. Our hypothesis is that while modifying toxic content and
keywords to fool filters can be easy, hiding sentiment is harder. In this
paper, we explore various aspects of sentiment detection and their correlation
to toxicity, and use our results to implement a toxicity detection tool. We
then test how adding the sentiment information helps detect toxicity in three
different real-world datasets, and incorporate subversion to these datasets to
simulate a user trying to circumvent the system. Our results show sentiment
information has a positive impact on toxicity detection against a subversive
user."
d2fbf34cf4b5b1fd82394124728b03003884409c,1909.07734,Which team achieved the top micro-F1 score of 81.5% on the Friends dataset for spoken dialogues in the EmotionX 2019 Challenge?,"[{'answer': 'IDEA', 'type': 'abstractive'}]",1909.07734.pdf,"['1909.07734.pdf', '1910.07481.pdf', '1712.00991.pdf', '1909.00252.pdf', '1712.05999.pdf', '1912.10011.pdf', '1911.08962.pdf', '1808.03430.pdf', '1910.11769.pdf', '1909.08041.pdf', '1707.08559.pdf', '2002.00652.pdf', '1804.05918.pdf', '1603.00968.pdf', '1810.09774.pdf', '1908.06151.pdf', '1901.02262.pdf']",FLOAT SELECTED: Table 6: F-scores for Friends (%),SocialNLP EmotionX 2019 Challenge Overview: Predicting Emotions in Spoken Dialogues and Chats,"We present an overview of the EmotionX 2019 Challenge, held at the 7th
International Workshop on Natural Language Processing for Social Media
(SocialNLP), in conjunction with IJCAI 2019. The challenge entailed predicting
emotions in spoken and chat-based dialogues using augmented EmotionLines
datasets. EmotionLines contains two distinct datasets: the first includes
excerpts from a US-based TV sitcom episode scripts (Friends) and the second
contains online chats (EmotionPush). A total of thirty-six teams registered to
participate in the challenge. Eleven of the teams successfully submitted their
predictions performance evaluation. The top-scoring team achieved a micro-F1
score of 81.5% for the spoken-based dialogues (Friends) and 79.5% for the
chat-based dialogues (EmotionPush)."
b540cd4fe9dc4394f64d5b76b0eaa4d9e30fb728,1906.05474,What evaluation metrics were used to assess the performance of BERT and ELMo across the ten datasets in the BLUE benchmark?,"[{'answer': 'BLUE utilizes different metrics for each of the tasks: Pearson correlation coefficient, F-1 scores, micro-averaging, and accuracy', 'type': 'abstractive'}]",1906.05474.pdf,"['1906.05474.pdf', '1904.01608.pdf', '1801.05147.pdf', '1908.07816.pdf', '1911.05153.pdf']",FLOAT SELECTED: Table 1: BLUE tasks,Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets,"Inspired by the success of the General Language Understanding Evaluation
benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE)
benchmark to facilitate research in the development of pre-training language
representations in the biomedicine domain. The benchmark consists of five tasks
with ten datasets that cover both biomedical and clinical texts with different
dataset sizes and difficulties. We also evaluate several baselines based on
BERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and
MIMIC-III clinical notes achieves the best results. We make the datasets,
pre-trained models, and codes publicly available at
https://github.com/ncbi-nlp/BLUE_Benchmark."
41173179efa6186eef17c96f7cbd8acb29105b0e,1906.05474,"What are the specific tasks in the BLUE benchmark in the paper evaluating BERT and ELMo on ten biomedical and clinical datasets, used to assess model performance across various NLP challenges?","[{'answer': 'Inference task\nThe aim of the inference task is to predict whether the premise sentence entails or contradicts the hypothesis sentence, Document multilabel classification\nThe multilabel classification task predicts multiple labels from the texts., Relation extraction\nThe aim of the relation extraction task is to predict relations and their types between the two entities mentioned in the sentences., Named entity recognition\nThe aim of the named entity recognition task is to predict mention spans given in the text , Sentence similarity\nThe sentence similarity task is to predict similarity scores based on sentence pairs', 'type': 'extractive'}]",1906.05474.pdf,"['1906.05474.pdf', '1707.05236.pdf', '1909.13695.pdf', '1705.01214.pdf', '1804.08050.pdf']",FLOAT SELECTED: Table 1: BLUE tasks,Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets,"Inspired by the success of the General Language Understanding Evaluation
benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE)
benchmark to facilitate research in the development of pre-training language
representations in the biomedicine domain. The benchmark consists of five tasks
with ten datasets that cover both biomedical and clinical texts with different
dataset sizes and difficulties. We also evaluate several baselines based on
BERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and
MIMIC-III clinical notes achieves the best results. We make the datasets,
pre-trained models, and codes publicly available at
https://github.com/ncbi-nlp/BLUE_Benchmark."
443d2448136364235389039cbead07e80922ec5c,1712.00991,Which summarization algorithms were evaluated using ROUGE unigram f1 scores for peer feedback summarization in the study on supervisor assessments and peer feedback in performance appraisals?,"[{'answer': 'LSA, TextRank, LexRank and ILP-based summary.', 'type': 'abstractive'}, {'answer': 'LSA, TextRank, LexRank', 'type': 'abstractive'}]",1712.00991.pdf,"['1712.00991.pdf', '1710.09340.pdf', '1801.05147.pdf', '1909.03544.pdf']","Table TABREF51 shows average and standard deviation of ROUGE unigram f1 scores for each algorithm, over the 100 summaries.",Mining Supervisor Evaluation and Peer Feedback in Performance Appraisals,"Performance appraisal (PA) is an important HR process to periodically measure
and evaluate every employee's performance vis-a-vis the goals established by
the organization. A PA process involves purposeful multi-step multi-modal
communication between employees, their supervisors and their peers, such as
self-appraisal, supervisor assessment and peer feedback. Analysis of the
structured data and text produced in PA is crucial for measuring the quality of
appraisals and tracking actual improvements. In this paper, we apply text
mining techniques to produce insights from PA text. First, we perform sentence
classification to identify strengths, weaknesses and suggestions of
improvements found in the supervisor assessments and then use clustering to
discover broad categories among them. Next we use multi-class multi-label
classification techniques to match supervisor assessments to predefined broad
perspectives on performance. Finally, we propose a short-text summarization
technique to produce a summary of peer feedback comments for a given employee
and compare it with manual summaries. All techniques are illustrated using a
real-life dataset of supervisor assessment and peer feedback text produced
during the PA of 4528 employees in a large multi-national IT company."
fb3d30d59ed49e87f63d3735b876d45c4c6b8939,1712.00991,What evaluation metrics are computed to assess the multi-label classification task of matching supervisor assessments to predefined performance perspectives in this paper?,"[{'answer': 'Precision, Recall, F-measure, accuracy', 'type': 'extractive'}, {'answer': 'Precision, Recall and F-measure', 'type': 'extractive'}]",1712.00991.pdf,"['1712.00991.pdf', '1711.00106.pdf', '2003.11563.pdf', '1705.00108.pdf', '1909.01247.pdf', '1911.02821.pdf', '1804.11346.pdf', '1905.06566.pdf', '1611.02550.pdf']","Precision, Recall and F-measure for this multi-label classification are computed using a strategy similar to the one described in BIBREF21 . ",Mining Supervisor Evaluation and Peer Feedback in Performance Appraisals,"Performance appraisal (PA) is an important HR process to periodically measure
and evaluate every employee's performance vis-a-vis the goals established by
the organization. A PA process involves purposeful multi-step multi-modal
communication between employees, their supervisors and their peers, such as
self-appraisal, supervisor assessment and peer feedback. Analysis of the
structured data and text produced in PA is crucial for measuring the quality of
appraisals and tracking actual improvements. In this paper, we apply text
mining techniques to produce insights from PA text. First, we perform sentence
classification to identify strengths, weaknesses and suggestions of
improvements found in the supervisor assessments and then use clustering to
discover broad categories among them. Next we use multi-class multi-label
classification techniques to match supervisor assessments to predefined broad
perspectives on performance. Finally, we propose a short-text summarization
technique to produce a summary of peer feedback comments for a given employee
and compare it with manual summaries. All techniques are illustrated using a
real-life dataset of supervisor assessment and peer feedback text produced
during the PA of 4528 employees in a large multi-national IT company."
197b276d0610ebfacd57ab46b0b29f3033c96a40,1712.00991,"Which specific machine learning classifiers, including both SciKit Learn implementations and custom-developed approaches, were tested using 5-fold cross-validation in the supervisor assessment text mining experiments?","[{'answer': 'Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK and Pattern-based', 'type': 'abstractive'}, {'answer': 'Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK, Pattern-based approach', 'type': 'abstractive'}]",1712.00991.pdf,"['1712.00991.pdf', '2003.12738.pdf', '1701.05574.pdf', '1803.09230.pdf', '1911.01799.pdf']","Table TABREF10 shows the results of 5-fold cross-validation experiments on dataset D1. For the first 5 classifiers, we used their implementation from the SciKit Learn library in Python (scikit-learn.org). The features used for these classifiers were simply the sentence words along with their frequencies. For the last 2 classifiers (in Table TABREF10 ), we used our own implementation.",Mining Supervisor Evaluation and Peer Feedback in Performance Appraisals,"Performance appraisal (PA) is an important HR process to periodically measure
and evaluate every employee's performance vis-a-vis the goals established by
the organization. A PA process involves purposeful multi-step multi-modal
communication between employees, their supervisors and their peers, such as
self-appraisal, supervisor assessment and peer feedback. Analysis of the
structured data and text produced in PA is crucial for measuring the quality of
appraisals and tracking actual improvements. In this paper, we apply text
mining techniques to produce insights from PA text. First, we perform sentence
classification to identify strengths, weaknesses and suggestions of
improvements found in the supervisor assessments and then use clustering to
discover broad categories among them. Next we use multi-class multi-label
classification techniques to match supervisor assessments to predefined broad
perspectives on performance. Finally, we propose a short-text summarization
technique to produce a summary of peer feedback comments for a given employee
and compare it with manual summaries. All techniques are illustrated using a
real-life dataset of supervisor assessment and peer feedback text produced
during the PA of 4528 employees in a large multi-national IT company."
e1b36927114969f3b759cba056cfb3756de474e4,2003.01769,"How much intelligibility improvement is achieved for the AECNN-T and AECNN-T-SM models when incorporating mimic loss for speech enhancement, based on experiments with CHiME-4 data?","[{'answer': 'Improved AECNN-T by 2.1 and AECNN-T-SM BY 0.9', 'type': 'abstractive'}]",2003.01769.pdf,"['2003.01769.pdf', '1912.01673.pdf', '1909.09484.pdf', '1701.03214.pdf', '1711.11221.pdf', '1609.00559.pdf', '1901.01010.pdf', '2004.01878.pdf']","In addition to the setting without any parallel data, we show results given parallel data. In Table TABREF10 we demonstrate that training the AECNN framework with mimic loss improves intelligibility over both the model trained with only time-domain loss (AECNN-T), as well as the model trained with both time-domain and spectral-domain losses (AECNN-T-SM). We only see a small improvement in the SI-SDR, likely due to the fact that the mimic loss technique is designed to improve the recognizablity of the results. In fact, seeing any improvement in SI-SDR at all is a surprising result.",Phonetic Feedback for Speech Enhancement With and Without Parallel Speech Data,"While deep learning systems have gained significant ground in speech
enhancement research, these systems have yet to make use of the full potential
of deep learning systems to provide high-level feedback. In particular,
phonetic feedback is rare in speech enhancement research even though it
includes valuable top-down information. We use the technique of mimic loss to
provide phonetic feedback to an off-the-shelf enhancement system, and find
gains in objective intelligibility scores on CHiME-4 data. This technique takes
a frozen acoustic model trained on clean speech to provide valuable feedback to
the enhancement model, even in the case where no parallel speech data is
available. Our work is one of the first to show intelligibility improvement for
neural enhancement systems without parallel speech data, and we show phonetic
feedback can improve a state-of-the-art neural enhancement system trained with
parallel speech data."
6e962f1f23061f738f651177346b38fd440ff480,2001.06286,Which models are listed as having previously held the state-of-the-art results on Dutch downstream tasks that RobBERT has now surpassed?,"[{'answer': 'BERTje BIBREF8, an ULMFiT model (Universal Language Model Fine-tuning for Text Classification model) BIBREF19., mBERT', 'type': 'extractive'}]",2001.06286.pdf,"['2001.06286.pdf', '2002.11402.pdf', '1909.13695.pdf', '1904.07904.pdf', '2003.03014.pdf', '1909.13714.pdf', '2003.05377.pdf', '1909.01383.pdf', '1902.00330.pdf', '2004.03744.pdf', '1810.06743.pdf', '1911.00069.pdf', '1807.07961.pdf', '1812.06705.pdf']","FLOAT SELECTED: Table 1: Results of RobBERT fine-tuned on several downstream tasks compared to the state of the art on the tasks. For accuracy, we also report the 95% confidence intervals. (Results annotated with * from van der Burgh and Verberne (2019), ** = from de Vries et al. (2019), *** from Allein et al. (2020))",RobBERT: a Dutch RoBERTa-based Language Model,"Pre-trained language models have been dominating the field of natural
language processing in recent years, and have led to significant performance
gains for various complex natural language tasks. One of the most prominent
pre-trained language models is BERT, which was released as an English as well
as a multilingual version. Although multilingual BERT performs well on many
tasks, recent studies show that BERT models trained on a single language
significantly outperform the multilingual version. Training a Dutch BERT model
thus has a lot of potential for a wide range of Dutch NLP tasks. While previous
approaches have used earlier implementations of BERT to train a Dutch version
of BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch
language model called RobBERT. We measured its performance on various tasks as
well as the importance of the fine-tuning dataset size. We also evaluated the
importance of language-specific tokenizers and the model's fairness. We found
that RobBERT improves state-of-the-art results for various tasks, and
especially significantly outperforms other models when dealing with smaller
datasets. These results indicate that it is a powerful pre-trained model for a
large variety of Dutch language tasks. The pre-trained and fine-tuned models
are publicly available to support further downstream Dutch NLP applications."
3b745f086fb5849e7ce7ce2c02ccbde7cfdedda5,2001.00137,"By how much does Stacked DeBERT outperform baseline models in F1-score in the Twitter Sentiment Classification task using Kaggle's Sentiment140 Corpus, and what is the average improvement in the intent classification task across the Chatbot NLU corpus?","[{'answer': 'In the sentiment classification task by 6% to 8% and in the intent classification task by 0.94% on average', 'type': 'abstractive'}]",2001.00137.pdf,"['2001.00137.pdf', '1712.03547.pdf', '2002.06675.pdf', '1909.09587.pdf']","Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table TABREF37, show that our model has better F1-micros scores, outperforming the baseline models by 6$\%$ to 8$\%$. ",Stacked DeBERT: All Attention in Incomplete Data for Text Classification,"In this paper, we propose Stacked DeBERT, short for Stacked Denoising
Bidirectional Encoder Representations from Transformers. This novel model
improves robustness in incomplete data, when compared to existing systems, by
designing a novel encoding scheme in BERT, a powerful language representation
model solely based on attention mechanisms. Incomplete data in natural language
processing refer to text with missing or incorrect words, and its presence can
hinder the performance of current models that were not implemented to withstand
such noises, but must still perform well even under duress. This is due to the
fact that current approaches are built for and trained with clean and complete
data, and thus are not able to extract features that can adequately represent
incomplete data. Our proposed approach consists of obtaining intermediate input
representations by applying an embedding layer to the input tokens followed by
vanilla transformers. These intermediate features are given as input to novel
denoising transformers which are responsible for obtaining richer input
representations. The proposed approach takes advantage of stacks of multilayer
perceptrons for the reconstruction of missing words' embeddings by extracting
more abstract and meaningful hidden feature vectors, and bidirectional
transformers for improved embedding representation. We consider two datasets
for training and evaluation: the Chatbot Natural Language Understanding
Evaluation Corpus and Kaggle's Twitter Sentiment Corpus. Our model shows
improved F1-scores and better robustness in informal/incorrect texts present in
tweets and in texts with Speech-to-Text error in the sentiment and intent
classification tasks."
b0376a7f67f1568a7926eff8ff557a93f434a253,1902.0033,"What is the exact micro F1 performance improvement of the proposed model over the highest performing baseline on the ACE2004 and CWEB datasets, as well as the overall average?","[{'answer': 'Comparing with the highest performing baseline: 1.3 points on ACE2004 dataset, 0.6 points on CWEB dataset, and 0.86 points in the average of all scores.', 'type': 'abstractive'}]",1902.00330.pdf,"['1902.00330.pdf', '1905.06566.pdf', '1911.13066.pdf', '1709.10217.pdf', '1904.01608.pdf', '1805.03710.pdf', '1901.09755.pdf', '2002.11402.pdf', '2002.06675.pdf', '2003.07996.pdf', '1811.02906.pdf', '2004.01980.pdf']",FLOAT SELECTED: Table 3: Compare our model with other baseline methods on different types of datasets. The evaluation metric is micro F1.,,
40e3639b79e2051bf6bce300d06548e7793daee0,1901.04899,Does the paper present a comparison of multiple approaches for utterance-level intent recognition in autonomous vehicle scenarios?,"[{'answer': 'Yes', 'type': 'boolean'}]",1901.04899.pdf,"['1901.04899.pdf', '1908.06379.pdf', '1705.01214.pdf', '2001.10161.pdf', '1912.01772.pdf', '1912.01673.pdf', '2002.05829.pdf', '1911.02086.pdf', '1910.07481.pdf']","The slot extraction and intent keywords extraction results are given in Table TABREF1 and Table TABREF2 , respectively. Table TABREF3 summarizes the results of various approaches we investigated for utterance-level intent understanding. Table TABREF4 shows the intent-wise detection results for our AMIE scenarios with the best performing utterance-level intent recognizer.",Conversational Intent Understanding for Passengers in Autonomous Vehicles,"Understanding passenger intents and extracting relevant slots are important
building blocks towards developing a contextual dialogue system responsible for
handling certain vehicle-passenger interactions in autonomous vehicles (AV).
When the passengers give instructions to AMIE (Automated-vehicle Multimodal
In-cabin Experience), the agent should parse such commands properly and trigger
the appropriate functionality of the AV system. In our AMIE scenarios, we
describe usages and support various natural commands for interacting with the
vehicle. We collected a multimodal in-cabin data-set with multi-turn dialogues
between the passengers and AMIE using a Wizard-of-Oz scheme. We explored
various recent Recurrent Neural Networks (RNN) based techniques and built our
own hierarchical models to recognize passenger intents along with relevant
slots associated with the action to be performed in AV scenarios. Our
experimental results achieved F1-score of 0.91 on utterance-level intent
recognition and 0.96 on slot extraction models."
0d7de323fd191a793858386d7eb8692cc924b432,1909.01247,What stylistic domains are represented in the annotated sentences from the Romanian newspaper corpus?,"[{'answer': 'current news, historical news, free time, sports, juridical news pieces, personal adverts, editorials.', 'type': 'abstractive'}]",1909.01247.pdf,"['1909.01247.pdf', '1911.07555.pdf', '2004.03788.pdf', '1911.08976.pdf', '1711.11221.pdf', '1901.05280.pdf', '1906.10551.pdf', '1904.05584.pdf', '1810.00663.pdf', '1606.00189.pdf', '1909.03405.pdf', '1811.02906.pdf', '1909.01013.pdf', '2001.08051.pdf', '1906.05474.pdf', '1905.11901.pdf']",FLOAT SELECTED: Table 1: Stylistic domains and examples (bold marks annotated entities),Introducing RONEC -- the Romanian Named Entity Corpus,"We present RONEC - the Named Entity Corpus for the Romanian language. The
corpus contains over 26000 entities in ~5000 annotated sentences, belonging to
16 distinct classes. The sentences have been extracted from a copy-right free
newspaper, covering several styles. This corpus represents the first initiative
in the Romanian language space specifically targeted for named entity
recognition. It is available in BRAT and CoNLL-U Plus formats, and it is free
to use and extend at github.com/dumitrescustefan/ronec ."
603fee7314fa65261812157ddfc2c544277fcf90,1911.10049,"How much larger are the training datasets used to create the new ELMo embeddings for the seven less-resourced languages, compared to those used in the CoNLL 2017 Shared Task, which sampled data from sources like Wikipedia dump and Common Crawl?","[{'answer': 'By 14 times.', 'type': 'abstractive'}, {'answer': 'up to 1.95 times larger', 'type': 'abstractive'}]",1911.10049.pdf,"['1911.10049.pdf', '1812.10479.pdf', '1910.04269.pdf', '2004.03354.pdf', '1804.11346.pdf', '1911.01680.pdf', '2002.06644.pdf', '1806.07711.pdf', '1701.05574.pdf', '1709.10367.pdf', '2003.05377.pdf', '2002.01984.pdf', '1611.03382.pdf', '1701.03214.pdf', '1909.05855.pdf', '1908.06379.pdf']","They used 20-million-words data randomly sampled from the raw text released by the CoNLL 2017 Shared Task - Automatically Annotated Raw Texts and Word Embeddings BIBREF8, which is a combination of Wikipedia dump and common crawl. ",High Quality ELMo Embeddings for Seven Less-Resourced Languages,"Recent results show that deep neural networks using contextual embeddings
significantly outperform non-contextual embeddings on a majority of text
classification task. We offer precomputed embeddings from popular contextual
ELMo model for seven languages: Croatian, Estonian, Finnish, Latvian,
Lithuanian, Slovenian, and Swedish. We demonstrate that the quality of
embeddings strongly depends on the size of training set and show that existing
publicly available ELMo embeddings for listed languages shall be improved. We
train new ELMo embeddings on much larger training sets and show their advantage
over baseline non-contextual FastText embeddings. In evaluation, we use two
benchmarks, the analogy task and the NER task."
09a1173e971e0fcdbf2fbecb1b077158ab08f497,1911.10049,What is the reported improvement in F1 score between ELMo and FastText embeddings for the Estonian NER task in the study on seven less-resourced languages?,"[{'answer': '5 percent points.', 'type': 'abstractive'}, {'answer': '0.05 F1', 'type': 'abstractive'}]",1911.10049.pdf,"['1911.10049.pdf', '1909.06937.pdf', '1603.00968.pdf', '1908.06379.pdf', '1805.03710.pdf', '1801.05147.pdf']","FLOAT SELECTED: Table 4: The results of NER evaluation task, averaged over 5 training and evaluation runs. The scores are average F1 score of the three named entity classes. The columns show FastText, ELMo, and the difference between them (∆(E − FT )).",High Quality ELMo Embeddings for Seven Less-Resourced Languages,"Recent results show that deep neural networks using contextual embeddings
significantly outperform non-contextual embeddings on a majority of text
classification task. We offer precomputed embeddings from popular contextual
ELMo model for seven languages: Croatian, Estonian, Finnish, Latvian,
Lithuanian, Slovenian, and Swedish. We demonstrate that the quality of
embeddings strongly depends on the size of training set and show that existing
publicly available ELMo embeddings for listed languages shall be improved. We
train new ELMo embeddings on much larger training sets and show their advantage
over baseline non-contextual FastText embeddings. In evaluation, we use two
benchmarks, the analogy task and the NER task."
5eda469a8a77f028d0c5f1acd296111085614537,1912.01214,"What language pairs, including the six zero-shot translation directions using Arabic, Spanish, and Russian from the MultiUN dataset, are evaluated through cross-lingual pre-training in the experiments outlined in this paper?","[{'answer': 'De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-En, En-Ru', 'type': 'abstractive'}, {'answer': 'French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation', 'type': 'extractive'}]",1912.01214.pdf,"['1912.01214.pdf', '1701.00185.pdf', '1904.01608.pdf', '1709.10217.pdf', '1810.10254.pdf', '2002.02070.pdf']","For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation. ",Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation,"Transfer learning between different language pairs has shown its
effectiveness for Neural Machine Translation (NMT) in low-resource scenario.
However, existing transfer methods involving a common target language are far
from success in the extreme scenario of zero-shot translation, due to the
language space mismatch problem between transferor (the parent model) and
transferee (the child model) on the source side. To address this challenge, we
propose an effective transfer learning approach based on cross-lingual
pre-training. Our key idea is to make all source languages share the same
feature space and thus enable a smooth transition for zero-shot translation. To
this end, we introduce one monolingual pre-training method and two bilingual
pre-training methods to obtain a universal encoder for different languages.
Once the universal encoder is constructed, the parent model built on such
encoder is trained with large-scale annotated data and then directly applied in
zero-shot translation scenario. Experiments on two public datasets show that
our approach significantly outperforms strong pivot-based baseline and various
multilingual NMT approaches."
a4d8fdcaa8adf99bdd1d7224f1a85c610659a9d3,1712.03547,How does the performance of the proposed KG embedding method using entity co-occurrence statistics compare to baseline methods in terms of maintaining comparable KG task performance while significantly improving interpretability?,"[{'answer': 'Performance was comparable, with the proposed method quite close and sometimes exceeding performance of baseline method.', 'type': 'abstractive'}]",1712.03547.pdf,"['1712.03547.pdf', '1906.11180.pdf', '1906.10225.pdf', '1812.10479.pdf', '1705.01214.pdf', '1904.10500.pdf', '2004.03788.pdf', '1912.01673.pdf', '1706.08032.pdf']",FLOAT SELECTED: Table 1: Results on test data. The proposed method significantly improves interpretability while maintaining comparable performance on KG tasks (Section 4.3).,Inducing Interpretability in Knowledge Graph Embeddings,"We study the problem of inducing interpretability in KG embeddings.
Specifically, we explore the Universal Schema (Riedel et al., 2013) and propose
a method to induce interpretability. There have been many vector space models
proposed for the problem, however, most of these methods don't address the
interpretability (semantics) of individual dimensions. In this work, we study
this problem and propose a method for inducing interpretability in KG
embeddings using entity co-occurrence statistics. The proposed method
significantly improves the interpretability, while maintaining comparable
performance in other KG tasks."
085147cd32153d46dd9901ab0f9195bfdbff6a85,1603.00968,"What baseline models are compared to MGNC-CNN in the experiments involving the use of word2vec, Glove, and syntactic embeddings?","[{'answer': 'MC-CNN\nMVCNN\nCNN', 'type': 'abstractive'}]",1603.00968.pdf,"['1603.00968.pdf', '1909.05855.pdf', '1703.06492.pdf', '1908.11365.pdf', '1701.02877.pdf', '1605.08675.pdf', '1804.00079.pdf', '1703.02507.pdf', '2004.03744.pdf', '1908.11047.pdf', '1909.08859.pdf', '1902.00672.pdf', '1912.10011.pdf', '2001.10161.pdf', '1908.06267.pdf']","We compared our proposed approaches to a standard CNN that exploits a single set of word embeddings BIBREF3 . We also compared to a baseline of simply concatenating embeddings for each word to form long vector inputs. We refer to this as Concatenation-CNN C-CNN. For all multiple embedding approaches (C-CNN, MG-CNN and MGNC-CNN), we explored two combined sets of embedding: word2vec+Glove, and word2vec+syntactic, and one three sets of embedding: word2vec+Glove+syntactic. For all models, we tuned the l2 norm constraint INLINEFORM0 over the range INLINEFORM1 on a validation set. For instantiations of MGNC-CNN in which we exploited two embeddings, we tuned both INLINEFORM2 , and INLINEFORM3 ; where we used three embedding sets, we tuned INLINEFORM4 and INLINEFORM5 .",MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification,"We introduce a novel, simple convolution neural network (CNN) architecture -
multi-group norm constraint CNN (MGNC-CNN) that capitalizes on multiple sets of
word embeddings for sentence classification. MGNC-CNN extracts features from
input embedding sets independently and then joins these at the penultimate
layer in the network to form a final feature vector. We then adopt a group
regularization strategy that differentially penalizes weights associated with
the subcomponents generated from the respective embedding sets. This model is
much simpler than comparable alternative architectures and requires
substantially less training time. Furthermore, it is flexible in that it does
not require input word embeddings to be of the same dimensionality. We show
that MGNC-CNN consistently outperforms baseline models."
c0035fb1c2b3de15146a7ce186ccd2e366fb4da2,1603.00968,"By how much does the MGNC-CNN model outperform the baseline models across different datasets like Subj, SST-1, SST-2, TREC, and Irony?","[{'answer': 'In terms of Subj the Average MGNC-CNN is better than the average score of baselines by 0.5.  Similarly, Scores of SST-1, SST-2, and TREC where MGNC-CNN has similar improvements. \nIn case of Irony the difference is about 2.0. \n', 'type': 'abstractive'}]",1603.00968.pdf,"['1603.00968.pdf', '1910.06036.pdf', '1611.03382.pdf', '2001.05970.pdf', '1911.01680.pdf', '1908.06264.pdf', '1908.05434.pdf', '1910.00912.pdf', '2001.08051.pdf', '1909.01013.pdf', '1605.07333.pdf', '1812.06864.pdf', '2004.03788.pdf', '1703.02507.pdf']","FLOAT SELECTED: Table 1: Results mean (min, max) achieved with each method. w2v:word2vec. Glv:GloVe. Syn: Syntactic embedding. Note that we experiment with using two and three sets of embeddings jointly, e.g., w2v+Syn+Glv indicates that we use all three of these.",MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification,"We introduce a novel, simple convolution neural network (CNN) architecture -
multi-group norm constraint CNN (MGNC-CNN) that capitalizes on multiple sets of
word embeddings for sentence classification. MGNC-CNN extracts features from
input embedding sets independently and then joins these at the penultimate
layer in the network to form a final feature vector. We then adopt a group
regularization strategy that differentially penalizes weights associated with
the subcomponents generated from the respective embedding sets. This model is
much simpler than comparable alternative architectures and requires
substantially less training time. Furthermore, it is flexible in that it does
not require input word embeddings to be of the same dimensionality. We show
that MGNC-CNN consistently outperforms baseline models."
34dd0ee1374a3afd16cf8b0c803f4ef4c6fec8ac,1603.00968,"What baseline and alternative architectures, including those that utilize single and concatenated word embeddings, are compared to MGNC-CNN according to the study?","[{'answer': 'standard CNN, C-CNN, MVCNN ', 'type': 'extractive'}]",1603.00968.pdf,"['1603.00968.pdf', '1901.05280.pdf', '1902.09393.pdf', '1908.06379.pdf', '1606.05320.pdf', '1806.11432.pdf', '1605.07333.pdf', '1902.09314.pdf']","We compared our proposed approaches to a standard CNN that exploits a single set of word embeddings BIBREF3 . We also compared to a baseline of simply concatenating embeddings for each word to form long vector inputs. We refer to this as Concatenation-CNN C-CNN. For all multiple embedding approaches (C-CNN, MG-CNN and MGNC-CNN), we explored two combined sets of embedding: word2vec+Glove, and word2vec+syntactic, and one three sets of embedding: word2vec+Glove+syntactic. ",MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification,"We introduce a novel, simple convolution neural network (CNN) architecture -
multi-group norm constraint CNN (MGNC-CNN) that capitalizes on multiple sets of
word embeddings for sentence classification. MGNC-CNN extracts features from
input embedding sets independently and then joins these at the penultimate
layer in the network to form a final feature vector. We then adopt a group
regularization strategy that differentially penalizes weights associated with
the subcomponents generated from the respective embedding sets. This model is
much simpler than comparable alternative architectures and requires
substantially less training time. Furthermore, it is flexible in that it does
not require input word embeddings to be of the same dimensionality. We show
that MGNC-CNN consistently outperforms baseline models."
e8029ec69b0b273954b4249873a5070c2a0edb8a,1905.1226,How does the exclusion of pixel data impact the crosslingual semantic similarity scores for the ImageVec model compared to previous methods across all six subtasks?,"[{'answer': 'performance is significantly degraded without pixel data', 'type': 'abstractive'}]",1905.12260.pdf,"['1905.12260.pdf', '1909.02480.pdf', '1909.00105.pdf', '1912.10806.pdf', '1910.05154.pdf', '1910.06592.pdf', '2001.08051.pdf', '1708.09609.pdf', '1912.01214.pdf', '1809.05752.pdf', '1806.04330.pdf', '2002.01984.pdf', '1810.00663.pdf', '2004.01980.pdf', '1909.11687.pdf']",FLOAT SELECTED: Table 1: Crosslingual semantic similarity scores (Spearman’s ρ) across six subtasks for ImageVec (our method) and previous work. Coverage is in brackets. The last column indicates the combined score across all subtasks. Best scores on each subtask are bolded.,,
8a5254ca726a2914214a4c0b6b42811a007ecfc6,2002.06675,What is the total duration of transcribed speech data available for the eight speakers in the speech corpus used to develop the Ainu language ASR system?,"[{'answer': 'Transcribed data is available for duration of 38h 54m 38s for 8 speakers.', 'type': 'abstractive'}]",2002.06675.pdf,"['2002.06675.pdf', '1910.11204.pdf', '1909.00279.pdf', '1808.09029.pdf', '1912.00864.pdf', '2002.01207.pdf', '1808.03430.pdf', '1705.00108.pdf', '1911.08976.pdf', '1910.08210.pdf', '1804.07789.pdf', '1812.10479.pdf']",The corpus we have prepared for ASR in this study is composed of text and speech. Table 1 shows the number of episodes and the total speech duration for each speaker.,Speech Corpus of Ainu Folklore and End-to-end Speech Recognition for Ainu Language,"Ainu is an unwritten language that has been spoken by Ainu people who are one
of the ethnic groups in Japan. It is recognized as critically endangered by
UNESCO and archiving and documentation of its language heritage is of paramount
importance. Although a considerable amount of voice recordings of Ainu folklore
has been produced and accumulated to save their culture, only a quite limited
parts of them are transcribed so far. Thus, we started a project of automatic
speech recognition (ASR) for the Ainu language in order to contribute to the
development of annotated language archives. In this paper, we report speech
corpus development and the structure and performance of end-to-end ASR for
Ainu. We investigated four modeling units (phone, syllable, word piece, and
word) and found that the syllable-based model performed best in terms of both
word and phone recognition accuracy, which were about 60% and over 85%
respectively in speaker-open condition. Furthermore, word and phone accuracy of
80% and 90% has been achieved in a speaker-closed setting. We also found out
that a multilingual ASR training with additional speech corpora of English and
Japanese further improves the speaker-open test accuracy."
5fa464a158dc8abf7cef8ca7d42a7080670c1edd,1912.0667,"Based on the Common Voice dataset, is the distribution of audio hours across the released languages described as balanced?","[{'answer': 'No', 'type': 'boolean'}]",1912.06670.pdf,"['1912.06670.pdf', '1703.07090.pdf', '1805.03710.pdf', '1911.01680.pdf', '1806.11432.pdf', '1707.05236.pdf', '1910.07481.pdf', '1902.09666.pdf', '1912.01673.pdf', '1904.03288.pdf', '1911.04952.pdf', '1603.00968.pdf', '1810.12885.pdf']",The data presented in Table (TABREF12) shows the currently available data. Each of the released languages is available for individual download as a compressed directory from the Mozilla Common Voice website. ,,
66c96c297c2cffdf5013bab5e95b59101cb38655,2003.03106,"What are the F1-scores of BERT for detection, relaxed classification, and strict classification tasks in the anonymisation of Spanish clinical text, and how do they compare to HUBES-PHI and MEDDOCAN as reported in the results?","[{'answer': 'F1 scores are:\nHUBES-PHI: Detection(0.965), Classification relaxed (0.95), Classification strict (0.937)\nMedoccan: Detection(0.972), Classification (0.967)', 'type': 'abstractive'}, {'answer': 'BERT remains only 0.3 F1-score points behind, and would have achieved the second position among all the MEDDOCAN shared task competitors. Taking into account that only 3% of the gold labels remain incorrectly annotated,  Table ', 'type': 'extractive'}]",2003.03106.pdf,"['2003.03106.pdf', '1704.08960.pdf', '1611.04798.pdf', '1901.05280.pdf', '1704.00939.pdf', '1909.11297.pdf', '1806.07711.pdf', '1701.06538.pdf', '1810.05241.pdf', '1909.00252.pdf', '2002.02492.pdf']","To finish with this experiment set, Table also shows the strict classification precision, recall and F1-score for the compared systems.",Sensitive Data Detection and Classification in Spanish Clinical Text: Experiments with BERT,"Massive digital data processing provides a wide range of opportunities and
benefits, but at the cost of endangering personal data privacy. Anonymisation
consists in removing or replacing sensitive information from data, enabling its
exploitation for different purposes while preserving the privacy of
individuals. Over the years, a lot of automatic anonymisation systems have been
proposed; however, depending on the type of data, the target language or the
availability of training documents, the task remains challenging still. The
emergence of novel deep-learning models during the last two years has brought
large improvements to the state of the art in the field of Natural Language
Processing. These advancements have been most noticeably led by BERT, a model
proposed by Google in 2018, and the shared language models pre-trained on
millions of documents. In this paper, we use a BERT-based sequence labelling
model to conduct a series of anonymisation experiments on several clinical
datasets in Spanish. We also compare BERT to other algorithms. The experiments
show that a simple BERT-based model with general-domain pre-training obtains
highly competitive results without any domain specific feature engineering."
c69f4df4943a2ca4c10933683a02b179a5e76f64,2003.12738,"Which variant of the Variational Transformer (GVT with global latent variables or SVT with sequential latent variables) achieves better performance in terms of perplexity, diversity, embeddings similarity, and human evaluation based on the experimental results presented in the paper?","[{'answer': 'PPL: SVT\nDiversity: GVT\nEmbeddings Similarity: SVT\nHuman Evaluation: SVT', 'type': 'abstractive'}]",2003.12738.pdf,"['2003.12738.pdf', '2002.04181.pdf', '1701.06538.pdf', '1704.08960.pdf', '1901.05280.pdf', '1611.03382.pdf', '1809.10644.pdf']","Compare to baseline models, the GVT achieves relatively lower reconstruction PPL, which suggests that the global latent variable contains rich latent information (e.g., topic) for response generation. Meanwhile, the sequential latent variables of the SVT encode fine-grained latent information and further improve the reconstruction PPL.",Variational Transformers for Diverse Response Generation,"Despite the great promise of Transformers in many sequence modeling tasks
(e.g., machine translation), their deterministic nature hinders them from
generalizing to high entropy tasks such as dialogue response generation.
Previous work proposes to capture the variability of dialogue responses with a
recurrent neural network (RNN)-based conditional variational autoencoder
(CVAE). However, the autoregressive computation of the RNN limits the training
efficiency. Therefore, we propose the Variational Transformer (VT), a
variational self-attentive feed-forward sequence model. The VT combines the
parallelizability and global receptive field of the Transformer with the
variational nature of the CVAE by incorporating stochastic latent variables
into Transformers. We explore two types of the VT: 1) modeling the
discourse-level diversity with a global latent variable; and 2) augmenting the
Transformer decoder with a sequence of fine-grained latent variables. Then, the
proposed models are evaluated on three conversational datasets with both
automatic metric and human evaluation. The experimental results show that our
models improve standard Transformers and other baselines in terms of diversity,
semantic relevance, and human judgment."
de12e059088e4800d7d89e4214a3997994dbc0d9,1902.09393,"Which baseline models are referenced on the ListOps dataset as comparisons, including those from Nangia and Bowman (2018), to evaluate the new recursive model's performance?","[{'answer': 'The system is compared to baseline models: LSTM, RL-SPINN and Gumbel Tree-LSTM', 'type': 'abstractive'}]",1902.09393.pdf,"['1902.09393.pdf', '1909.08089.pdf', '1911.08976.pdf', '2002.05829.pdf', '2002.01207.pdf', '2004.01878.pdf', '1902.10525.pdf', '1906.05474.pdf', '1911.03597.pdf', '1908.11546.pdf', '1812.06705.pdf']",FLOAT SELECTED: Table 1: Accuracy on the ListOps dataset. All models have 128 dimensions. Results for models with * are taken from Nangia and Bowman (2018).,Cooperative Learning of Disjoint Syntax and Semantics,"There has been considerable attention devoted to models that learn to jointly
infer an expression's syntactic structure and its semantics. Yet,
\citet{NangiaB18} has recently shown that the current best systems fail to
learn the correct parsing strategy on mathematical expressions generated from a
simple context-free grammar. In this work, we present a recursive model
inspired by \newcite{ChoiYL18} that reaches near perfect accuracy on this task.
Our model is composed of two separated modules for syntax and semantics. They
are cooperatively trained with standard continuous and discrete optimization
schemes. Our model does not require any linguistic structure for supervision
and its recursive nature allows for out-of-domain generalization with little
loss in performance. Additionally, our approach performs competitively on
several natural language tasks, such as Natural Language Inference or Sentiment
Analysis."
a3efe43a72b76b8f5e5111b54393d00e6a5c97ab,1810.05241,"""According to 'One Size Does Not Fit All: Generating and Evaluating Variable Number of Keyphrases,' what is the approximate size of the StackEx dataset, in terms of the number of questions, used for keyphrase generation?""","[{'answer': 'around 330k questions', 'type': 'abstractive'}]",1810.05241.pdf,"['1810.05241.pdf', '2003.03014.pdf', '1810.03459.pdf', '1910.12795.pdf', '1708.09609.pdf', '2001.06888.pdf', '1910.07481.pdf', '1909.00175.pdf', '1809.01202.pdf', '1709.10367.pdf', '1904.05584.pdf', '1806.04511.pdf', '1910.10288.pdf', '1707.03569.pdf', '1712.00991.pdf']","FLOAT SELECTED: Table 1: Statistics of datasets we use in this work. Avg# and Var# indicate the mean and variance of numbers of target phrases per data point, respectively.",One Size Does Not Fit All: Generating and Evaluating Variable Number of Keyphrases,"Different texts shall by nature correspond to different number of keyphrases.
This desideratum is largely missing from existing neural keyphrase generation
models. In this study, we address this problem from both modeling and
evaluation perspectives.
  We first propose a recurrent generative model that generates multiple
keyphrases as delimiter-separated sequences. Generation diversity is further
enhanced with two novel techniques by manipulating decoder hidden states. In
contrast to previous approaches, our model is capable of generating diverse
keyphrases and controlling number of outputs.
  We further propose two evaluation metrics tailored towards the
variable-number generation. We also introduce a new dataset StackEx that
expands beyond the only existing genre (i.e., academic writing) in keyphrase
generation tasks. With both previous and new evaluation metrics, our model
outperforms strong baselines on all datasets."
f1e90a553a4185a4b0299bd179f4f156df798bce,1810.05241,What baseline models are listed for comparison against the proposed recurrent generative model on the present-keyphrase portion of the KP20k dataset?,"[{'answer': 'CopyRNN (Meng et al., 2017), Multi-Task (Ye and Wang, 2018), and TG-Net (Chen et al., 2018b)', 'type': 'abstractive'}, {'answer': 'CopyRNN BIBREF0, KEA BIBREF4 and Maui BIBREF8, CopyRNN*', 'type': 'extractive'}]",1810.05241.pdf,"['1810.05241.pdf', '1810.09774.pdf', '1909.05855.pdf', '1908.07195.pdf', '1912.01772.pdf', '1910.14537.pdf', '1702.03342.pdf', '1904.09678.pdf', '2001.08051.pdf']",We report our model's performance on the present-keyphrase portion of the KP20k dataset in Table TABREF35 .,One Size Does Not Fit All: Generating and Evaluating Variable Number of Keyphrases,"Different texts shall by nature correspond to different number of keyphrases.
This desideratum is largely missing from existing neural keyphrase generation
models. In this study, we address this problem from both modeling and
evaluation perspectives.
  We first propose a recurrent generative model that generates multiple
keyphrases as delimiter-separated sequences. Generation diversity is further
enhanced with two novel techniques by manipulating decoder hidden states. In
contrast to previous approaches, our model is capable of generating diverse
keyphrases and controlling number of outputs.
  We further propose two evaluation metrics tailored towards the
variable-number generation. We also introduce a new dataset StackEx that
expands beyond the only existing genre (i.e., academic writing) in keyphrase
generation tasks. With both previous and new evaluation metrics, our model
outperforms strong baselines on all datasets."
27275fe9f6a9004639f9ac33c3a5767fea388a98,2003.11645,"What hyperparameters are evaluated in the Word2Vec study, and how do they impact tasks such as Named Entity Recognition (NER), Sentiment Analysis (SA), and vector quality?","[{'answer': 'Dimension size, window size, architecture, algorithm, epochs, hidden dimension size, learning rate, loss function, optimizer algorithm.', 'type': 'abstractive'}, {'answer': 'Hyperparameters explored were: dimension size, window size, architecture, algorithm and epochs.', 'type': 'abstractive'}]",2003.11645.pdf,"['2003.11645.pdf', '1909.09270.pdf', '2002.05058.pdf', '2004.04721.pdf', '1804.08050.pdf', '1908.11047.pdf', '1909.00361.pdf', '1910.11769.pdf', '1910.13215.pdf', '2003.05377.pdf', '1810.05241.pdf', '1809.06537.pdf', '1709.05413.pdf', '1810.09774.pdf', '2001.08868.pdf']",FLOAT SELECTED: Table 1: Hyper-parameter choices,Word2Vec: Optimal Hyper-Parameters and Their Impact on NLP Downstream Tasks,"Word2Vec is a prominent model for natural language processing (NLP) tasks.
Similar inspiration is found in distributed embeddings for new state-of-the-art
(SotA) deep neural networks. However, wrong combination of hyper-parameters can
produce poor quality vectors. The objective of this work is to empirically show
optimal combination of hyper-parameters exists and evaluate various
combinations. We compare them with the released, pre-trained original word2vec
model. Both intrinsic and extrinsic (downstream) evaluations, including named
entity recognition (NER) and sentiment analysis (SA) were carried out. The
downstream tasks reveal that the best model is usually task-specific, high
analogy scores don't necessarily correlate positively with F1 scores and the
same applies to focus on data alone. Increasing vector dimension size after a
point leads to poor quality or performance. If ethical considerations to save
time, energy and the environment are made, then reasonably smaller corpora may
do just as well or even better in some cases. Besides, using a small corpus, we
obtain better human-assigned WordSim scores, corresponding Spearman correlation
and better downstream performances (with significance tests) compared to the
original model, trained on 100 billion-word corpus."
c2d1387e08cf25cb6b1f482178cca58030e85b70,2003.11645,Does the study on Word2Vec hyper-parameters present choices relevant to both the skip-gram and CBOW models?,"[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]",2003.11645.pdf,"['2003.11645.pdf', '1909.11297.pdf', '1902.09666.pdf', '1705.01265.pdf', '1809.09795.pdf', '1902.00672.pdf', '2002.02070.pdf', '1811.12254.pdf']",FLOAT SELECTED: Table 1: Hyper-parameter choices,Word2Vec: Optimal Hyper-Parameters and Their Impact on NLP Downstream Tasks,"Word2Vec is a prominent model for natural language processing (NLP) tasks.
Similar inspiration is found in distributed embeddings for new state-of-the-art
(SotA) deep neural networks. However, wrong combination of hyper-parameters can
produce poor quality vectors. The objective of this work is to empirically show
optimal combination of hyper-parameters exists and evaluate various
combinations. We compare them with the released, pre-trained original word2vec
model. Both intrinsic and extrinsic (downstream) evaluations, including named
entity recognition (NER) and sentiment analysis (SA) were carried out. The
downstream tasks reveal that the best model is usually task-specific, high
analogy scores don't necessarily correlate positively with F1 scores and the
same applies to focus on data alone. Increasing vector dimension size after a
point leads to poor quality or performance. If ethical considerations to save
time, energy and the environment are made, then reasonably smaller corpora may
do just as well or even better in some cases. Besides, using a small corpus, we
obtain better human-assigned WordSim scores, corresponding Spearman correlation
and better downstream performances (with significance tests) compared to the
original model, trained on 100 billion-word corpus."
e2e31ab279d3092418159dfd24760f0f0566e9d3,1704.00939,How did word-representations and basic pre-processing influence the final test set performance in the SemEval-2017 Task 5 (subtask 2) according to the reported results?,"[{'answer': 'beneficial impact of word-representations and basic pre-processing', 'type': 'extractive'}]",1704.00939.pdf,"['1704.00939.pdf', '1909.07734.pdf', '1909.01958.pdf', '1911.00069.pdf', '1912.03457.pdf', '1706.08032.pdf', '2004.03744.pdf', '1704.08960.pdf', '1911.07228.pdf', '1911.02821.pdf', '1810.09774.pdf', '1906.10225.pdf', '1909.04002.pdf', '1909.06162.pdf', '1612.08205.pdf', '1908.07245.pdf']","In Table TABREF17 we show model's performances on the challenge training data, in a 5-fold cross-validation setting.

Further, the final performances obtained with our approach on the challenge test set are reported in Table TABREF18 . Consistently with the cross-validation performances shown earlier, we observe the beneficial impact of word-representations and basic pre-processing.",Fortia-FBK at SemEval-2017 Task 5: Bullish or Bearish? Inferring Sentiment towards Brands from Financial News Headlines,"In this paper, we describe a methodology to infer Bullish or Bearish
sentiment towards companies/brands. More specifically, our approach leverages
affective lexica and word embeddings in combination with convolutional neural
networks to infer the sentiment of financial news headlines towards a target
company. Such architecture was used and evaluated in the context of the SemEval
2017 challenge (task 5, subtask 2), in which it obtained the best performance."
579941de2838502027716bae88e33e79e69997a6,1909.13375,"What is the difference in Exact Match (EM) and F1 metrics between the LARGE-SQUAD and MTMSNlarge models for single-span, number-type, and date-type questions from the DROP development set?","[{'answer': 'For single-span questions, the proposed LARGE-SQUAD improve performance of the MTMSNlarge baseline for 2.1 EM and 1.55 F1.\nFor number type question,  MTMSNlarge baseline  have improvement over LARGE-SQUAD for 3,11  EM and  2,98 F1. \nFor date question,  LARGE-SQUAD have improvements in 2,02 EM but MTMSNlarge have improvement of 4,39 F1.', 'type': 'abstractive'}]",1909.13375.pdf,"['1909.13375.pdf', '1909.11687.pdf', '1611.04798.pdf', '1803.09230.pdf', '1905.12260.pdf', '1910.13215.pdf', '1909.00361.pdf', '1708.09609.pdf', '1711.11221.pdf', '1901.09755.pdf', '1909.06937.pdf', '1906.10225.pdf', '1709.05413.pdf', '1911.01799.pdf']",FLOAT SELECTED: Table 2. Performance of different models on DROP’s development set in terms of Exact Match (EM) and F1.,A Simple and Effective Model for Answering Multi-span Questions,"Models for reading comprehension (RC) commonly restrict their output space to
the set of all single contiguous spans from the input, in order to alleviate
the learning problem and avoid the need for a model that generates text
explicitly. However, forcing an answer to be a single span can be restrictive,
and some recent datasets also include multi-span questions, i.e., questions
whose answer is a set of non-contiguous spans in the text. Naturally, models
that return single spans cannot answer these questions. In this work, we
propose a simple architecture for answering multi-span questions by casting the
task as a sequence tagging problem, namely, predicting for each input token
whether it should be part of the output or not. Our model substantially
improves performance on span extraction questions from DROP and Quoref by 9.9
and 5.5 EM points respectively."
9a65cfff4d99e4f9546c72dece2520cae6231810,1909.13375,What are the precise EM and F1 scores achieved by the proposed multi-span sequence tagging model on DROP's development and test sets?,"[{'answer': 'The proposed model achieves  EM 77,63 and F1 80,73  on the test and EM  76,95 and  F1 80,25 on the dev', 'type': 'abstractive'}]",1909.13375.pdf,"['1909.13375.pdf', '1709.10217.pdf', '1911.08962.pdf', '1701.02877.pdf', '1901.09755.pdf', '1804.11346.pdf', '1908.05434.pdf', '1911.11951.pdf', '1707.08559.pdf', '1810.06743.pdf', '1811.12254.pdf', '1603.04513.pdf', '2004.04721.pdf']","Table TABREF25 shows the results on DROP's test set, with our model being the best overall as of the time of writing, and not just on multi-span questions.",A Simple and Effective Model for Answering Multi-span Questions,"Models for reading comprehension (RC) commonly restrict their output space to
the set of all single contiguous spans from the input, in order to alleviate
the learning problem and avoid the need for a model that generates text
explicitly. However, forcing an answer to be a single span can be restrictive,
and some recent datasets also include multi-span questions, i.e., questions
whose answer is a set of non-contiguous spans in the text. Naturally, models
that return single spans cannot answer these questions. In this work, we
propose a simple architecture for answering multi-span questions by casting the
task as a sequence tagging problem, namely, predicting for each input token
whether it should be part of the output or not. Our model substantially
improves performance on span extraction questions from DROP and Quoref by 9.9
and 5.5 EM points respectively."
06eb9f2320451df83e27362c22eb02f4a426a018,1610.07809,"What are the three incremental levels of document preprocessing implemented in the study to evaluate the robustness of keyphrase extraction models over noisy scientific articles, as described in the SemEval-2010 dataset analysis?","[{'answer': 'raw text, text cleaning through document logical structure detection, removal of keyphrase sparse sections of the document', 'type': 'extractive'}, {'answer': 'Level 1, Level 2 and Level 3.', 'type': 'abstractive'}]",1610.07809.pdf,"['1610.07809.pdf', '1910.10288.pdf', '1902.00672.pdf', '1705.08142.pdf', '2002.02492.pdf', '1905.00563.pdf', '1705.01214.pdf', '1701.05574.pdf', '1609.00559.pdf', '2003.04866.pdf', '1604.00400.pdf', '2001.10161.pdf', '1812.10479.pdf', '1712.05999.pdf', '1802.06024.pdf', '1909.00252.pdf', '1809.02286.pdf', '1908.06379.pdf']","Three incremental levels of document preprocessing are experimented with: raw text, text cleaning through document logical structure detection, and removal of keyphrase sparse sections of the document. In doing so, we present the first consistent comparison of different keyphrase extraction models and study their robustness over noisy text.",How Document Pre-processing affects Keyphrase Extraction Performance,"The SemEval-2010 benchmark dataset has brought renewed attention to the task
of automatic keyphrase extraction. This dataset is made up of scientific
articles that were automatically converted from PDF format to plain text and
thus require careful preprocessing so that irrevelant spans of text do not
negatively affect keyphrase extraction performance. In previous work, a wide
range of document preprocessing techniques were described but their impact on
the overall performance of keyphrase extraction models is still unexplored.
Here, we re-assess the performance of several keyphrase extraction models and
measure their robustness against increasingly sophisticated levels of document
preprocessing."
c165ea43256d7ee1b1fb6f5c0c8af5f7b585e60d,1909.09484,"What improvements in the BPRA, APRA, and BLEU metrics does the proposed generative dialog policy (GDP) model achieve over the baseline models on the DSTC2 and Maluuba task-oriented dialogue datasets?","[{'answer': 'most of the models have similar performance on BPRA: DSTC2 (+0.0015), Maluuba (+0.0729)\nGDP achieves the best performance in APRA: DSTC2 (+0.2893), Maluuba (+0.2896)\nGDP significantly outperforms the baselines on BLEU: DSTC2 (+0.0791), Maluuba (+0.0492)', 'type': 'abstractive'}]",1909.09484.pdf,"['1909.09484.pdf', '1910.06036.pdf', '2002.11402.pdf', '1901.04899.pdf']","FLOAT SELECTED: Table 2: The performance of baselines and proposed model on DSTC2 and Maluuba dataset. T imefull is the time spent on training the whole model, T imeDP is the time spent on training the dialogue policy maker.",Generative Dialog Policy for Task-oriented Dialog Systems,"There is an increasing demand for task-oriented dialogue systems which can
assist users in various activities such as booking tickets and restaurant
reservations. In order to complete dialogues effectively, dialogue policy plays
a key role in task-oriented dialogue systems. As far as we know, the existing
task-oriented dialogue systems obtain the dialogue policy through
classification, which can assign either a dialogue act and its corresponding
parameters or multiple dialogue acts without their corresponding parameters for
a dialogue action. In fact, a good dialogue policy should construct multiple
dialogue acts and their corresponding parameters at the same time. However,
it's hard for existing classification-based methods to achieve this goal. Thus,
to address the issue above, we propose a novel generative dialogue policy
learning method. Specifically, the proposed method uses attention mechanism to
find relevant segments of given dialogue context and input utterance and then
constructs the dialogue policy by a seq2seq way for task-oriented dialogue
systems. Extensive experiments on two benchmark datasets show that the proposed
model significantly outperforms the state-of-the-art baselines. In addition, we
have publicly released our codes."
5908d7fb6c48f975c5dfc5b19bb0765581df2b25,1912.13109,"How many rows of labeled text messages are present in the dataset used to train the classifier for categorizing Hinglish social media content into abusive, hate-inducing, and not offensive categories?","[{'answer': '3189 rows of text messages', 'type': 'extractive'}, {'answer': 'Resulting dataset was 7934 messages for train and 700 messages for test.', 'type': 'abstractive'}]",1912.13109.pdf,"['1912.13109.pdf', '1911.02086.pdf', '1910.10288.pdf', '1910.11235.pdf', '1809.00540.pdf', '1809.09194.pdf', '1605.07333.pdf', '1703.06492.pdf', '1912.01214.pdf', '1908.11365.pdf', '1712.03547.pdf', '1603.07044.pdf', '1909.06937.pdf', '1811.02906.pdf', '2001.08051.pdf', '1908.06151.pdf']","Dataset: Based on some earlier work, only available labelled dataset had 3189 rows of text messages of average length of 116 words and with a range of 1, 1295.","""Hinglish"" Language -- Modeling a Messy Code-Mixed Language","With a sharp rise in fluency and users of ""Hinglish"" in linguistically
diverse country, India, it has increasingly become important to analyze social
content written in this language in platforms such as Twitter, Reddit,
Facebook. This project focuses on using deep learning techniques to tackle a
classification problem in categorizing social content written in Hindi-English
into Abusive, Hate-Inducing and Not offensive categories. We utilize
bi-directional sequence models with easy text augmentation techniques such as
synonym replacement, random insertion, random swap, and random deletion to
produce a state of the art classifier that outperforms the previous work done
on analyzing this dataset."
b68f72aed961d5ba152e9dc50345e1e832196a76,1909.01383,What is the average BLEU score improvement reported when using the DocRepair model for refining 4-sentence fragment translations in the English-Russian translation task?,"[{'answer': 'On average 0.64 ', 'type': 'abstractive'}]",1909.01383.pdf,"['1909.01383.pdf', '1908.06267.pdf', '1906.03538.pdf', '1910.04269.pdf', '1911.01799.pdf', '2003.03044.pdf', '1910.08987.pdf', '1709.10217.pdf', '1909.11467.pdf', '1804.05918.pdf', '1910.03467.pdf', '1910.13215.pdf', '1905.10810.pdf', '1909.08041.pdf', '1712.03547.pdf']",The BLEU scores are provided in Table TABREF24 (we evaluate translations of 4-sentence fragments).,Context-Aware Monolingual Repair for Neural Machine Translation,"Modern sentence-level NMT systems often produce plausible translations of
isolated sentences. However, when put in context, these translations may end up
being inconsistent with each other. We propose a monolingual DocRepair model to
correct inconsistencies between sentence-level translations. DocRepair performs
automatic post-editing on a sequence of sentence-level translations, refining
translations of sentences in context of each other. For training, the DocRepair
model requires only monolingual document-level data in the target language. It
is trained as a monolingual sequence-to-sequence model that maps inconsistent
groups of sentences into consistent ones. The consistent groups come from the
original training data; the inconsistent groups are obtained by sampling
round-trip translations for each isolated sentence. We show that this approach
successfully imitates inconsistencies we aim to fix: using contrastive
evaluation, we show large improvements in the translation of several contextual
phenomena in an English-Russian translation task, as well as improvements in
the BLEU score. We also conduct a human evaluation and show a strong preference
of the annotators to corrected translations over the baseline ones. Moreover,
we analyze which discourse phenomena are hard to capture using monolingual data
only."
0af16b164db20d8569df4ce688d5a62c861ace0b,1908.06264,What baseline models are listed in the validation results for the Friends dataset used to compare with Emotion BERT in the task of dialogue emotion prediction?,"[{'answer': 'BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN', 'type': 'abstractive'}, {'answer': 'bag-of-words (BOW), term frequency–inverse document frequency (TFIDF), neural-based word embedding, Logistic Regression (LR), Random Forest (RF), TextCNN BIBREF10 with initial word embedding as GloVe', 'type': 'extractive'}]",1908.06264.pdf,"['1908.06264.pdf', '2003.03044.pdf', '1706.08032.pdf', '2001.08868.pdf']",The experiment results of validation on Friends are shown in Table TABREF19. ,EmotionX-IDEA: Emotion BERT -- an Affectional Model for Conversation,"In this paper, we investigate the emotion recognition ability of the
pre-training language model, namely BERT. By the nature of the framework of
BERT, a two-sentence structure, we adapt BERT to continues dialogue emotion
prediction tasks, which rely heavily on the sentence-level context-aware
understanding. The experiments show that by mapping the continues dialogue into
a causal utterance pair, which is constructed by the utterance and the reply
utterance, models can better capture the emotions of the reply utterance. The
present method has achieved 0.815 and 0.885 micro F1 score in the testing
dataset of Friends and EmotionPush, respectively."
6a14379fee26a39631aebd0e14511ce3756e42ad,1908.06264,"In the EmotionX-IDEA study, which BERT model configurations were used for validation on the Friends dataset?","[{'answer': 'BERT-base, BERT-large, BERT-uncased, BERT-cased', 'type': 'abstractive'}]",1908.06264.pdf,"['1908.06264.pdf', '2003.03044.pdf', '1911.13066.pdf', '1911.08976.pdf', '1901.01010.pdf']",FLOAT SELECTED: Table 6: Validation Results (Friends),EmotionX-IDEA: Emotion BERT -- an Affectional Model for Conversation,"In this paper, we investigate the emotion recognition ability of the
pre-training language model, namely BERT. By the nature of the framework of
BERT, a two-sentence structure, we adapt BERT to continues dialogue emotion
prediction tasks, which rely heavily on the sentence-level context-aware
understanding. The experiments show that by mapping the continues dialogue into
a causal utterance pair, which is constructed by the utterance and the reply
utterance, models can better capture the emotions of the reply utterance. The
present method has achieved 0.815 and 0.885 micro F1 score in the testing
dataset of Friends and EmotionPush, respectively."
98eb245c727c0bd050d7686d133fa7cd9d25a0fb,1910.13215,What evaluation metric was calculated on the validation set to select the best model for inference in the cascaded multimodal speech translation system developed for the IWSLT 2019 evaluation?,"[{'answer': 'BLEU scores', 'type': 'abstractive'}]",1910.13215.pdf,"['1910.13215.pdf', '1901.08079.pdf', '1810.05241.pdf', '1902.09666.pdf', '2002.00652.pdf']","After the training finishes, we evaluate all the checkpoints on the validation set and compute the real BIBREF30 scores, based on which we select the best model for inference on the test set.",Transformer-based Cascaded Multimodal Speech Translation,"This paper describes the cascaded multimodal speech translation systems
developed by Imperial College London for the IWSLT 2019 evaluation campaign.
The architecture consists of an automatic speech recognition (ASR) system
followed by a Transformer-based multimodal machine translation (MMT) system.
While the ASR component is identical across the experiments, the MMT model
varies in terms of the way of integrating the visual context (simple
conditioning vs. attention), the type of visual features exploited (pooled,
convolutional, action categories) and the underlying architecture. For the
latter, we explore both the canonical transformer and its deliberation version
with additive and cascade variants which differ in how they integrate the
textual attention. Upon conducting extensive experiments, we found that (i) the
explored visual integration schemes often harm the translation performance for
the transformer and additive deliberation, but considerably improve the cascade
deliberation; (ii) the transformer and cascade deliberation integrate the
visual modality better than the additive deliberation, as shown by the
incongruence analysis."
c1f4d632da78714308dc502fe4e7b16ea6f76f81,1910.07481,"According to the results, which translation direction—English-French or French-English—demonstrated better performance based on both BLEU and TER metrics?","[{'answer': 'French-English', 'type': 'abstractive'}]",1910.07481.pdf,"['1910.07481.pdf', '1704.06194.pdf', '1911.12579.pdf', '1909.00578.pdf', '1809.04960.pdf', '2003.12218.pdf', '1803.09230.pdf']","FLOAT SELECTED: Table 5: Results obtained for the English-French and French-English translation tasks, scored on three test sets using BLEU and TER metrics. p-values are denoted by * and correspond to the following values: ∗< .05, ∗∗< .01, ∗∗∗< .001.",Using Whole Document Context in Neural Machine Translation,"In Machine Translation, considering the document as a whole can help to
resolve ambiguities and inconsistencies. In this paper, we propose a simple yet
promising approach to add contextual information in Neural Machine Translation.
We present a method to add source context that capture the whole document with
accurate boundaries, taking every word into account. We provide this additional
information to a Transformer model and study the impact of our method on three
language pairs. The proposed approach obtains promising results in the
English-German, English-French and French-English document-level translation
tasks. We observe interesting cross-sentential behaviors where the model learns
to use document-level information to improve translation coherence."
a99fdd34422f4231442c220c97eafc26c76508dd,1809.0054,Does the clustering algorithm involve the use of graphical models in comparison to CluStream for multilingual datasets?,"[{'answer': 'No', 'type': 'boolean'}, {'answer': 'No', 'type': 'boolean'}]",1809.00540.pdf,"['1809.00540.pdf', '2004.04721.pdf', '2002.06424.pdf', '1909.00430.pdf', '1701.09123.pdf']",FLOAT SELECTED: Table 2: Clustering results on the labeled dataset. We compare our algorithm (with and without timestamps) with the online micro-clustering routine of Aggarwal and Yu (2006) (denoted by CluStream). The F1 values are for the precision (P) and recall (R) in the following columns. See Table 3 for a legend of the different models. Best result for each language is in bold.,,
d604f5fb114169f75f9a38fab18c1e866c5ac28b,1809.0054,"What evaluation metrics are used to compare the clustering performance, including precision, recall, and F1 scores, of the proposed algorithm against CluStream?","[{'answer': 'F1, precision, recall, accuracy', 'type': 'abstractive'}, {'answer': 'Precision, recall, F1, accuracy', 'type': 'abstractive'}]",1809.00540.pdf,"['1809.00540.pdf', '1909.00279.pdf', '1909.11687.pdf', '1704.06194.pdf', '1911.00069.pdf', '1908.06083.pdf', '1707.05236.pdf', '1910.06592.pdf']",FLOAT SELECTED: Table 2: Clustering results on the labeled dataset. We compare our algorithm (with and without timestamps) with the online micro-clustering routine of Aggarwal and Yu (2006) (denoted by CluStream). The F1 values are for the precision (P) and recall (R) in the following columns. See Table 3 for a legend of the different models. Best result for each language is in bold.,,
8b3d3953454c88bde88181897a7a2c0c8dd87e23,1609.00559,What word embedding methods are compared against the proposed vector-res and vector-faith methods in this paper's evaluation of improving correlation with human judgments using semantic similarity integration?,"[{'answer': 'Skip–gram, CBOW', 'type': 'extractive'}, {'answer': 'integrated vector-res, vector-faith, Skip–gram, CBOW', 'type': 'extractive'}]",1609.00559.pdf,"['1609.00559.pdf', '1909.07734.pdf', '1903.09722.pdf', '1902.09393.pdf', '1705.01214.pdf', '2002.04181.pdf', '2004.01980.pdf', '2002.11910.pdf', '1911.02086.pdf', '1901.01010.pdf', '1611.02550.pdf', '1909.00694.pdf', '1911.00069.pdf']",chiu2016how evaluated both the the Skip–gram and CBOW models over the PMC corpus and PubMed.,Improving Correlation with Human Judgments by Integrating Semantic Similarity with Second--Order Vectors,"Vector space methods that measure semantic similarity and relatedness often
rely on distributional information such as co--occurrence frequencies or
statistical measures of association to weight the importance of particular
co--occurrences. In this paper, we extend these methods by incorporating a
measure of semantic similarity based on a human curated taxonomy into a
second--order vector representation. This results in a measure of semantic
relatedness that combines both the contextual information available in a
corpus--based vector space representation with the semantic knowledge found in
a biomedical ontology. Our results show that incorporating semantic similarity
into a second order co--occurrence matrices improves correlation with human
judgments for both similarity and relatedness, and that our method compares
favorably to various different word embedding methods that have recently been
evaluated on the same reference standards we have used."
1cbca15405632a2e9d0a7061855642d661e3b3a7,2004.03788,"What is the percentage improvement of the GTRS model over both SVM and Pawlak rough set models in detecting satirical news tweets, as presented in the experimental results?","[{'answer': 'Their GTRS approach got an improvement of 3.89% compared to SVM and 27.91% compared to Pawlak.', 'type': 'abstractive'}]",2004.03788.pdf,"['2004.03788.pdf', '2002.01664.pdf', '1910.08987.pdf', '1805.03710.pdf', '1909.13695.pdf', '1704.00939.pdf', '1904.07904.pdf', '1909.04002.pdf', '2003.12738.pdf', '1705.08142.pdf', '1611.03382.pdf', '2002.00652.pdf', '1810.12885.pdf', '1909.06162.pdf', '2003.03106.pdf', '1909.02480.pdf', '1701.03214.pdf']",FLOAT SELECTED: Table 7. Experimental results,Satirical News Detection with Semantic Feature Extraction and Game-theoretic Rough Sets,"Satirical news detection is an important yet challenging task to prevent
spread of misinformation. Many feature based and end-to-end neural nets based
satirical news detection systems have been proposed and delivered promising
results. Existing approaches explore comprehensive word features from satirical
news articles, but lack semantic metrics using word vectors for tweet form
satirical news. Moreover, the vagueness of satire and news parody determines
that a news tweet can hardly be classified with a binary decision, that is,
satirical or legitimate. To address these issues, we collect satirical and
legitimate news tweets, and propose a semantic feature based approach. Features
are extracted by exploring inconsistencies in phrases, entities, and between
main and relative clauses. We apply game-theoretic rough set model to detect
satirical news, in which probabilistic thresholds are derived by game
equilibrium and repetition learning mechanism. Experimental results on the
collected dataset show the robustness and improvement of the proposed approach
compared with Pawlak rough set model and SVM."
5a9f94ae296dda06c8aec0fb389ce2f68940ea88,1804.0805,"Based on the experimental results, by how many percentage points does the proposed multi-head decoder method improve the Character Error Rate compared to the best-performing multi-head attention model?","[{'answer': 'Their average improvement in Character Error Rate over the best MHA model was 0.33 percent points.', 'type': 'abstractive'}]",1804.08050.pdf,"['1804.08050.pdf', '2002.06424.pdf', '1803.09230.pdf', '1906.01081.pdf', '2002.00652.pdf', '2002.04181.pdf', '1908.11546.pdf']",FLOAT SELECTED: Table 2: Experimental results.,,
85912b87b16b45cde79039447a70bd1f6f1f8361,1804.0805,"In the experimental conditions described, what is the total size, in utterances, of the Corpus of Spontaneous Japanese used for evaluating the multi-head decoder model?","[{'answer': '449050', 'type': 'abstractive'}]",1804.08050.pdf,"['1804.08050.pdf', '1611.00514.pdf', '1707.03569.pdf', '1909.00105.pdf', '1909.08041.pdf', '2002.01207.pdf', '1905.06566.pdf', '1901.02257.pdf', '1908.11546.pdf', '2002.08899.pdf']",FLOAT SELECTED: Table 1: Experimental conditions.,,
33d864153822bd378a98a732ace720e2c06a6bc6,1910.11204,What were the F1 scores of the RelAwe with DepPath&RelPath model in the closed and open settings for the Chinese SRL task on the CoNLL-2009 dataset?,"[{'answer': 'In closed setting 84.22 F1 and in open 87.35 F1.', 'type': 'abstractive'}]",1910.11204.pdf,"['1910.11204.pdf', '1804.07789.pdf', '1906.10551.pdf', '1903.09722.pdf', '1809.00540.pdf', '2002.06644.pdf', '1809.02279.pdf', '1809.08298.pdf', '1906.10225.pdf', '1911.08673.pdf', '1606.00189.pdf', '1806.11432.pdf', '1604.00400.pdf', '1807.07961.pdf', '1911.07228.pdf', '1909.01247.pdf']","Table TABREF46 shows that our Open model achieves more than 3 points of f1-score than the state-of-the-art result, and RelAwe with DepPath&RelPath achieves the best in both Closed and Open settings.",Syntax-Enhanced Self-Attention-Based Semantic Role Labeling,"As a fundamental NLP task, semantic role labeling (SRL) aims to discover the
semantic roles for each predicate within one sentence. This paper investigates
how to incorporate syntactic knowledge into the SRL task effectively. We
present different approaches of encoding the syntactic information derived from
dependency trees of different quality and representations; we propose a
syntax-enhanced self-attention model and compare it with other two strong
baseline methods; and we conduct experiments with newly published deep
contextualized word representations as well. The experiment results demonstrate
that with proper incorporation of the high quality syntactic information, our
model achieves a new state-of-the-art performance for the Chinese SRL task on
the CoNLL-2009 dataset."
bab8c69e183bae6e30fc362009db9b46e720225e,1910.11204,Which two strong baseline methods are compared to the syntax-enhanced self-attention model in the SRL results for the Chinese CoNLL-2009 dataset?,"[{'answer': 'Marcheggiani and Titov (2017) and Cai et al. (2018)', 'type': 'abstractive'}]",1910.11204.pdf,"['1910.11204.pdf', '1912.00864.pdf', '1712.00991.pdf', '1905.07464.pdf', '1910.06592.pdf', '1709.05413.pdf', '1911.13066.pdf', '1608.06757.pdf', '1809.03449.pdf', '1902.10525.pdf', '1909.13375.pdf']",FLOAT SELECTED: Table 7: SRL results on the Chinese test set. We choose the best settings for each configuration of our model.,Syntax-Enhanced Self-Attention-Based Semantic Role Labeling,"As a fundamental NLP task, semantic role labeling (SRL) aims to discover the
semantic roles for each predicate within one sentence. This paper investigates
how to incorporate syntactic knowledge into the SRL task effectively. We
present different approaches of encoding the syntactic information derived from
dependency trees of different quality and representations; we propose a
syntax-enhanced self-attention model and compare it with other two strong
baseline methods; and we conduct experiments with newly published deep
contextualized word representations as well. The experiment results demonstrate
that with proper incorporation of the high quality syntactic information, our
model achieves a new state-of-the-art performance for the Chinese SRL task on
the CoNLL-2009 dataset."
3a9d391d25cde8af3334ac62d478b36b30079d74,2003.07723,Does the paper include macro F1 for the BERT model’s performance on the test set for classifying emotions in the annotated poetry dataset?,"[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]",2003.07723.pdf,"['2003.07723.pdf', '1904.10500.pdf', '1909.03242.pdf', '1806.07711.pdf', '2002.00652.pdf', '1603.07044.pdf', '1605.07333.pdf']",FLOAT SELECTED: Table 7: Recall and precision scores of the best model (dbmdz) for each emotion on the test set. ‘Support’ signifies the number of labels.,"PO-EMO: Conceptualization, Annotation, and Modeling of Aesthetic Emotions in German and English Poetry","Most approaches to emotion analysis of social media, literature, news, and
other domains focus exclusively on basic emotion categories as defined by Ekman
or Plutchik. However, art (such as literature) enables engagement in a broader
range of more complex and subtle emotions. These have been shown to also
include mixed emotional responses. We consider emotions in poetry as they are
elicited in the reader, rather than what is expressed in the text or intended
by the author. Thus, we conceptualize a set of aesthetic emotions that are
predictive of aesthetic appreciation in the reader, and allow the annotation of
multiple labels per line to capture mixed emotions within their context. We
evaluate this novel setting in an annotation experiment both with carefully
trained experts and via crowdsourcing. Our annotation with experts leads to an
acceptable agreement of kappa = .70, resulting in a consistent dataset for
future large scale analysis. Finally, we conduct first emotion classification
experiments based on BERT, showing that identifying aesthetic emotions is
challenging in our data, with up to .52 F1-micro on the German subset. Data and
resources are available at https://github.com/tnhaider/poetry-emotion"
11dde2be9a69a025f2fc29ce647201fb5a4df580,1710.0934,What are the UAS (Unlabeled Attachment Score) and LAS (Labeled Attachment Score) achieved by the proposed non-local transition-based parser compared to the strongest state-of-the-art greedy and overall parsers on the PT-SD dataset?,"[{'answer': 'Proposed method achieves 94.5 UAS and 92.4 LAS  compared to 94.3 and 92.2 of best state-of-the -art greedy based parser. Best state-of-the art parser overall achieves 95.8 UAS and 94.6 LAS.', 'type': 'abstractive'}]",1710.09340.pdf,"['1710.09340.pdf', '1909.00754.pdf', '1910.11204.pdf', '1909.07734.pdf', '1908.07195.pdf', '1911.10049.pdf', '1606.05320.pdf', '1812.10479.pdf', '1910.06748.pdf', '1910.08210.pdf', '1910.14497.pdf']","Table TABREF12 compares our novel system with other state-of-the-art transition-based dependency parsers on the PT-SD. Greedy parsers are in the first block, beam-search and dynamic programming parsers in the second block. The third block shows the best result on this benchmark, obtained with constituent parsing with generative re-ranking and conversion to dependencies. Despite being the only non-projective parser tested on a practically projective dataset, our parser achieves the highest score among greedy transition-based models (even above those trained with a dynamic oracle).

We even slightly outperform the arc-swift system of Qi2017, with the same model architecture, implementation and training setup, but based on the projective arc-eager transition-based parser instead.",,
46570c8faaeefecc8232cfc2faab0005faaba35f,1809.09795,"What are the 7 benchmark datasets that were used to evaluate sarcasm and irony detection across Tweets, Reddit posts, and online debates in the study on deep contextualized word representations?","[{'answer': 'SemEval 2018 Task 3, BIBREF20, BIBREF4, SARC 2.0, SARC 2.0 pol, Sarcasm Corpus V1 (SC-V1), Sarcasm Corpus V2 (SC-V2)', 'type': 'extractive'}]",1809.09795.pdf,"['1809.09795.pdf', '1711.02013.pdf', '1912.01214.pdf', '1909.06937.pdf', '1712.03556.pdf']","FLOAT SELECTED: Table 1: Benchmark datasets: Tweets, Reddit posts and online debates for sarcasm and irony detection.",Deep contextualized word representations for detecting sarcasm and irony,"Predicting context-dependent and non-literal utterances like sarcastic and
ironic expressions still remains a challenging task in NLP, as it goes beyond
linguistic patterns, encompassing common sense and shared knowledge as crucial
components. To capture complex morpho-syntactic features that can usually serve
as indicators for irony or sarcasm across dynamic contexts, we propose a model
that uses character-level vector representations of words, based on ELMo. We
test our model on 7 different datasets derived from 3 different data sources,
providing state-of-the-art performance in 6 of them, and otherwise offering
competitive results."
1771a55236823ed44d3ee537de2e85465bf03eaf,2002.11402,"What are the differences in recall scores between the proposed BERT-Multilingual, Bi-GRU, CRF-based model and traditional NER systems such as Stanford, SpaCy, and Flair when evaluated using both Traditional NERs and Wikipedia titles as references?","[{'answer': 'Between the model and Stanford, Spacy and Flair the differences are 42.91, 25.03, 69.8 with Traditional NERs as reference and  49.88, 43.36, 62.43 with Wikipedia titles as reference.', 'type': 'abstractive'}]",2002.11402.pdf,"['2002.11402.pdf', '1805.03710.pdf', '1909.07734.pdf', '1912.01673.pdf', '2002.04181.pdf']",FLOAT SELECTED: Table 2. Comparison with Traditional NERs as reference,"Detecting Potential Topics In News Using BERT, CRF and Wikipedia","For a news content distribution platform like Dailyhunt, Named Entity
Recognition is a pivotal task for building better user recommendation and
notification algorithms. Apart from identifying names, locations, organisations
from the news for 13+ Indian languages and use them in algorithms, we also need
to identify n-grams which do not necessarily fit in the definition of
Named-Entity, yet they are important. For example, ""me too movement"", ""beef
ban"", ""alwar mob lynching"". In this exercise, given an English language text,
we are trying to detect case-less n-grams which convey important information
and can be used as topics and/or hashtags for a news. Model is built using
Wikipedia titles data, private English news corpus and BERT-Multilingual
pre-trained model, Bi-GRU and CRF architecture. It shows promising results when
compared with industry best Flair, Spacy and Stanford-caseless-NER in terms of
F1 and especially Recall."
1d74fd1d38a5532d20ffae4abbadaeda225b6932,2002.11402,What are the F1 score and recall values reported when comparing your BERT-CRF model's performance on detecting case-less n-grams against traditional Named Entity Recognition systems?,"[{'answer': 'F1 score and Recall are 68.66, 80.08 with Traditional NERs as reference and 59.56, 69.76 with Wikipedia titles as reference.', 'type': 'abstractive'}]",2002.11402.pdf,"['2002.11402.pdf', '1707.08559.pdf', '2003.12738.pdf', '1809.06537.pdf', '2002.01664.pdf', '2003.03106.pdf', '1810.12085.pdf', '1903.09722.pdf', '1701.00185.pdf', '1603.04513.pdf']",FLOAT SELECTED: Table 2. Comparison with Traditional NERs as reference,"Detecting Potential Topics In News Using BERT, CRF and Wikipedia","For a news content distribution platform like Dailyhunt, Named Entity
Recognition is a pivotal task for building better user recommendation and
notification algorithms. Apart from identifying names, locations, organisations
from the news for 13+ Indian languages and use them in algorithms, we also need
to identify n-grams which do not necessarily fit in the definition of
Named-Entity, yet they are important. For example, ""me too movement"", ""beef
ban"", ""alwar mob lynching"". In this exercise, given an English language text,
we are trying to detect case-less n-grams which convey important information
and can be used as topics and/or hashtags for a news. Model is built using
Wikipedia titles data, private English news corpus and BERT-Multilingual
pre-trained model, Bi-GRU and CRF architecture. It shows promising results when
compared with industry best Flair, Spacy and Stanford-caseless-NER in terms of
F1 and especially Recall."
44104668796a6ca10e2ea3ecf706541da1cec2cf,1905.1081,"What is the accuracy difference between the interpretable system leveraging edit distance and cosine similarity with semantic vectors, and the LSTM model enhanced with ELMo embeddings for spelling correction on the PlEWi dataset?","[{'answer': 'Accuracy of best interpretible system was 0.3945 while accuracy of LSTM-ELMo net was 0.6818.', 'type': 'abstractive'}]",1905.10810.pdf,"['1905.10810.pdf', '1910.08987.pdf', '2001.08868.pdf', '1809.06537.pdf']",The experimental results are presented in Table TABREF4 .,,
4a8bceb3b6d45f14c4749115d6aa83912f0b0a6e,1807.07961,Are the experimental datasets used in the paper's proposed emoji-based sentiment analysis exclusively from English-language tweets?,"[{'answer': 'Yes', 'type': 'boolean'}]",1807.07961.pdf,"['1807.07961.pdf', '1906.11180.pdf', '1909.05855.pdf', '2003.12738.pdf', '1909.00694.pdf', '1901.05280.pdf', '2001.05493.pdf']",FLOAT SELECTED: Table 1: Tweet examples with emojis. The sentiment ground truth is given in the second column. The examples show that inconsistent sentiments exist between emojis and texts.,Twitter Sentiment Analysis via Bi-sense Emoji Embedding and Attention-based LSTM,"Sentiment analysis on large-scale social media data is important to bridge
the gaps between social media contents and real world activities including
political election prediction, individual and public emotional status
monitoring and analysis, and so on. Although textual sentiment analysis has
been well studied based on platforms such as Twitter and Instagram, analysis of
the role of extensive emoji uses in sentiment analysis remains light. In this
paper, we propose a novel scheme for Twitter sentiment analysis with extra
attention on emojis. We first learn bi-sense emoji embeddings under positive
and negative sentimental tweets individually, and then train a sentiment
classifier by attending on these bi-sense emoji embeddings with an
attention-based long short-term memory network (LSTM). Our experiments show
that the bi-sense embedding is effective for extracting sentiment-aware
embeddings of emojis and outperforms the state-of-the-art models. We also
visualize the attentions to show that the bi-sense emoji embedding provides
better guidance on the attention mechanism to obtain a more robust
understanding of the semantics and sentiments."
a3f108f60143d13fe38d911b1cc3b17bdffde3bd,1809.10644,"What are the F1 scores for the SR, HATE, and HAR datasets achieved by the proposed model, as reported in the paper on predictive embeddings for hate speech detection?","[{'answer': 'Proposed model achieves 0.86, 0.924, 0.71 F1 score on SR, HATE, HAR datasets respectively.', 'type': 'abstractive'}]",1809.10644.pdf,"['1809.10644.pdf', '1911.03310.pdf', '1808.09029.pdf', '2004.03354.pdf', '1711.02013.pdf', '1909.00279.pdf', '1902.09314.pdf', '1704.00939.pdf']",FLOAT SELECTED: Table 2: F1 Results3,Predictive Embeddings for Hate Speech Detection on Twitter,"We present a neural-network based approach to classifying online hate speech
in general, as well as racist and sexist speech in particular. Using
pre-trained word embeddings and max/mean pooling from simple, fully-connected
transformations of these embeddings, we are able to predict the occurrence of
hate speech on three commonly used publicly available datasets. Our models
match or outperform state of the art F1 performance on all three datasets using
significantly fewer parameters and minimal feature preprocessing compared to
previous methods."
ea6764a362bac95fb99969e9f8c773a61afd8f39,1607.06025,"What is the highest classification accuracy reported for a model trained on the combined original and best generated dataset, with the generated datasets filtered at a threshold of 0.6?","[{'answer': '82.0%', 'type': 'abstractive'}]",1607.06025.pdf,"['1607.06025.pdf', '1909.01247.pdf', '1910.05154.pdf', '1606.05320.pdf', '1908.06264.pdf', '1709.10367.pdf']",FLOAT SELECTED: Table 4: The performance of classifiers trained on the original and generated datasets. The classifiers were tested on original test set. The generated datasets were generated by the models from Table 3. The generated datasets were filtered with threshold 0.6.,Constructing a Natural Language Inference Dataset using Generative Neural Networks,"Natural Language Inference is an important task for Natural Language
Understanding. It is concerned with classifying the logical relation between
two sentences. In this paper, we propose several text generative neural
networks for generating text hypothesis, which allows construction of new
Natural Language Inference datasets. To evaluate the models, we propose a new
metric -- the accuracy of the classifier trained on the generated dataset. The
accuracy obtained by our best generative model is only 2.7% lower than the
accuracy of the classifier trained on the original, human crafted dataset.
Furthermore, the best generated dataset combined with the original dataset
achieves the highest accuracy. The best model learns a mapping embedding for
each training example. By comparing various metrics we show that datasets that
obtain higher ROUGE or METEOR scores do not necessarily yield higher
classification accuracies. We also provide analysis of what are the
characteristics of a good dataset including the distinguishability of the
generated datasets from the original one."
8958465d1eaf81c8b781ba4d764a4f5329f026aa,1910.14497,"What are the three bias evaluation metrics that show reduction when applying the probabilistic bias mitigation method, as indicated by the cosine distance measurements in the experimental results?","[{'answer': 'RIPA, Neighborhood Metric, WEAT', 'type': 'abstractive'}]",1910.14497.pdf,"['1910.14497.pdf', '1910.11204.pdf', '1804.11346.pdf', '1711.02013.pdf', '2003.07996.pdf', '1701.05574.pdf', '1901.02262.pdf', '1909.01958.pdf', '1705.00108.pdf', '1909.00578.pdf', '1804.07789.pdf', '1810.00663.pdf', '1909.00279.pdf']",Geometric bias mitigation uses the cosine distances between words to both measure and remove gender bias BIBREF0.,Probabilistic Bias Mitigation in Word Embeddings,"It has been shown that word embeddings derived from large corpora tend to
incorporate biases present in their training data. Various methods for
mitigating these biases have been proposed, but recent work has demonstrated
that these methods hide but fail to truly remove the biases, which can still be
observed in word nearest-neighbor statistics. In this work we propose a
probabilistic view of word embedding bias. We leverage this framework to
present a novel method for mitigating bias which relies on probabilistic
observations to yield a more robust bias mitigation algorithm. We demonstrate
that this method effectively reduces bias according to three separate measures
of bias while maintaining embedding quality across various popular benchmark
semantic tasks"
c35806cf68220b2b9bb082b62f493393b9bdff86,1809.02279,"What is the state-of-the-art accuracy achieved by the proposed CAS-LSTM architecture on the SNLI benchmark, and how does it leverage the fusion of hidden and memory cell states to outperform existing models?","[{'answer': 'In SNLI, our best model achieves the new state-of-the-art accuracy of 87.0%,  we can see that our models outperform other models by large margin, achieving the new state of the art., Our models achieve the new state-of-the-art accuracy on SST-2 and competitive accuracy on SST-5', 'type': 'extractive'}, {'answer': 'accuracy of 87.0%', 'type': 'extractive'}]",1809.02279.pdf,"['1809.02279.pdf', '1901.04899.pdf', '1606.05320.pdf', '2002.11402.pdf', '1611.00514.pdf', '1911.08962.pdf', '2002.01664.pdf', '1703.02507.pdf']","In SNLI, our best model achieves the new state-of-the-art accuracy of 87.0% with relatively fewer parameters.",Cell-aware Stacked LSTMs for Modeling Sentences,"We propose a method of stacking multiple long short-term memory (LSTM) layers
for modeling sentences. In contrast to the conventional stacked LSTMs where
only hidden states are fed as input to the next layer, the suggested
architecture accepts both hidden and memory cell states of the preceding layer
and fuses information from the left and the lower context using the soft gating
mechanism of LSTMs. Thus the architecture modulates the amount of information
to be delivered not only in horizontal recurrence but also in vertical
connections, from which useful features extracted from lower layers are
effectively conveyed to upper layers. We dub this architecture Cell-aware
Stacked LSTM (CAS-LSTM) and show from experiments that our models bring
significant performance gain over the standard LSTMs on benchmark datasets for
natural language inference, paraphrase detection, sentiment classification, and
machine translation. We also conduct extensive qualitative analysis to
understand the internal behavior of the suggested approach."
bbb77f2d6685c9257763ca38afaaef29044b4018,1701.05574,What system achieved the best F-score in sarcasm detection after incorporating cognitive features from eye-tracking data with traditional features?,"[{'answer': 'Gaze Sarcasm using Multi Instance Logistic Regression.', 'type': 'abstractive'}, {'answer': 'the MILR classifier', 'type': 'extractive'}]",1701.05574.pdf,"['1701.05574.pdf', '1909.11687.pdf', '2002.02492.pdf', '1911.05153.pdf', '1906.10225.pdf', '1908.06151.pdf', '1908.11047.pdf', '1910.06036.pdf']","FLOAT SELECTED: Table 3: Classification results for different feature combinations. P→ Precision, R→Recall, F→ F˙score, Kappa→ Kappa statistics show agreement with the gold labels. Subscripts 1 and -1 correspond to sarcasm and non-sarcasm classes respectively.",Harnessing Cognitive Features for Sarcasm Detection,"In this paper, we propose a novel mechanism for enriching the feature vector,
for the task of sarcasm detection, with cognitive features extracted from
eye-movement patterns of human readers. Sarcasm detection has been a
challenging research problem, and its importance for NLP applications such as
review summarization, dialog systems and sentiment analysis is well recognized.
Sarcasm can often be traced to incongruity that becomes apparent as the full
sentence unfolds. This presence of incongruity- implicit or explicit- affects
the way readers eyes move through the text. We observe the difference in the
behaviour of the eye, while reading sarcastic and non sarcastic sentences.
Motivated by his observation, we augment traditional linguistic and stylistic
features for sarcasm detection with the cognitive features obtained from
readers eye movement data. We perform statistical classification using the
enhanced feature set so obtained. The augmented cognitive features improve
sarcasm detection by 3.7% (in terms of F-score), over the performance of the
best reported system."
74b338d5352fe1a6fd592e38269a4c81fe79b866,1701.05574,"What specific eye-movement-based cognitive features, including fixation and saccade metrics, are listed as contributing to the sarcasm detection model in this paper?","[{'answer': 'Readability (RED),  Number of Words (LEN), Avg. Fixation Duration (FDUR), Avg. Fixation Count (FC), Avg. Saccade Length (SL), Regression Count (REG), Skip count (SKIP), Count of regressions from second half\nto first half of the sentence (RSF), Largest Regression Position (LREG),  Edge density of the saliency gaze\ngraph (ED),  Fixation Duration at Left/Source\n(F1H, F1S),  Fixation Duration at Right/Target\n(F2H, F2S),  Forward Saccade Word Count of\nSource (PSH, PSS),  Forward SaccadeWord Count of Destination\n(PSDH, PSDS), Regressive Saccade Word Count of\nSource (RSH, RSS),  Regressive Saccade Word Count of\nDestination (RSDH, RSDS)', 'type': 'abstractive'}]",1701.05574.pdf,"['1701.05574.pdf', '1909.00361.pdf', '1809.09795.pdf', '2002.04181.pdf', '1902.09393.pdf']",FLOAT SELECTED: Table 2: The complete set of features used in our system.,Harnessing Cognitive Features for Sarcasm Detection,"In this paper, we propose a novel mechanism for enriching the feature vector,
for the task of sarcasm detection, with cognitive features extracted from
eye-movement patterns of human readers. Sarcasm detection has been a
challenging research problem, and its importance for NLP applications such as
review summarization, dialog systems and sentiment analysis is well recognized.
Sarcasm can often be traced to incongruity that becomes apparent as the full
sentence unfolds. This presence of incongruity- implicit or explicit- affects
the way readers eyes move through the text. We observe the difference in the
behaviour of the eye, while reading sarcastic and non sarcastic sentences.
Motivated by his observation, we augment traditional linguistic and stylistic
features for sarcasm detection with the cognitive features obtained from
readers eye movement data. We perform statistical classification using the
enhanced feature set so obtained. The augmented cognitive features improve
sarcasm detection by 3.7% (in terms of F-score), over the performance of the
best reported system."
df0257ab04686ddf1c6c4d9b0529a7632330b98e,2001.08868,How does Go-Explore's phase 1 exploration compare to DQN++ and DRQN++ in terms of interaction efficiency and trajectory optimality when solving the CoinCollector text-based game?,"[{'answer': ' On the other hand, phase 1 of Go-Explore finds an optimal trajectory with approximately half the interactions with the environment, Moreover, the trajectory length found by Go-Explore is always optimal (i.e. 30 steps) whereas both DQN++ and DRQN++ have an average length of 38 and 42 respectively., Especially interesting is that the performance of DRRN is substantially lower than that of the Go-Explore Seq2Seq model', 'type': 'extractive'}, {'answer': 'On Coin Collector, proposed model finds shorter path in fewer number of interactions with enironment.\nOn Cooking World, proposed model uses smallest amount of steps and on average has bigger score and number of wins by significant margin.', 'type': 'abstractive'}]",2001.08868.pdf,"['2001.08868.pdf', '1605.08675.pdf', '1701.05574.pdf', '1912.10435.pdf', '1910.14497.pdf', '1603.07044.pdf', '1909.11467.pdf', '1709.10217.pdf', '1908.07195.pdf', '1908.11365.pdf', '1904.03288.pdf', '1706.08032.pdf', '2002.02492.pdf', '1703.07090.pdf', '1808.09029.pdf']","Results ::: CoinCollector
In this setting, we compare the number of actions played in the environment (frames) and the score achieved by the agent (i.e. +1 reward if the coin is collected). In Go-Explore we also count the actions used to restore the environment to a selected cell, i.e. to bring the agent to the state represented in the selected cell. This allows a one-to-one comparison of the exploration efficiency between Go-Explore and algorithms that use a count-based reward in text-based games. Importantly, BIBREF8 showed that DQN and DRQN, without such counting rewards, could never find a successful trajectory in hard games such as the ones used in our experiments. Figure FIGREF17 shows the number of interactions with the environment (frames) versus the maximum score obtained, averaged over 10 games of the same difficulty. As shown by BIBREF8, DRQN++ finds a trajectory with the maximum score faster than to DQN++. On the other hand, phase 1 of Go-Explore finds an optimal trajectory with approximately half the interactions with the environment. Moreover, the trajectory length found by Go-Explore is always optimal (i.e. 30 steps) whereas both DQN++ and DRQN++ have an average length of 38 and 42 respectively.",Exploration Based Language Learning for Text-Based Games,"This work presents an exploration and imitation-learning-based agent capable
of state-of-the-art performance in playing text-based computer games.
Text-based computer games describe their world to the player through natural
language and expect the player to interact with the game using text. These
games are of interest as they can be seen as a testbed for language
understanding, problem-solving, and language generation by artificial agents.
Moreover, they provide a learning environment in which these skills can be
acquired through interactions with an environment rather than using fixed
corpora. One aspect that makes these games particularly challenging for
learning agents is the combinatorially large action space. Existing methods for
solving text-based games are limited to games that are either very simple or
have an action space restricted to a predetermined set of admissible actions.
In this work, we propose to use the exploration approach of Go-Explore for
solving text-based games. More specifically, in an initial exploration phase,
we first extract trajectories with high rewards, after which we train a policy
to solve the game by imitating these trajectories. Our experiments show that
this approach outperforms existing solutions in solving text-based games, and
it is more sample efficient in terms of the number of interactions with the
environment. Moreover, we show that the learned policy can generalize better
than existing solutions to unseen games without using any restriction on the
action space."
286078813136943dfafb5155ee15d2429e7601d9,1802.06024,"What are the F1 score improvements of the LiLi model over the ""Single"" model on the Freebase and WordNet datasets, specifically for known, unknown, and all relation types?","[{'answer': 'In case of Freebase knowledge base, LiLi model had better F1 score than the single model by 0.20 , 0.01, 0.159 for kwn, unk, and all test Rel type.  The values for WordNet are 0.25, 0.1, 0.2. \n', 'type': 'abstractive'}]",1802.06024.pdf,"['1802.06024.pdf', '1911.10049.pdf', '1805.03710.pdf', '1612.05270.pdf', '1909.11467.pdf', '1910.08987.pdf', '1909.07734.pdf', '1810.05241.pdf', '1909.09270.pdf', '1912.10011.pdf', '1909.00754.pdf', '1906.05474.pdf', '1709.05413.pdf', '1910.11204.pdf']","Baselines. As none of the existing KBC methods can solve the OKBC problem, we choose various versions of LiLi as baselines.

Single: Version of LiLi where we train a single prediction model INLINEFORM0 for all test relations.

Sep: We do not transfer (past learned) weights for initializing INLINEFORM0 , i.e., we disable LL.

F-th): Here, we use a fixed prediction threshold 0.5 instead of relation-specific threshold INLINEFORM0 .

BG: The missing or connecting links (when the user does not respond) are filled with “@-RelatedTo-@"" blindly, no guessing mechanism.

w/o PTS: LiLi does not ask for additional clues via past task selection for skillset improvement.",Towards a Continuous Knowledge Learning Engine for Chatbots,"Although chatbots have been very popular in recent years, they still have
some serious weaknesses which limit the scope of their applications. One major
weakness is that they cannot learn new knowledge during the conversation
process, i.e., their knowledge is fixed beforehand and cannot be expanded or
updated during conversation. In this paper, we propose to build a general
knowledge learning engine for chatbots to enable them to continuously and
interactively learn new knowledge during conversations. As time goes by, they
become more and more knowledgeable and better and better at learning and
conversation. We model the task as an open-world knowledge base completion
problem and propose a novel technique called lifelong interactive learning and
inference (LiLi) to solve it. LiLi works by imitating how humans acquire
knowledge and perform inference during an interactive conversation. Our
experimental results show LiLi is highly promising."
1adbdb5f08d67d8b05328ccc86d297ac01bf076c,1810.03459,What are the specific BABEL languages used for training the multilingual sequence-to-sequence model and the target languages for transfer learning experiments as described in the paper?,"[{'answer': 'Train languages are: Cantonese, Bengali, Pashto, Turkish, Vietnamese, Haitian, Tamil, Kurdish, Tokpisin and Georgian, while Assamese, Tagalog, Swahili, Lao are used as target languages.', 'type': 'abstractive'}]",1810.03459.pdf,"['1810.03459.pdf', '2003.04642.pdf', '1709.10217.pdf', '1910.07481.pdf']",Table TABREF14 presents the details of the languages used in this work for training and evaluation.,"Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling","Sequence-to-sequence (seq2seq) approach for low-resource ASR is a relatively
new direction in speech research. The approach benefits by performing model
training without using lexicon and alignments. However, this poses a new
problem of requiring more data compared to conventional DNN-HMM systems. In
this work, we attempt to use data from 10 BABEL languages to build a
multi-lingual seq2seq model as a prior model, and then port them towards 4
other BABEL languages using transfer learning approach. We also explore
different architectures for improving the prior multilingual seq2seq model. The
paper also discusses the effect of integrating a recurrent neural network
language model (RNNLM) with a seq2seq model during decoding. Experimental
results show that the transfer learning approach from the multilingual model
shows substantial gains over monolingual models across all 4 BABEL languages.
Incorporating an RNNLM also brings significant improvements in terms of %WER,
and achieves recognition performance comparable to the models trained with
twice more training data."
79ed71a3505cf6f5e8bf121fd7ec1518cab55cae,2002.08899,How do the authors simulate Wernicke’s and Broca’s aphasia in the LLA-LSTM model by damaging neural modules in the paper?,"[{'answer': 'Damage to neural modules is done by randomly initializing their weights, causing the loss of all learned information.', 'type': 'abstractive'}]",2002.08899.pdf,"['2002.08899.pdf', '1804.05918.pdf', '1910.12129.pdf', '1910.05154.pdf', '1908.07245.pdf', '1707.05236.pdf']","FLOAT SELECTED: Table 2: Results for artificial Wernicke’s and Broca’s aphasia induced in the LLA-LSTM model. Damage to neural modules is done by randomly initializing their weights, causing the loss of all learned information. The inputs that we present are arbitrarily chosen, subject to the constraints listed in the text. Mean precision (Prec.) results on the test sets are also provided to demonstrate corpus-level results. An ellipses represents the repetition of the preceding word at least 1000 times.",Compositional Neural Machine Translation by Removing the Lexicon from Syntax,"The meaning of a natural language utterance is largely determined from its
syntax and words. Additionally, there is evidence that humans process an
utterance by separating knowledge about the lexicon from syntax knowledge.
Theories from semantics and neuroscience claim that complete word meanings are
not encoded in the representation of syntax. In this paper, we propose neural
units that can enforce this constraint over an LSTM encoder and decoder. We
demonstrate that our model achieves competitive performance across a variety of
domains including semantic parsing, syntactic parsing, and English to Mandarin
Chinese translation. In these cases, our model outperforms the standard LSTM
encoder and decoder architecture on many or all of our metrics. To demonstrate
that our model achieves the desired separation between the lexicon and syntax,
we analyze its weights and explore its behavior when different neural modules
are damaged. When damaged, we find that the model displays the knowledge
distortions that aphasics are evidenced to have."
74fb77a624ea9f1821f58935a52cca3086bb0981,1902.09666,"How many tweets are annotated in the OLID dataset, which focuses on the distribution of label combinations for categorizing offensive content by type and target?","[{'answer': '14,100 tweets', 'type': 'abstractive'}, {'answer': 'Dataset contains total of 14100 annotations.', 'type': 'abstractive'}]",1902.09666.pdf,"['1902.09666.pdf', '2002.01359.pdf', '1801.05147.pdf', '1806.04330.pdf', '1912.08960.pdf', '1809.10644.pdf', '1809.02286.pdf', '1908.08345.pdf', '1910.14497.pdf', '1802.06024.pdf', '1811.02906.pdf', '1908.10084.pdf', '1703.07090.pdf', '1709.10367.pdf', '1810.00663.pdf']",FLOAT SELECTED: Table 3: Distribution of label combinations in OLID.,Predicting the Type and Target of Offensive Posts in Social Media,"As offensive content has become pervasive in social media, there has been
much research in identifying potentially offensive messages. However, previous
work on this topic did not consider the problem as a whole, but rather focused
on detecting very specific types of offensive content, e.g., hate speech,
cyberbulling, or cyber-aggression. In contrast, here we target several
different kinds of offensive content. In particular, we model the task
hierarchically, identifying the type and the target of offensive messages in
social media. For this purpose, we complied the Offensive Language
Identification Dataset (OLID), a new dataset with tweets annotated for
offensive content using a fine-grained three-layer annotation scheme, which we
make publicly available. We discuss the main similarities and differences
between OLID and pre-existing datasets for hate speech identification,
aggression detection, and similar tasks. We further experiment with and we
compare the performance of different machine learning models on OLID."
1b72aa2ec3ce02131e60626639f0cf2056ec23ca,1902.09666,"How many tweets are annotated at each hierarchical level (A, B, and C) in the OLID dataset in the paper on identifying the type and target of offensive posts in social media?","[{'answer': 'Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets', 'type': 'abstractive'}]",1902.09666.pdf,"['1902.09666.pdf', '1908.07245.pdf', '1908.06083.pdf', '1902.00330.pdf', '1910.13215.pdf', '1909.11687.pdf', '1909.09270.pdf', '1911.05153.pdf', '2002.01664.pdf', '1909.06937.pdf', '1909.08859.pdf', '1911.08673.pdf', '2002.10361.pdf']", The breakdown of the data into training and testing for the labels from each level is shown in Table TABREF15 .,Predicting the Type and Target of Offensive Posts in Social Media,"As offensive content has become pervasive in social media, there has been
much research in identifying potentially offensive messages. However, previous
work on this topic did not consider the problem as a whole, but rather focused
on detecting very specific types of offensive content, e.g., hate speech,
cyberbulling, or cyber-aggression. In contrast, here we target several
different kinds of offensive content. In particular, we model the task
hierarchically, identifying the type and the target of offensive messages in
social media. For this purpose, we complied the Offensive Language
Identification Dataset (OLID), a new dataset with tweets annotated for
offensive content using a fine-grained three-layer annotation scheme, which we
make publicly available. We discuss the main similarities and differences
between OLID and pre-existing datasets for hate speech identification,
aggression detection, and similar tasks. We further experiment with and we
compare the performance of different machine learning models on OLID."
12ac76b77f22ed3bcb6430bcd0b909441d79751b,1910.11235,"Which models are compared to the proposed method, where BLEU scores and road exam results are reported for the EMNLP2017 WMT News dataset?","[{'answer': 'TEACHER FORCING (TF), SCHEDULED SAMPLING (SS),  SEQGAN, RANKGAN, LEAKGAN.', 'type': 'abstractive'}]",1910.11235.pdf,"['1910.11235.pdf', '1701.05574.pdf', '1704.00939.pdf', '1708.09609.pdf', '1809.02286.pdf', '1907.09369.pdf', '1805.04033.pdf', '1909.01383.pdf', '1909.03405.pdf', '2003.11645.pdf', '1903.09722.pdf', '1906.10551.pdf', '1604.00400.pdf']",FLOAT SELECTED: Table 2: Results on EMNLP2017 WMT News dataset. The 95 % confidence intervals from multiple trials are reported.,Rethinking Exposure Bias In Language Modeling,"Exposure bias describes the phenomenon that a language model trained under
the teacher forcing schema may perform poorly at the inference stage when its
predictions are conditioned on its previous predictions unseen from the
training corpus. Recently, several generative adversarial networks (GANs) and
reinforcement learning (RL) methods have been introduced to alleviate this
problem. Nonetheless, a common issue in RL and GANs training is the sparsity of
reward signals. In this paper, we adopt two simple strategies, multi-range
reinforcing, and multi-entropy sampling, to amplify and denoise the reward
signal. Our model produces an improvement over competing models with regards to
BLEU scores and road exam, a new metric we designed to measure the robustness
against exposure bias in language models."
4a4616e1a9807f32cca9b92ab05e65b05c2a1bf5,1905.07464,What were the number of drug labels and sentences in Test Set 1 and Test Set 2 used to evaluate the performance of your system in the TAC 2018 track for drug-drug interaction extraction from structured product labels?,"[{'answer': 'Test set 1 contained 57 drug labels and 8208 sentences and test set 2 contained 66 drug labels and 4224 sentences', 'type': 'abstractive'}]",1905.07464.pdf,"['1905.07464.pdf', '1711.02013.pdf', '1804.07789.pdf', '1904.10503.pdf', '1906.01081.pdf', '1909.09270.pdf', '1908.05828.pdf', '1912.01214.pdf', '1912.00864.pdf']","Two test sets of 57 and 66 drug labels, referred to as Test Set 1 and 2 respectively, with gold standard annotations are used to evaluate participating systems.",A Multi-Task Learning Framework for Extracting Drugs and Their Interactions from Drug Labels,"Preventable adverse drug reactions as a result of medical errors present a
growing concern in modern medicine. As drug-drug interactions (DDIs) may cause
adverse reactions, being able to extracting DDIs from drug labels into
machine-readable form is an important effort in effectively deploying drug
safety information. The DDI track of TAC 2018 introduces two large
hand-annotated test sets for the task of extracting DDIs from structured
product labels with linkage to standard terminologies. Herein, we describe our
approach to tackling tasks one and two of the DDI track, which corresponds to
named entity recognition (NER) and sentence-level relation extraction
respectively. Namely, our approach resembles a multi-task learning framework
designed to jointly model various sub-tasks including NER and interaction type
and outcome prediction. On NER, our system ranked second (among eight teams) at
33.00% and 38.25% F1 on Test Sets 1 and 2 respectively. On relation extraction,
our system ranked second (among four teams) at 21.59% and 23.55% on Test Sets 1
and 2 respectively."
701571680724c05ca70c11bc267fb1160ea1460a,1806.11432,"Does the GAN model using the Diehl-Martinez-Kamalu (DMK) loss function in this paper show state-of-the-art performance when predicting Airbnb booking success from listing descriptions, based on development accuracy and cross-entropy loss?","[{'answer': 'No', 'type': 'boolean'}]",1806.11432.pdf,"['1806.11432.pdf', '1912.08960.pdf', '1810.12196.pdf', '1710.09340.pdf', '1901.04899.pdf', '1902.00672.pdf', '2002.06675.pdf', '1711.02013.pdf', '1612.05270.pdf', '1911.08962.pdf', '1910.11204.pdf', '1612.08205.pdf']","That said, these results, though they do show a marginal increase in dev accuracy and a decrease in CE loss, suggest that perhaps listing description is not too predictive of occupancy rate given our parameterizations. ",Using General Adversarial Networks for Marketing: A Case Study of Airbnb,"In this paper, we examine the use case of general adversarial networks (GANs)
in the field of marketing. In particular, we analyze how GAN models can
replicate text patterns from successful product listings on Airbnb, a
peer-to-peer online market for short-term apartment rentals. To do so, we
define the Diehl-Martinez-Kamalu (DMK) loss function as a new class of
functions that forces the model's generated output to include a set of
user-defined keywords. This allows the general adversarial network to recommend
a way of rewording the phrasing of a listing description to increase the
likelihood that it is booked. Although we tailor our analysis to Airbnb data,
we believe this framework establishes a more general model for how generative
algorithms can be used to produce text samples for the purposes of marketing."
600b097475b30480407ce1de81c28c54a0b3b2f8,1806.11432,"What GloVe-based baseline configurations, including ensembling and non-ensembling approaches, are compared in the study on using GANs to generate Airbnb listing descriptions with the DMK loss function?","[{'answer': 'GloVe vectors trained on Wikipedia Corpus with ensembling, and GloVe vectors trained on Airbnb Data without ensembling', 'type': 'abstractive'}]",1806.11432.pdf,"['1806.11432.pdf', '1812.06864.pdf', '1703.07090.pdf', '1909.01958.pdf', '1712.03547.pdf', '1909.00361.pdf', '1912.01772.pdf', '1905.11901.pdf', '1909.13375.pdf', '1809.02286.pdf', '2001.10161.pdf', '1901.04899.pdf', '1911.02821.pdf', '1909.03242.pdf', '1708.09609.pdf', '1910.06592.pdf', '1902.00330.pdf', '1910.11204.pdf']",FLOAT SELECTED: Table 1: Results of RNN/LSTM,Using General Adversarial Networks for Marketing: A Case Study of Airbnb,"In this paper, we examine the use case of general adversarial networks (GANs)
in the field of marketing. In particular, we analyze how GAN models can
replicate text patterns from successful product listings on Airbnb, a
peer-to-peer online market for short-term apartment rentals. To do so, we
define the Diehl-Martinez-Kamalu (DMK) loss function as a new class of
functions that forces the model's generated output to include a set of
user-defined keywords. This allows the general adversarial network to recommend
a way of rewording the phrasing of a listing description to increase the
likelihood that it is booked. Although we tailor our analysis to Airbnb data,
we believe this framework establishes a more general model for how generative
algorithms can be used to produce text samples for the purposes of marketing."
3fb4334e5a4702acd44bd24eb1831bb7e9b98d31,1909.00361,"What are the exact numbers of questions in the CMRC 2018 and DRCD Chinese machine reading comprehension datasets, as listed in the evaluations of this paper?","[{'answer': 'Evaluation datasets used:\nCMRC 2018 - 18939 questions, 10 answers\nDRCD - 33953 questions, 5 answers\nNIST MT02/03/04/05/06/08 Chinese-English - Not specified\n\nSource language train data:\nSQuAD - Not specified', 'type': 'abstractive'}]",1909.00361.pdf,"['1909.00361.pdf', '1909.00430.pdf', '1911.12579.pdf', '1905.11901.pdf', '1901.02257.pdf', '1706.08032.pdf', '1807.07961.pdf', '1704.08960.pdf', '1612.08205.pdf', '2004.04721.pdf', '1707.00110.pdf', '1909.03242.pdf']",We evaluate our approaches on two public Chinese span-extraction machine reading comprehension datasets: CMRC 2018 (simplified Chinese) BIBREF8 and DRCD (traditional Chinese) BIBREF9. The statistics of the two datasets are listed in Table TABREF29.,Cross-Lingual Machine Reading Comprehension,"Though the community has made great progress on Machine Reading Comprehension
(MRC) task, most of the previous works are solving English-based MRC problems,
and there are few efforts on other languages mainly due to the lack of
large-scale training data. In this paper, we propose Cross-Lingual Machine
Reading Comprehension (CLMRC) task for the languages other than English.
Firstly, we present several back-translation approaches for CLMRC task, which
is straightforward to adopt. However, to accurately align the answer into
another language is difficult and could introduce additional noise. In this
context, we propose a novel model called Dual BERT, which takes advantage of
the large-scale training data provided by rich-resource language (such as
English) and learn the semantic relations between the passage and question in a
bilingual context, and then utilize the learned knowledge to improve reading
comprehension performance of low-resource language. We conduct experiments on
two Chinese machine reading comprehension datasets CMRC 2018 and DRCD. The
results show consistent and significant improvements over various
state-of-the-art systems by a large margin, which demonstrate the potentials in
CLMRC task. Resources available: https://github.com/ymcui/Cross-Lingual-MRC"
9ec1f88ceec84a10dc070ba70e90a792fba8ce71,2002.01984,"What was the highest MRR score achieved by the UNCC system for Factoid questions in Batch 3 of the BioASQ Task-7B, Phase-B competition?","[{'answer': '0.5115', 'type': 'abstractive'}, {'answer': '0.6103', 'type': 'extractive'}]",2002.01984.pdf,"['2002.01984.pdf', '1909.00015.pdf', '1803.09230.pdf', '1905.10810.pdf', '1711.00106.pdf', '1809.00540.pdf', '1810.12885.pdf']",FLOAT SELECTED: Table 1: Factoid Questions. In Batch 3 we obtained the highest score. Also the relative distance between our best system and the top performing system shrunk between Batch 4 and 5.,"UNCC Biomedical Semantic Question Answering Systems. BioASQ: Task-7B, Phase-B","In this paper, we detail our submission to the 2019, 7th year, BioASQ
competition. We present our approach for Task-7b, Phase B, Exact Answering
Task. These Question Answering (QA) tasks include Factoid, Yes/No, List Type
Question answering. Our system is based on a contextual word embedding model.
We have used a Bidirectional Encoder Representations from Transformers(BERT)
based system, fined tuned for biomedical question answering task using BioBERT.
In the third test batch set, our system achieved the highest MRR score for
Factoid Question Answering task. Also, for List type question answering task
our system achieved the highest recall score in the fourth test batch set.
Along with our detailed approach, we present the results for our submissions,
and also highlight identified downsides for our current approach and ways to
improve them in our future experiments."
75b69eef4a38ec16df63d60be9708a3c44a79c56,2002.05058,"How does the proposed comparative evaluator's Pearson correlation with human judgment in story generation and dialogue response tasks, at both the sample and model levels, compare to the next best automated evaluation metric?","[{'answer': 'Pearson correlation to human judgement - proposed vs next best metric\nSample level comparison:\n- Story generation: 0.387 vs 0.148\n- Dialogue: 0.472 vs 0.341\nModel level comparison:\n- Story generation:  0.631 vs 0.302\n- Dialogue: 0.783 vs 0.553', 'type': 'abstractive'}]",2002.05058.pdf,"['2002.05058.pdf', '1810.10254.pdf', '1902.09393.pdf', '1905.12260.pdf', '1911.13066.pdf', '1911.02711.pdf', '1703.06492.pdf', '2002.00652.pdf', '1908.07195.pdf', '2002.08899.pdf', '1908.06267.pdf']",The experimental results are summarized in Table 1. We can see that the proposed comparative evaluator correlates far better with human judgment than BLEU and perplexity.,Learning to Compare for Better Training and Evaluation of Open Domain Natural Language Generation Models,"Automated evaluation of open domain natural language generation (NLG) models
remains a challenge and widely used metrics such as BLEU and Perplexity can be
misleading in some cases. In our paper, we propose to evaluate natural language
generation models by learning to compare a pair of generated sentences by
fine-tuning BERT, which has been shown to have good natural language
understanding ability. We also propose to evaluate the model-level quality of
NLG models with sample-level comparison results with skill rating system. While
able to be trained in a fully self-supervised fashion, our model can be further
fine-tuned with a little amount of human preference annotation to better
imitate human judgment. In addition to evaluating trained models, we propose to
apply our model as a performance indicator during training for better
hyperparameter tuning and early-stopping. We evaluate our approach on both
story generation and chit-chat dialogue response generation. Experimental
results show that our model correlates better with human preference compared
with previous automated evaluation approaches. Training with the proposed
metric yields better performance in human evaluation, which further
demonstrates the effectiveness of the proposed model."
92294820ac0d9421f086139e816354970f066d8a,1910.06036,"What are the specific improvements and slight degradations in automatic evaluation metrics for the proposed model compared to the baselines, especially for the Bleu1 scores on the Zhou split and DuSplit datasets?","[{'answer': 'Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1', 'type': 'abstractive'}]",1910.06036.pdf,"['1910.06036.pdf', '1809.04960.pdf', '1806.04330.pdf', '1902.09666.pdf', '1908.08345.pdf', '1807.07961.pdf', '1904.05584.pdf', '1910.03814.pdf', '1910.02339.pdf', '2003.11563.pdf', '1705.00108.pdf', '1912.01772.pdf', '1906.01081.pdf']",Table TABREF30 shows automatic evaluation results for our model and baselines (copied from their papers).,Improving Question Generation With to the Point Context,"Question generation (QG) is the task of generating a question from a
reference sentence and a specified answer within the sentence. A major
challenge in QG is to identify answer-relevant context words to finish the
declarative-to-interrogative sentence transformation. Existing
sequence-to-sequence neural models achieve this goal by proximity-based answer
position encoding under the intuition that neighboring words of answers are of
high possibility to be answer-relevant. However, such intuition may not apply
to all cases especially for sentences with complex answer-relevant relations.
Consequently, the performance of these models drops sharply when the relative
distance between the answer fragment and other non-stop sentence words that
also appear in the ground truth question increases. To address this issue, we
propose a method to jointly model the unstructured sentence and the structured
answer-relevant relation (extracted from the sentence in advance) for question
generation. Specifically, the structured answer-relevant relation acts as the
to the point context and it thus naturally helps keep the generated question to
the point, while the unstructured sentence provides the full information.
Extensive experiments show that to the point context helps our question
generation model achieve significant improvements on several automatic
evaluation metrics. Furthermore, our model is capable of generating diverse
questions for a sentence which conveys multiple relations of its answer
fragment."
a02696d4ab728ddd591f84a352df9375faf7d1b4,1605.07683,"How many dialogs are in the training, validation, and test sets for Task 6, derived from the 2nd Dialog State Tracking Challenge, as reported in the Learning End-to-End Goal-Oriented Dialog paper?","[{'answer': '1,618 training dialogs, 500 validation dialogs, and 1,117 test dialogs', 'type': 'abstractive'}]",1605.07683.pdf,"['1605.07683.pdf', '1908.06379.pdf', '1710.06700.pdf', '2004.01878.pdf']","FLOAT SELECTED: Table 1: Data used in this paper. Tasks 1-5 were generated using our simulator and share the same KB. Task 6 was converted from the 2nd Dialog State Tracking Challenge (Henderson et al., 2014a). Concierge is made of chats extracted from a real online concierge service. (∗) Tasks 1-5 have two test sets, one using the vocabulary of the training set and the other using out-of-vocabulary words.",Learning End-to-End Goal-Oriented Dialog,"Traditional dialog systems used in goal-oriented applications require a lot
of domain-specific handcrafting, which hinders scaling up to new domains.
End-to-end dialog systems, in which all components are trained from the dialogs
themselves, escape this limitation. But the encouraging success recently
obtained in chit-chat dialog may not carry over to goal-oriented settings. This
paper proposes a testbed to break down the strengths and shortcomings of
end-to-end dialog systems in goal-oriented applications. Set in the context of
restaurant reservation, our tasks require manipulating sentences and symbols,
so as to properly conduct conversations, issue API calls and use the outputs of
such calls. We show that an end-to-end dialog system based on Memory Networks
can reach promising, yet imperfect, performance and learn to perform
non-trivial operations. We confirm those results by comparing our system to a
hand-crafted slot-filling baseline on data from the second Dialog State
Tracking Challenge (Henderson et al., 2014a). We show similar result patterns
on data extracted from an online concierge service."
40c0f97c3547232d6aa039fcb330f142668dea4b,1709.10367,"Does the empirical evaluation of the S-EFE model, which includes datasets like U.S. Senate speeches, ArXiv papers, and grocery purchases, involve only English-language content?","[{'answer': 'No', 'type': 'boolean'}]",1709.10367.pdf,"['1709.10367.pdf', '1909.08041.pdf', '1910.10288.pdf', '1905.10810.pdf', '1611.04798.pdf']","Data. We apply the sefe on three datasets: ArXiv papers, U.S. Senate speeches, and purchases on supermarket grocery shopping data. We describe these datasets below, and we provide a summary of the datasets in Table TABREF17 .",Structured Embedding Models for Grouped Data,"Word embeddings are a powerful approach for analyzing language, and
exponential family embeddings (EFE) extend them to other types of data. Here we
develop structured exponential family embeddings (S-EFE), a method for
discovering embeddings that vary across related groups of data. We study how
the word usage of U.S. Congressional speeches varies across states and party
affiliation, how words are used differently across sections of the ArXiv, and
how the co-purchase patterns of groceries can vary across seasons. Key to the
success of our method is that the groups share statistical information. We
develop two sharing strategies: hierarchical modeling and amortization. We
demonstrate the benefits of this approach in empirical studies of speeches,
abstracts, and shopping baskets. We show how S-EFE enables group-specific
interpretation of word usage, and outperforms EFE in predicting held-out data."
52f9cd05d8312ae3c7a43689804bac63f7cac34b,1809.03449,"Does the paper introducing the Knowledge Aided Reader (KAR) suggest that humans' robustness to noise in comprehension tasks stems from their ability to use general knowledge, such as the semantic connections extracted via WordNet?","[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]",1809.03449.pdf,"['1809.03449.pdf', '1611.03382.pdf', '1808.09920.pdf', '1904.07904.pdf', '1807.07961.pdf', '1901.05280.pdf', '1809.05752.pdf', '1605.07333.pdf', '1906.03538.pdf', '1903.09722.pdf', '1907.03060.pdf', '1912.01214.pdf']","To verify the effectiveness of general knowledge, we first study the relationship between the amount of general knowledge and the performance of KAR. As shown in Table TABREF13 , by increasing INLINEFORM0 from 0 to 5 in the data enrichment method, the amount of general knowledge rises monotonically, but the performance of KAR first rises until INLINEFORM1 reaches 3 and then drops down.",Explicit Utilization of General Knowledge in Machine Reading Comprehension,"To bridge the gap between Machine Reading Comprehension (MRC) models and
human beings, which is mainly reflected in the hunger for data and the
robustness to noise, in this paper, we explore how to integrate the neural
networks of MRC models with the general knowledge of human beings. On the one
hand, we propose a data enrichment method, which uses WordNet to extract
inter-word semantic connections as general knowledge from each given
passage-question pair. On the other hand, we propose an end-to-end MRC model
named as Knowledge Aided Reader (KAR), which explicitly uses the above
extracted general knowledge to assist its attention mechanisms. Based on the
data enrichment method, KAR is comparable in performance with the
state-of-the-art MRC models, and significantly more robust to noise than them.
When only a subset (20%-80%) of the training examples are available, KAR
outperforms the state-of-the-art MRC models by a large margin, and is still
reasonably robust to noise."
b1bc9ae9d40e7065343c12f860a461c7c730a612,1912.0896,"Which specific ShapeWorldICE datasets, including OneShape, MultiShapes, TwoShapes, Count, and Ratio, are described in the paper for evaluating model performance?","[{'answer': 'Existential (OneShape, MultiShapes), Spacial (TwoShapes, Multishapes), Quantification (Count, Ratio) datasets are generated from ShapeWorldICE', 'type': 'abstractive'}, {'answer': 'ShapeWorldICE datasets: OneShape, MultiShapes, TwoShapes, MultiShapes, Count, and Ratio', 'type': 'abstractive'}]",1912.08960.pdf,"['1912.08960.pdf', '1909.06162.pdf', '2002.02492.pdf', '1812.01704.pdf', '1810.05241.pdf', '1909.03544.pdf', '1909.00430.pdf', '1911.13066.pdf', '1707.03569.pdf', '1910.08210.pdf', '1711.02013.pdf', '1704.00939.pdf', '1911.03597.pdf', '1805.03710.pdf', '1911.02711.pdf', '1904.03288.pdf', '1611.03382.pdf']","We develop a variety of ShapeWorldICE datasets, with a similar idea to the “skill tasks” in the bAbI framework BIBREF22. Table TABREF4 gives an overview for different ShapeWorldICE datasets we use in this paper.",,
66125cfdf11d3bf8e59728428e02021177142c3a,1911.0331,"How does the data demonstrate that mBERT's language-neutral component leads to better word-alignment performance than FastAlign, even when using large parallel corpora, and what role does explicit projection learning play in this outcome?","[{'answer': 'Table TABREF15 shows that word-alignment based on mBERT representations surpasses the outputs of the standard FastAlign tool even if it was provided large parallel corpus. This suggests that word-level semantics are well captured by mBERT contextual embeddings. For this task, learning an explicit projection had a negligible effect on the performance.', 'type': 'extractive'}, {'answer': 'explicit projection had a negligible effect on the performance', 'type': 'extractive'}]",1911.03310.pdf,"['1911.03310.pdf', '2003.01769.pdf', '1909.09587.pdf', '1902.09393.pdf', '1809.04960.pdf']","Following BIBREF3, we hypothesize that a sentence representation in mBERT is composed of a language-specific component, which identifies the language of the sentence, and a language-neutral component, which captures the meaning of the sentence in a language-independent way. We assume that the language-specific component is similar across all sentences in the language.",,
47a30eb4d0d6f5f2ff4cdf6487265a25c1b18fd8,1909.0043,Does the system trained using expectation regularization (XR) with sentence-level (S) and noisy sentence-level (N) data outperform the fully supervised neural system trained solely with aspect-level data (A) in terms of accuracy and Macro-F1 score for Aspect-based Sentiment Classification?,"[{'answer': 'Yes', 'type': 'boolean'}]",1909.00430.pdf,"['1909.00430.pdf', '2001.00137.pdf', '1904.10500.pdf', '1903.09722.pdf', '2004.01980.pdf', '1908.11365.pdf', '2003.03044.pdf']","FLOAT SELECTED: Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method’s result is significantly better than all baseline methods, † indicates that the method’s result is significantly better than all baselines methods that use the aspect-based data only, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a). Numbers for Semisupervised are from (He et al., 2018b).",,
e42fbf6c183abf1c6c2321957359c7683122b48e,1909.0043,"What are the reported accuracy values for the BiLSTM-XR and BiLSTM-XR-Dev Estimation models, trained using only XR loss on the SemEval-15 and SemEval-16 datasets, in the study on aspect-based sentiment classification using Sentence-level, Noisy sentence-level, and Aspect-level data?","[{'answer': 'BiLSTM-XR-Dev Estimation accuracy is 83.31 for SemEval-15 and 87.68 for SemEval-16.\nBiLSTM-XR accuracy is 83.31 for SemEval-15 and 88.12 for SemEval-16.\n', 'type': 'abstractive'}]",1909.00430.pdf,"['1909.00430.pdf', '1811.12254.pdf', '1911.02086.pdf', '1910.00458.pdf', '1905.12260.pdf', '2003.12218.pdf', '1905.11901.pdf', '1904.10503.pdf', '1810.09774.pdf', '1805.03710.pdf', '2001.05467.pdf', '1701.03214.pdf', '1909.03405.pdf', '1712.00991.pdf', '1909.00361.pdf', '1910.11204.pdf', '1912.08960.pdf', '1609.00559.pdf']","FLOAT SELECTED: Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method’s result is significantly better than all baseline methods, † indicates that the method’s result is significantly better than all baselines methods that use the aspect-based data only, with p < 0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from (He et al., 2018a). Numbers for Semisupervised are from (He et al., 2018b).",,
52e8f79814736fea96fd9b642881b476243e1698,1909.13695,"What specific speaker verification systems, including adaptations and fine-tunings of VoxCeleb-based models, are evaluated for their % EER on BULATS and Linguaskill test sets?","[{'answer': 'BULATS i-vector/PLDA\nBULATS x-vector/PLDA\nVoxCeleb x-vector/PLDA\nPLDA adaptation (X1)\n Extractor fine-tuning (X2) ', 'type': 'abstractive'}]",1909.13695.pdf,"['1909.13695.pdf', '1912.06670.pdf', '1810.12885.pdf', '1912.00864.pdf', '2002.01359.pdf', '1810.12196.pdf', '2004.01878.pdf', '1804.07789.pdf', '2003.12738.pdf', '2004.03744.pdf', '1902.09393.pdf', '1911.02711.pdf']",FLOAT SELECTED: Table 2. % EER performance of VoxCeleb-based systems on BULATS and Linguaskill test sets.,Non-native Speaker Verification for Spoken Language Assessment,"Automatic spoken language assessment systems are becoming more popular in
order to handle increasing interests in second language learning. One challenge
for these systems is to detect malpractice. Malpractice can take a range of
forms, this paper focuses on detecting when a candidate attempts to impersonate
another in a speaking test. This form of malpractice is closely related to
speaker verification, but applied in the specific domain of spoken language
assessment. Advanced speaker verification systems, which leverage deep-learning
approaches to extract speaker representations, have been successfully applied
to a range of native speaker verification tasks. These systems are explored for
non-native spoken English data in this paper. The data used for speaker
enrolment and verification is mainly taken from the BULATS test, which assesses
English language skills for business. Performance of systems trained on
relatively limited amounts of BULATS data, and standard large speaker
verification corpora, is compared. Experimental results on large-scale test
sets with millions of trials show that the best performance is achieved by
adapting the imported model to non-native data. Breakdown of impostor trials
across different first languages (L1s) and grades is analysed, which shows that
inter-L1 impostors are more challenging for speaker verification systems."
5a65ad10ff954d0f27bb3ccd9027e3d8f7f6bb76,1904.10503,Which models are directly compared with the authors' ELMo and Wikidata-based approach for fine-grained entity recognition using the Wiki(gold) dataset?,"[{'answer': 'Akbik et al. (2018), Link et al. (2012)', 'type': 'abstractive'}, {'answer': 'They compare to Akbik et al. (2018) and Link et al. (2012).', 'type': 'abstractive'}]",1904.10503.pdf,"['1904.10503.pdf', '2003.11645.pdf', '1909.04002.pdf', '1809.02286.pdf', '1707.00110.pdf', '1906.03538.pdf', '2001.00137.pdf', '1606.05320.pdf', '2002.01359.pdf', '2003.07996.pdf', '2001.08051.pdf', '2001.08868.pdf', '1909.01958.pdf', '1910.06592.pdf', '1909.00279.pdf', '1810.10254.pdf', '1911.08673.pdf', '2003.05377.pdf']",FLOAT SELECTED: Table 3: Comparison with existing models.,Fine-Grained Named Entity Recognition using ELMo and Wikidata,"Fine-grained Named Entity Recognition is a task whereby we detect and
classify entity mentions to a large set of types. These types can span diverse
domains such as finance, healthcare, and politics. We observe that when the
type set spans several domains the accuracy of the entity detection becomes a
limitation for supervised learning models. The primary reason being the lack of
datasets where entity boundaries are properly annotated, whilst covering a
large spectrum of entity types. Furthermore, many named entity systems suffer
when considering the categorization of fine grained entity types. Our work
attempts to address these issues, in part, by combining state-of-the-art deep
learning models (ELMo) with an expansive knowledge base (Wikidata). Using our
framework, we cross-validate our model on the 112 fine-grained entity types
based on the hierarchy given from the Wiki(gold) dataset."
f17ca24b135f9fe6bb25dc5084b13e1637ec7744,1804.05918,"Based on the results reported for the paragraph-level neural network model trained on the PDTB corpus, which explicit discourse relations demonstrate the highest and lowest classification performance?","[{'answer': 'explicit discourse relations', 'type': 'extractive'}, {'answer': 'Best: Expansion (Exp). Worst: Comparison (Comp).', 'type': 'abstractive'}]",1804.05918.pdf,"['1804.05918.pdf', '1902.10525.pdf', '1909.13714.pdf', '1704.05907.pdf', '2003.05377.pdf', '1905.12260.pdf', '1910.03814.pdf', '1909.05855.pdf', '1905.06566.pdf', '1910.11769.pdf', '1809.00530.pdf', '1606.05320.pdf', '1603.00968.pdf', '1804.00079.pdf', '1902.09314.pdf', '1910.12129.pdf']","the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 ).",Improving Implicit Discourse Relation Classification by Modeling Inter-dependencies of Discourse Units in a Paragraph,"We argue that semantic meanings of a sentence or clause can not be
interpreted independently from the rest of a paragraph, or independently from
all discourse relations and the overall paragraph-level discourse structure.
With the goal of improving implicit discourse relation classification, we
introduce a paragraph-level neural networks that model inter-dependencies
between discourse units as well as discourse relation continuity and patterns,
and predict a sequence of discourse relations in a paragraph. Experimental
results show that our model outperforms the previous state-of-the-art systems
on the benchmark corpus of PDTB."
fc8bc6a3c837a9d1c869b7ee90cf4e3c39bcd102,1905.06566,"Does the paper provide a comparison between hierarchical models like HiBERT and non-hierarchical models such as BERT, evaluated as baselines on the CNNDM dataset?","[{'answer': 'There were hierarchical and non-hierarchical baselines; BERT was one of those baselines', 'type': 'abstractive'}]",1905.06566.pdf,"['1905.06566.pdf', '1909.13375.pdf', '2002.01664.pdf', '1910.00825.pdf', '1910.06592.pdf', '1901.03866.pdf', '1908.11047.pdf', '1809.02286.pdf', '1911.03310.pdf']","FLOAT SELECTED: Table 1: Results of various models on the CNNDM test set using full-length F1 ROUGE-1 (R-1), ROUGE-2 (R2), and ROUGE-L (R-L).",HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization,"Neural extractive summarization models usually employ a hierarchical encoder
for document encoding and they are trained using sentence-level labels, which
are created heuristically using rule-based methods. Training the hierarchical
encoder with these \emph{inaccurate} labels is challenging. Inspired by the
recent work on pre-training transformer sentence encoders
\cite{devlin:2018:arxiv}, we propose {\sc Hibert} (as shorthand for {\bf
HI}erachical {\bf B}idirectional {\bf E}ncoder {\bf R}epresentations from {\bf
T}ransformers) for document encoding and a method to pre-train it using
unlabeled data. We apply the pre-trained {\sc Hibert} to our summarization
model and it outperforms its randomly initialized counterpart by 1.25 ROUGE on
the CNN/Dailymail dataset and by 2.0 ROUGE on a version of New York Times
dataset. We also achieve the state-of-the-art performance on these two
datasets."
4704cbb35762d0172f5ac6c26b67550921567a65,1811.02906,What are the percentage improvements in F1 score and accuracy for the top-performing transfer learning strategy in Task 1 in the paper on BiLSTM-CNN neural networks and background knowledge transfer for offensive language detection on German Twitter data?,"[{'answer': 'In task 1 best transfer learning strategy improves F1 score by 4.4% and accuracy score by 3.3%, in task 2 best transfer learning strategy improves F1 score by 2.9% and accuracy score by 1.7%', 'type': 'abstractive'}]",1811.02906.pdf,"['1811.02906.pdf', '1910.06036.pdf', '1912.01214.pdf', '1612.08205.pdf', '1910.07481.pdf', '1909.13375.pdf', '1908.08345.pdf']",FLOAT SELECTED: Table 2: Transfer learning performance (Task 1),Transfer Learning from LDA to BiLSTM-CNN for Offensive Language Detection in Twitter,"We investigate different strategies for automatic offensive language
classification on German Twitter data. For this, we employ a sequentially
combined BiLSTM-CNN neural network. Based on this model, three transfer
learning tasks to improve the classification performance with background
knowledge are tested. We compare 1. Supervised category transfer: social media
data annotated with near-offensive language categories, 2. Weakly-supervised
category transfer: tweets annotated with emojis they contain, 3. Unsupervised
category transfer: tweets annotated with topic clusters obtained by Latent
Dirichlet Allocation (LDA). Further, we investigate the effect of three
different strategies to mitigate negative effects of 'catastrophic forgetting'
during transfer learning. Our results indicate that transfer learning in
general improves offensive language detection. Best results are achieved from
pre-training our model on the unsupervised topic clustering of tweets in
combination with thematic user cluster information."
4e9684fd68a242cb354fa6961b0e3b5c35aae4b6,1910.03814,"What are the F-score, AUC, and mean accuracy values comparing the unimodal LSTM model and the best multimodal model (FCM) for hate speech detection on the MMHS150K dataset?","[{'answer': 'Unimodal LSTM vs Best Multimodal (FCM)\n- F score: 0.703 vs 0.704\n- AUC: 0.732 vs 0.734 \n- Mean Accuracy: 68.3 vs 68.4 ', 'type': 'abstractive'}]",1910.03814.pdf,"['1910.03814.pdf', '1705.01265.pdf', '2003.03044.pdf', '1809.01202.pdf', '1905.06566.pdf', '1910.12129.pdf', '1607.06025.pdf', '1704.05907.pdf', '1908.07195.pdf', '1908.08345.pdf', '1909.03405.pdf', '1711.11221.pdf', '1905.00563.pdf', '1910.11204.pdf', '1804.08139.pdf']","Table TABREF31 shows the F-score, the Area Under the ROC Curve (AUC) and the mean accuracy (ACC) of the proposed models when different inputs are available.",Exploring Hate Speech Detection in Multimodal Publications,"In this work we target the problem of hate speech detection in multimodal
publications formed by a text and an image. We gather and annotate a large
scale dataset from Twitter, MMHS150K, and propose different models that jointly
analyze textual and visual information for hate speech detection, comparing
them with unimodal detection. We provide quantitative and qualitative results
and analyze the challenges of the proposed task. We find that, even though
images are useful for the hate speech detection task, current multimodal models
cannot outperform models analyzing only text. We discuss why and open the field
and the dataset for further research."
01edeca7b902ae3fd66264366bf548acea1db364,1808.0343,"What recall scores for positions 1, 2, and 5 were reported for the introduced method when evaluated with 10 candidates in the comparison of different models?","[{'answer': 'Their model resulted in values of 0.476, 0.672 and 0.893 for recall at position 1,2 and 5 respectively in 10 candidates.', 'type': 'abstractive'}]",1808.03430.pdf,"['1808.03430.pdf', '1909.11687.pdf', '1909.00175.pdf', '1910.06592.pdf', '1909.00430.pdf', '1605.07683.pdf', '1704.05907.pdf']",FLOAT SELECTED: Table 1: Comparison of different models.,,
7c794fa0b2818d354ca666969107818a2ffdda0c,1910.00912,"Apart from entity tagging, what combined metrics for intent and entities are reported in HERMIT NLU's evaluation results?","[{'answer': 'We also report the metrics in BIBREF7 for consistency, we report the span F1,  Exact Match (EM) accuracy of the entire sequence of labels, metric that combines intent and entities', 'type': 'extractive'}]",1910.00912.pdf,"['1910.00912.pdf', '1909.03405.pdf', '1605.07683.pdf', '1910.02339.pdf', '1908.06267.pdf']","Following BIBREF7, we then evaluated a metric that combines intent and entities, computed by simply summing up the two confusion matrices (Table TABREF23). Results highlight the contribution of the entity tagging task, where HERMIT outperforms the other approaches. Paired-samples t-tests were conducted to compare the HERMIT combined F1 against the other systems.",Hierarchical Multi-Task Natural Language Understanding for Cross-domain Conversational AI: HERMIT NLU,"We present a new neural architecture for wide-coverage Natural Language
Understanding in Spoken Dialogue Systems. We develop a hierarchical multi-task
architecture, which delivers a multi-layer representation of sentence meaning
(i.e., Dialogue Acts and Frame-like structures). The architecture is a
hierarchy of self-attention mechanisms and BiLSTM encoders followed by CRF
tagging layers. We describe a variety of experiments, showing that our approach
obtains promising results on a dataset annotated with Dialogue Acts and Frame
Semantics. Moreover, we demonstrate its applicability to a different, publicly
available NLU dataset annotated with domain-specific intents and corresponding
semantic roles, providing overall performance higher than state-of-the-art
tools such as RASA, Dialogflow, LUIS, and Watson. For example, we show an
average 4.45% improvement in entity tagging F-score over Rasa, Dialogflow and
LUIS."
9ecde59ffab3c57ec54591c3c7826a9188b2b270,2003.04642,"Which MRC gold standards published between 2016 and 2019, meeting the citation-based thresholds and aligned with the problem definition in this study, were analyzed for features like lexical ambiguity, factual correctness, and the presence of lexical cues as part of their answer selection styles?","[{'answer': 'fit our problem definition and were published in the years 2016 to 2019, have at least $(2019 - publication\\ year) \\times 20$ citations', 'type': 'extractive'}, {'answer': 'MSMARCO,  HOTPOTQA, RECORD,  MULTIRC, NEWSQA, and DROP.', 'type': 'abstractive'}]",2003.04642.pdf,"['2003.04642.pdf', '1910.03814.pdf', '2003.03014.pdf', '1609.00559.pdf', '1611.00514.pdf', '2004.03354.pdf', '1909.01013.pdf', '1810.00663.pdf', '1910.03467.pdf']","Concretely, we selected gold standards that fit our problem definition and were published in the years 2016 to 2019, have at least $(2019 - publication\ year) \times 20$ citations, and bucket them according to the answer selection styles as described in Section SECREF4",A Framework for Evaluation of Machine Reading Comprehension Gold Standards,"Machine Reading Comprehension (MRC) is the task of answering a question over
a paragraph of text. While neural MRC systems gain popularity and achieve
noticeable performance, issues are being raised with the methodology used to
establish their performance, particularly concerning the data design of gold
standards that are used to evaluate them. There is but a limited understanding
of the challenges present in this data, which makes it hard to draw comparisons
and formulate reliable hypotheses. As a first step towards alleviating the
problem, this paper proposes a unifying framework to systematically investigate
the present linguistic features, required reasoning and background knowledge
and factual correctness on one hand, and the presence of lexical cues as a
lower bound for the requirement of understanding on the other hand. We propose
a qualitative annotation schema for the first and a set of approximative
metrics for the latter. In a first application of the framework, we analyse
modern MRC gold standards and present our findings: the absence of features
that contribute towards lexical ambiguity, the varying factual correctness of
the expected answers and the presence of lexical cues, all of which potentially
lower the reading comprehension complexity and quality of the evaluation data."
a1064307a19cd7add32163a70b6623278a557946,1911.12579,"According to the corpus statistics presented in the Sindhi word embeddings study, how many unique tokens were retained in the final cleaned vocabulary after preprocessing?","[{'answer': '908456 unique words are available in collected corpus.', 'type': 'abstractive'}]",1911.12579.pdf,"['1911.12579.pdf', '1910.11769.pdf', '1807.07961.pdf', '1904.05584.pdf', '1704.08960.pdf', '1904.07904.pdf', '1904.10500.pdf', '1701.00185.pdf', '1803.09230.pdf', '1808.09029.pdf', '1910.04269.pdf', '1701.03214.pdf', '1908.10084.pdf', '1910.06036.pdf', '1908.07195.pdf', '1910.00825.pdf', '1804.05918.pdf']","The large corpus acquired from multiple resources is rich in vocabulary. We present the complete statistics of collected corpus (see Table TABREF52) with number of sentences, words and unique tokens.",Word Embedding based New Corpus for Low-resourced Language: Sindhi,"Representing words and phrases into dense vectors of real numbers which
encode semantic and syntactic properties is a vital constituent in natural
language processing (NLP). The success of neural network (NN) models in NLP
largely rely on such dense word representations learned on the large unlabeled
corpus. Sindhi is one of the rich morphological language, spoken by large
population in Pakistan and India lacks corpora which plays an essential role of
a test-bed for generating word embeddings and developing language independent
NLP systems. In this paper, a large corpus of more than 61 million words is
developed for low-resourced Sindhi language for training neural word
embeddings. The corpus is acquired from multiple web-resources using
web-scrappy. Due to the unavailability of open source preprocessing tools for
Sindhi, the prepossessing of such large corpus becomes a challenging problem
specially cleaning of noisy data extracted from web resources. Therefore, a
preprocessing pipeline is employed for the filtration of noisy text.
Afterwards, the cleaned vocabulary is utilized for training Sindhi word
embeddings with state-of-the-art GloVe, Skip-Gram (SG), and Continuous Bag of
Words (CBoW) word2vec algorithms. The intrinsic evaluation approach of cosine
similarity matrix and WordSim-353 are employed for the evaluation of generated
Sindhi word embeddings. Moreover, we compare the proposed word embeddings with
recently revealed Sindhi fastText (SdfastText) word representations. Our
intrinsic evaluation results demonstrate the high quality of our generated
Sindhi word embeddings using SG, CBoW, and GloVe as compare to SdfastText word
representations."
46c9e5f335b2927db995a55a18b7c7621fd3d051,2003.03044,"How many distinct clinical phenotypes were manually annotated in the dataset of Discharge Summaries and Nursing Progress Notes, focusing on the risk of recurrent ICU readmissions?","[{'answer': '15 clinical patient phenotypes', 'type': 'extractive'}, {'answer': 'Thirteen different phenotypes are present in the dataset.', 'type': 'abstractive'}]",2003.03044.pdf,"['2003.03044.pdf', '1911.02821.pdf', '1912.10806.pdf', '1809.05752.pdf']","We have created a dataset of discharge summaries and nursing notes, all in the English language, with a focus on frequently readmitted patients, labeled with 15 clinical patient phenotypes believed to be associated with risk of recurrent Intensive Care Unit (ICU) readmission per our domain experts (co-authors LAC, PAT, DAG) as well as the literature. BIBREF10 BIBREF11 BIBREF12",A Corpus for Detecting High-Context Medical Conditions in Intensive Care Patient Notes Focusing on Frequently Readmitted Patients,"A crucial step within secondary analysis of electronic health records (EHRs)
is to identify the patient cohort under investigation. While EHRs contain
medical billing codes that aim to represent the conditions and treatments
patients may have, much of the information is only present in the patient
notes. Therefore, it is critical to develop robust algorithms to infer
patients' conditions and treatments from their written notes. In this paper, we
introduce a dataset for patient phenotyping, a task that is defined as the
identification of whether a patient has a given medical condition (also
referred to as clinical indication or phenotype) based on their patient note.
Nursing Progress Notes and Discharge Summaries from the Intensive Care Unit of
a large tertiary care hospital were manually annotated for the presence of
several high-context phenotypes relevant to treatment and risk of
re-hospitalization. This dataset contains 1102 Discharge Summaries and 1000
Nursing Progress Notes. Each Discharge Summary and Progress Note has been
annotated by at least two expert human annotators (one clinical researcher and
one resident physician). Annotated phenotypes include treatment non-adherence,
chronic pain, advanced/metastatic cancer, as well as 10 other phenotypes. This
dataset can be utilized for academic and industrial research in medicine and
computer science, particularly within the field of medical natural language
processing."
6e8c587b6562fafb43a7823637b84cd01487059a,1707.0011,"How do the BLEU scores vary based on different values of K and sequence lengths, and how do these scores compare to the baseline models with and without attention?","[{'answer': 'Ranges from 44.22 to 100.00 depending on K and the sequence length.', 'type': 'abstractive'}]",1707.00110.pdf,"['1707.00110.pdf', '2002.05058.pdf', '1908.07245.pdf', '1908.06151.pdf', '1809.08298.pdf', '2001.05493.pdf', '1710.06700.pdf', '1905.11901.pdf']",FLOAT SELECTED: Table 1: BLEU scores and computation times with varyingK and sequence length compared to baseline models with and without attention.,,
2ddb51b03163d309434ee403fef42d6b9aecc458,1904.03288,What baseline models are listed for comparison in the Jasper paper when evaluating performance on the Hub5'00 conversational dataset?,"[{'answer': 'LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC', 'type': 'abstractive'}]",1904.03288.pdf,"['1904.03288.pdf', '1805.03710.pdf', '2004.04721.pdf', '1802.06024.pdf', '1909.00430.pdf', '1803.09230.pdf', '1912.01673.pdf']", We compare to other models trained using the same data and report Hub5'00 results in Table TABREF31 .,Jasper: An End-to-End Convolutional Neural Acoustic Model,"In this paper, we report state-of-the-art results on LibriSpeech among
end-to-end speech recognition models without any external training data. Our
model, Jasper, uses only 1D convolutions, batch normalization, ReLU, dropout,
and residual connections. To improve training, we further introduce a new
layer-wise optimizer called NovoGrad. Through experiments, we demonstrate that
the proposed deep architecture performs as well or better than more complex
choices. Our deepest Jasper variant uses 54 convolutional layers. With this
architecture, we achieve 2.95% WER using a beam-search decoder with an external
neural language model and 3.86% WER with a greedy decoder on LibriSpeech
test-clean. We also report competitive results on the Wall Street Journal and
the Hub5'00 conversational evaluation datasets."
e587559f5ab6e42f7d981372ee34aebdc92b646e,1904.03288,"What are the WER scores achieved by Jasper's best model on the WSJ nov93/nov92 subsets and the Hub5'00 dataset’s SWB and CHM subsets, and how do these results compare to the current state-of-the-art?","[{'answer': 'In case of read speech datasets,  their best model got the highest nov93 score of 16.1 and the highest nov92 score of 13.3.\nIn case of Conversational Speech, their best model got the highest SWB of 8.3 and the highest CHM of 19.3. ', 'type': 'abstractive'}, {'answer': ""On WSJ datasets author's best approach achieves 9.3 and 6.9 WER compared to best results of 7.5 and 4.1 on nov93 and nov92 subsets.\nOn Hub5'00 datasets author's best approach achieves WER of 7.8 and 16.2 compared to best result of 7.3 and 14.2 on Switchboard (SWB) and Callhome (CHM) subsets."", 'type': 'abstractive'}]",1904.03288.pdf,"['1904.03288.pdf', '2003.06044.pdf', '1911.03310.pdf', '1610.07809.pdf', '2002.01984.pdf', '1904.10500.pdf', '1908.07816.pdf', '1807.07961.pdf', '1612.08205.pdf', '2004.01878.pdf', '1912.03457.pdf', '1804.05918.pdf', '1911.03597.pdf']",We trained a smaller Jasper 10x3 model with SGD with momentum optimizer for 400 epochs on a combined WSJ dataset (80 hours): LDC93S6A (WSJ0) and LDC94S13A (WSJ1). The results are provided in Table TABREF29 .,Jasper: An End-to-End Convolutional Neural Acoustic Model,"In this paper, we report state-of-the-art results on LibriSpeech among
end-to-end speech recognition models without any external training data. Our
model, Jasper, uses only 1D convolutions, batch normalization, ReLU, dropout,
and residual connections. To improve training, we further introduce a new
layer-wise optimizer called NovoGrad. Through experiments, we demonstrate that
the proposed deep architecture performs as well or better than more complex
choices. Our deepest Jasper variant uses 54 convolutional layers. With this
architecture, we achieve 2.95% WER using a beam-search decoder with an external
neural language model and 3.86% WER with a greedy decoder on LibriSpeech
test-clean. We also report competitive results on the Wall Street Journal and
the Hub5'00 conversational evaluation datasets."
564dcaf8d0bcc274ab64c784e4c0f50d7a2c17ee,1810.06743,From which 31 languages are evaluated for token-level recall in the proposed conversion from Universal Dependencies tags to UniMorph tags?,"[{'answer': 'Ar, Bg, Ca, Cs, Da, De, En, Es, Eu, Fa, Fi, Fr, Ga, He, Hi, Hu, It, La, Lt, Lv, Nb, Nl, Nn, PL, Pt, Ro, Ru, Sl, Sv, Tr, Uk, Ur', 'type': 'abstractive'}, {'answer': 'We apply this conversion to the 31 languages, Arabic, Hindi, Lithuanian, Persian, and Russian. , Dutch, Spanish', 'type': 'extractive'}]",1810.06743.pdf,"['1810.06743.pdf', '1811.01088.pdf', '1908.08345.pdf', '1905.12260.pdf', '1712.03556.pdf']",FLOAT SELECTED: Table 3: Token-level recall when converting Universal Dependencies tags to UniMorph tags. CSV refers to the lookup-based system. Post-editing refers to the proposed method.,Marrying Universal Dependencies and Universal Morphology,"The Universal Dependencies (UD) and Universal Morphology (UniMorph) projects
each present schemata for annotating the morphosyntactic details of language.
Each project also provides corpora of annotated text in many languages - UD at
the token level and UniMorph at the type level. As each corpus is built by
different annotators, language-specific decisions hinder the goal of universal
schemata. With compatibility of tags, each project's annotations could be used
to validate the other's. Additionally, the availability of both type- and
token-level resources would be a boon to tasks such as parsing and homograph
disambiguation. To ease this interoperability, we present a deterministic
mapping from Universal Dependencies v2 features into the UniMorph schema. We
validate our approach by lookup in the UniMorph corpora and find a
macro-average of 64.13% recall. We also note incompatibilities due to paucity
of data on either side. Finally, we present a critical evaluation of the
foundations, strengths, and weaknesses of the two annotation projects."
2858620e0498db2f2224bfbed5263432f0570832,1908.06267,"In the ablation study results for the non-hierarchical MPAD model, performed on the Reuters, Polarity, and IMDB datasets, which modification resulted in the smallest performance change compared to the baseline?","[{'answer': 'Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets.', 'type': 'abstractive'}]",1908.06267.pdf,"['1908.06267.pdf', '1909.00578.pdf', '1910.11235.pdf', '1709.05413.pdf', '1909.08859.pdf', '1710.09340.pdf', '1611.00514.pdf', '1706.08032.pdf', '1608.06757.pdf', '1912.08960.pdf']","Results and ablations ::: Ablation studies
To understand the impact of some hyperparameters on performance, we conducted additional experiments on the Reuters, Polarity, and IMDB datasets, with the non-hierarchical version of MPAD. Results are shown in Table TABREF29.",Message Passing Attention Networks for Document Understanding,"Graph neural networks have recently emerged as a very effective framework for
processing graph-structured data. These models have achieved state-of-the-art
performance in many tasks. Most graph neural networks can be described in terms
of message passing, vertex update, and readout functions. In this paper, we
represent documents as word co-occurrence networks and propose an application
of the message passing framework to NLP, the Message Passing Attention network
for Document understanding (MPAD). We also propose several hierarchical
variants of MPAD. Experiments conducted on 10 standard text classification
datasets show that our architectures are competitive with the state-of-the-art.
Ablation studies reveal further insights about the impact of the different
components on performance. Code is publicly available at:
https://github.com/giannisnik/mpad ."
545e92833b0ad4ba32eac5997edecf97a366a244,1908.06267,"How does increasing the number of message passing iterations and removing the master node affect the classification performance of the non-hierarchical MPAD on the Reuters, Polarity, and IMDB datasets?","[{'answer': 'Increasing number of message passing iterations showed consistent improvement in performance - around 1 point improvement compared between 1 and 4 iterations', 'type': 'abstractive'}, {'answer': 'Removing the master node deteriorates performance across all datasets', 'type': 'extractive'}]",1908.06267.pdf,"['1908.06267.pdf', '1908.05828.pdf', '1912.08960.pdf', '1911.07555.pdf', '1912.00864.pdf', '1904.10503.pdf', '1910.12129.pdf', '1911.01680.pdf', '1605.08675.pdf', '2003.03044.pdf', '1908.06379.pdf', '2003.03106.pdf', '2001.06888.pdf', '2001.05467.pdf', '1810.00663.pdf']","Results and ablations ::: Ablation studies
To understand the impact of some hyperparameters on performance, we conducted additional experiments on the Reuters, Polarity, and IMDB datasets, with the non-hierarchical version of MPAD. Results are shown in Table TABREF29.

Number of MP iterations. First, we varied the number of message passing iterations from 1 to 4. We can clearly see in Table TABREF29 that having more iterations improves performance. We attribute this to the fact that we are reading out at each iteration from 1 to $T$ (see Eq. DISPLAY_FORM18), which enables the final graph representation to encode a mixture of low-level and high-level features. Indeed, in initial experiments involving readout at $t$=$T$ only, setting $T\ge 2$ was always decreasing performance, despite the GRU-based updates (Eq. DISPLAY_FORM14). These results were consistent with that of BIBREF53 and BIBREF9, who both are reading out only at $t$=$T$ too. We hypothesize that node features at $T\ge 2$ are too diffuse to be entirely relied upon during readout. More precisely, initially at $t$=0, node representations capture information about words, at $t$=1, about their 1-hop neighborhood (bigrams), at $t$=2, about compositions of bigrams, etc. Thus, pretty quickly, node features become general and diffuse. In such cases, considering also the lower-level, more precise features of the earlier iterations when reading out may be necessary.",Message Passing Attention Networks for Document Understanding,"Graph neural networks have recently emerged as a very effective framework for
processing graph-structured data. These models have achieved state-of-the-art
performance in many tasks. Most graph neural networks can be described in terms
of message passing, vertex update, and readout functions. In this paper, we
represent documents as word co-occurrence networks and propose an application
of the message passing framework to NLP, the Message Passing Attention network
for Document understanding (MPAD). We also propose several hierarchical
variants of MPAD. Experiments conducted on 10 standard text classification
datasets show that our architectures are competitive with the state-of-the-art.
Ablation studies reveal further insights about the impact of the different
components on performance. Code is publicly available at:
https://github.com/giannisnik/mpad ."
458dbf217218fcab9153e33045aac08a2c8a38c6,1612.0527,"Based on the dataset breakdown, how many annotated data points are provided for SemEval 2015, SemEval 2016, TASS 2015, and SENTIPOLC 2014 in the study?","[{'answer': ""Total number of annotated data:\nSemeval'15: 10712\nSemeval'16: 28632\nTass'15: 69000\nSentipol'14: 6428"", 'type': 'abstractive'}]",1612.05270.pdf,"['1612.05270.pdf', '1911.01799.pdf', '1901.04899.pdf', '1905.06566.pdf', '1909.03242.pdf', '1909.11297.pdf', '1810.10254.pdf', '1704.08960.pdf', '1911.04952.pdf']",FLOAT SELECTED: Table 3: Datasets details from each competition tested in this work,,
cebf3e07057339047326cb2f8863ee633a62f49f,1612.0527,"In the paper analyzing B4MSA for multilingual sentiment classification, which languages, outside of SemEval, TASS, and SENTIPOLC, did B4MSA demonstrate superior performance compared to the reported results?","[{'answer': 'Arabic, German, Portuguese, Russian, Swedish', 'type': 'abstractive'}]",1612.05270.pdf,"['1612.05270.pdf', '1904.03288.pdf', '1909.13714.pdf', '2003.03044.pdf', '1809.09194.pdf', '1912.10806.pdf', '1707.05236.pdf', '1810.06743.pdf', '1901.02257.pdf', '2003.04642.pdf', '1901.03866.pdf', '1808.03430.pdf']","Table TABREF24 supports the idea of choosing B4MSA as a bootstrapping sentiment classifier because, in the overall, B4MSA reaches superior performances regardless of the language.",,
a6665074b067abb2676d5464f36b2cb07f6919d3,1908.06379,What are the state-of-the-art dependency parsing metrics (UAS/LAS) and constituent parsing F1 scores achieved on the PTB and CTB datasets in the paper on Concurrent Parsing of Constituency and Dependency?,"[{'answer': '. On PTB, our model achieves 93.90 F1 score of constituent parsing and 95.91 UAS and 93.86 LAS of dependency parsing., On CTB, our model achieves a new state-of-the-art result on both constituent and dependency parsing.', 'type': 'extractive'}]",1908.06379.pdf,"['1908.06379.pdf', '1911.00069.pdf', '1610.07809.pdf', '1909.00754.pdf', '1910.03467.pdf', '1908.11546.pdf', '2002.06644.pdf']",FLOAT SELECTED: Table 3: Dependency parsing on PTB and CTB.,Concurrent Parsing of Constituency and Dependency,"Constituent and dependency representation for syntactic structure share a lot
of linguistic and computational characteristics, this paper thus makes the
first attempt by introducing a new model that is capable of parsing constituent
and dependency at the same time, so that lets either of the parsers enhance
each other. Especially, we evaluate the effect of different shared network
components and empirically verify that dependency parsing may be much more
beneficial from constituent parsing structure.
  The proposed parser achieves new state-of-the-art performance for both
parsing tasks, constituent and dependency on PTB and CTB benchmarks."
ef4dba073d24042f24886580ae77add5326f2130,1801.05147,What are the F1 scores achieved by the proposed LSTM-CRF model for Chinese NER on the DL-PS dataset (dialogue domain) and the EC-MT/EC-UQ datasets (e-commerce domain)?,"[{'answer': 'F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ', 'type': 'abstractive'}, {'answer': 'F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)', 'type': 'abstractive'}]",1801.05147.pdf,"['1801.05147.pdf', '1907.09369.pdf', '1910.06592.pdf', '1711.02013.pdf', '1911.02821.pdf', '1910.12129.pdf', '1711.00106.pdf', '1908.07816.pdf', '2001.05493.pdf', '1909.05855.pdf', '1906.10551.pdf']",FLOAT SELECTED: Table 2: Main results on the DL-PS data.,Adversarial Learning for Chinese NER from Crowd Annotations,"To quickly obtain new labeled data, we can choose crowdsourcing as an
alternative way at lower cost in a short time. But as an exchange, crowd
annotations from non-experts may be of lower quality than those from experts.
In this paper, we propose an approach to performing crowd annotation learning
for Chinese Named Entity Recognition (NER) to make full use of the noisy
sequence labels from multiple annotators. Inspired by adversarial learning, our
approach uses a common Bi-LSTM and a private Bi-LSTM for representing
annotator-generic and -specific information. The annotator-generic information
is the common knowledge for entities easily mastered by the crowd. Finally, we
build our Chinese NE tagger based on the LSTM-CRF model. In our experiments, we
create two data sets for Chinese NER tasks from two domains. The experimental
results show that our system achieves better scores than strong baseline
systems."
dd5c9a370652f6550b4fd13e2ac317eaf90973a8,2001.0597,"Based on the linear regression results, what is the exact correlation between the prevalence of #MeToo social media activity and official reports of sexual harassment?","[{'answer': '0.9098 correlation', 'type': 'abstractive'}]",2001.05970.pdf,"['2001.05970.pdf', '1905.07464.pdf', '1810.10254.pdf', '1907.09369.pdf', '1909.03242.pdf', '1711.02013.pdf', '1904.10503.pdf', '1901.02262.pdf', '2004.04721.pdf', '2003.01769.pdf', '1806.07711.pdf', '1909.06937.pdf', '1909.11467.pdf', '1909.09587.pdf']",FLOAT SELECTED: Table 2: Linear regression results.,,
b9c0049a7a5639c33efdb6178c2594b8efdefabb,1911.03597,"How do the experiments demonstrate the performance of the transformer-based multilingual paraphrasing model in terms of relevance and fluency, compared to the pivoting method?","[{'answer': 'our method outperforms the baseline in both relevance and fluency significantly.', 'type': 'extractive'}]",1911.03597.pdf,"['1911.03597.pdf', '1810.12196.pdf', '1911.03310.pdf', '1811.12254.pdf', '1910.00825.pdf', '1908.07816.pdf', '1902.09393.pdf', '1904.10500.pdf', '1605.07333.pdf', '1905.07464.pdf', '1904.10503.pdf', '1911.01680.pdf', '1806.07711.pdf', '1912.10011.pdf', '1704.08960.pdf']","First we compare our models with the conventional pivoting method, i.e., round-trip translation. As shown in Figure FIGREF15 (a)(b), either the bilingual or the multilingual model is better than the baseline in terms of relevance and diversity in most cases. In other words, with the same generation diversity (measured by both Distinct-2 and Self-BLEU), our models can generate paraphrase with more semantically similarity to the input sentence.",Zero-Shot Paraphrase Generation with Multilingual Language Models,"Leveraging multilingual parallel texts to automatically generate paraphrases
has drawn much attention as size of high-quality paraphrase corpus is limited.
Round-trip translation, also known as the pivoting method, is a typical
approach to this end. However, we notice that the pivoting process involves
multiple machine translation models and is likely to incur semantic drift
during the two-step translations. In this paper, inspired by the
Transformer-based language models, we propose a simple and unified paraphrasing
model, which is purely trained on multilingual parallel data and can conduct
zero-shot paraphrase generation in one step. Compared with the pivoting
approach, paraphrases generated by our model is more semantically similar to
the input sentence. Moreover, since our model shares the same architecture as
GPT (Radford et al., 2018), we are able to pre-train the model on large-scale
unparallel corpus, which further improves the fluency of the output sentences.
In addition, we introduce the mechanism of denoising auto-encoder (DAE) to
improve diversity and robustness of the model. Experimental results show that
our model surpasses the pivoting method in terms of relevance, diversity,
fluency and efficiency."
bde6fa2057fa21b38a91eeb2bb6a3ae7fb3a2c62,1704.05907,What accuracy is reported for the multi-view network (MVN) model on the Stanford Sentiment Treebank 5-class classification task?,"[{'answer': '51.5', 'type': 'abstractive'}]",1704.05907.pdf,"['1704.05907.pdf', '1701.03214.pdf', '1605.07333.pdf', '1810.10254.pdf', '1701.06538.pdf', '1909.00175.pdf', '1908.06267.pdf', '1912.06670.pdf', '1910.08210.pdf']","FLOAT SELECTED: Table 1: Accuracies on the Stanford Sentiment Treebank 5-class classification task; except for the MVN, all results are drawn from (Lei et al., 2015).",End-to-End Multi-View Networks for Text Classification,"We propose a multi-view network for text classification. Our method
automatically creates various views of its input text, each taking the form of
soft attention weights that distribute the classifier's focus among a set of
base features. For a bag-of-words representation, each view focuses on a
different subset of the text's words. Aggregating many such views results in a
more discriminative and robust representation. Through a novel architecture
that both stacks and concatenates views, we produce a network that emphasizes
both depth and width, allowing training to converge quickly. Using our
multi-view architecture, we establish new state-of-the-art accuracies on two
benchmark tasks."
fd2c6c26fd0ab3c10aae4f2550c5391576a77491,1907.09369,"Are the results in the ""Emotion Detection in Text"" paper focused exclusively on datasets consisting of English text, including the new dataset tested on?","[{'answer': 'Yes', 'type': 'boolean'}]",1907.09369.pdf,"['1907.09369.pdf', '1910.14537.pdf', '2002.11402.pdf', '1909.08859.pdf', '1910.06036.pdf', '1705.00108.pdf', '2002.00652.pdf', '1810.09774.pdf', '1802.06024.pdf', '1606.00189.pdf', '1704.08960.pdf', '1701.09123.pdf', '1809.03449.pdf', '1809.05752.pdf', '1905.06566.pdf']",FLOAT SELECTED: Table 2: Results of final classification in Wang et al.,Emotion Detection in Text: Focusing on Latent Representation,"In recent years, emotion detection in text has become more popular due to its
vast potential applications in marketing, political science, psychology,
human-computer interaction, artificial intelligence, etc. In this work, we
argue that current methods which are based on conventional machine learning
models cannot grasp the intricacy of emotional language by ignoring the
sequential nature of the text, and the context. These methods, therefore, are
not sufficient to create an applicable and generalizable emotion detection
methodology. Understanding these limitations, we present a new network based on
a bidirectional GRU model to show that capturing more meaningful information
from text can significantly improve the performance of these models. The
results show significant improvement with an average of 26.8 point increase in
F-measure on our test data and 38.6 increase on the totally new dataset."
5bc1dc6ebcb88fd0310b21d2a74939e35a4c1a11,2004.04721,"What languages were evaluated to measure the cross-lingual transfer learning performance on the XNLI development set, highlighting the impact of translation artifacts?","[{'answer': 'English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish', 'type': 'abstractive'}, {'answer': 'English, Spanish, Finnish', 'type': 'extractive'}]",2004.04721.pdf,"['2004.04721.pdf', '1605.08675.pdf', '1611.04798.pdf', '1810.09774.pdf', '1904.07904.pdf']",FLOAT SELECTED: Table 1: XNLI dev results (acc). BT-XX and MT-XX consistently outperform ORIG in all cases.,Translation Artifacts in Cross-lingual Transfer Learning,"Both human and machine translation play a central role in cross-lingual
transfer learning: many multilingual datasets have been created through
professional translation services, and using machine translation to translate
either the test set or the training set is a widely used transfer technique. In
this paper, we show that such translation process can introduce subtle
artifacts that have a notable impact in existing cross-lingual models. For
instance, in natural language inference, translating the premise and the
hypothesis independently can reduce the lexical overlap between them, which
current models are highly sensitive to. We show that some previous findings in
cross-lingual transfer learning need to be reconsidered in the light of this
phenomenon. Based on the gained insights, we also improve the state-of-the-art
in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points,
respectively."
63403ffc0232ff041f3da8fa6c30827cfd6404b7,1808.0992,What performance metric is used to evaluate the accuracy of models on the WIKIHOP closed test and public validation sets in the Entity-GCN paper?,"[{'answer': 'Accuracy', 'type': 'extractive'}]",1808.09920.pdf,"['1808.09920.pdf', '1712.00991.pdf', '1805.04033.pdf', '1812.06705.pdf', '2004.01980.pdf', '2002.06675.pdf']",FLOAT SELECTED: Table 2: Accuracy of different models on WIKIHOP closed test set and public validation set. Our Entity-GCN outperforms recent prior work without learning any language model to process the input but relying on a pretrained one (ELMo – without fine-tunning it) and applying R-GCN to reason among entities in the text. * with coreference for unmasked dataset and without coreference for the masked one.,,
a25c1883f0a99d2b6471fed48c5121baccbbae82,1808.0992,"What are the testing accuracies reported for the Entity-GCN model (with and without coreference) and the ensemble of 5 models on the WIKIHOP closed test set, in the paper that uses R-GCN for reasoning among entities without fine-tuning a language model?","[{'answer': 'During testing: 67.6 for single model without coreference, 66.4 for single model with coreference, 71.2 for ensemble of 5 models', 'type': 'abstractive'}]",1808.09920.pdf,"['1808.09920.pdf', '1910.11204.pdf', '1704.08960.pdf', '1911.10049.pdf', '1910.06592.pdf', '1910.11769.pdf']",FLOAT SELECTED: Table 2: Accuracy of different models on WIKIHOP closed test set and public validation set. Our Entity-GCN outperforms recent prior work without learning any language model to process the input but relying on a pretrained one (ELMo – without fine-tunning it) and applying R-GCN to reason among entities in the text. * with coreference for unmasked dataset and without coreference for the masked one.,,
447eb98e602616c01187960c9c3011c62afd7c27,1911.04952,"What are the twenty distinct lyrical topics identified through the LDA topic model from the corpus of 124,288 metal song lyrics?","[{'answer': 'Table TABREF10 displays the twenty resulting topics', 'type': 'extractive'}]",1911.04952.pdf,"['1911.04952.pdf', '1605.08675.pdf', '1902.00330.pdf', '1909.07734.pdf', '1904.10500.pdf', '1910.02339.pdf', '1907.03060.pdf']",Table TABREF10 displays the twenty resulting topics found within the text corpus using LDA.,"'Warriors of the Word' -- Deciphering Lyrical Topics in Music and Their Connection to Audio Feature Dimensions Based on a Corpus of Over 100,000 Metal Songs","We look into the connection between the musical and lyrical content of metal
music by combining automated extraction of high-level audio features and
quantitative text analysis on a corpus of 124.288 song lyrics from this genre.
Based on this text corpus, a topic model was first constructed using Latent
Dirichlet Allocation (LDA). For a subsample of 503 songs, scores for predicting
perceived musical hardness/heaviness and darkness/gloominess were extracted
using audio feature models. By combining both audio feature and text analysis,
we (1) offer a comprehensive overview of the lyrical topics present within the
metal genre and (2) are able to establish whether or not levels of hardness and
other music dimensions are associated with the occurrence of particularly harsh
(and other) textual topics. Twenty typical topics were identified and projected
into a topic space using multidimensional scaling (MDS). After Bonferroni
correction, positive correlations were found between musical hardness and
darkness and textual topics dealing with 'brutal death', 'dystopia', 'archaisms
and occultism', 'religion and satanism', 'battle' and '(psychological)
madness', while there is a negative associations with topics like 'personal
life' and 'love and romance'."
8c0a0747a970f6ea607ff9b18cfeb738502d9a95,1911.01799,What is the EER(%) performance of the i-vector and x-vector systems trained on VoxCeleb and evaluated using the CN-Celeb evaluation dataset?,"[{'answer': 'ERR of 19.05 with i-vectors and 15.52 with x-vectors', 'type': 'abstractive'}]",1911.01799.pdf,"['1911.01799.pdf', '1810.06743.pdf', '1909.08041.pdf', '1911.01680.pdf', '1908.08345.pdf', '1909.01958.pdf', '1701.02877.pdf', '1604.00400.pdf', '1804.08050.pdf', '1912.03457.pdf', '1901.08079.pdf', '2003.11563.pdf', '2003.12218.pdf', '1909.07734.pdf', '1607.06025.pdf', '1808.09029.pdf']",FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets.,CN-CELEB: a challenging Chinese speaker recognition dataset,"Recently, researchers set an ambitious goal of conducting speaker recognition
in unconstrained conditions where the variations on ambient, channel and
emotion could be arbitrary. However, most publicly available datasets are
collected under constrained environments, i.e., with little noise and limited
channel variation. These datasets tend to deliver over optimistic performance
and do not meet the request of research on speaker recognition in unconstrained
conditions. In this paper, we present CN-Celeb, a large-scale speaker
recognition dataset collected `in the wild'. This dataset contains more than
130,000 utterances from 1,000 Chinese celebrities, and covers 11 different
genres in real world. Experiments conducted with two state-of-the-art speaker
recognition approaches (i-vector and x-vector) show that the performance on
CN-Celeb is far inferior to the one obtained on VoxCeleb, a widely used speaker
recognition dataset. This result demonstrates that in real-life conditions, the
performance of existing techniques might be much worse than it was thought. Our
database is free for researchers and can be downloaded from
http://project.cslt.org."
a2be2bd84e5ae85de2ab9968147b3d49c84dfb7f,1911.01799,What are the 11 genres listed in the CN-Celeb speaker recognition dataset for evaluating speaker recognition in unconstrained conditions?,"[{'answer': 'genre, entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement', 'type': 'abstractive'}]",1911.01799.pdf,"['1911.01799.pdf', '1909.00754.pdf', '1911.04952.pdf', '1810.12885.pdf', '1804.08050.pdf']",FLOAT SELECTED: Table 1. The distribution over genres.,CN-CELEB: a challenging Chinese speaker recognition dataset,"Recently, researchers set an ambitious goal of conducting speaker recognition
in unconstrained conditions where the variations on ambient, channel and
emotion could be arbitrary. However, most publicly available datasets are
collected under constrained environments, i.e., with little noise and limited
channel variation. These datasets tend to deliver over optimistic performance
and do not meet the request of research on speaker recognition in unconstrained
conditions. In this paper, we present CN-Celeb, a large-scale speaker
recognition dataset collected `in the wild'. This dataset contains more than
130,000 utterances from 1,000 Chinese celebrities, and covers 11 different
genres in real world. Experiments conducted with two state-of-the-art speaker
recognition approaches (i-vector and x-vector) show that the performance on
CN-Celeb is far inferior to the one obtained on VoxCeleb, a widely used speaker
recognition dataset. This result demonstrates that in real-life conditions, the
performance of existing techniques might be much worse than it was thought. Our
database is free for researchers and can be downloaded from
http://project.cslt.org."
944d5dbe0cfc64bf41ea36c11b1d378c408d40b8,1911.01799,"Based on the results presented in the CN-Celeb paper, which model—i-vector or x-vector—showed better performance in terms of EER (%) when evaluated on the CN-Celeb dataset?","[{'answer': 'x-vector', 'type': 'abstractive'}]",1911.01799.pdf,"['1911.01799.pdf', '1908.06151.pdf', '1904.10503.pdf', '1909.07734.pdf', '1909.09270.pdf', '1911.05153.pdf', '1810.12085.pdf']",FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets.,CN-CELEB: a challenging Chinese speaker recognition dataset,"Recently, researchers set an ambitious goal of conducting speaker recognition
in unconstrained conditions where the variations on ambient, channel and
emotion could be arbitrary. However, most publicly available datasets are
collected under constrained environments, i.e., with little noise and limited
channel variation. These datasets tend to deliver over optimistic performance
and do not meet the request of research on speaker recognition in unconstrained
conditions. In this paper, we present CN-Celeb, a large-scale speaker
recognition dataset collected `in the wild'. This dataset contains more than
130,000 utterances from 1,000 Chinese celebrities, and covers 11 different
genres in real world. Experiments conducted with two state-of-the-art speaker
recognition approaches (i-vector and x-vector) show that the performance on
CN-Celeb is far inferior to the one obtained on VoxCeleb, a widely used speaker
recognition dataset. This result demonstrates that in real-life conditions, the
performance of existing techniques might be much worse than it was thought. Our
database is free for researchers and can be downloaded from
http://project.cslt.org."
327e6c6609fbd4c6ae76284ca639951f03eb4a4c,1911.01799,"*According to the CN-Celeb paper, what is the percentage increase in EER for both the i-vector and x-vector systems when evaluated on CN-Celeb compared to VoxCeleb?*","[{'answer': 'For i-vector system, performances are 11.75% inferior to voxceleb. For x-vector system, performances are 10.74% inferior to voxceleb', 'type': 'abstractive'}]",1911.01799.pdf,"['1911.01799.pdf', '1910.07481.pdf', '1805.04033.pdf', '2002.00652.pdf', '1612.08205.pdf', '1902.00330.pdf', '2003.06044.pdf', '1901.05280.pdf', '1804.00079.pdf', '1704.05907.pdf', '2003.12218.pdf', '1911.03310.pdf', '1908.05828.pdf', '1804.05918.pdf', '1909.09587.pdf', '1712.03547.pdf', '1907.09369.pdf', '1910.00825.pdf']",FLOAT SELECTED: Table 4. EER(%) results of the i-vector and x-vector systems trained on VoxCeleb and evaluated on three evaluation sets.,CN-CELEB: a challenging Chinese speaker recognition dataset,"Recently, researchers set an ambitious goal of conducting speaker recognition
in unconstrained conditions where the variations on ambient, channel and
emotion could be arbitrary. However, most publicly available datasets are
collected under constrained environments, i.e., with little noise and limited
channel variation. These datasets tend to deliver over optimistic performance
and do not meet the request of research on speaker recognition in unconstrained
conditions. In this paper, we present CN-Celeb, a large-scale speaker
recognition dataset collected `in the wild'. This dataset contains more than
130,000 utterances from 1,000 Chinese celebrities, and covers 11 different
genres in real world. Experiments conducted with two state-of-the-art speaker
recognition approaches (i-vector and x-vector) show that the performance on
CN-Celeb is far inferior to the one obtained on VoxCeleb, a widely used speaker
recognition dataset. This result demonstrates that in real-life conditions, the
performance of existing techniques might be much worse than it was thought. Our
database is free for researchers and can be downloaded from
http://project.cslt.org."
6f2f304ef292d8bcd521936f93afeec917cbe28a,2002.02492,How effective are the consistent variants of nucleus and top-k sampling in reducing the non-termination ratio compared to baseline decoding algorithms?,"[{'answer': 'It eliminates non-termination in some models fixing for some models up to 6% of non-termination ratio.', 'type': 'abstractive'}]",2002.02492.pdf,"['2002.02492.pdf', '1903.09588.pdf', '1804.11346.pdf', '1904.05584.pdf', '1609.00559.pdf', '2003.05377.pdf', '1612.08205.pdf', '1902.09393.pdf', '1810.05241.pdf', '1905.11901.pdf', '2003.12218.pdf', '1909.06937.pdf', '2002.11910.pdf', '2001.05493.pdf', '1901.05280.pdf']","Table TABREF44 shows that consistent nucleus and top-$k$ sampling (§SECREF28) resulted in only terminating sequences, except for a few cases that we attribute to the finite limit $L$ used to measure the non-termination ratio. The example continuations in Table TABREF46 show that the sampling tends to preserve language modeling quality on prefixes that led to termination with the baseline (first row). On prefixes that led to non-termination with the baseline (second & third rows), the quality tends to improve since the continuation now terminates. Since the model's non-$\left<\text{eos}\right>$ token probabilities at each step are only modified by a multiplicative constant, the sampling process can still enter a repetitive cycle (e.g. when the constant is close to 1), though the cycle is guaranteed to eventually terminate.",Consistency of a Recurrent Language Model With Respect to Incomplete Decoding,"Despite strong performance on a variety of tasks, neural sequence models
trained with maximum likelihood have been shown to exhibit issues such as
length bias and degenerate repetition. We study the related issue of receiving
infinite-length sequences from a recurrent language model when using common
decoding algorithms. To analyze this issue, we first define inconsistency of a
decoding algorithm, meaning that the algorithm can yield an infinite-length
sequence that has zero probability under the model. We prove that commonly used
incomplete decoding algorithms - greedy search, beam search, top-k sampling,
and nucleus sampling - are inconsistent, despite the fact that recurrent
language models are trained to produce sequences of finite length. Based on
these insights, we propose two remedies which address inconsistency: consistent
variants of top-k and nucleus sampling, and a self-terminating recurrent
language model. Empirical results show that inconsistency occurs in practice,
and that the proposed methods prevent inconsistency."
2d274c93901c193cf7ad227ab28b1436c5f410af,1901.02262,Which models are listed as baselines in this paper for comparing Masque's performance on the MS MARCO V2 leaderboard?,"[{'answer': 'BiDAF, Deep Cascade QA, S-Net+CES2S, BERT+Multi-PGNet, Selector+CCG, VNET, DECAPROP, MHPGM+NOIC, ConZNet, RMR+A2D', 'type': 'abstractive'}]",1901.02262.pdf,"['1901.02262.pdf', '1908.07245.pdf', '1902.09666.pdf', '1806.04330.pdf', '1604.00400.pdf', '2003.04642.pdf', '2002.01207.pdf', '1908.10084.pdf', '1909.03405.pdf', '1909.01247.pdf', '1909.11467.pdf', '1910.12795.pdf', '1809.02286.pdf', '1906.01081.pdf', '2003.03106.pdf', '1712.03556.pdf', '1904.05584.pdf', '2002.06644.pdf']","FLOAT SELECTED: Table 2: Performance of our and competing models on the MS MARCO V2 leaderboard (4 March 2019). aSeo et al. (2017); bYan et al. (2019); cShao (unpublished), a variant of Tan et al. (2018); dLi (unpublished), a model using Devlin et al. (2018) and See et al. (2017); eQian (unpublished); fWu et al. (2018). Whether the competing models are ensemble models or not is unreported.",Multi-style Generative Reading Comprehension,"This study tackles generative reading comprehension (RC), which consists of
answering questions based on textual evidence and natural language generation
(NLG). We propose a multi-style abstractive summarization model for question
answering, called Masque. The proposed model has two key characteristics.
First, unlike most studies on RC that have focused on extracting an answer span
from the provided passages, our model instead focuses on generating a summary
from the question and multiple passages. This serves to cover various answer
styles required for real-world applications. Second, whereas previous studies
built a specific model for each answer style because of the difficulty of
acquiring one general model, our approach learns multi-style answers within a
model to improve the NLG capability for all styles involved. This also enables
our model to give an answer in the target style. Experiments show that our
model achieves state-of-the-art performance on the Q&A task and the Q&A + NLG
task of MS MARCO 2.1 and the summary task of NarrativeQA. We observe that the
transfer of the style-independent NLG capability to the target style is the key
to its success."
e63bde5c7b154fbe990c3185e2626d13a1bad171,1901.02262,"How does the Masque model perform on the NarrativeQA test set in terms of BLEU-1, BLEU-4, METEOR, and ROUGE-L scores?","[{'answer': 'Bleu-1: 54.11, Bleu-4: 30.43, METEOR: 26.13, ROUGE-L: 59.87', 'type': 'abstractive'}]",1901.02262.pdf,"['1901.02262.pdf', '2002.02492.pdf', '1912.10435.pdf', '1612.05270.pdf', '1909.05855.pdf', '2001.05970.pdf', '1711.02013.pdf']",FLOAT SELECTED: Table 5: Performance of our and competing models on the NarrativeQA test set. aSeo et al. (2017); bTay et al. (2018); cBauer et al. (2018); dIndurthi et al. (2018); eHu et al. (2018). fResults on the NarrativeQA validation set.,Multi-style Generative Reading Comprehension,"This study tackles generative reading comprehension (RC), which consists of
answering questions based on textual evidence and natural language generation
(NLG). We propose a multi-style abstractive summarization model for question
answering, called Masque. The proposed model has two key characteristics.
First, unlike most studies on RC that have focused on extracting an answer span
from the provided passages, our model instead focuses on generating a summary
from the question and multiple passages. This serves to cover various answer
styles required for real-world applications. Second, whereas previous studies
built a specific model for each answer style because of the difficulty of
acquiring one general model, our approach learns multi-style answers within a
model to improve the NLG capability for all styles involved. This also enables
our model to give an answer in the target style. Experiments show that our
model achieves state-of-the-art performance on the Q&A task and the Q&A + NLG
task of MS MARCO 2.1 and the summary task of NarrativeQA. We observe that the
transfer of the style-independent NLG capability to the target style is the key
to its success."
6aa2a1e2e3666f2b2a1f282d4cbdd1ca325eb9de,1809.0053,"Based on the dataset summary, how many labeled data points are there for the datasets Book, Electronics, Beauty, Music, IMDB, Yelp, Cell Phone, and Baby?","[{'answer': '719313', 'type': 'abstractive'}, {'answer': 'Book, Electronics, Beauty and Music each have 6000, IMDB 84919, Yelp 231163, Cell Phone 194792 and Baby 160792 labeled data.', 'type': 'abstractive'}]",1809.00530.pdf,"['1809.00530.pdf', '1910.02339.pdf', '1909.06162.pdf', '1911.10049.pdf', '1804.05918.pdf', '1611.02550.pdf', '1707.00110.pdf', '1702.03342.pdf', '1901.04899.pdf', '1902.00672.pdf', '1811.12254.pdf', '2002.01664.pdf', '1908.06083.pdf', '2002.08899.pdf', '1809.09795.pdf']",FLOAT SELECTED: Table 1: Summary of datasets.,,
9176d2ba1c638cdec334971c4c7f1bb959495a8e,1809.0053,Which specific domains were selected as the source and target for sentiment classification in this research?,"[{'answer': 'Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen', 'type': 'abstractive'}, {'answer': 'we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)', 'type': 'extractive'}]",1809.00530.pdf,"['1809.00530.pdf', '1910.02339.pdf', '1808.09920.pdf', '2003.12218.pdf', '1904.01608.pdf', '1911.03597.pdf']",FLOAT SELECTED: Table 1: Summary of datasets.,,
e2db361ae9ad9dbaa9a85736c5593eb3a471983d,1908.10084,Which sentence embedding methods are evaluated alongside SBERT and SRoBERTa based on Spearman rank correlation for the STS tasks in the paper?,"[{'answer': 'GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent', 'type': 'abstractive'}, {'answer': 'Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder.', 'type': 'abstractive'}]",1908.10084.pdf,"['1908.10084.pdf', '2002.06424.pdf', '1611.04798.pdf', '2003.12218.pdf']","FLOAT SELECTED: Table 1: Spearman rank correlation ρ between the cosine similarity of sentence representations and the gold labels for various Textual Similarity (STS) tasks. Performance is reported by convention as ρ × 100. STS12-STS16: SemEval 2012-2016, STSb: STSbenchmark, SICK-R: SICK relatedness dataset.",Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,"BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new
state-of-the-art performance on sentence-pair regression tasks like semantic
textual similarity (STS). However, it requires that both sentences are fed into
the network, which causes a massive computational overhead: Finding the most
similar pair in a collection of 10,000 sentences requires about 50 million
inference computations (~65 hours) with BERT. The construction of BERT makes it
unsuitable for semantic similarity search as well as for unsupervised tasks
like clustering.
  In this publication, we present Sentence-BERT (SBERT), a modification of the
pretrained BERT network that use siamese and triplet network structures to
derive semantically meaningful sentence embeddings that can be compared using
cosine-similarity. This reduces the effort for finding the most similar pair
from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while
maintaining the accuracy from BERT.
  We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning
tasks, where it outperforms other state-of-the-art sentence embeddings methods."
d51dc36fbf6518226b8e45d4c817e07e8f642003,1908.05828,"According to the ""Dataset statistics"" section of the Nepali NER study, how many sentences are present in both the training and testing sets of the dataset?","[{'answer': '3606', 'type': 'abstractive'}, {'answer': '6946', 'type': 'extractive'}]",1908.05828.pdf,"['1908.05828.pdf', '1606.00189.pdf', '2002.01359.pdf', '1910.10288.pdf', '1910.03814.pdf', '2004.01878.pdf', '1612.08205.pdf', '1611.03382.pdf', '1909.11687.pdf', '1912.01772.pdf', '1809.04960.pdf', '1806.04330.pdf', '1909.13695.pdf', '2002.05058.pdf', '1809.02286.pdf', '1907.03060.pdf', '1909.00105.pdf']",FLOAT SELECTED: Table 1: Dataset statistics,Named Entity Recognition for Nepali Language,"Named Entity Recognition have been studied for different languages like
English, German, Spanish and many others but no study have focused on Nepali
language. In this paper we propose a neural based Nepali NER using latest
state-of-the-art architecture based on grapheme-level which doesn't require any
hand-crafted features and no data pre-processing. Our novel neural based model
gained relative improvement of 33% to 50% compared to feature based SVM model
and up to 10% improvement over state-of-the-art neural based model developed
for languages beside Nepali."
cb77d6a74065cb05318faf57e7ceca05e126a80d,1908.05828,Which baseline models are used to compare the proposed grapheme-level neural-based Nepali NER to other models for performance evaluation?,"[{'answer': 'CNN modelBIBREF0, Stanford CRF modelBIBREF21', 'type': 'extractive'}, {'answer': 'Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec', 'type': 'abstractive'}]",1908.05828.pdf,"['1908.05828.pdf', '1705.01214.pdf', '1809.01541.pdf', '1910.08210.pdf', '1811.02906.pdf', '1909.07734.pdf', '1712.03556.pdf', '1910.11769.pdf', '1901.04899.pdf', '1809.10644.pdf', '1909.11687.pdf']","First we are comparing BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2 models with CNN modelBIBREF0 and Stanford CRF modelBIBREF21.",Named Entity Recognition for Nepali Language,"Named Entity Recognition have been studied for different languages like
English, German, Spanish and many others but no study have focused on Nepali
language. In this paper we propose a neural based Nepali NER using latest
state-of-the-art architecture based on grapheme-level which doesn't require any
hand-crafted features and no data pre-processing. Our novel neural based model
gained relative improvement of 33% to 50% compared to feature based SVM model
and up to 10% improvement over state-of-the-art neural based model developed
for languages beside Nepali."
a1b3e2107302c5a993baafbe177684ae88d6f505,1908.05828,What are the total number of sentences and entities in the ILPRL and OurNepali datasets in the neural-based Nepali NER study?,"[{'answer': 'Dataset contains 3606 total sentences and 79087 total entities.', 'type': 'abstractive'}, {'answer': 'ILPRL contains 548 sentences, OurNepali contains 3606 sentences', 'type': 'abstractive'}]",1908.05828.pdf,"['1908.05828.pdf', '1909.00015.pdf', '1909.09270.pdf', '1909.00361.pdf', '1701.05574.pdf', '1704.00939.pdf', '1909.00105.pdf', '1903.09722.pdf', '1804.11346.pdf', '1809.01541.pdf', '1910.02339.pdf', '1701.02877.pdf', '2002.01207.pdf', '1802.06024.pdf', '1703.07090.pdf', '1909.13375.pdf', '1701.00185.pdf']",The statistics of both the dataset is presented in table TABREF23.,Named Entity Recognition for Nepali Language,"Named Entity Recognition have been studied for different languages like
English, German, Spanish and many others but no study have focused on Nepali
language. In this paper we propose a neural based Nepali NER using latest
state-of-the-art architecture based on grapheme-level which doesn't require any
hand-crafted features and no data pre-processing. Our novel neural based model
gained relative improvement of 33% to 50% compared to feature based SVM model
and up to 10% improvement over state-of-the-art neural based model developed
for languages beside Nepali."
1462eb312944926469e7cee067dfc7f1267a2a8c,1908.05828,"How many distinct entity types are listed for the OurNepali and ILPRL datasets in the ""Named Entity Recognition for Nepali Language"" paper?","[{'answer': 'OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities', 'type': 'abstractive'}, {'answer': 'three', 'type': 'extractive'}]",1908.05828.pdf,"['1908.05828.pdf', '1608.06757.pdf', '2002.01359.pdf', '1706.08032.pdf', '1910.04269.pdf', '1904.09678.pdf', '1908.06083.pdf', '1903.09588.pdf', '1707.08559.pdf', '1907.09369.pdf', '2002.05058.pdf', '1910.02339.pdf', '1701.00185.pdf', '1904.03288.pdf', '2004.04721.pdf']",FLOAT SELECTED: Table 1: Dataset statistics,Named Entity Recognition for Nepali Language,"Named Entity Recognition have been studied for different languages like
English, German, Spanish and many others but no study have focused on Nepali
language. In this paper we propose a neural based Nepali NER using latest
state-of-the-art architecture based on grapheme-level which doesn't require any
hand-crafted features and no data pre-processing. Our novel neural based model
gained relative improvement of 33% to 50% compared to feature based SVM model
and up to 10% improvement over state-of-the-art neural based model developed
for languages beside Nepali."
f59f1f5b528a2eec5cfb1e49c87699e0c536cc45,1908.05828,"In the dataset statistics section of the paper on neural-based Nepali NER, how many sentences and entities are present in the newly created dataset?","[{'answer': '3606 sentences', 'type': 'abstractive'}, {'answer': 'Dataset contains 3606 total sentences and 79087 total entities.', 'type': 'abstractive'}]",1908.05828.pdf,"['1908.05828.pdf', '1706.08032.pdf', '1810.12196.pdf', '1909.00694.pdf', '1910.04269.pdf']",FLOAT SELECTED: Table 1: Dataset statistics,Named Entity Recognition for Nepali Language,"Named Entity Recognition have been studied for different languages like
English, German, Spanish and many others but no study have focused on Nepali
language. In this paper we propose a neural based Nepali NER using latest
state-of-the-art architecture based on grapheme-level which doesn't require any
hand-crafted features and no data pre-processing. Our novel neural based model
gained relative improvement of 33% to 50% compared to feature based SVM model
and up to 10% improvement over state-of-the-art neural based model developed
for languages beside Nepali."
9bd080bb2a089410fd7ace82e91711136116af6c,1908.05828,What is the performance comparison between the grapheme-level and character-level BiLSTM+CNN models on the OurNepali and ILPRL test datasets as reported in the Nepali NER paper?,"[{'answer': 'On OurNepali test dataset Grapheme-level representation model achieves average 0.16% improvement, on ILPRL test dataset it achieves maximum 1.62% improvement', 'type': 'abstractive'}, {'answer': 'BiLSTM+CNN(grapheme-level) which turns out to be performing on par with BiLSTM+CNN(character-level) under the same configuration', 'type': 'extractive'}]",1908.05828.pdf,"['1908.05828.pdf', '1704.05907.pdf', '1911.03310.pdf', '1905.07464.pdf', '1706.08032.pdf', '1909.08824.pdf', '1909.00105.pdf', '2002.01207.pdf', '2002.01664.pdf', '1902.00672.pdf']",FLOAT SELECTED: Table 5: Comparison of different variation of our models,Named Entity Recognition for Nepali Language,"Named Entity Recognition have been studied for different languages like
English, German, Spanish and many others but no study have focused on Nepali
language. In this paper we propose a neural based Nepali NER using latest
state-of-the-art architecture based on grapheme-level which doesn't require any
hand-crafted features and no data pre-processing. Our novel neural based model
gained relative improvement of 33% to 50% compared to feature based SVM model
and up to 10% improvement over state-of-the-art neural based model developed
for languages beside Nepali."
3070d6d6a52aa070f0c0a7b4de8abddd3da4f056,1711.02013,What performance metrics are reported to evaluate the PRPN model's variants on word/character-level language modeling tasks after the removal of different model components?,"[{'answer': 'BPC, Perplexity', 'type': 'abstractive'}]",1711.02013.pdf,"['1711.02013.pdf', '1804.05918.pdf', '1909.07734.pdf', '1704.08960.pdf', '1908.10084.pdf', '1809.08298.pdf', '1904.05584.pdf', '1805.04033.pdf', '1909.00175.pdf', '2004.01878.pdf', '2003.08385.pdf', '1909.13714.pdf']","In Table TABREF40 , we show the value of test perplexity for different variants of PRPN, each variant remove part of the model. ",Neural Language Modeling by Jointly Learning Syntax and Lexicon,"We propose a neural language model capable of unsupervised syntactic
structure induction. The model leverages the structure information to form
better semantic representations and better language modeling. Standard
recurrent neural networks are limited by their structure and fail to
efficiently use syntactic information. On the other hand, tree-structured
recursive networks usually require additional structural supervision at the
cost of human expert annotation. In this paper, We propose a novel neural
language model, called the Parsing-Reading-Predict Networks (PRPN), that can
simultaneously induce the syntactic structure from unannotated sentences and
leverage the inferred structure to learn a better language model. In our model,
the gradient can be directly back-propagated from the language model loss into
the neural parsing network. Experiments show that the proposed model can
discover the underlying syntactic structure and achieve state-of-the-art
performance on word/character-level language model tasks."
0c7823b27326b3f5dff51f32f45fc69c91a4e06d,1703.06492,"In the evaluation results, for which VQA task did the method achieve state-of-the-art accuracy of 60.34%, especially using basic questions and VGG?","[{'answer': 'in open-ended task esp. for counting-type questions ', 'type': 'abstractive'}]",1703.06492.pdf,"['1703.06492.pdf', '1908.07816.pdf', '1712.05999.pdf', '1610.07809.pdf', '1910.08987.pdf', '1810.12196.pdf', '2002.11402.pdf', '1704.05907.pdf', '2002.04181.pdf', '1701.03214.pdf', '2003.06044.pdf', '1910.10288.pdf']","FLOAT SELECTED: Table 4. Evaluation results on VQA dataset [1]. ”-” indicates the results are not available, and the Ours+VGG(1) and Ours+VGG(2) are the results by using different thresholds. Note that our VGGNet is same as CoAtt+VGG.",VQABQ: Visual Question Answering by Basic Questions,"Taking an image and question as the input of our method, it can output the
text-based answer of the query question about the given image, so called Visual
Question Answering (VQA). There are two main modules in our algorithm. Given a
natural language question about an image, the first module takes the question
as input and then outputs the basic questions of the main given question. The
second module takes the main question, image and these basic questions as input
and then outputs the text-based answer of the main question. We formulate the
basic questions generation problem as a LASSO optimization problem, and also
propose a criterion about how to exploit these basic questions to help answer
main question. Our method is evaluated on the challenging VQA dataset and
yields state-of-the-art accuracy, 60.34% in open-ended task."
37be0d479480211291e068d0d3823ad0c13321d3,1909.09587,"What are the exact EM and F1 scores for multi-BERT when fine-tuned on English and tested on Chinese, as compared to QANet trained on Chinese?","[{'answer': 'Table TABREF6, Table TABREF8', 'type': 'extractive'}, {'answer': 'when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En', 'type': 'extractive'}]",1909.09587.pdf,"['1909.09587.pdf', '1910.11769.pdf', '1901.04899.pdf', '2002.01207.pdf', '1810.09774.pdf', '1705.00108.pdf', '1909.13375.pdf', '1905.10810.pdf', '2001.08051.pdf', '1902.09666.pdf']","Table TABREF6 shows the result of different models trained on either Chinese or English and tested on Chinese. In row (f), multi-BERT is fine-tuned on English but tested on Chinese, which achieves competitive performance compared with QANet trained on Chinese. We also find that multi-BERT trained on English has relatively lower EM compared with the model with comparable F1 scores. ",Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model,"Because it is not feasible to collect training data for every language, there
is a growing interest in cross-lingual transfer learning. In this paper, we
systematically explore zero-shot cross-lingual transfer learning on reading
comprehension tasks with a language representation model pre-trained on
multi-lingual corpus. The experimental results show that with pre-trained
language representation zero-shot learning is feasible, and translating the
source data into the target language is not necessary and even degrades the
performance. We further explore what does the model learn in zero-shot setting."
a3d9b101765048f4b61cbd3eaa2439582ebb5c77,1909.09587,What source-target language pairs were fine-tuned and tested for zero-shot cross-lingual reading comprehension tasks in this paper's multi-BERT experiments?,"[{'answer': 'En-Fr, En-Zh, En-Jp, En-Kr, Zh-En, Zh-Fr, Zh-Jp, Zh-Kr to English, Chinese or Korean', 'type': 'abstractive'}, {'answer': 'English , Chinese', 'type': 'extractive'}, {'answer': 'English, Chinese, Korean, we translated the English and Chinese datasets into more languages, with Google Translate', 'type': 'extractive'}]",1909.09587.pdf,"['1909.09587.pdf', '1809.09194.pdf', '1908.05434.pdf', '1912.08960.pdf']","FLOAT SELECTED: Table 2: EM/F1 score of multi-BERTs fine-tuned on different training sets and tested on different languages (En: English, Fr: French, Zh: Chinese, Jp: Japanese, Kr: Korean, xx-yy: translated from xx to yy). The text in bold means training data language is the same as testing data language.",Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model,"Because it is not feasible to collect training data for every language, there
is a growing interest in cross-lingual transfer learning. In this paper, we
systematically explore zero-shot cross-lingual transfer learning on reading
comprehension tasks with a language representation model pre-trained on
multi-lingual corpus. The experimental results show that with pre-trained
language representation zero-shot learning is feasible, and translating the
source data into the target language is not necessary and even degrades the
performance. We further explore what does the model learn in zero-shot setting."
cc608df2884e1e82679f663ed9d9d67a4b6c03f3,1705.01214,What performance metrics were used to evaluate the classifiers in the first version of the training set for your proposed hybrid architecture in the finance domain?,"[{'answer': 'precision, recall, F1 and accuracy', 'type': 'abstractive'}, {'answer': 'Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy.', 'type': 'abstractive'}]",1705.01214.pdf,"['1705.01214.pdf', '1607.06025.pdf', '1707.00110.pdf', '1701.00185.pdf', '1905.06566.pdf', '1611.02550.pdf', '1912.06670.pdf', '1906.10551.pdf']",FLOAT SELECTED: Table 15: Evaluation of different classifiers in the first version of the training set,A Hybrid Architecture for Multi-Party Conversational Systems,"Multi-party Conversational Systems are systems with natural language
interaction between one or more people or systems. From the moment that an
utterance is sent to a group, to the moment that it is replied in the group by
a member, several activities must be done by the system: utterance
understanding, information search, reasoning, among others. In this paper we
present the challenges of designing and building multi-party conversational
systems, the state of the art, our proposed hybrid architecture using both
rules and machine learning and some insights after implementing and evaluating
one on the finance domain."
dac2591f19f5bbac3d4a7fa038ff7aa09f6f0d96,1911.08976,"What three methods did Red Dragon AI employ for explanation regeneration in their TextGraphs-13 Shared Task submission, and how are they ranked by MAP scoring?","[{'answer': 'Optimized TF-IDF, iterated TF-IDF, BERT re-ranking.', 'type': 'abstractive'}]",1911.08976.pdf,"['1911.08976.pdf', '1704.06194.pdf', '1710.06700.pdf', '1912.01673.pdf', '1809.01202.pdf', '1804.08139.pdf', '1607.06025.pdf', '1809.03449.pdf', '1806.04511.pdf', '2003.05377.pdf', '1701.02877.pdf', '2004.03744.pdf']","FLOAT SELECTED: Table 2: MAP scoring of new methods. The timings are in seconds for the whole dev-set, and the BERT Re-ranking figure includes the initial Iterated TF-IDF step.",Red Dragon AI at TextGraphs 2019 Shared Task: Language Model Assisted Explanation Generation,"The TextGraphs-13 Shared Task on Explanation Regeneration asked participants
to develop methods to reconstruct gold explanations for elementary science
questions. Red Dragon AI's entries used the language of the questions and
explanation text directly, rather than a constructing a separate graph-like
representation. Our leaderboard submission placed us 3rd in the competition,
but we present here three methods of increasing sophistication, each of which
scored successively higher on the test set after the competition close."
39a450ac15688199575798e72a2cc016ef4316b5,1712.03556,"What are the specific improvements in exact match (EM) and F1 scores of the Stochastic Answer Network (SAN) compared to baseline models, and how does SAN rank in F1 performance on the SQuAD test set?","[{'answer': 'Compared to baselines SAN (Table 1) shows  improvement of 1.096% on EM and 0.689% F1. Compared to other published SQuAD results (Table 2) SAN is ranked second. ', 'type': 'abstractive'}]",1712.03556.pdf,"['1712.03556.pdf', '1610.00879.pdf', '1611.04642.pdf', '1909.03242.pdf', '1908.06379.pdf', '1909.08089.pdf', '1704.05907.pdf', '1909.13714.pdf', '2004.01878.pdf', '1912.01673.pdf', '1909.13695.pdf', '1904.10500.pdf', '1910.08987.pdf', '1909.08041.pdf', '2002.10361.pdf', '1611.02550.pdf']",FLOAT SELECTED: Table 2: Test performance on SQuAD. Results are sorted by Test F1.,Stochastic Answer Networks for Machine Reading Comprehension,"We propose a simple yet robust stochastic answer network (SAN) that simulates
multi-step reasoning in machine reading comprehension. Compared to previous
work such as ReasoNet which used reinforcement learning to determine the number
of steps, the unique feature is the use of a kind of stochastic prediction
dropout on the answer module (final layer) of the neural network during the
training. We show that this simple trick improves robustness and achieves
results competitive to the state-of-the-art on the Stanford Question Answering
Dataset (SQuAD), the Adversarial SQuAD, and the Microsoft MAchine Reading
COmprehension Dataset (MS MARCO)."
5c90e1ed208911dbcae7e760a553e912f8c237a5,1911.00069,"What are the exact document counts for the in-house and ACE05 datasets in the training, development, and test sets in the paper on neural cross-lingual relation extraction using bilingual word embedding mapping?","[{'answer': 'In-house dataset consists of  3716 documents \nACE05 dataset consists of  1635 documents', 'type': 'abstractive'}]",1911.00069.pdf,"['1911.00069.pdf', '1605.07683.pdf', '1909.00015.pdf', '1908.11546.pdf', '2002.10361.pdf', '1912.13109.pdf', '1812.01704.pdf', '2002.11910.pdf', '2003.03014.pdf', '1912.01673.pdf', '1910.11204.pdf', '1905.00563.pdf', '1605.07333.pdf', '1912.01214.pdf', '1904.10500.pdf', '1804.08050.pdf']",FLOAT SELECTED: Table 2: Number of documents in the training/dev/test sets of the in-house and ACE05 datasets.,Neural Cross-Lingual Relation Extraction Based on Bilingual Word Embedding Mapping,"Relation extraction (RE) seeks to detect and classify semantic relationships
between entities, which provides useful information for many NLP applications.
Since the state-of-the-art RE models require large amounts of manually
annotated data and language-specific resources to achieve high accuracy, it is
very challenging to transfer an RE model of a resource-rich language to a
resource-poor language. In this paper, we propose a new approach for
cross-lingual RE model transfer based on bilingual word embedding mapping. It
projects word embeddings from a target language to a source language, so that a
well-trained source-language neural network RE model can be directly applied to
the target language. Experiment results show that the proposed approach
achieves very good performance for a number of target languages on both
in-house and open datasets, using a small bilingual dictionary with only 1K
word pairs."
9e04730907ad728d62049f49ac828acb4e0a1a2a,1701.00185,"What are the ACC and NMI performance results of the STC^2 framework for the SearchSnippets, StackOverflow, and Biomedical datasets?","[{'answer': 'On SearchSnippets dataset ACC 77.01%, NMI 62.94%, on StackOverflow dataset ACC 51.14%, NMI 49.08%, on Biomedical dataset ACC 43.00%, NMI 38.18%', 'type': 'abstractive'}]",1701.00185.pdf,"['1701.00185.pdf', '2003.12218.pdf', '1704.08960.pdf', '1901.04899.pdf', '1611.04642.pdf', '2002.01664.pdf', '1712.03556.pdf', '1909.11467.pdf', '1911.10049.pdf', '2003.01769.pdf', '1605.08675.pdf']","FLOAT SELECTED: Table 6: Comparison of ACC of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.",Self-Taught Convolutional Neural Networks for Short Text Clustering,"Short text clustering is a challenging problem due to its sparseness of text
representation. Here we propose a flexible Self-Taught Convolutional neural
network framework for Short Text Clustering (dubbed STC^2), which can flexibly
and successfully incorporate more useful semantic features and learn non-biased
deep text representation in an unsupervised manner. In our framework, the
original raw text features are firstly embedded into compact binary codes by
using one existing unsupervised dimensionality reduction methods. Then, word
embeddings are explored and fed into convolutional neural networks to learn
deep feature representations, meanwhile the output units are used to fit the
pre-trained binary codes in the training process. Finally, we get the optimal
clusters by employing K-means to cluster the learned representations. Extensive
experimental results demonstrate that the proposed framework is effective,
flexible and outperform several popular clustering methods when tested on three
public short text datasets."
5a0841cc0628e872fe473874694f4ab9411a1d10,1701.00185,By what percentage did the STC^2 model outperform other non-biased clustering methods in terms of ACC and NMI on both the SearchSnippets and Biomedical datasets?,"[{'answer': 'on SearchSnippets dataset by 6.72% in ACC, by 6.94% in NMI; on Biomedical dataset by 5.77% in ACC, 3.91% in NMI', 'type': 'abstractive'}]",1701.00185.pdf,"['1701.00185.pdf', '1707.08559.pdf', '1910.12129.pdf', '1909.06162.pdf', '1809.00540.pdf', '2003.07723.pdf', '1809.09795.pdf', '1611.02550.pdf', '1905.00563.pdf', '1911.03597.pdf', '2004.03788.pdf']","FLOAT SELECTED: Table 6: Comparison of ACC of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.",Self-Taught Convolutional Neural Networks for Short Text Clustering,"Short text clustering is a challenging problem due to its sparseness of text
representation. Here we propose a flexible Self-Taught Convolutional neural
network framework for Short Text Clustering (dubbed STC^2), which can flexibly
and successfully incorporate more useful semantic features and learn non-biased
deep text representation in an unsupervised manner. In our framework, the
original raw text features are firstly embedded into compact binary codes by
using one existing unsupervised dimensionality reduction methods. Then, word
embeddings are explored and fed into convolutional neural networks to learn
deep feature representations, meanwhile the output units are used to fit the
pre-trained binary codes in the training process. Finally, we get the optimal
clusters by employing K-means to cluster the learned representations. Extensive
experimental results demonstrate that the proposed framework is effective,
flexible and outperform several popular clustering methods when tested on three
public short text datasets."
34fab25d9ceb9c5942daf4ebdab6c5dd4ff9d3db,1911.02821,What are the four datasets reported for evaluating the improvements brought by the Multi-source Word-aligned Attention (MWA) method in this paper?,"[{'answer': 'weibo-100k, Ontonotes, LCQMC and XNLI', 'type': 'abstractive'}]",1911.02821.pdf,"['1911.02821.pdf', '1909.09484.pdf', '1605.08675.pdf', '1911.12579.pdf', '2002.11910.pdf', '1806.11432.pdf', '2002.06644.pdf', '1804.07789.pdf', '1907.03060.pdf']",Table TABREF14 shows the experiment measuring improvements from the MWA attention on test sets of four datasets.,Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention,"Most Chinese pre-trained models take character as the basic unit and learn
representation according to character's external contexts, ignoring the
semantics expressed in the word, which is the smallest meaningful utterance in
Chinese. Hence, we propose a novel word-aligned attention to exploit explicit
word information, which is complementary to various character-based Chinese
pre-trained language models. Specifically, we devise a pooling mechanism to
align the character-level attention to the word level and propose to alleviate
the potential issue of segmentation error propagation by multi-source
information fusion. As a result, word and character information are explicitly
integrated at the fine-tuning procedure. Experimental results on five Chinese
NLP benchmark tasks demonstrate that our model could bring another significant
gain over several pre-trained models."
1fb73176394ef59adfaa8fc7827395525f9a5af7,1903.00172,What are the two question-answer datasets used to evaluate NeurON’s tuple extraction performance?,"[{'answer': 'AmazonQA and ConciergeQA datasets', 'type': 'abstractive'}]",1903.00172.pdf,"['1903.00172.pdf', '1911.08976.pdf', '1909.05855.pdf', '1902.09314.pdf', '2001.06888.pdf', '1908.06264.pdf', '1910.11235.pdf', '1908.08345.pdf', '1605.07333.pdf', '1609.00559.pdf', '1911.10049.pdf', '1708.09609.pdf', '2001.08868.pdf', '1704.05907.pdf', '1901.09755.pdf', '2004.03788.pdf']","FLOAT SELECTED: Table 3: Precision (P), Recall (R), and Relative Coverage (RC) results on ConciergeQA.",Open Information Extraction from Question-Answer Pairs,"Open Information Extraction (OpenIE) extracts meaningful structured tuples
from free-form text. Most previous work on OpenIE considers extracting data
from one sentence at a time. We describe NeurON, a system for extracting tuples
from question-answer pairs. Since real questions and answers often contain
precisely the information that users care about, such information is
particularly desirable to extend a knowledge base with.
  NeurON addresses several challenges. First, an answer text is often hard to
understand without knowing the question, and second, relevant information can
span multiple sentences. To address these, NeurON formulates extraction as a
multi-source sequence-to-sequence learning task, wherein it combines
distributed representations of a question and an answer to generate knowledge
facts. We describe experiments on two real-world datasets that demonstrate that
NeurON can find a significant number of new and interesting facts to extend a
knowledge base compared to state-of-the-art OpenIE methods."
d70ba6053e245ee4179c26a5dabcad37561c6af0,1903.00172,Which two real-world datasets are listed in the NeurON paper for evaluating the system's performance in Open Information Extraction from question-answer pairs?,"[{'answer': 'ConciergeQA and AmazonQA', 'type': 'abstractive'}]",1903.00172.pdf,"['1903.00172.pdf', '1607.06025.pdf', '1812.06705.pdf', '1909.07734.pdf', '2002.11910.pdf', '1908.07195.pdf', '2002.04181.pdf', '1909.08041.pdf', '1809.02279.pdf', '1906.10225.pdf', '1911.08962.pdf']",FLOAT SELECTED: Table 1: Various types of training instances.,Open Information Extraction from Question-Answer Pairs,"Open Information Extraction (OpenIE) extracts meaningful structured tuples
from free-form text. Most previous work on OpenIE considers extracting data
from one sentence at a time. We describe NeurON, a system for extracting tuples
from question-answer pairs. Since real questions and answers often contain
precisely the information that users care about, such information is
particularly desirable to extend a knowledge base with.
  NeurON addresses several challenges. First, an answer text is often hard to
understand without knowing the question, and second, relevant information can
span multiple sentences. To address these, NeurON formulates extraction as a
multi-source sequence-to-sequence learning task, wherein it combines
distributed representations of a question and an answer to generate knowledge
facts. We describe experiments on two real-world datasets that demonstrate that
NeurON can find a significant number of new and interesting facts to extend a
knowledge base compared to state-of-the-art OpenIE methods."
efe9bad55107a6be7704ed97ecce948a8ca7b1d2,1909.11687,"What specific compression techniques, including NoKD and PKD, were compared in the evaluation of mixed-vocabulary training models in the ""Extremely Small BERT Models from Mixed-Vocabulary Training"" paper?","[{'answer': 'baseline without knowledge distillation (termed NoKD), Patient Knowledge Distillation (PKD)', 'type': 'extractive'}, {'answer': 'NoKD, PKD, BERTBASE teacher model', 'type': 'extractive'}]",1909.11687.pdf,"['1909.11687.pdf', '1710.06700.pdf', '2004.03354.pdf', '1811.01088.pdf', '1910.00912.pdf', '1909.09270.pdf', '1909.11297.pdf', '1909.13714.pdf']","For the language modeling evaluation, we also evaluate a baseline without knowledge distillation (termed NoKD), with a model parameterized identically to the distilled student models but trained directly on the teacher model objective from scratch. For downstream tasks, we compare with NoKD as well as Patient Knowledge Distillation (PKD) from BIBREF34, who distill the 12-layer BERTBASE model into 3 and 6-layer BERT models by using the teacher model's hidden states.",Extremely Small BERT Models from Mixed-Vocabulary Training,"Pretrained language models like BERT have achieved good results on NLP tasks,
but are impractical on resource-limited devices due to memory footprint. A
large fraction of this footprint comes from the input embeddings with large
input vocabulary and embedding dimensions. Existing knowledge distillation
methods used for model compression cannot be directly applied to train student
models with reduced vocabulary sizes. To this end, we propose a distillation
method to align the teacher and student embeddings via mixed-vocabulary
training. Our method compresses BERT-LARGE to a task-agnostic model with
smaller vocabulary and hidden dimensions, which is an order of magnitude
smaller than other distilled BERT models and offers a better size-accuracy
trade-off on language understanding benchmarks as well as a practical dialogue
task."
c32adef59efcb9d1a5b10e1d7c999a825c9e6d9a,1809.01541,What languages are reported in the official CoNLL-SIGMORPHON 2018 shared task test set results for evaluating the Copenhagen system's morphological reinflection approach?,"[{'answer': 'German, English, Spanish, Finnish, French, Russian,  Swedish.', 'type': 'abstractive'}]",1809.01541.pdf,"['1809.01541.pdf', '2003.11645.pdf', '1606.05320.pdf', '1902.09666.pdf', '1909.00512.pdf', '1909.01383.pdf', '1701.02877.pdf', '1705.01214.pdf']",FLOAT SELECTED: Table 2: Official shared task test set results.,Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding,"This paper documents the Team Copenhagen system which placed first in the
CoNLL--SIGMORPHON 2018 shared task on universal morphological reinflection,
Task 2 with an overall accuracy of 49.87. Task 2 focuses on morphological
inflection in context: generating an inflected word form, given the lemma of
the word and the context it occurs in. Previous SIGMORPHON shared tasks have
focused on context-agnostic inflection---the ""inflection in context"" task was
introduced this year. We approach this with an encoder-decoder architecture
over character sequences with three core innovations, all contributing to an
improvement in performance: (1) a wide context window; (2) a multi-task
learning approach with the auxiliary task of MSD prediction; (3) training
models in a multilingual fashion."
32a3c248b928d4066ce00bbb0053534ee62596e7,1809.01541,"What specific type of MSD tag prediction (e.g., V;PST;V.PTCP;PASS) is used as an auxiliary task in the Copenhagen system for the inflection-in-context task in CoNLL--SIGMORPHON 2018, particularly highlighted in Track 1?","[{'answer': 'The task of predicting MSD tags: V, PST, V.PCTP, PASS.', 'type': 'abstractive'}, {'answer': 'morphosyntactic descriptions (MSD)', 'type': 'extractive'}]",1809.01541.pdf,"['1809.01541.pdf', '1608.06757.pdf', '1911.10049.pdf', '2002.06675.pdf', '1902.10525.pdf', '1904.09678.pdf', '2002.02070.pdf', '1707.00110.pdf', '1804.08139.pdf', '2003.05377.pdf', '1804.07789.pdf', '1806.07711.pdf', '1607.06025.pdf', '1909.08089.pdf', '1809.02286.pdf', '2001.06888.pdf', '1711.11221.pdf', '1910.03814.pdf']","FLOAT SELECTED: Table 1: Example input sentence. Context MSD tags and lemmas, marked in gray, are only available in Track 1. The cyan square marks the main objective of predicting the word form made. The magenta square marks the auxiliary objective of predicting the MSD tag V;PST;V.PTCP;PASS.",Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding,"This paper documents the Team Copenhagen system which placed first in the
CoNLL--SIGMORPHON 2018 shared task on universal morphological reinflection,
Task 2 with an overall accuracy of 49.87. Task 2 focuses on morphological
inflection in context: generating an inflected word form, given the lemma of
the word and the context it occurs in. Previous SIGMORPHON shared tasks have
focused on context-agnostic inflection---the ""inflection in context"" task was
introduced this year. We approach this with an encoder-decoder architecture
over character sequences with three core innovations, all contributing to an
improvement in performance: (1) a wide context window; (2) a multi-task
learning approach with the auxiliary task of MSD prediction; (3) training
models in a multilingual fashion."
f68508adef6f4bcdc0cc0a3ce9afc9a2b6333cc5,1909.13714,"What was the specific improvement in F1 scores for both intent detection and slot filling when incorporating multimodal inputs (speech embeddings, audio, vision) in the NLU models?","[{'answer': 'by 2.3-6.8 points in f1 score for intent recognition and 0.8-3.5 for slot filling', 'type': 'abstractive'}, {'answer': 'F1 score increased from 0.89 to 0.92', 'type': 'abstractive'}]",1909.13714.pdf,"['1909.13714.pdf', '1804.08139.pdf', '1911.10049.pdf', '1703.02507.pdf', '1912.10011.pdf', '1911.02086.pdf', '1909.03405.pdf', '1701.09123.pdf', '1909.03135.pdf']",FLOAT SELECTED: Table 1: Speech Embeddings Experiments: Precision/Recall/F1-scores (%) of NLU Models,Towards Multimodal Understanding of Passenger-Vehicle Interactions in Autonomous Vehicles: Intent/Slot Recognition Utilizing Audio-Visual Data,"Understanding passenger intents from spoken interactions and car's vision
(both inside and outside the vehicle) are important building blocks towards
developing contextual dialog systems for natural interactions in autonomous
vehicles (AV). In this study, we continued exploring AMIE (Automated-vehicle
Multimodal In-cabin Experience), the in-cabin agent responsible for handling
certain multimodal passenger-vehicle interactions. When the passengers give
instructions to AMIE, the agent should parse such commands properly considering
available three modalities (language/text, audio, video) and trigger the
appropriate functionality of the AV system. We had collected a multimodal
in-cabin dataset with multi-turn dialogues between the passengers and AMIE
using a Wizard-of-Oz scheme via realistic scavenger hunt game. In our previous
explorations, we experimented with various RNN-based models to detect
utterance-level intents (set destination, change route, go faster, go slower,
stop, park, pull over, drop off, open door, and others) along with intent
keywords and relevant slots (location, position/direction, object,
gesture/gaze, time-guidance, person) associated with the action to be performed
in our AV scenarios. In this recent work, we propose to discuss the benefits of
multimodal understanding of in-cabin utterances by incorporating
verbal/language input (text and speech embeddings) together with the
non-verbal/acoustic and visual input from inside and outside the vehicle (i.e.,
passenger gestures and gaze from in-cabin video stream, referred objects
outside of the vehicle from the road view camera stream). Our experimental
results outperformed text-only baselines and with multimodality, we achieved
improved performances for utterance-level intent detection and slot filling."
2a6003a74d051d0ebbe62e8883533a5f5e55078b,1702.03342,"Which neural embedding model, CRX or 3C, achieves the highest accuracy in the dataless concept categorization task as presented in the paper?","[{'answer': 'the CRX model', 'type': 'abstractive'}, {'answer': '3C model', 'type': 'extractive'}]",1702.03342.pdf,"['1702.03342.pdf', '1801.05147.pdf', '1909.07734.pdf', '1912.01673.pdf', '1910.03814.pdf']",FLOAT SELECTED: Table 5 Accuracy of concept categorization,Learning Concept Embeddings for Efficient Bag-of-Concepts Densification,"Explicit concept space models have proven efficacy for text representation in
many natural language and text mining applications. The idea is to embed
textual structures into a semantic space of concepts which captures the main
ideas, objects, and the characteristics of these structures. The so called Bag
of Concepts (BoC) representation suffers from data sparsity causing low
similarity scores between similar texts due to low concept overlap. To address
this problem, we propose two neural embedding models to learn continuous
concept vectors. Once they are learned, we propose an efficient vector
aggregation method to generate fully continuous BoC representations. We
evaluate our concept embedding models on three tasks: 1) measuring entity
semantic relatedness and ranking where we achieve 1.6% improvement in
correlation scores, 2) dataless concept categorization where we achieve
state-of-the-art performance and reduce the categorization error rate by more
than 5% compared to five prior word and entity embedding models, and 3)
dataless document classification where our models outperform the sparse BoC
representations. In addition, by exploiting our efficient linear time vector
aggregation method, we achieve better accuracy scores with much less concept
dimensions compared to previous BoC densification methods which operate in
polynomial time and require hundreds of dimensions in the BoC representation."
3415762847ed13acc3c90de60e3ef42612bc49af,1910.12795,"How does the data manipulation method in this paper improve text classification accuracy on low-data datasets like SST-5, TREC, and IMDB, as well as imbalanced label distributions such as 100:1000 and 20:1000?","[{'answer': 'Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000', 'type': 'abstractive'}]",1910.12795.pdf,"['1910.12795.pdf', '1809.03449.pdf', '1812.06705.pdf', '1606.00189.pdf', '1910.06592.pdf', '1909.11297.pdf', '1909.11687.pdf', '1905.07464.pdf', '2002.11910.pdf', '1910.10288.pdf']","FLOAT SELECTED: Table 1: Accuracy of Data Manipulation on Text Classification. All results are averaged over 15 runs ± one standard deviation. The numbers in parentheses next to the dataset names indicate the size of the datasets. For example, (40+2) denotes 40 training instances and 2 validation instances per class.",Learning Data Manipulation for Augmentation and Weighting,"Manipulating data, such as weighting data examples or augmenting with new
instances, has been increasingly used to improve model training. Previous work
has studied various rule- or learning-based approaches designed for specific
types of data manipulation. In this work, we propose a new method that supports
learning different manipulation schemes with the same gradient-based algorithm.
Our approach builds upon a recent connection of supervised learning and
reinforcement learning (RL), and adapts an off-the-shelf reward learning
algorithm from RL for joint data manipulation learning and model training.
Different parameterization of the ""data reward"" function instantiates different
manipulation schemes. We showcase data augmentation that learns a text
transformation network, and data weighting that dynamically adapts the data
sample importance. Experiments show the resulting algorithms significantly
improve the image and text classification performance in low data regime and
class-imbalance problems."
dc1fe3359faa2d7daa891c1df33df85558bc461b,1910.04269,Does the paper indicate that the language identification models combine both log-Mel spectrograms and raw waveforms as input features simultaneously?,"[{'answer': 'No', 'type': 'boolean'}]",1910.04269.pdf,"['1910.04269.pdf', '1912.00864.pdf', '2001.05493.pdf', '1910.12129.pdf', '1909.00015.pdf']",FLOAT SELECTED: Table 4: Results of the two models and all its variations,Spoken Language Identification using ConvNets,"Language Identification (LI) is an important first step in several speech
processing systems. With a growing number of voice-based assistants, speech LI
has emerged as a widely researched field. To approach the problem of
identifying languages, we can either adopt an implicit approach where only the
speech for a language is present or an explicit one where text is available
with its corresponding transcript. This paper focuses on an implicit approach
due to the absence of transcriptive data. This paper benchmarks existing models
and proposes a new attention based model for language identification which uses
log-Mel spectrogram images as input. We also present the effectiveness of raw
waveforms as features to neural network models for LI tasks. For training and
evaluation of models, we classified six languages (English, French, German,
Spanish, Russian and Italian) with an accuracy of 95.4% and four languages
(English, French, German, Spanish) with an accuracy of 96.3% obtained from the
VoxForge dataset. This approach can further be scaled to incorporate more
languages."
37eba8c3cfe23778498d95a7dfddf8dfb725f8e2,1703.02507,Which unsupervised models are compared with the proposed sentence embedding method on supervised evaluation tasks in the study?,"[{'answer': 'Sequential (Denoising) Autoencoder, TF-IDF BOW, SkipThought, FastSent, Siamese C-BOW, C-BOW, C-PHRASE, ParagraphVector', 'type': 'extractive'}]",1703.02507.pdf,"['1703.02507.pdf', '2001.08051.pdf', '1910.06592.pdf', '1910.03814.pdf']","FLOAT SELECTED: Table 1: Comparison of the performance of different models on different supervised evaluation tasks. An underline indicates the best performance for the dataset. Top 3 performances in each data category are shown in bold. The average is calculated as the average of accuracy for each category (For MSRP, we take the accuracy). )",Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features,"The recent tremendous success of unsupervised word embeddings in a multitude
of applications raises the obvious question if similar methods could be derived
to improve embeddings (i.e. semantic representations) of word sequences as
well. We present a simple but efficient unsupervised objective to train
distributed representations of sentences. Our method outperforms the
state-of-the-art unsupervised models on most benchmark tasks, highlighting the
robustness of the produced general-purpose sentence embeddings."
ba6422e22297c7eb0baa381225a2f146b9621791,1909.0248,What is the BLEU score difference between FlowSeq and state-of-the-art baselines that utilize advanced decoding techniques like iterative refinement and NPD rescoring on NMT tasks?,"[{'answer': 'Difference is around 1 BLEU score lower on average than state of the art methods.', 'type': 'abstractive'}]",1909.02480.pdf,"['1909.02480.pdf', '1909.00512.pdf', '1703.02507.pdf', '1908.07195.pdf', '1810.12085.pdf', '1806.07711.pdf', '1908.06151.pdf', '1911.08673.pdf', '1903.09722.pdf', '1909.00578.pdf', '1910.00825.pdf', '1909.00279.pdf', '2003.12218.pdf']","Table TABREF40 illustrates the BLEU scores of FlowSeq and baselines with advanced decoding methods such as iterative refinement, IWD and NPD rescoring.",,
d8de12f5eff64d0e9c9e88f6ebdabc4cdf042c22,1603.04513,"What are the performance improvements in test scores for MVCNN with pretraining across the classification tasks Binary, Fine-Grained, Senti140, and Subjectivity?","[{'answer': '0.8 points on Binary; 0.7 points on Fine-Grained; 0.6 points on Senti140; 0.7 points on Subj', 'type': 'abstractive'}]",1603.04513.pdf,"['1603.04513.pdf', '1904.10503.pdf', '1905.10810.pdf', '1905.12260.pdf', '1806.11432.pdf', '1905.06566.pdf', '1808.09029.pdf', '1909.09270.pdf', '1809.06537.pdf', '1810.12196.pdf', '1711.11221.pdf', '1712.03547.pdf', '1706.08032.pdf', '1612.08205.pdf', '1909.09587.pdf', '1909.00105.pdf', '1711.00106.pdf', '1911.08673.pdf']","FLOAT SELECTED: Table 3: Test set results of our CNN model against other methods. RAE: Recursive Autoencoders with pretrained word embeddings from Wikipedia (Socher et al., 2011b). MV-RNN: Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). RNTN: Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). DCNN, MAX-TDNN, NBOW: Dynamic Convolution Neural Network with k-max pooling, Time-Delay Neural Networks with Max-pooling (Collobert and Weston, 2008), Neural Bag-of-Words Models (Kalchbrenner et al., 2014). Paragraph-Vec: Logistic regression on top of paragraph vectors (Le and Mikolov, 2014). SVM, BINB, MAXENT: Support Vector Machines, Naive Bayes with unigram features and bigram features, Maximum Entropy (Go et al., 2009). NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from Wang and Manning (2012). CNN-rand/static/multichannel/nonstatic: CNN with word embeddings randomly initialized / initialized by pretrained vectors and kept static during training / initialized with two copies (each is a “channel”) of pretrained embeddings / initialized with pretrained embeddings while fine-tuned during training (Kim, 2014). G-Dropout, F-Dropout: Gaussian Dropout and Fast Dropout from Wang and Manning (2013). Minus sign “-” in MVCNN (-Huang) etc. means “Huang” is not used. “versions / filters / tricks / layers” denote the MVCNN variants with different setups: discard certain embedding version / discard certain filter size / discard mutual-learning or pretraining / different numbers of convolution layer.",Multichannel Variable-Size Convolution for Sentence Classification,"We propose MVCNN, a convolution neural network (CNN) architecture for
sentence classification. It (i) combines diverse versions of pretrained word
embeddings and (ii) extracts features of multigranular phrases with
variable-size convolution filters. We also show that pretraining MVCNN is
critical for good performance. MVCNN achieves state-of-the-art performance on
four tasks: on small-scale binary, small-scale multi-class and largescale
Twitter sentiment prediction and on subjectivity classification."
9cba2ee1f8e1560e48b3099d0d8cf6c854ddea2e,1603.04513,How do the variable-size convolution filters (especially sizes 5 and 7) contribute to the extraction of multigranular phrase features and overall performance in the MVCNN architecture?,"[{'answer': 'The system benefits from filters of each size., features of multigranular phrases are extracted with variable-size convolution filters.', 'type': 'extractive'}]",1603.04513.pdf,"['1603.04513.pdf', '1611.04798.pdf', '1908.10084.pdf', '1910.08987.pdf']","The block “filters” indicates the contribution of each filter size. The system benefits from filters of each size. Sizes 5 and 7 are most important for high performance, especially 7 (rows 25 and 26).",Multichannel Variable-Size Convolution for Sentence Classification,"We propose MVCNN, a convolution neural network (CNN) architecture for
sentence classification. It (i) combines diverse versions of pretrained word
embeddings and (ii) extracts features of multigranular phrases with
variable-size convolution filters. We also show that pretraining MVCNN is
critical for good performance. MVCNN achieves state-of-the-art performance on
four tasks: on small-scale binary, small-scale multi-class and largescale
Twitter sentiment prediction and on subjectivity classification."
7975c3e1f61344e3da3b38bb12e1ac6dcb153a18,1603.04513,How does the removal of individual pretrained word embedding versions in the MVCNN model affect sentence classification performance?,"[{'answer': 'each embedding version is crucial for good performance', 'type': 'extractive'}]",1603.04513.pdf,"['1603.04513.pdf', '2002.05058.pdf', '1707.03569.pdf', '1911.03597.pdf', '1911.05153.pdf', '2004.04721.pdf', '1908.06083.pdf', '1911.00069.pdf', '1912.13109.pdf', '1701.03214.pdf', '1901.09755.pdf', '1912.08960.pdf', '1701.09123.pdf', '1610.00879.pdf', '1804.11346.pdf']","In the block “versions”, we see that each embedding version is crucial for good performance: performance drops in every single case. ",Multichannel Variable-Size Convolution for Sentence Classification,"We propose MVCNN, a convolution neural network (CNN) architecture for
sentence classification. It (i) combines diverse versions of pretrained word
embeddings and (ii) extracts features of multigranular phrases with
variable-size convolution filters. We also show that pretraining MVCNN is
critical for good performance. MVCNN achieves state-of-the-art performance on
four tasks: on small-scale binary, small-scale multi-class and largescale
Twitter sentiment prediction and on subjectivity classification."
b43fa27270eeba3e80ff2a03754628b5459875d6,2002.01359,"What are the domains and services listed in the Schema-Guided Dialogue State Tracking Task dataset, as described for intents and dialogues across the train, dev, and test sets?","[{'answer': 'Alarm, Banks, Buses, Calendar, Events, Flights, Homes, Hotels, Media, Messaging, Movies, Music, Payment, Rental Cars, Restaurants, Ride Sharing, Services, Train, Travel, Weather', 'type': 'abstractive'}]",2002.01359.pdf,"['2002.01359.pdf', '1809.10644.pdf', '1910.12795.pdf', '1605.07333.pdf', '2002.01207.pdf', '1806.04511.pdf', '1811.01088.pdf', '2003.07723.pdf', '1707.03569.pdf', '1910.11204.pdf', '1711.11221.pdf', '2003.05377.pdf', '1909.11297.pdf', '1911.07555.pdf']","FLOAT SELECTED: Table 2: The total number of intents (services in parentheses) and dialogues for each domain across train1, dev2 and test3 sets. Superscript indicates the datasets in which dialogues from the domain are present. Multi-domain dialogues contribute to counts of each domain. The domain Services includes salons, dentists, doctors, etc.",Schema-Guided Dialogue State Tracking Task at DSTC8,"This paper gives an overview of the Schema-Guided Dialogue State Tracking
task of the 8th Dialogue System Technology Challenge. The goal of this task is
to develop dialogue state tracking models suitable for large-scale virtual
assistants, with a focus on data-efficient joint modeling across domains and
zero-shot generalization to new APIs. This task provided a new dataset
consisting of over 16000 dialogues in the training set spanning 16 domains to
highlight these challenges, and a baseline model capable of zero-shot
generalization to new APIs. Twenty-five teams participated, developing a range
of neural network models, exceeding the performance of the baseline model by a
very high margin. The submissions incorporated a variety of pre-trained
encoders and data augmentation techniques. This paper describes the task
definition, dataset and evaluation methodology. We also summarize the approach
and results of the submitted systems to highlight the overall trends in the
state-of-the-art."
79620a2b4b121b6d3edd0f7b1d4a8cc7ada0b516,1911.11951,Which stance detection model achieved state-of-the-art weighted accuracy and standard accuracy on the Fake News Challenge Stage 1 (FNC-I) benchmark by leveraging a RoBERTa-based deep bidirectional transformer with bidirectional cross-attention between claim-article pairs?,"[{'answer': 'To the best of our knowledge, our method achieves state-of-the-art results in weighted-accuracy and standard accuracy on the dataset', 'type': 'extractive'}]",1911.11951.pdf,"['1911.11951.pdf', '1911.02711.pdf', '1901.09755.pdf', '1901.02257.pdf', '1810.05241.pdf', '1605.07333.pdf', '1712.00991.pdf', '1904.01608.pdf', '1912.13109.pdf']","FLOAT SELECTED: Table 2: Performance of various methods on the FNC-I benchmark. The first and second groups are methods introduced during and after the challenge period, respectively. Best results are in bold.",Taking a Stance on Fake News: Towards Automatic Disinformation Assessment via Deep Bidirectional Transformer Language Models for Stance Detection,"The exponential rise of social media and digital news in the past decade has
had the unfortunate consequence of escalating what the United Nations has
called a global topic of concern: the growing prevalence of disinformation.
Given the complexity and time-consuming nature of combating disinformation
through human assessment, one is motivated to explore harnessing AI solutions
to automatically assess news articles for the presence of disinformation. A
valuable first step towards automatic identification of disinformation is
stance detection, where given a claim and a news article, the aim is to predict
if the article agrees, disagrees, takes no position, or is unrelated to the
claim. Existing approaches in literature have largely relied on hand-engineered
features or shallow learned representations (e.g., word embeddings) to encode
the claim-article pairs, which can limit the level of representational
expressiveness needed to tackle the high complexity of disinformation
identification. In this work, we explore the notion of harnessing large-scale
deep bidirectional transformer language models for encoding claim-article pairs
in an effort to construct state-of-the-art stance detection geared for
identifying disinformation. Taking advantage of bidirectional cross-attention
between claim-article pairs via pair encoding with self-attention, we construct
a large-scale language model for stance detection by performing transfer
learning on a RoBERTa deep bidirectional transformer language model, and were
able to achieve state-of-the-art performance (weighted accuracy of 90.01%) on
the Fake News Challenge Stage 1 (FNC-I) benchmark. These promising results
serve as motivation for harnessing such large-scale language models as powerful
building blocks for creating effective AI solutions to combat disinformation."
e79a5b6b6680bd2f63e9f4adbaae1d7795d81e38,1806.04511,"Which non-English language achieved the highest sentiment analysis accuracy after translation using the RNN model trained on English reviews, as reported in the experimental results with Russian, Spanish, Turkish, and Dutch?","[{'answer': 'Russian', 'type': 'extractive'}, {'answer': 'Russsian', 'type': 'abstractive'}]",1806.04511.pdf,"['1806.04511.pdf', '1912.01673.pdf', '1906.01081.pdf', '1909.00578.pdf', '2003.05377.pdf', '1901.02257.pdf', '1911.11951.pdf', '1805.03710.pdf', '1908.06379.pdf', '1611.00514.pdf', '1905.06566.pdf', '1908.06083.pdf']","Considering the improvements over the majority baseline achieved by the RNN model for both non-English (on the average 22.76% relative improvement; 15.82% relative improvement on Spanish, 72.71% vs. 84.21%, 30.53% relative improvement on Turkish, 56.97% vs. 74.36%, 37.13% relative improvement on Dutch, 59.63% vs. 81.77%, and 7.55% relative improvement on Russian, 79.60% vs. 85.62%) and English test sets (27.34% relative improvement), we can draw the conclusion that our model is robust to handle multiple languages.",Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data,"Sentiment analysis is a widely studied NLP task where the goal is to
determine opinions, emotions, and evaluations of users towards a product, an
entity or a service that they are reviewing. One of the biggest challenges for
sentiment analysis is that it is highly language dependent. Word embeddings,
sentiment lexicons, and even annotated data are language specific. Further,
optimizing models for each language is very time consuming and labor intensive
especially for recurrent neural network models. From a resource perspective, it
is very challenging to collect data for different languages.
  In this paper, we look for an answer to the following research question: can
a sentiment analysis model trained on a language be reused for sentiment
analysis in other languages, Russian, Spanish, Turkish, and Dutch, where the
data is more limited? Our goal is to build a single model in the language with
the largest dataset available for the task, and reuse it for languages that
have limited resources. For this purpose, we train a sentiment analysis model
using recurrent neural networks with reviews in English. We then translate
reviews in other languages and reuse this model to evaluate the sentiments.
Experimental results show that our robust approach of single model trained on
English reviews statistically significantly outperforms the baselines in
several different languages."
02417455c05f09d89c2658f39705ac1df1daa0cd,2002.05829,"Based on the cost benchmarks of the HULK energy efficiency benchmark platform, what is the minimal fine-tuning cost for a model using a 4-core TPU v3 priced at $8/hour?","[{'answer': '$1,728', 'type': 'abstractive'}]",2002.05829.pdf,"['2002.05829.pdf', '1910.00458.pdf', '1609.00559.pdf', '2002.01664.pdf', '1911.13066.pdf', '1909.03242.pdf', '1904.09678.pdf', '1606.00189.pdf', '1909.00361.pdf', '1902.09314.pdf', '1805.04033.pdf', '1707.03569.pdf', '1908.06379.pdf', '1809.08298.pdf', '1804.05918.pdf', '1910.12129.pdf', '1901.09755.pdf']","FLOAT SELECTED: Table 1: Pretraining costs of baseline models. Hardware and pretraining time are collected from original papers, with which costs are estimated with current TPU price at $8 per hour with 4 core TPU v3 chips and V100 GPU at $3.06 per hour. DistilBERT model is trained upon a pretrained BERT model. Parameter numbers are estimated using the pretrained models implemented in the Transformers (https://github.com/huggingface/ transformers) library (Wolf et al., 2019), shown in million.",HULK: An Energy Efficiency Benchmark Platform for Responsible Natural Language Processing,"Computation-intensive pretrained models have been taking the lead of many
natural language processing benchmarks such as GLUE. However, energy efficiency
in the process of model training and inference becomes a critical bottleneck.
We introduce HULK, a multi-task energy efficiency benchmarking platform for
responsible natural language processing. With HULK, we compare pretrained
models' energy efficiency from the perspectives of time and cost. Baseline
benchmarking results are provided for further analysis. The fine-tuning
efficiency of different pretrained models can differ a lot among different
tasks and fewer parameter number does not necessarily imply better efficiency.
We analyzed such phenomenon and demonstrate the method of comparing the
multi-task efficiency of pretrained models. Our platform is available at
https://sites.engineering.ucsb.edu/~xiyou/hulk/."
6ce057d3b88addf97a30cb188795806239491154,2002.05829,"What pretrained models are evaluated in the HULK benchmark paper for their pretraining costs, using hardware metrics and current TPU and V100 GPU pricing?","[{'answer': 'BERT, XLNET RoBERTa, ALBERT, DistilBERT', 'type': 'abstractive'}]",2002.05829.pdf,"['2002.05829.pdf', '1804.08050.pdf', '1911.03310.pdf', '2003.06044.pdf', '2002.04181.pdf', '1809.03449.pdf']","FLOAT SELECTED: Table 1: Pretraining costs of baseline models. Hardware and pretraining time are collected from original papers, with which costs are estimated with current TPU price at $8 per hour with 4 core TPU v3 chips and V100 GPU at $3.06 per hour. DistilBERT model is trained upon a pretrained BERT model. Parameter numbers are estimated using the pretrained models implemented in the Transformers (https://github.com/huggingface/ transformers) library (Wolf et al., 2019), shown in million.",HULK: An Energy Efficiency Benchmark Platform for Responsible Natural Language Processing,"Computation-intensive pretrained models have been taking the lead of many
natural language processing benchmarks such as GLUE. However, energy efficiency
in the process of model training and inference becomes a critical bottleneck.
We introduce HULK, a multi-task energy efficiency benchmarking platform for
responsible natural language processing. With HULK, we compare pretrained
models' energy efficiency from the perspectives of time and cost. Baseline
benchmarking results are provided for further analysis. The fine-tuning
efficiency of different pretrained models can differ a lot among different
tasks and fewer parameter number does not necessarily imply better efficiency.
We analyzed such phenomenon and demonstrate the method of comparing the
multi-task efficiency of pretrained models. Our platform is available at
https://sites.engineering.ucsb.edu/~xiyou/hulk/."
3aee5c856e0ee608a7664289ffdd11455d153234,1810.00663,"What were the exact match (EM), generalization metric (GM), edit distance (ED), and F1 scores of the proposed model for both the test-repeated and test-new datasets?","[{'answer': 'For test-repeated set, EM score of 61.17, F1 of 93.54, ED of 0.75 and GM of 61.36. For test-new set, EM score of 41.71, F1 of 91.02, ED of 1.22 and GM of 41.81', 'type': 'abstractive'}]",1810.00663.pdf,"['1810.00663.pdf', '1810.06743.pdf', '2001.05970.pdf', '1911.10049.pdf', '1909.05855.pdf', '1804.07789.pdf', '2002.01359.pdf']","FLOAT SELECTED: Table 3: Performance of different models on the test datasets. EM and GM report percentages, and ED corresponds to average edit distance. The symbol ↑ indicates that higher results are better in the corresponding column; ↓ indicates that lower is better.",Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation,"We propose an end-to-end deep learning model for translating free-form
natural language instructions to a high-level plan for behavioral robot
navigation. We use attention models to connect information from both the user
instructions and a topological representation of the environment. We evaluate
our model's performance on a new dataset containing 10,050 pairs of navigation
instructions. Our model significantly outperforms baseline approaches.
Furthermore, our results suggest that it is possible to leverage the
environment map as a relevant knowledge base to facilitate the translation of
free-form navigational instruction."
8434974090491a3c00eed4f22a878f0b70970713,1902.09314,What is the exact parameter count and memory size (in MB) of the Attentional Encoder Network (AEN) for targeted sentiment classification?,"[{'answer': 'Proposed model has 1.16 million parameters and 11.04 MB.', 'type': 'abstractive'}]",1902.09314.pdf,"['1902.09314.pdf', '1904.09678.pdf', '1605.07333.pdf', '1906.05474.pdf', '1909.04002.pdf', '1810.06743.pdf', '2004.04721.pdf', '1909.00279.pdf', '1910.11235.pdf', '1611.04642.pdf', '1909.02480.pdf', '1911.04952.pdf', '1908.11546.pdf', '1809.01541.pdf', '2004.01878.pdf']",Statistical results are reported in Table TABREF37 .,Attentional Encoder Network for Targeted Sentiment Classification,"Targeted sentiment classification aims at determining the sentimental
tendency towards specific targets. Most of the previous approaches model
context and target words with RNN and attention. However, RNNs are difficult to
parallelize and truncated backpropagation through time brings difficulty in
remembering long-term patterns. To address this issue, this paper proposes an
Attentional Encoder Network (AEN) which eschews recurrence and employs
attention based encoders for the modeling between context and target. We raise
the label unreliability issue and introduce label smoothing regularization. We
also apply pre-trained BERT to this task and obtain new state-of-the-art
results. Experiments and analysis demonstrate the effectiveness and lightweight
of our model."
234ccc1afcae4890e618ff2a7b06fc1e513ea640,1911.05153,"How do data augmentation techniques (es, cs, cs+es) influence adversarial performance improvements in the task-oriented dialog system, specifically for the systems, and by what percentage do adversarial sets (Adv es, Adv cs) improve compared to the baseline?","[{'answer': 'Data augmentation (es)  improved Adv es by 20% comparing to baseline \nData augmentation (cs) improved Adv cs by 16.5% comparing to baseline\nData augmentation (cs+es) improved both Adv cs and Adv es by at least 10% comparing to baseline \nAll models show improvements over adversarial sets  \n', 'type': 'abstractive'}]",1911.05153.pdf,"['1911.05153.pdf', '1804.08050.pdf', '1809.06537.pdf', '2003.03106.pdf', '1904.10500.pdf']","The performance of the base model described in the previous section is shown in the first row of Table TABREF8 for the Nematus cs-en ($\bar{cs}$), FB MT system cs-en (cs) and es-en (es), sequence autoencoder (seq2seq), and the average of the adversarial sets (avg).",Improving Robustness of Task Oriented Dialog Systems,"Task oriented language understanding in dialog systems is often modeled using
intents (task of a query) and slots (parameters for that task). Intent
detection and slot tagging are, in turn, modeled using sentence classification
and word tagging techniques respectively. Similar to adversarial attack
problems with computer vision models discussed in existing literature, these
intent-slot tagging models are often over-sensitive to small variations in
input -- predicting different and often incorrect labels when small changes are
made to a query, thus reducing their accuracy and reliability. However,
evaluating a model's robustness to these changes is harder for language since
words are discrete and an automated change (e.g. adding `noise') to a query
sometimes changes the meaning and thus labels of a query. In this paper, we
first describe how to create an adversarial test set to measure the robustness
of these models. Furthermore, we introduce and adapt adversarial training
methods as well as data augmentation using back-translation to mitigate these
issues. Our experiments show that both techniques improve the robustness of the
system substantially and can be combined to yield the best results."
4d28c99750095763c81bcd5544491a0ba51d9070,1909.04002,"Which 15 celebrities from domains such as politics, business, music, and entertainment provided the dataset of tweets used in the analysis of characterization scores and tweet popularity in the study on identifying a subject's most representative tweets?","[{'answer': 'Amitabh Bachchan, Ariana Grande, Barack Obama, Bill Gates, Donald Trump,\nEllen DeGeneres, J K Rowling, Jimmy Fallon, Justin Bieber, Kevin Durant, Kim Kardashian, Lady Gaga, LeBron James,Narendra Modi, Oprah Winfrey', 'type': 'abstractive'}, {'answer': 'Celebrities from varioius domains - Acting, Music, Politics, Business, TV, Author, Sports, Modeling. ', 'type': 'abstractive'}]",1909.04002.pdf,"['1909.04002.pdf', '1809.10644.pdf', '1610.00879.pdf', '2001.08051.pdf', '1903.09722.pdf', '1911.02086.pdf', '1605.07333.pdf', '2004.01878.pdf', '1912.10806.pdf', '2002.00652.pdf', '1911.02711.pdf', '1710.09340.pdf', '1806.11432.pdf', '1604.00400.pdf', '1910.04269.pdf']","FLOAT SELECTED: Table 1: Twitter celebrities in our dataset, with tweet counts before and after filtering (Foll. denotes followers in millions)",The Trumpiest Trump? Identifying a Subject's Most Characteristic Tweets,"The sequence of documents produced by any given author varies in style and
content, but some documents are more typical or representative of the source
than others. We quantify the extent to which a given short text is
characteristic of a specific person, using a dataset of tweets from fifteen
celebrities. Such analysis is useful for generating excerpts of high-volume
Twitter profiles, and understanding how representativeness relates to tweet
popularity. We first consider the related task of binary author detection (is x
the author of text T?), and report a test accuracy of 90.37% for the best of
five approaches to this problem. We then use these models to compute
characterization scores among all of an author's texts. A user study shows
human evaluators agree with our characterization model for all 15 celebrities
in our dataset, each with p-value < 0.05. We use these classifiers to show
surprisingly strong correlations between characterization scores and the
popularity of the associated texts. Indeed, we demonstrate a statistically
significant correlation between this score and tweet popularity
(likes/replies/retweets) for 13 of the 15 celebrities in our study."
5fda8539a97828e188ba26aad5cda1b9dd642bc8,1910.14537,"What are the F1 scores reported for the proposed model's performance on the MSR and AS datasets, and how do they compare to the baseline models in the closed test evaluation?","[{'answer': 'F1 score of 97.5 on MSR and 95.7 on AS', 'type': 'abstractive'}, {'answer': 'MSR: 97.7 compared to 97.5 of baseline\nAS: 95.7 compared to 95.6 of baseline', 'type': 'abstractive'}]",1910.14537.pdf,"['1910.14537.pdf', '1905.11901.pdf', '1606.05320.pdf', '1809.05752.pdf', '1908.11546.pdf', '1603.00968.pdf', '1611.04798.pdf', '1905.00563.pdf', '2002.11910.pdf', '1712.03547.pdf', '1909.01013.pdf', '2003.04866.pdf', '1809.03449.pdf', '1703.06492.pdf', '2003.05377.pdf', '2002.02070.pdf', '1909.00015.pdf', '1909.08824.pdf']","FLOAT SELECTED: Table 5: Results on PKU and MSR compared with previous models in closed test. The asterisks indicate the result of model with unsupervised label from (Wang et al., 2019).",Attention Is All You Need for Chinese Word Segmentation,"Taking greedy decoding algorithm as it should be, this work focuses on
further strengthening the model itself for Chinese word segmentation (CWS),
which results in an even more fast and more accurate CWS model. Our model
consists of an attention only stacked encoder and a light enough decoder for
the greedy segmentation plus two highway connections for smoother training, in
which the encoder is composed of a newly proposed Transformer variant,
Gaussian-masked Directional (GD) Transformer, and a biaffine attention scorer.
With the effective encoder design, our model only needs to take unigram
features for scoring. Our model is evaluated on SIGHAN Bakeoff benchmark
datasets. The experimental results show that with the highest segmentation
speed, the proposed model achieves new state-of-the-art or comparable
performance against strong baselines in terms of strict closed test setting."
fabcd71644bb63559d34b38d78f6ef87c256d475,1910.14537,What are the baseline models that the proposed CWS model is compared against in the closed test setting?,"[{'answer': 'Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019', 'type': 'abstractive'}]",1910.14537.pdf,"['1910.14537.pdf', '1810.03459.pdf', '1912.06670.pdf', '1905.12260.pdf', '1911.13066.pdf', '2003.04642.pdf']",Tables TABREF25 and TABREF26 reports the performance of recent models and ours in terms of closed test setting.,Attention Is All You Need for Chinese Word Segmentation,"Taking greedy decoding algorithm as it should be, this work focuses on
further strengthening the model itself for Chinese word segmentation (CWS),
which results in an even more fast and more accurate CWS model. Our model
consists of an attention only stacked encoder and a light enough decoder for
the greedy segmentation plus two highway connections for smoother training, in
which the encoder is composed of a newly proposed Transformer variant,
Gaussian-masked Directional (GD) Transformer, and a biaffine attention scorer.
With the effective encoder design, our model only needs to take unigram
features for scoring. Our model is evaluated on SIGHAN Bakeoff benchmark
datasets. The experimental results show that with the highest segmentation
speed, the proposed model achieves new state-of-the-art or comparable
performance against strong baselines in terms of strict closed test setting."
e82fa03f1638a8c59ceb62bb9a6b41b498950e1f,1908.07245,"What types of systems are compared in the GlossBERT paper for fine-grained English all-words WSD, including the baseline, knowledge-based, traditional supervised, and neural-based approaches evaluated in the Raganato et al. (2017b) framework?","[{'answer': 'Two knowledge-based systems,\ntwo traditional word expert supervised systems, six recent neural-based systems, and one BERT feature-based system.', 'type': 'abstractive'}]",1908.07245.pdf,"['1908.07245.pdf', '1604.00400.pdf', '1808.09029.pdf', '1906.10225.pdf', '1703.06492.pdf', '1810.05241.pdf', '2003.03014.pdf', '1811.01088.pdf']","FLOAT SELECTED: Table 3: F1-score (%) for fine-grained English all-words WSD on the test sets in the framework of Raganato et al. (2017b) (including the development set SE07). Bold font indicates best systems. The five blocks list the MFS baseline, two knowledge-based systems, two traditional word expert supervised systems, six recent neural-based systems and our systems, respectively. Results in first three blocks come from Raganato et al. (2017b), and others from the corresponding papers.",GlossBERT: BERT for Word Sense Disambiguation with Gloss Knowledge,"Word Sense Disambiguation (WSD) aims to find the exact sense of an ambiguous
word in a particular context. Traditional supervised methods rarely take into
consideration the lexical resources like WordNet, which are widely utilized in
knowledge-based methods. Recent studies have shown the effectiveness of
incorporating gloss (sense definition) into neural networks for WSD. However,
compared with traditional word expert supervised methods, they have not
achieved much improvement. In this paper, we focus on how to better leverage
gloss knowledge in a supervised neural WSD system. We construct context-gloss
pairs and propose three BERT-based models for WSD. We fine-tune the pre-trained
BERT model on SemCor3.0 training corpus and the experimental results on several
English all-words WSD benchmark datasets show that our approach outperforms the
state-of-the-art systems."
babe72f0491e65beff0e5889380e8e32d7a81f78,1902.00672,"How does the TL-TranSum model, incorporating sentence length and hyperedge coverage, perform compared to the MMR (Maximal Marginal Relevance) baseline in terms of redundancy and relevance on the DUC benchmark datasets?","[{'answer': ' Moreover, our TL-TranSum method also outperforms other approaches such as MaxCover ( $5\\%$ ) and MRMR ( $7\\%$ )', 'type': 'extractive'}]",1902.00672.pdf,"['1902.00672.pdf', '1909.08089.pdf', '2002.00652.pdf', '1909.04002.pdf', '1909.09587.pdf', '2001.10161.pdf', '1901.09755.pdf', '1909.00105.pdf', '1704.06194.pdf', '1809.09194.pdf', '1903.09722.pdf', '1909.00578.pdf', '1910.04269.pdf']","While long sentences are expected to cover more themes and induce a larger increase in the total weight of covered hyperedges, the division by the node weights (i.e. the sentence lengths) balances this tendency and allows the inclusion of short sentences as well. In contrast, the methods of sentence selection based on a maximal relevance and a minimal redundancy such as, for instance, the maximal marginal relevance approach of BIBREF31 , tend to favor the selection of long sentences only.",Query-oriented text summarization based on hypergraph transversals,"Existing graph- and hypergraph-based algorithms for document summarization
represent the sentences of a corpus as the nodes of a graph or a hypergraph in
which the edges represent relationships of lexical similarities between
sentences. Each sentence of the corpus is then scored individually, using
popular node ranking algorithms, and a summary is produced by extracting highly
scored sentences. This approach fails to select a subset of jointly relevant
sentences and it may produce redundant summaries that are missing important
topics of the corpus. To alleviate this issue, a new hypergraph-based
summarizer is proposed in this paper, in which each node is a sentence and each
hyperedge is a theme, namely a group of sentences sharing a topic. Themes are
weighted in terms of their prominence in the corpus and their relevance to a
user-defined query. It is further shown that the problem of identifying a
subset of sentences covering the relevant themes of the corpus is equivalent to
that of finding a hypergraph transversal in our theme-based hypergraph. Two
extensions of the notion of hypergraph transversal are proposed for the purpose
of summarization, and polynomial time algorithms building on the theory of
submodular functions are proposed for solving the associated discrete
optimization problems. The worst-case time complexity of the proposed
algorithms is squared in the number of terms, which makes it cheaper than the
existing hypergraph-based methods. A thorough comparative analysis with related
models on DUC benchmark datasets demonstrates the effectiveness of our
approach, which outperforms existing graph- or hypergraph-based methods by at
least 6% of ROUGE-SU4 score."
761de1610e934189850e8fda707dc5239dd58092,1907.0306,"Which baseline models and advanced methods, including PBSMT, NMT, and their variations, did the authors evaluate in addressing the low-resource Japanese--Russian translation problem prior to introducing their back-translation and multi-directional approaches?","[{'answer': 'pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17', 'type': 'extractive'}, {'answer': 'M2M Transformer', 'type': 'abstractive'}]",1907.03060.pdf,"['1907.03060.pdf', '1909.11687.pdf', '1707.05236.pdf', '1911.08962.pdf', '1906.05474.pdf', '1704.00939.pdf', '1808.09920.pdf', '1705.00108.pdf', '1908.11047.pdf', '1909.07734.pdf', '1906.10551.pdf', '1909.00175.pdf', '1909.08089.pdf', '1909.05855.pdf', '1908.06264.pdf', '1609.00559.pdf']","We began with evaluating standard MT paradigms, i.e., PBSMT BIBREF3 and NMT BIBREF1 . As for PBSMT, we also examined two advanced methods: pivot-based translation relying on a helping language BIBREF10 and induction of phrase tables from monolingual data BIBREF14 .

As for NMT, we compared two types of encoder-decoder architectures: attentional RNN-based model (RNMT) BIBREF2 and the Transformer model BIBREF18 . In addition to standard uni-directional modeling, to cope with the low-resource problem, we examined two multi-directional models: bi-directional model BIBREF11 and multi-to-multi (M2M) model BIBREF8 .

After identifying the best model, we also examined the usefulness of a data augmentation method based on back-translation BIBREF17 .",,
37edc25e39515ffc2d92115d2fcd9e6ceb18898b,1707.03569,What baseline models are listed comparing the performance of different data representations in fine-grained sentiment classification against the multitask learning approach?,"[{'answer': 'SVMs, LR, BIBREF2', 'type': 'extractive'}, {'answer': 'SVM INLINEFORM0, SVM INLINEFORM1, LR INLINEFORM2, MaxEnt', 'type': 'extractive'}]",1707.03569.pdf,"['1707.03569.pdf', '1905.00563.pdf', '2001.00137.pdf', '1711.00106.pdf', '2003.03044.pdf', '1608.06757.pdf', '2002.06644.pdf', '2003.12738.pdf', '1709.05413.pdf', '1908.06151.pdf', '1703.02507.pdf', '1902.00330.pdf', '1901.05280.pdf', '1910.07481.pdf', '1911.08673.pdf', '1812.01704.pdf', '1909.11687.pdf', '1910.06036.pdf']","Experimental results Table TABREF9 illustrates the performance of the models for the different data representations. The upper part of the Table summarizes the performance of the baselines. The entry “Balikas et al.” stands for the winning system of the 2016 edition of the challenge BIBREF2 , which to the best of our knowledge holds the state-of-the-art.",Multitask Learning for Fine-Grained Twitter Sentiment Analysis,"Traditional sentiment analysis approaches tackle problems like ternary
(3-category) and fine-grained (5-category) classification by learning the tasks
separately. We argue that such classification tasks are correlated and we
propose a multitask approach based on a recurrent neural network that benefits
by jointly learning them. Our study demonstrates the potential of multitask
models on this type of problems and improves the state-of-the-art results in
the fine-grained sentiment classification problem."
e431661f17347607c3d3d9764928385a8f3d9650,1707.03569,What was the reported MAE reduction achieved by the multitask biLSTM model for fine-grained sentiment classification in comparison to state-of-the-art methods?,"[{'answer': 'They decrease MAE in 0.34', 'type': 'abstractive'}]",1707.03569.pdf,"['1707.03569.pdf', '1901.04899.pdf', '1912.08960.pdf', '1908.07195.pdf', '1810.10254.pdf', '1906.10551.pdf', '1911.05153.pdf', '1909.09484.pdf', '2004.01980.pdf', '1809.00530.pdf', '1910.11204.pdf', '1904.05584.pdf']","In this work, we use an extended version of LSTM called bidirectional LSTM (biLSTM)",Multitask Learning for Fine-Grained Twitter Sentiment Analysis,"Traditional sentiment analysis approaches tackle problems like ternary
(3-category) and fine-grained (5-category) classification by learning the tasks
separately. We argue that such classification tasks are correlated and we
propose a multitask approach based on a recurrent neural network that benefits
by jointly learning them. Our study demonstrates the potential of multitask
models on this type of problems and improves the state-of-the-art results in
the fine-grained sentiment classification problem."
dafa760e1466e9eaa73ad8cb39b229abd5babbda,1809.08298,What is the total number of run-on and non-run-on sentences mentioned in the paper on using machine learning models for run-on sentence correction?,"[{'answer': '4.756 million sentences', 'type': 'abstractive'}]",1809.08298.pdf,"['1809.08298.pdf', '1910.03467.pdf', '1906.03538.pdf', '2002.02070.pdf', '2002.01984.pdf', '1904.10503.pdf', '1907.03060.pdf', '1912.13109.pdf', '1707.08559.pdf', '1701.09123.pdf']",FLOAT SELECTED: Table 3: Number of run-on (RO) and non-run-on (Non-RO) sentences in our datasets.,How do you correct run-on sentences it's not as easy as it seems,"Run-on sentences are common grammatical mistakes but little research has
tackled this problem to date. This work introduces two machine learning models
to correct run-on sentences that outperform leading methods for related tasks,
punctuation restoration and whole-sentence grammatical error correction. Due to
the limited annotated data for this error, we experiment with artificially
generating training data from clean newswire text. Our findings suggest
artificial training data is viable for this task. We discuss implications for
correcting run-ons and other types of mistakes that have low coverage in
error-annotated corpora."
2d536961c6e1aec9f8491e41e383dc0aac700e0a,1912.01673,"What are the 15 modification types detailed in the annotation instructions for sentence transformations in the Czech-based COSTRA 1.0 dataset, which are used to illustrate various sentence-level changes?","[{'answer': '- paraphrase 1\n- paraphrase 2\n- different meaning\n- opposite meaning\n- nonsense\n- minimal change\n- generalization\n- gossip\n- formal sentence\n- non-standard sentence\n- simple sentence\n- possibility\n- ban\n- future\n- past', 'type': 'abstractive'}]",1912.01673.pdf,"['1912.01673.pdf', '1901.05280.pdf', '1912.10435.pdf', '2002.06675.pdf', '1911.01799.pdf', '1909.00430.pdf', '1603.00968.pdf', '1901.08079.pdf', '1810.12885.pdf', '2001.05970.pdf', '1901.03866.pdf', '1801.05147.pdf', '1808.03430.pdf', '1611.00514.pdf', '1912.08960.pdf']",We selected 15 modifications types to collect COSTRA 1.0. They are presented in annotationinstructions.,COSTRA 1.0: A Dataset of Complex Sentence Transformations,"We present COSTRA 1.0, a dataset of complex sentence transformations. The
dataset is intended for the study of sentence-level embeddings beyond simple
word alternations or standard paraphrasing. This first version of the dataset
is limited to sentences in Czech but the construction method is universal and
we plan to use it also for other languages. The dataset consist of 4,262 unique
sentences with average length of 10 words, illustrating 15 types of
modifications such as simplification, generalization, or formal and informal
language variation. The hope is that with this dataset, we should be able to
test semantic properties of sentence embeddings and perhaps even to find some
topologically interesting 'skeleton' in the sentence embedding space. A
preliminary analysis using LASER, multi-purpose multi-lingual sentence
embeddings suggests that the LASER space does not exhibit the desired
properties."
70e9210fe64f8d71334e5107732d764332a81cb1,1812.06864,"What architecture is reported as state-of-the-art on WSJ, combining HMM with convolutional, recurrent, and fully connected layers?","[{'answer': 'CNN-DNN-BLSTM-HMM', 'type': 'abstractive'}, {'answer': 'HMM-based system', 'type': 'extractive'}]",1812.06864.pdf,"['1812.06864.pdf', '1605.07333.pdf', '1709.10217.pdf', '1904.07904.pdf', '1908.06379.pdf']","Table TABREF11 shows Word Error Rates (WER) on WSJ for the current state-of-the-art and our models. The current best model trained on this dataset is an HMM-based system which uses a combination of convolutional, recurrent and fully connected layers, as well as speaker adaptation, and reaches INLINEFORM0 WER on nov92.",Fully Convolutional Speech Recognition,"Current state-of-the-art speech recognition systems build on recurrent neural
networks for acoustic and/or language modeling, and rely on feature extraction
pipelines to extract mel-filterbanks or cepstral coefficients. In this paper we
present an alternative approach based solely on convolutional neural networks,
leveraging recent advances in acoustic models from the raw waveform and
language modeling. This fully convolutional approach is trained end-to-end to
predict characters from the raw waveform, removing the feature extraction step
altogether. An external convolutional language model is used to decode words.
On Wall Street Journal, our model matches the current state-of-the-art. On
Librispeech, we report state-of-the-art performance among end-to-end models,
including Deep Speech 2 trained with 12 times more acoustic data and
significantly more linguistic data."
8e2b125426d1220691cceaeaf1875f76a6049cbd,1909.08824,"What are the absolute improvements in BLEU scores for xIntent, xReact, and oReact on both the Event2Mind and Atomic datasets using the context-aware variational autoencoder model, as reported in comparison to previous state-of-the-art methods?","[{'answer': 'ON Event2Mind, the accuracy of proposed method is improved by  absolute BLUE  2.9,  10.87, 1.79 for xIntent, xReact and oReact respectively.\nOn Atomic dataset, the accuracy of proposed method is improved by  absolute BLUE 3.95.   4.11, 4.49 for xIntent, xReact and oReact.respectively.', 'type': 'abstractive'}]",1909.08824.pdf,"['1909.08824.pdf', '1911.13066.pdf', '1910.12129.pdf', '1911.00069.pdf', '1804.08139.pdf', '1909.05855.pdf', '2004.01980.pdf']","Further, we employ BLEU score to evaluate the accuracy of generations BIBREF21, and the number of distinct n-gram to evaluate the diversity of generations BIBREF6. ",Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder,"Understanding event and event-centered commonsense reasoning are crucial for
natural language processing (NLP). Given an observed event, it is trivial for
human to infer its intents and effects, while this type of If-Then reasoning
still remains challenging for NLP systems. To facilitate this, a If-Then
commonsense reasoning dataset Atomic is proposed, together with an RNN-based
Seq2Seq model to conduct such reasoning. However, two fundamental problems
still need to be addressed: first, the intents of an event may be multiple,
while the generations of RNN-based Seq2Seq models are always semantically
close; second, external knowledge of the event background may be necessary for
understanding events and conducting the If-Then reasoning. To address these
issues, we propose a novel context-aware variational autoencoder effectively
learning event background information to guide the If-Then reasoning.
Experimental results show that our approach improves the accuracy and diversity
of inferences compared with state-of-the-art baseline methods."
42bc4e0cd0f3e238a4891142f1b84ebcd6594bf1,1909.08824,"Which baseline models, including RNN-based Seq2Seq and CWVAE-Unpretrained, are compared with the proposed context-aware variational autoencoder on the Atomic dataset for If-Then reasoning?","[{'answer': 'RNN-based Seq2Seq, Variational Seq2Seq, VRNMT , CWVAE-Unpretrained', 'type': 'extractive'}]",1909.08824.pdf,"['1909.08824.pdf', '1804.07789.pdf', '1908.06379.pdf', '1611.03382.pdf', '1909.00430.pdf', '1809.01202.pdf', '1804.08139.pdf', '1811.12254.pdf', '1909.13375.pdf', '1909.00279.pdf', '2001.05493.pdf', '1707.08559.pdf']","We compared our proposed model with the following four baseline methods:

RNN-based Seq2Seq proposed by BIBREF4 (BIBREF4) for the If-Then reasoning on Atomic.

Variational Seq2Seq combines a latent variable with the encoder-decoder structure through converting the last hidden state of RNN encoder into a Gaussian distributed latent variable BIBREF8.

VRNMT Propose by BIBREF19 (BIBREF19), VRNMT combines CVAE with attention-based encoder-decoder framework through introduces a latent variable to model the semantic distribution of targets.

CWVAE-Unpretrained refers to the CWVAE model without the pretrain stage.

Note that, for each baseline method, we train distinct models for each distinct inference dimension, respectively.",Modeling Event Background for If-Then Commonsense Reasoning Using Context-aware Variational Autoencoder,"Understanding event and event-centered commonsense reasoning are crucial for
natural language processing (NLP). Given an observed event, it is trivial for
human to infer its intents and effects, while this type of If-Then reasoning
still remains challenging for NLP systems. To facilitate this, a If-Then
commonsense reasoning dataset Atomic is proposed, together with an RNN-based
Seq2Seq model to conduct such reasoning. However, two fundamental problems
still need to be addressed: first, the intents of an event may be multiple,
while the generations of RNN-based Seq2Seq models are always semantically
close; second, external knowledge of the event background may be necessary for
understanding events and conducting the If-Then reasoning. To address these
issues, we propose a novel context-aware variational autoencoder effectively
learning event background information to guide the If-Then reasoning.
Experimental results show that our approach improves the accuracy and diversity
of inferences compared with state-of-the-art baseline methods."
097ab15f58cb1fce5b5ffb5082b8d7bbee720659,1902.10525,"Based on the results, which language shows the smallest reduction in character error rate when applying all components of the system?","[{'answer': 'thai', 'type': 'abstractive'}]",1902.10525.pdf,"['1902.10525.pdf', '1908.06267.pdf', '1809.02279.pdf', '1711.00106.pdf', '1809.04960.pdf', '2003.07996.pdf', '1810.05241.pdf', '1703.07090.pdf', '1803.09230.pdf', '2002.06424.pdf', '2003.03106.pdf', '1912.13109.pdf', '1911.02086.pdf', '1908.07195.pdf', '2001.06888.pdf', '1705.01214.pdf', '2003.08385.pdf', '1901.05280.pdf']","FLOAT SELECTED: Table 9 Character error rates on the validation data using successively more of the system components described above for English (en), Spanish (es), German (de), Arabic (ar), Korean (ko), Thai (th), Hindi (hi), and Chinese (zh) along with the respective number of items and characters in the test sets. Average latencies for all languages and models were computed on an Intel Xeon E5-2690 CPU running at 2.6GHz.",Fast Multi-language LSTM-based Online Handwriting Recognition,"We describe an online handwriting system that is able to support 102
languages using a deep neural network architecture. This new system has
completely replaced our previous Segment-and-Decode-based system and reduced
the error rate by 20%-40% relative for most languages. Further, we report new
state-of-the-art results on IAM-OnDB for both the open and closed dataset
setting. The system combines methods from sequence recognition with a new input
encoding using B\'ezier curves. This leads to up to 10x faster recognition
times compared to our previous system. Through a series of experiments we
determine the optimal configuration of our models and report the results of our
setup on a number of additional public datasets."
e2f269997f5a01949733c2ec8169f126dabd7571,1804.00079,"What are the specific datasets from which sentence pairs were drawn for each task in the multi-task learning framework, according to the paper?","[{'answer': '- En-Fr (WMT14)\n- En-De (WMT15)\n- Skipthought (BookCorpus)\n- AllNLI (SNLI + MultiNLI)\n- Parsing (PTB + 1-billion word)', 'type': 'abstractive'}]",1804.00079.pdf,"['1804.00079.pdf', '1912.10435.pdf', '2002.01207.pdf', '1908.06151.pdf', '1902.09393.pdf', '1910.14497.pdf', '1906.11180.pdf', '1905.12260.pdf', '1909.03135.pdf', '2002.05829.pdf', '1911.07555.pdf', '2002.05058.pdf', '1903.09588.pdf', '1805.03710.pdf']",FLOAT SELECTED: Table 1: An approximate number of sentence pairs for each task.,Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning,"A lot of the recent success in natural language processing (NLP) has been
driven by distributed vector representations of words trained on large amounts
of text in an unsupervised manner. These representations are typically used as
general purpose features for words across a range of NLP problems. However,
extending this success to learning representations of sequences of words, such
as sentences, remains an open problem. Recent work has explored unsupervised as
well as supervised learning techniques with different training objectives to
learn general purpose fixed-length sentence representations. In this work, we
present a simple, effective multi-task learning framework for sentence
representations that combines the inductive biases of diverse training
objectives in a single model. We train this model on several data sources with
multiple training objectives on over 100 million sentences. Extensive
experiments demonstrate that sharing a single recurrent sentence encoder across
weakly related tasks leads to consistent improvements over previous methods. We
present substantial improvements in the context of transfer learning and
low-resource settings using our learned general-purpose representations."
79f9468e011670993fd162543d1a4b3dd811ac5d,1908.07195,"Based on the automatic evaluation results, how does ARAML improve in reverse perplexity, Self-BLEU, grammaticality, and relevance compared to baseline methods on the COCO and EMNLP2017 WMT datasets?","[{'answer': 'ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.', 'type': 'abstractive'}, {'answer': 'Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively.', 'type': 'abstractive'}]",1908.07195.pdf,"['1908.07195.pdf', '1912.01772.pdf', '1908.06267.pdf', '1910.02339.pdf', '1908.06151.pdf', '1908.05828.pdf', '1701.06538.pdf', '1805.03710.pdf']",FLOAT SELECTED: Table 4: Automatic evaluation on COCO and EMNLP2017 WMT. Each metric is presented with mean and standard deviation.,ARAML: A Stable Adversarial Training Framework for Text Generation,"Most of the existing generative adversarial networks (GAN) for text
generation suffer from the instability of reinforcement learning training
algorithms such as policy gradient, leading to unstable performance. To tackle
this problem, we propose a novel framework called Adversarial Reward Augmented
Maximum Likelihood (ARAML). During adversarial training, the discriminator
assigns rewards to samples which are acquired from a stationary distribution
near the data rather than the generator's distribution. The generator is
optimized with maximum likelihood estimation augmented by the discriminator's
rewards instead of policy gradient. Experiments show that our model can
outperform state-of-the-art text GANs with a more stable training process."
9c44df7503720709eac933a15569e5761b378046,1805.0371,"Which language's subword segmentations, including examples like ""〈hell, o, o, o〉"" and ""〈louis, ana〉,"" are used to generate word embeddings based on cosine distance in the paper?","[{'answer': 'English', 'type': 'abstractive'}]",1805.03710.pdf,"['1805.03710.pdf', '1910.11769.pdf', '1706.08032.pdf', '1705.01265.pdf', '1910.14497.pdf', '1909.00694.pdf', '1709.10367.pdf', '1707.03569.pdf', '1909.03242.pdf', '1707.00110.pdf', '1809.02286.pdf', '1907.03060.pdf', '1906.11180.pdf']","FLOAT SELECTED: Table 2: We generate vectors for OOV using subword information and search for the nearest (cosine distance) words in the embedding space. The LV-M segmentation for each word is: {〈hell, o, o, o〉}, {〈marvel, i, cious〉}, {〈louis, ana〉}, {〈re, re, read〉}, {〈 tu, z, read〉}. We omit the LV-N and FT n-grams as they are trivial and too numerous to list.",,
35b3ce3a7499070e9b280f52e2cb0c29b0745380,2003.08385,"Does the paper list the macro-average F1 scores for the 'favor' and 'against' classes in a zero-shot cross-lingual stance detection setting involving German, French, and Italian languages?","[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]",2003.08385.pdf,"['2003.08385.pdf', '1605.08675.pdf', '1909.03405.pdf', '1908.06267.pdf', '1902.00672.pdf', '1908.07195.pdf', '2002.06675.pdf', '1909.01383.pdf', '2003.03014.pdf']","FLOAT SELECTED: Table 3: Baseline scores in the cross-lingual setting. No Italian samples were seen during training, making this a case of zero-shot cross-lingual transfer. The scores are reported as the macro-average of the F1scores for ‘favor’ and for ‘against’.",X-Stance: A Multilingual Multi-Target Dataset for Stance Detection,"We extract a large-scale stance detection dataset from comments written by
candidates of elections in Switzerland. The dataset consists of German, French
and Italian text, allowing for a cross-lingual evaluation of stance detection.
It contains 67 000 comments on more than 150 political issues (targets). Unlike
stance detection models that have specific target issues, we use the dataset to
train a single model on all the issues. To make learning across targets
possible, we prepend to each instance a natural question that represents the
target (e.g. ""Do you support X?""). Baseline results from multilingual BERT show
that zero-shot cross-lingual and cross-target transfer of stance detection is
moderately successful with this approach."
71ba1b09bb03f5977d790d91702481cc406b3767,2003.08385,What F1 macro score and accuracy did M-BERT achieve on the supervised stance detection benchmarks in the X-Stance paper?,"[{'answer': 'M-Bert had 76.6 F1 macro score.', 'type': 'abstractive'}, {'answer': '75.1% and 75.6% accuracy', 'type': 'abstractive'}]",2003.08385.pdf,"['2003.08385.pdf', '2002.10361.pdf', '1809.01541.pdf', '1708.09609.pdf', '1910.00912.pdf', '1701.03214.pdf', '1810.12085.pdf', '1810.12196.pdf', '1912.01772.pdf', '2001.08868.pdf', '2003.04642.pdf', '1709.05413.pdf', '1910.03467.pdf', '2002.02070.pdf', '1808.03430.pdf']",FLOAT SELECTED: Table 6: Performance of BERT-like models on different supervised stance detection benchmarks.,X-Stance: A Multilingual Multi-Target Dataset for Stance Detection,"We extract a large-scale stance detection dataset from comments written by
candidates of elections in Switzerland. The dataset consists of German, French
and Italian text, allowing for a cross-lingual evaluation of stance detection.
It contains 67 000 comments on more than 150 political issues (targets). Unlike
stance detection models that have specific target issues, we use the dataset to
train a single model on all the issues. To make learning across targets
possible, we prepend to each instance a natural question that represents the
target (e.g. ""Do you support X?""). Baseline results from multilingual BERT show
that zero-shot cross-lingual and cross-target transfer of stance detection is
moderately successful with this approach."
bd40f33452da7711b65faaa248aca359b27fddb6,2003.08385,What was the F1 macro score that multilingual BERT achieved on the Swiss election comments dataset for multilingual multi-target stance detection detailed in the X-Stance paper?,"[{'answer': 'BERT had 76.6 F1 macro score on x-stance dataset.', 'type': 'abstractive'}]",2003.08385.pdf,"['2003.08385.pdf', '2002.01359.pdf', '2003.06044.pdf', '1910.12795.pdf', '1912.10011.pdf', '1904.01608.pdf', '1804.08050.pdf', '1804.11346.pdf', '1805.03710.pdf', '2001.06888.pdf', '1806.04330.pdf']",To put the supervised score into context we list scores that variants of Bert have achieved on other stance detection datasets in Table TABREF46.,X-Stance: A Multilingual Multi-Target Dataset for Stance Detection,"We extract a large-scale stance detection dataset from comments written by
candidates of elections in Switzerland. The dataset consists of German, French
and Italian text, allowing for a cross-lingual evaluation of stance detection.
It contains 67 000 comments on more than 150 political issues (targets). Unlike
stance detection models that have specific target issues, we use the dataset to
train a single model on all the issues. To make learning across targets
possible, we prepend to each instance a natural question that represents the
target (e.g. ""Do you support X?""). Baseline results from multilingual BERT show
that zero-shot cross-lingual and cross-target transfer of stance detection is
moderately successful with this approach."
3288a50701a80303fd71c8c5ede81cbee14fa2c7,1908.11365,Does the model with DS-Init and MAtt have fewer parameters than the baseline Transformer for the WMT14 En-De translation task?,"[{'answer': 'No', 'type': 'boolean'}]",1908.11365.pdf,"['1908.11365.pdf', '1603.00968.pdf', '1808.09920.pdf', '1910.14497.pdf', '1910.06592.pdf', '1901.09755.pdf']",FLOAT SELECTED: Table 3: Tokenized case-sensitive BLEU (in parentheses: sacreBLEU) on WMT14 En-De translation task. #Param: number of model parameters. 4Dec: decoding time (seconds)/speedup on newstest2014 dataset with a batch size of 32. 4Train: training time (seconds)/speedup per training step evaluated on 0.5K steps with a batch size of 1K target tokens. Time is averaged over 3 runs using Tensorflow on a single TITAN X (Pascal). “-”: optimization failed and no result. “?”: the same as model 1©. † and ‡: comparison against 11© and 14© respectively rather than 1©. Base: the baseline Transformer with base setting. Bold indicates best BLEU score. dpa and dpr: dropout rate on attention weights and residual connection. bs: batch size in tokens.,Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention,"The general trend in NLP is towards increasing model capacity and performance
via deeper neural networks. However, simply stacking more layers of the popular
Transformer architecture for machine translation results in poor convergence
and high computational overhead. Our empirical analysis suggests that
convergence is poor due to gradient vanishing caused by the interaction between
residual connections and layer normalization. We propose depth-scaled
initialization (DS-Init), which decreases parameter variance at the
initialization stage, and reduces output variance of residual connections so as
to ease gradient back-propagation through normalization layers. To address
computational cost, we propose a merged attention sublayer (MAtt) which
combines a simplified averagebased self-attention sublayer and the
encoderdecoder attention sublayer on the decoder side. Results on WMT and IWSLT
translation tasks with five translation directions show that deep Transformers
with DS-Init and MAtt can substantially outperform their base counterpart in
terms of BLEU (+1.1 BLEU on average for 12-layer models), while matching the
decoding speed of the baseline model thanks to the efficiency improvements of
MAtt."
96c09ece36a992762860cde4c110f1653c110d96,1709.10217,"Based on the evaluation results, what were the highest F1 scores for Task 1 and the top performance metrics for Task 2, including Ratio, Satisfaction, Fluency, Turns, and Guide, in the iFLYTEK Corporation's evaluation of Chinese human-computer dialogue technology?","[{'answer': 'For task 1 best F1 score was 0.9391 on closed and 0.9414 on open test.\nFor task2 best result had: Ratio 0.3175 , Satisfaction 64.53, Fluency 0, Turns -1 and Guide 2', 'type': 'abstractive'}]",1709.10217.pdf,"['1709.10217.pdf', '1909.08041.pdf', '1909.00754.pdf', '1909.02480.pdf', '1909.00015.pdf', '1909.03135.pdf']",Table TABREF14 and TABREF15 show the evaluation results of the closed test and open test of the task 1 respectively.,The First Evaluation of Chinese Human-Computer Dialogue Technology,"In this paper, we introduce the first evaluation of Chinese human-computer
dialogue technology. We detail the evaluation scheme, tasks, metrics and how to
collect and annotate the data for training, developing and test. The evaluation
includes two tasks, namely user intent classification and online testing of
task-oriented dialogue. To consider the different sources of the data for
training and developing, the first task can also be divided into two sub tasks.
Both the two tasks are coming from the real problems when using the
applications developed by industry. The evaluation data is provided by the
iFLYTEK Corporation. Meanwhile, in this paper, we publish the evaluation
results to present the current performance of the participants in the two tasks
of Chinese human-computer dialogue technology. Moreover, we analyze the
existing problems of human-computer dialogue as well as the evaluation scheme
itself."
a5b67470a1c4779877f0d8b7724879bbb0a3b313,1705.00108,What is the micro-averaged evaluation metric officially reported in the results for assessing the performance of the semi-supervised bidirectional language model on named entity recognition and chunking tasks?,"[{'answer': 'micro-averaged F1', 'type': 'abstractive'}]",1705.00108.pdf,"['1705.00108.pdf', '1909.03405.pdf', '1812.06864.pdf', '1809.05752.pdf', '2004.03744.pdf', '2001.05493.pdf', '1908.11365.pdf', '1709.10217.pdf', '1911.05153.pdf']",We report the official evaluation metric (micro-averaged INLINEFORM0 ). ,Semi-supervised sequence tagging with bidirectional language models,"Pre-trained word embeddings learned from unlabeled text have become a
standard component of neural network architectures for NLP tasks. However, in
most cases, the recurrent network that operates on word-level representations
to produce context sensitive representations is trained on relatively little
labeled data. In this paper, we demonstrate a general semi-supervised approach
for adding pre- trained context embeddings from bidirectional language models
to NLP systems and apply it to sequence labeling tasks. We evaluate our model
on two standard datasets for named entity recognition (NER) and chunking, and
in both cases achieve state of the art results, surpassing previous systems
that use other forms of transfer or joint learning with additional labeled data
and task specific gazetteers."
4640793d82aa7db30ad7b88c0bf0a1030e636558,1705.00108,"In the comparison of sequence tagging models without incorporating additional labeled data or task-specific gazetteers, which systems are listed in the semi-supervised sequence tagging with bidirectional language models paper?","[{'answer': 'Chiu and Nichols (2016), Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), Hashimoto et al. (2016), Søgaard and Goldberg (2016) ', 'type': 'abstractive'}]",1705.00108.pdf,"['1705.00108.pdf', '1908.11546.pdf', '1911.02086.pdf', '2002.08899.pdf', '1609.00559.pdf', '1902.09314.pdf', '1909.01013.pdf', '1904.10500.pdf', '1701.00185.pdf', '1905.10810.pdf', '1810.06743.pdf', '1809.09795.pdf', '1805.04033.pdf', '1911.01799.pdf', '1804.08139.pdf']",Tables TABREF15 and TABREF16 compare results from TagLM with previously published state of the art results without additional labeled data or task specific gazetteers. Tables TABREF17 and TABREF18 compare results of TagLM to other systems that include additional labeled data or gazetteers. ,Semi-supervised sequence tagging with bidirectional language models,"Pre-trained word embeddings learned from unlabeled text have become a
standard component of neural network architectures for NLP tasks. However, in
most cases, the recurrent network that operates on word-level representations
to produce context sensitive representations is trained on relatively little
labeled data. In this paper, we demonstrate a general semi-supervised approach
for adding pre- trained context embeddings from bidirectional language models
to NLP systems and apply it to sequence labeling tasks. We evaluate our model
on two standard datasets for named entity recognition (NER) and chunking, and
in both cases achieve state of the art results, surpassing previous systems
that use other forms of transfer or joint learning with additional labeled data
and task specific gazetteers."
4547818a3bbb727c4bb4a76554b5a5a7b5c5fedb,1905.11901,"What were the different German-English training data sizes, including subsets of the IWSLT14 corpus, used in the TED shared translation task experiments, and how was the development set split in this study?","[{'answer': 'Training data with 159000, 80000, 40000, 20000, 10000 and 5000 sentences, and 7584 sentences for development', 'type': 'abstractive'}, {'answer': 'ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words)', 'type': 'extractive'}]",1905.11901.pdf,"['1905.11901.pdf', '2003.07723.pdf', '1705.01214.pdf', '1909.11687.pdf', '1911.05153.pdf', '1707.05236.pdf', '1810.05241.pdf', '1611.04642.pdf', '1809.09795.pdf']","We use the TED data from the IWSLT 2014 German INLINEFORM0 English shared translation task BIBREF38 . We use the same data cleanup and train/dev split as BIBREF39 , resulting in 159000 parallel sentences of training data, and 7584 for development.",Revisiting Low-Resource Neural Machine Translation: A Case Study,"It has been shown that the performance of neural machine translation (NMT)
drops starkly in low-resource conditions, underperforming phrase-based
statistical machine translation (PBSMT) and requiring large amounts of
auxiliary data to achieve competitive results. In this paper, we re-assess the
validity of these results, arguing that they are the result of lack of system
adaptation to low-resource settings. We discuss some pitfalls to be aware of
when training low-resource NMT systems, and recent techniques that have shown
to be especially helpful in low-resource settings, resulting in a set of best
practices for low-resource NMT. In our experiments on German--English with
different amounts of IWSLT14 training data, we show that, without the use of
any auxiliary monolingual or multilingual data, an optimized NMT system can
outperform PBSMT with far less data than previously claimed. We also apply
these techniques to a low-resource Korean-English dataset, surpassing
previously reported results by 4 BLEU."
9d578ddccc27dd849244d632dd0f6bf27348ad81,1909.00694,"What are the accuracy results of the BiGRU and BERT models on the ACP test set when trained with different configurations, including the full dataset and subsets, in the paper titled ""Minimally Supervised Learning of Affective Events Using Discourse Relations""?","[{'answer': 'Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.', 'type': 'abstractive'}]",1909.00694.pdf,"['1909.00694.pdf', '1912.03457.pdf', '1911.08976.pdf', '1908.11365.pdf', '1912.13109.pdf', '1910.06036.pdf', '1905.00563.pdf']",FLOAT SELECTED: Table 3: Performance of various models on the ACP test set.,Minimally Supervised Learning of Affective Events Using Discourse Relations,"Recognizing affective events that trigger positive or negative sentiment has
a wide range of natural language processing applications but remains a
challenging problem mainly because the polarity of an event is not necessarily
predictable from its constituent words. In this paper, we propose to propagate
affective polarity using discourse relations. Our method is simple and only
requires a very small seed lexicon and a large raw corpus. Our experiments
using Japanese data show that our method learns affective events effectively
without manually labeled data. It also improves supervised learning results
when labeled data are small."
44c4bd6decc86f1091b5fc0728873d9324cdde4e,1909.00694,How many affective event pairs were extracted from both the Japanese web corpus and the ACP corpus in your minimally supervised learning of affective events via discourse relations?,"[{'answer': '7000000 pairs of events were extracted from the Japanese Web corpus, 529850 pairs of events were extracted from the ACP corpus', 'type': 'abstractive'}, {'answer': 'The ACP corpus has around 700k events split into positive and negative polarity ', 'type': 'abstractive'}]",1909.00694.pdf,"['1909.00694.pdf', '1904.05584.pdf', '2004.03744.pdf', '1908.07195.pdf', '1811.02906.pdf', '1909.11467.pdf', '1611.00514.pdf', '1710.09340.pdf', '1710.06700.pdf', '1912.01673.pdf', '1604.00400.pdf', '2002.02492.pdf']","As a raw corpus, we used a Japanese web corpus that was compiled through the procedures proposed by BIBREF13. ",Minimally Supervised Learning of Affective Events Using Discourse Relations,"Recognizing affective events that trigger positive or negative sentiment has
a wide range of natural language processing applications but remains a
challenging problem mainly because the polarity of an event is not necessarily
predictable from its constituent words. In this paper, we propose to propagate
affective polarity using discourse relations. Our method is simple and only
requires a very small seed lexicon and a large raw corpus. Our experiments
using Japanese data show that our method learns affective events effectively
without manually labeled data. It also improves supervised learning results
when labeled data are small."
c029deb7f99756d2669abad0a349d917428e9c12,1909.00694,"What is the percentage improvement in BERT's performance when using the minimally supervised method that propagates affective polarity through discourse relations, when trained on a small set of labeled Japanese data compared to the baseline supervised approach?","[{'answer': '3%', 'type': 'abstractive'}]",1909.00694.pdf,"['1909.00694.pdf', '1704.00939.pdf', '1909.01247.pdf', '2003.03106.pdf', '1912.03457.pdf', '2002.06644.pdf', '1909.00015.pdf', '1805.04033.pdf', '1806.07711.pdf', '2001.08051.pdf', '1809.09795.pdf', '1705.00108.pdf', '1703.06492.pdf', '1704.08960.pdf']","FLOAT SELECTED: Table 4: Results for small labeled training data. Given the performance with the full dataset, we show BERT trained only with the AL data.",Minimally Supervised Learning of Affective Events Using Discourse Relations,"Recognizing affective events that trigger positive or negative sentiment has
a wide range of natural language processing applications but remains a
challenging problem mainly because the polarity of an event is not necessarily
predictable from its constituent words. In this paper, we propose to propagate
affective polarity using discourse relations. Our method is simple and only
requires a very small seed lexicon and a large raw corpus. Our experiments
using Japanese data show that our method learns affective events effectively
without manually labeled data. It also improves supervised learning results
when labeled data are small."
efb3a87845460655c53bd7365bcb8393c99358ec,1706.08032,"What were the accuracy results reported for the STS, Sanders, and HCR datasets when evaluated using the proposed DeepCNN-BiLSTM model for binary sentiment classification?","[{'answer': 'accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR', 'type': 'abstractive'}]",1706.08032.pdf,"['1706.08032.pdf', '1911.02711.pdf', '1611.02550.pdf', '2003.11563.pdf', '1701.02877.pdf', '1809.09194.pdf', '1911.02086.pdf', '2003.05377.pdf']",FLOAT SELECTED: Table IV ACCURACY OF DIFFERENT MODELS FOR BINARY CLASSIFICATION,A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking,"This paper introduces a novel deep learning framework including a
lexicon-based approach for sentence-level prediction of sentiment label
distribution. We propose to first apply semantic rules and then use a Deep
Convolutional Neural Network (DeepCNN) for character-level embeddings in order
to increase information for word-level embedding. After that, a Bidirectional
Long Short-Term Memory Network (Bi-LSTM) produces a sentence-wide feature
representation from the word-level embedding. We evaluate our approach on three
Twitter sentiment classification datasets. Experimental results show that our
model can improve the classification accuracy of sentence-level sentiment
analysis in Twitter social networking."
d60a3887a0d434abc0861637bbcd9ad0c596caf4,1706.08032,"Which semantic rules, as referenced in the Twitter sentiment classification model, are excluded from the main method because they are used to compute polarity after POS tagging or parsing steps?","[{'answer': 'rules that compute polarity of words after POS tagging or parsing steps', 'type': 'abstractive'}]",1706.08032.pdf,"['1706.08032.pdf', '1909.01958.pdf', '1909.08824.pdf', '1701.09123.pdf', '1702.03342.pdf', '1908.07816.pdf', '2004.03354.pdf', '1912.06670.pdf', '1607.06025.pdf', '1704.08960.pdf', '1810.12885.pdf', '2003.03106.pdf', '2003.01769.pdf', '1704.06194.pdf', '1910.00825.pdf', '1902.00672.pdf', '2002.08899.pdf', '1908.05828.pdf']","In Twitter social networking, people express their opinions containing sub-sentences. These sub-sentences using specific PoS particles (Conjunction and Conjunctive adverbs), like ""but, while, however, despite, however"" have different polarities. However, the overall sentiment of tweets often focus on certain sub-sentences. For example:

@lonedog bwahahah...you are amazing! However, it was quite the letdown.

@kirstiealley my dentist is great but she's expensive...=(

In two tweets above, the overall sentiment is negative. However, the main sentiment is only in the sub-sentences following but and however. This inspires a processing step to remove unessential parts in a tweet. Rule-based approach can assists these problems in handling negation and dealing with specific PoS particles led to effectively affect the final output of classification BIBREF11 BIBREF16 . BIBREF11 summarized a full presentation of their semantic rules approach and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after using the rules. Table TABREF16 illustrates the number of processed sentences on each dataset.",A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking,"This paper introduces a novel deep learning framework including a
lexicon-based approach for sentence-level prediction of sentiment label
distribution. We propose to first apply semantic rules and then use a Deep
Convolutional Neural Network (DeepCNN) for character-level embeddings in order
to increase information for word-level embedding. After that, a Bidirectional
Long Short-Term Memory Network (Bi-LSTM) produces a sentence-wide feature
representation from the word-level embedding. We evaluate our approach on three
Twitter sentiment classification datasets. Experimental results show that our
model can improve the classification accuracy of sentence-level sentiment
analysis in Twitter social networking."
4f243056e63a74d1349488983dc1238228ca76a7,2003.12218,Does the paper provide a complete list of all 75 fine-grained entity types annotated within the CORD-NER dataset?,"[{'answer': 'No', 'type': 'boolean'}]",2003.12218.pdf,"['2003.12218.pdf', '1703.07090.pdf', '1911.00069.pdf', '1909.00361.pdf']",FLOAT SELECTED: Table 2: Examples of the most frequent entities annotated in CORD-NER.,Comprehensive Named Entity Recognition on CORD-19 with Distant or Weak Supervision,"We created this CORD-NER dataset with comprehensive named entity recognition
(NER) on the COVID-19 Open Research Dataset Challenge (CORD-19) corpus
(2020-03-13). This CORD-NER dataset covers 75 fine-grained entity types: In
addition to the common biomedical entity types (e.g., genes, chemicals and
diseases), it covers many new entity types related explicitly to the COVID-19
studies (e.g., coronaviruses, viral proteins, evolution, materials, substrates
and immune responses), which may benefit research on COVID-19 related virus,
spreading mechanisms, and potential vaccines. CORD-NER annotation is a
combination of four sources with different NER methods. The quality of CORD-NER
annotation surpasses SciSpacy (over 10% higher on the F1 score based on a
sample set of documents), a fully supervised BioNER tool. Moreover, CORD-NER
supports incrementally adding new documents as well as adding new entity types
when needed by adding dozens of seeds as the input examples. We will constantly
update CORD-NER based on the incremental updates of the CORD-19 corpus and the
improvement of our system."
891c2001d6baaaf0da4e65b647402acac621a7d2,1909.00512,"What method is used in the paper to compute static embeddings from the contextualized word representations across different layers of ELMo, BERT, and GPT-2?","[{'answer': ""They use the first principal component of a word's contextualized representation in a given layer as its static embedding."", 'type': 'abstractive'}, {'answer': ' by taking the first principal component (PC) of its contextualized representations in a given layer', 'type': 'extractive'}]",1909.00512.pdf,"['1909.00512.pdf', '2004.03788.pdf', '1701.06538.pdf', '2001.08051.pdf', '1911.07228.pdf', '2003.12218.pdf', '1711.00106.pdf', '1701.09123.pdf', '1908.06267.pdf', '1910.14537.pdf', '1906.10551.pdf', '2001.05970.pdf', '1902.09314.pdf', '1906.01081.pdf', '2001.00137.pdf']","FLOAT SELECTED: Table 1: The performance of various static embeddings on word embedding benchmark tasks. The best result for each task is in bold. For the contextualizing models (ELMo, BERT, GPT-2), we use the first principal component of a word’s contextualized representations in a given layer as its static embedding. The static embeddings created using ELMo and BERT’s contextualized representations often outperform GloVe and FastText vectors.","How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings","Replacing static word embeddings with contextualized word representations has
yielded significant improvements on many NLP tasks. However, just how
contextual are the contextualized representations produced by models such as
ELMo and BERT? Are there infinitely many context-specific representations for
each word, or are words essentially assigned one of a finite number of
word-sense representations? For one, we find that the contextualized
representations of all words are not isotropic in any layer of the
contextualizing model. While representations of the same word in different
contexts still have a greater cosine similarity than those of two different
words, this self-similarity is much lower in upper layers. This suggests that
upper layers of contextualizing models produce more context-specific
representations, much like how upper layers of LSTMs produce more task-specific
representations. In all layers of ELMo, BERT, and GPT-2, on average, less than
5% of the variance in a word's contextualized representations can be explained
by a static embedding for that word, providing some justification for the
success of contextualized representations."
3d6015d722de6e6297ba7bfe7cb0f8a67f660636,1909.11467,"What are the 12 educational subjects listed in the Kurdish Textbooks Corpus (KTC) paper, specifically related to the Sorani dialect's K-12 textbooks, with 693,800 tokens categorized into these subjects?","[{'answer': 'Economics, Genocide, Geography, History, Human Rights, Kurdish, Kurdology, Philosophy, Physics, Theology, Sociology, Social Study', 'type': 'abstractive'}]",1909.11467.pdf,"['1909.11467.pdf', '1909.03135.pdf', '1909.09484.pdf', '1805.03710.pdf', '1606.05320.pdf']","FLOAT SELECTED: Table 1: Statistics of the corpus - In the Course Level column, (i) represents Institute2 .",Developing a Fine-Grained Corpus for a Less-resourced Language: the case of Kurdish,"Kurdish is a less-resourced language consisting of different dialects written
in various scripts. Approximately 30 million people in different countries
speak the language. The lack of corpora is one of the main obstacles in Kurdish
language processing. In this paper, we present KTC-the Kurdish Textbooks
Corpus, which is composed of 31 K-12 textbooks in Sorani dialect. The corpus is
normalized and categorized into 12 educational subjects containing 693,800
tokens (110,297 types). Our resource is publicly available for non-commercial
use under the CC BY-NC-SA 4.0 license."
b8cee4782e05afaeb9647efdb8858554490feba5,1709.05413,Are all Twitter customer service conversations used in the evaluation datasets focused exclusively on English?,"[{'answer': 'Yes', 'type': 'boolean'}]",1709.05413.pdf,"['1709.05413.pdf', '2002.04181.pdf', '1909.02480.pdf', '1910.12129.pdf', '1909.01013.pdf', '1908.07195.pdf', '1807.07961.pdf', '1701.05574.pdf', '1908.05828.pdf', '1701.09123.pdf', '2002.02070.pdf', '1904.03288.pdf', '2003.04642.pdf', '1810.12085.pdf', '1910.11769.pdf', '1912.03457.pdf', '1809.02286.pdf', '2001.00137.pdf']",FLOAT SELECTED: Table 1: Example Twitter Customer Service Conversation,"""How May I Help You?"": Modeling Twitter Customer Service Conversations Using Fine-Grained Dialogue Acts","Given the increasing popularity of customer service dialogue on Twitter,
analysis of conversation data is essential to understand trends in customer and
agent behavior for the purpose of automating customer service interactions. In
this work, we develop a novel taxonomy of fine-grained ""dialogue acts""
frequently observed in customer service, showcasing acts that are more suited
to the domain than the more generic existing taxonomies. Using a sequential
SVM-HMM model, we model conversation flow, predicting the dialogue act of a
given turn in real-time. We characterize differences between customer and agent
behavior in Twitter customer service conversations, and investigate the effect
of testing our system on different customer service industries. Finally, we use
a data-driven approach to predict important conversation outcomes: customer
satisfaction, customer frustration, and overall problem resolution. We show
that the type and location of certain dialogue acts in a conversation have a
significant effect on the probability of desirable and undesirable outcomes,
and present actionable rules based on our findings. The patterns and rules we
derive can be used as guidelines for outcome-driven automated customer service
platforms."
8a0a51382d186e8d92bf7e78277a1d48958758da,1908.11546,"How does the gCAS model's performance in terms of Entity F1 and Success F1 compare across the movie, taxi, and restaurant domains, and in which domains does it particularly excel?","[{'answer': 'For entity  F1 in the movie, taxi and restaurant domain it results in scores of 50.86, 64, and 60.35. For success, it results it outperforms in the movie and restaurant domain with scores of 77.95 and 71.52', 'type': 'abstractive'}]",1908.11546.pdf,"['1908.11546.pdf', '1908.11365.pdf', '1702.03342.pdf', '1705.01265.pdf', '1707.08559.pdf']",FLOAT SELECTED: Table 5: Entity F1 and Success F1 at dialogue level.,Modeling Multi-Action Policy for Task-Oriented Dialogues,"Dialogue management (DM) plays a key role in the quality of the interaction
with the user in a task-oriented dialogue system. In most existing approaches,
the agent predicts only one DM policy action per turn. This significantly
limits the expressive power of the conversational agent and introduces unwanted
turns of interactions that may challenge users' patience. Longer conversations
also lead to more errors and the system needs to be more robust to handle them.
In this paper, we compare the performance of several models on the task of
predicting multiple acts for each turn. A novel policy model is proposed based
on a recurrent cell called gated Continue-Act-Slots (gCAS) that overcomes the
limitations of the existing models. Experimental results show that gCAS
outperforms other approaches. The code is available at
https://leishu02.github.io/"
c1c611409b5659a1fd4a870b6cc41f042e2e9889,1711.11221,"What specific evaluation metrics, such as BLEU scores and coherence-related measures, were used to assess the proposed dynamic and topic cache models on the NIST Chinese-English translation tasks?","[{'answer': 'BLEU scores, exact matches of words in both translations and topic cache, and cosine similarities of adjacent sentences for coherence.', 'type': 'abstractive'}]",1711.11221.pdf,"['1711.11221.pdf', '1911.00069.pdf', '1603.00968.pdf', '1908.11365.pdf', '1712.05999.pdf', '1811.12254.pdf', '1909.00512.pdf', '2003.08385.pdf', '1809.01541.pdf', '1812.01704.pdf', '1906.10225.pdf', '1702.03342.pdf', '1612.08205.pdf', '1809.04960.pdf', '1809.02286.pdf']","FLOAT SELECTED: Table 1: Experiment results on the NIST Chinese-English translation tasks. [+Cd] is the proposed model with the dynamic cache. [+Cd,Ct] is the proposed model with both the dynamic and topic cache. The BLEU scores are case-insensitive. Avg means the average BLEU score on all test sets.",Modeling Coherence for Neural Machine Translation with Dynamic and Topic Caches,"Sentences in a well-formed text are connected to each other via various links
to form the cohesive structure of the text. Current neural machine translation
(NMT) systems translate a text in a conventional sentence-by-sentence fashion,
ignoring such cross-sentence links and dependencies. This may lead to generate
an incoherent target text for a coherent source text. In order to handle this
issue, we propose a cache-based approach to modeling coherence for neural
machine translation by capturing contextual information either from recently
translated sentences or the entire document. Particularly, we explore two types
of caches: a dynamic cache, which stores words from the best translation
hypotheses of preceding sentences, and a topic cache, which maintains a set of
target-side topical words that are semantically related to the document to be
translated. On this basis, we build a new layer to score target words in these
two caches with a cache-based neural model. Here the estimated probabilities
from the cache-based neural model are combined with NMT probabilities into the
final word prediction probabilities via a gating mechanism. Finally, the
proposed cache-based neural model is trained jointly with NMT system in an
end-to-end manner. Experiments and analysis presented in this paper demonstrate
that the proposed cache-based model achieves substantial improvements over
several state-of-the-art SMT and NMT baselines."
6dcbe941a3b0d5193f950acbdc574f1cfb007845,1909.05855,"What are the 17 domains, including the one excluded from training, in the Schema-Guided Dialogue Dataset paper?","[{'answer': 'Alarm\nBank\nBus\nCalendar\nEvent\nFlight\nHome\nHotel\nMedia\nMovie\nMusic\nRentalCar\nRestaurant\nRideShare\nService\nTravel\nWeather', 'type': 'abstractive'}]",1909.05855.pdf,"['1909.05855.pdf', '1806.04511.pdf', '1909.07734.pdf', '1911.01799.pdf', '1809.09795.pdf', '1909.08859.pdf', '1906.01081.pdf', '1801.05147.pdf', '1605.08675.pdf', '1606.00189.pdf', '1911.08673.pdf', '1901.08079.pdf', '1910.12795.pdf', '1603.07044.pdf', '1809.05752.pdf', '1911.07228.pdf', '1811.01088.pdf']",The 17 domains (`Alarm' domain not included in training) present in our dataset are listed in Table TABREF5. We create synthetic implementations of a total of 34 services or APIs over these domains. ,Towards Scalable Multi-domain Conversational Agents: The Schema-Guided Dialogue Dataset,"Virtual assistants such as Google Assistant, Alexa and Siri provide a
conversational interface to a large number of services and APIs spanning
multiple domains. Such systems need to support an ever-increasing number of
services with possibly overlapping functionality. Furthermore, some of these
services have little to no training data available. Existing public datasets
for task-oriented dialogue do not sufficiently capture these challenges since
they cover few domains and assume a single static ontology per domain. In this
work, we introduce the the Schema-Guided Dialogue (SGD) dataset, containing
over 16k multi-domain conversations spanning 16 domains. Our dataset exceeds
the existing task-oriented dialogue corpora in scale, while also highlighting
the challenges associated with building large-scale virtual assistants. It
provides a challenging testbed for a number of tasks including language
understanding, slot filling, dialogue state tracking and response generation.
Along the same lines, we present a schema-guided paradigm for task-oriented
dialogue, in which predictions are made over a dynamic set of intents and
slots, provided as input, using their natural language descriptions. This
allows a single dialogue system to easily support a large number of services
and facilitates simple integration of new services without requiring additional
training data. Building upon the proposed paradigm, we release a model for
dialogue state tracking capable of zero-shot generalization to new APIs, while
remaining competitive in the regular setting."
75df70ce7aa714ec4c6456d0c51f82a16227f2cb,2002.01664,What are the 7 Indian languages listed in the paper that implements Ghost-VLAD pooling for language identification experiments?,"[{'answer': 'Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam', 'type': 'abstractive'}, {'answer': 'Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)', 'type': 'abstractive'}]",2002.01664.pdf,"['2002.01664.pdf', '1909.13714.pdf', '1908.07195.pdf', '1901.08079.pdf', '1909.13695.pdf', '1803.09230.pdf', '1909.08089.pdf', '2004.03744.pdf', '1603.00968.pdf', '1901.04899.pdf', '2002.01984.pdf', '1903.00172.pdf']",FLOAT SELECTED: Table 1: Dataset,Identification of Indian Languages using Ghost-VLAD pooling,"In this work, we propose a new pooling strategy for language identification
by considering Indian languages. The idea is to obtain utterance level features
for any variable length audio for robust language recognition. We use the
GhostVLAD approach to generate an utterance level feature vector for any
variable length input audio by aggregating the local frame level features
across time. The generated feature vector is shown to have very good language
discriminative features and helps in getting state of the art results for
language identification task. We conduct our experiments on 635Hrs of audio
data for 7 Indian languages. Our method outperforms the previous state of the
art x-vector [11] method by an absolute improvement of 1.88% in F1-score and
achieves 98.43% F1-score on the held-out test data. We compare our system with
various pooling approaches and show that GhostVLAD is the best pooling approach
for this task. We also provide visualization of the utterance level embeddings
generated using Ghost-VLAD pooling and show that this method creates embeddings
which has very good language discriminative features."
0fd678d24c86122b9ab27b73ef20216bbd9847d1,1804.08139,What accuracy metrics are presented to assess the performance of the proposed multi-task learning model on each of the 16 text classification tasks and as an overall average across all tasks?,"[{'answer': 'Accuracy on each dataset and the average accuracy on all datasets.', 'type': 'abstractive'}]",1804.08139.pdf,"['1804.08139.pdf', '1801.05147.pdf', '1712.03556.pdf', '1809.09194.pdf', '1912.00864.pdf', '2002.08899.pdf', '1909.09587.pdf', '1907.09369.pdf', '2001.08868.pdf', '1905.07464.pdf']",Table TABREF34 shows the performances of the different methods.,"Same Representation, Different Attentions: Shareable Sentence Representation Learning from Multiple Tasks","Distributed representation plays an important role in deep learning based
natural language processing. However, the representation of a sentence often
varies in different tasks, which is usually learned from scratch and suffers
from the limited amounts of training data. In this paper, we claim that a good
sentence representation should be invariant and can benefit the various
subsequent tasks. To achieve this purpose, we propose a new scheme of
information sharing for multi-task learning. More specifically, all tasks share
the same sentence representation and each task can select the task-specific
information from the shared sentence representation with attention mechanism.
The query vector of each task's attention could be either static parameters or
generated dynamically. We conduct extensive experiments on 16 different text
classification tasks, which demonstrate the benefits of our architecture."
9555aa8de322396a16a07a5423e6a79dcd76816a,1611.03382,What is the percentage improvement of the model presented over the state-of-the-art systems on the Gigaword dataset in terms of both Rouge-1 and Rouge-L scores?,"[{'answer': 'w.r.t Rouge-1 their model outperforms by 0.98% and w.r.t Rouge-L their model outperforms by 0.45%', 'type': 'abstractive'}]",1611.03382.pdf,"['1611.03382.pdf', '1912.06670.pdf', '1901.02262.pdf', '2002.02070.pdf', '1909.03544.pdf', '2001.05970.pdf', '1806.04330.pdf', '2002.00652.pdf', '1909.08859.pdf', '1701.02877.pdf', '1910.06592.pdf']","As shown in Table TABREF35 , our method outperforms all previous methods on Rouge-1 and Rouge-L, and is comparable on Rouge-2.",Efficient Summarization with Read-Again and Copy Mechanism,"Encoder-decoder models have been widely used to solve sequence to sequence
prediction tasks. However current approaches suffer from two shortcomings.
First, the encoders compute a representation of each word taking into account
only the history of the words it has read so far, yielding suboptimal
representations. Second, current decoders utilize large vocabularies in order
to minimize the problem of unknown words, resulting in slow decoding times. In
this paper we address both shortcomings. Towards this goal, we first introduce
a simple mechanism that first reads the input sequence before committing to a
representation of each word. Furthermore, we propose a simple copy mechanism
that is able to exploit very small vocabularies and handle out-of-vocabulary
words. We demonstrate the effectiveness of our approach on the Gigaword dataset
and DUC competition outperforming the state-of-the-art."
bc9c31b3ce8126d1d148b1025c66f270581fde10,1905.00563,What are the specific knowledge graphs that were used to evaluate the performance and robustness of the link prediction models in this study?,"[{'answer': ' Kinship and Nations knowledge graphs, YAGO3-10 and WN18KGs knowledge graphs ', 'type': 'abstractive'}, {'answer': 'WN18 and YAGO3-10', 'type': 'extractive'}]",1905.00563.pdf,"['1905.00563.pdf', '1812.06864.pdf', '1901.05280.pdf', '2003.11645.pdf', '2002.05058.pdf']",FLOAT SELECTED: Table 2: Data Statistics of the benchmarks.,Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications,"Representing entities and relations in an embedding space is a well-studied
approach for machine learning on relational data. Existing approaches, however,
primarily focus on improving accuracy and overlook other aspects such as
robustness and interpretability. In this paper, we propose adversarial
modifications for link prediction models: identifying the fact to add into or
remove from the knowledge graph that changes the prediction for a target fact
after the model is retrained. Using these single modifications of the graph, we
identify the most influential fact for a predicted link and evaluate the
sensitivity of the model to the addition of fake facts. We introduce an
efficient approach to estimate the effect of such modifications by
approximating the change in the embeddings when the knowledge graph changes. To
avoid the combinatorial search over all possible facts, we train a network to
decode embeddings to their corresponding graph components, allowing the use of
gradient-based optimization to identify the adversarial modification. We use
these techniques to evaluate the robustness of link prediction models (by
measuring sensitivity to additional facts), study interpretability through the
facts most responsible for predictions (by identifying the most influential
neighbors), and detect incorrect facts in the knowledge base."
93b299acfb6fad104b9ebf4d0585d42de4047051,1901.09755,"Which specific datasets, including those from the ABSA restaurants domain for languages like English, Spanish, French, and Russian, are detailed with token counts, target counts, and multiword targets?","[{'answer': 'ABSA SemEval 2014-2016 datasets\nYelp Academic Dataset\nWikipedia dumps', 'type': 'abstractive'}]",1901.09755.pdf,"['1901.09755.pdf', '1809.01541.pdf', '1810.12085.pdf', '1707.08559.pdf']","Table TABREF7 shows the ABSA datasets from the restaurants domain for English, Spanish, French, Dutch, Russian and Turkish. From left to right each row displays the number of tokens, number of targets and the number of multiword targets for each training and test set. For English, it should be noted that the size of the 2015 set is less than half with respect to the 2014 dataset in terms of tokens, and only one third in number of targets. The French, Spanish and Dutch datasets are quite similar in terms of tokens although the number of targets in the Dutch dataset is comparatively smaller, possibly due to the tendency to construct compound terms in that language. The Russian dataset is the largest whereas the Turkish set is by far the smallest one.",Language Independent Sequence Labelling for Opinion Target Extraction,"In this research note we present a language independent system to model
Opinion Target Extraction (OTE) as a sequence labelling task. The system
consists of a combination of clustering features implemented on top of a simple
set of shallow local features. Experiments on the well known Aspect Based
Sentiment Analysis (ABSA) benchmarks show that our approach is very competitive
across languages, obtaining best results for six languages in seven different
datasets. Furthermore, the results provide further insights into the behaviour
of clustering features for sequence labelling tasks. The system and models
generated in this work are available for public use and to facilitate
reproducibility of results."
1f63ccc379f01ecdccaa02ed0912970610c84b72,1711.00106,"Based on the paper’s ablation study, how much improvement is achieved in exact match (EM) and F1 scores when using the mixed objective, compared to only using cross-entropy, on the SQuAD development set?","[{'answer': 'The mixed objective improves EM by 2.5% and F1 by 2.2%', 'type': 'abstractive'}]",1711.00106.pdf,"['1711.00106.pdf', '1610.00879.pdf', '1707.00110.pdf', '1905.12260.pdf', '1811.02906.pdf', '1812.01704.pdf', '1909.01247.pdf', '2003.07723.pdf']",FLOAT SELECTED: Table 2: Ablation study on the development set of SQuAD.,DCN+: Mixed Objective and Deep Residual Coattention for Question Answering,"Traditional models for question answering optimize using cross entropy loss,
which encourages exact answers at the cost of penalizing nearby or overlapping
answers that are sometimes equally accurate. We propose a mixed objective that
combines cross entropy loss with self-critical policy learning. The objective
uses rewards derived from word overlap to solve the misalignment between
evaluation metric and optimization objective. In addition to the mixed
objective, we improve dynamic coattention networks (DCN) with a deep residual
coattention encoder that is inspired by recent work in deep self-attention and
residual networks. Our proposals improve model performance across question
types and input lengths, especially for long questions that requires the
ability to capture long-term dependencies. On the Stanford Question Answering
Dataset, our model achieves state-of-the-art results with 75.1% exact match
accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy
and 86.0% F1."
1bb7eb5c3d029d95d1abf9f2892c1ec7b6eef306,1703.0709,"What are the character error rates (CER) for the 9-layer, 2-layer regular-trained, and 2-layer distilled LSTM models as reported in the paper's experiments on Shenma voice search data?","[{'answer': '2.49% for  layer-wise training, 2.63% for distillation, 6.26% for transfer learning.', 'type': 'abstractive'}, {'answer': 'Their best model achieved a 2.49% Character Error Rate.', 'type': 'abstractive'}]",1703.07090.pdf,"['1703.07090.pdf', '1909.00015.pdf', '1909.01013.pdf', '2003.06044.pdf', '1704.00939.pdf', '1909.09587.pdf', '1909.00512.pdf', '1811.12254.pdf', '1605.07683.pdf', '1910.08987.pdf', '1912.06670.pdf', '1611.04798.pdf']","FLOAT SELECTED: Table 3. The CER and RTF of 9-layers, 2-layers regular-trained and 2-laryers distilled LSTM.",,
c0af8b7bf52dc15e0b33704822c4a34077e09cd1,1703.0709,What were the different layer configurations of the unidirectional LSTM models specifically evaluated for real-time streaming speech recognition in the study?,"[{'answer': 'Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers.', 'type': 'abstractive'}]",1703.07090.pdf,"['1703.07090.pdf', '1904.01608.pdf', '1909.00175.pdf', '2002.01359.pdf']","Unidirectional LSTM network is applied, rather than bidirectional one, because it is well suited to real-time streaming speech recognition.",,
a3a871ca2417b2ada9df1438d282c45e4b4ad668,2003.06044,How do previous methods perform on the Switchboard Dialogue Act and DailyDialog datasets compared to the proposed hierarchical attention-based model’s reported accuracies of 80.34% and 85.81%?,"[{'answer': 'Table TABREF20 , Table TABREF22, Table TABREF23', 'type': 'extractive'}]",2003.06044.pdf,"['2003.06044.pdf', '1910.10288.pdf', '1801.05147.pdf', '1810.10254.pdf', '2004.01878.pdf', '1909.08824.pdf', '1810.03459.pdf', '1902.09314.pdf', '1908.06083.pdf', '1707.05236.pdf', '2002.08899.pdf', '1811.02906.pdf', '1912.01673.pdf', '1908.06151.pdf', '2002.11402.pdf', '1811.01088.pdf', '1802.06024.pdf']",We evaluate the performance of our model on two high-quality datasets: Switchboard Dialogue Act Corpus (SwDA) BIBREF4 and DailyDialog BIBREF24. ,Local Contextual Attention with Hierarchical Structure for Dialogue Act Recognition,"Dialogue act recognition is a fundamental task for an intelligent dialogue
system. Previous work models the whole dialog to predict dialog acts, which may
bring the noise from unrelated sentences. In this work, we design a
hierarchical model based on self-attention to capture intra-sentence and
inter-sentence information. We revise the attention distribution to focus on
the local and contextual semantic information by incorporating the relative
position information between utterances. Based on the found that the length of
dialog affects the performance, we introduce a new dialog segmentation
mechanism to analyze the effect of dialog length and context padding length
under online and offline settings. The experiment shows that our method
achieves promising performance on two datasets: Switchboard Dialogue Act and
DailyDialog with the accuracy of 80.34\% and 85.81\% respectively.
Visualization of the attention weights shows that our method can learn the
context dependency between utterances explicitly."
0fcac64544842dd06d14151df8c72fc6de5d695c,2003.06044,Which baseline methods are compared against the proposed hierarchical self-attention-based model for the Switchboard Dialogue Act (SwDA) dataset?,"[{'answer': 'BLSTM+Attention+BLSTM\nHierarchical BLSTM-CRF\nCRF-ASN\nHierarchical CNN (window 4)\nmLSTM-RNN\nDRLM-Conditional\nLSTM-Softmax\nRCNN\nCNN\nCRF\nLSTM\nBERT', 'type': 'abstractive'}]",2003.06044.pdf,"['2003.06044.pdf', '1711.11221.pdf', '1704.00939.pdf', '1910.03467.pdf', '2003.08385.pdf', '1812.01704.pdf', '1608.06757.pdf', '1809.10644.pdf', '1911.02086.pdf', '1909.01247.pdf']",FLOAT SELECTED: Table 4: Comparison results with the previous approaches and our approaches on SwDA dataset.,Local Contextual Attention with Hierarchical Structure for Dialogue Act Recognition,"Dialogue act recognition is a fundamental task for an intelligent dialogue
system. Previous work models the whole dialog to predict dialog acts, which may
bring the noise from unrelated sentences. In this work, we design a
hierarchical model based on self-attention to capture intra-sentence and
inter-sentence information. We revise the attention distribution to focus on
the local and contextual semantic information by incorporating the relative
position information between utterances. Based on the found that the length of
dialog affects the performance, we introduce a new dialog segmentation
mechanism to analyze the effect of dialog length and context padding length
under online and offline settings. The experiment shows that our method
achieves promising performance on two datasets: Switchboard Dialogue Act and
DailyDialog with the accuracy of 80.34\% and 85.81\% respectively.
Visualization of the attention weights shows that our method can learn the
context dependency between utterances explicitly."
a7510ec34eaec2c7ac2869962b69cc41031221e5,1909.0927,"What F1 score is reported for the Bengali NER corpus annotated by non-speakers using noisy and incomplete data, as highlighted by the authors' improvement over the previous state of the art?","[{'answer': '52.0%', 'type': 'abstractive'}]",1909.09270.pdf,"['1909.09270.pdf', '2002.05058.pdf', '1706.08032.pdf', '1703.07090.pdf', '2001.10161.pdf']",FLOAT SELECTED: Table 5: Bengali manual annotation results. Our methods improve on state of the art scores by over 5 points F1 given a relatively small amount of noisy and incomplete annotations from non-speakers.,,
13d92cbc2c77134626e26166c64ca5c00aec0bf5,1909.08041,"Which baseline systems and corresponding authors are compared against in the evaluation on both the HotpotQA and FEVER datasets in the ""Revealing the Importance of Semantic Retrieval for Machine Reading at Scale"" paper?","[{'answer': 'HotspotQA: Yang, Ding, Muppet\nFever: Hanselowski, Yoneda, Nie', 'type': 'abstractive'}]",1909.08041.pdf,"['1909.08041.pdf', '1912.10435.pdf', '1701.05574.pdf', '2002.10361.pdf', '1701.02877.pdf', '1710.09340.pdf', '1611.02550.pdf', '1901.08079.pdf', '1704.08960.pdf', '1909.07734.pdf', '1609.00559.pdf', '1909.04002.pdf', '2002.05058.pdf', '1910.11769.pdf', '1909.11687.pdf']","We chose the best system based on the dev set, and used that for submitting private test predictions on both FEVER and HotpotQA .",Revealing the Importance of Semantic Retrieval for Machine Reading at Scale,"Machine Reading at Scale (MRS) is a challenging task in which a system is
given an input query and is asked to produce a precise output by ""reading""
information from a large knowledge base. The task has gained popularity with
its natural combination of information retrieval (IR) and machine comprehension
(MC). Advancements in representation learning have led to separated progress in
both IR and MC; however, very few studies have examined the relationship and
combined design of retrieval and comprehension at different levels of
granularity, for development of MRS systems. In this work, we give general
guidelines on system design for MRS by proposing a simple yet effective
pipeline system with special consideration on hierarchical semantic retrieval
at both paragraph and sentence level, and their potential effects on the
downstream task. The system is evaluated on both fact verification and
open-domain multihop QA, achieving state-of-the-art results on the leaderboard
test sets of both FEVER and HOTPOTQA. To further demonstrate the importance of
semantic retrieval, we present ablation and analysis studies to quantify the
contribution of neural retrieval modules at both paragraph-level and
sentence-level, and illustrate that intermediate semantic retrieval modules are
vital for not only effectively filtering upstream information and thus saving
downstream computation, but also for shaping upstream data distribution and
providing better data for downstream modeling. Code/data made publicly
available at: https://github.com/easonnie/semanticRetrievalMRS"
ac54a9c30c968e5225978a37032158a6ffd4ddb8,1909.08041,"How does the effectiveness of sentence-level versus paragraph-level neural retrieval differ in terms of their impact on FEVER Score, Label Accuracy, and classification F1 scores across the SUPPORT, REFUTE, and NOT ENOUGH INFO labels?","[{'answer': 'This seems to indicate that the downstream QA module relies more on the upstream paragraph-level retrieval whereas the verification module relies more on the upstream sentence-level retrieval.', 'type': 'extractive'}]",1909.08041.pdf,"['1909.08041.pdf', '2001.06888.pdf', '1910.10288.pdf', '1705.01265.pdf', '1605.08675.pdf', '1806.04330.pdf', '1909.13695.pdf', '1810.12885.pdf', '2003.01769.pdf', '1909.09484.pdf']","FLOAT SELECTED: Table 4: Ablation over the paragraph-level and sentence-level neural retrieval sub-modules on FEVER. “LA”=Label Accuracy; “FS”=FEVER Score; “Orcl.” is the oracle upperbound of FEVER Score assuming all downstream modules are perfect. “L-F1 (S/R/N)” means the classification f1 scores on the three verification labels: SUPPORT, REFUTE, and NOT ENOUGH INFO.",Revealing the Importance of Semantic Retrieval for Machine Reading at Scale,"Machine Reading at Scale (MRS) is a challenging task in which a system is
given an input query and is asked to produce a precise output by ""reading""
information from a large knowledge base. The task has gained popularity with
its natural combination of information retrieval (IR) and machine comprehension
(MC). Advancements in representation learning have led to separated progress in
both IR and MC; however, very few studies have examined the relationship and
combined design of retrieval and comprehension at different levels of
granularity, for development of MRS systems. In this work, we give general
guidelines on system design for MRS by proposing a simple yet effective
pipeline system with special consideration on hierarchical semantic retrieval
at both paragraph and sentence level, and their potential effects on the
downstream task. The system is evaluated on both fact verification and
open-domain multihop QA, achieving state-of-the-art results on the leaderboard
test sets of both FEVER and HOTPOTQA. To further demonstrate the importance of
semantic retrieval, we present ablation and analysis studies to quantify the
contribution of neural retrieval modules at both paragraph-level and
sentence-level, and illustrate that intermediate semantic retrieval modules are
vital for not only effectively filtering upstream information and thus saving
downstream computation, but also for shaping upstream data distribution and
providing better data for downstream modeling. Code/data made publicly
available at: https://github.com/easonnie/semanticRetrievalMRS"
3f326c003be29c8eac76b24d6bba9608c75aa7ea,1908.06083,"What evaluation metrics are reported for the multi-turn adversarial task, specifically comparing models that incorporate dialogue context with BERT dialogue segment features?","[{'answer': 'F1 and Weighted-F1', 'type': 'abstractive'}]",1908.06083.pdf,"['1908.06083.pdf', '1905.11901.pdf', '1608.06757.pdf', '1701.09123.pdf', '1909.02480.pdf', '2001.00137.pdf', '1910.12795.pdf', '1912.10435.pdf']",FLOAT SELECTED: Table 10: Results of experiments on the multi-turn adversarial task. We denote the average and one standard deviation from the results of five runs. Models that use the context as input (“with context”) perform better. Encoding this in the architecture as well (via BERT dialogue segment features) gives us the best results.,Build it Break it Fix it for Dialogue Safety: Robustness from Adversarial Human Attack,"The detection of offensive language in the context of a dialogue has become
an increasingly important application of natural language processing. The
detection of trolls in public forums (Gal\'an-Garc\'ia et al., 2016), and the
deployment of chatbots in the public domain (Wolf et al., 2017) are two
examples that show the necessity of guarding against adversarially offensive
behavior on the part of humans. In this work, we develop a training scheme for
a model to become robust to such human attacks by an iterative build it, break
it, fix it strategy with humans and models in the loop. In detailed experiments
we show this approach is considerably more robust than previous systems.
Further, we show that offensive language used within a conversation critically
depends on the dialogue context, and cannot be viewed as a single sentence
offensive detection task as in most previous work. Our newly collected tasks
and methods will be made open source and publicly available."
3e1829e96c968cbd8ad8e9ce850e3a92a76b26e4,1910.06592,What are the total number of Twitter accounts and tweets in the dataset specifically used for detecting fake news on Twitter through neural recurrent models and timeline-based profiling?,"[{'answer': 'Total dataset size: 171 account (522967 tweets)', 'type': 'abstractive'}, {'answer': '212 accounts', 'type': 'abstractive'}]",1910.06592.pdf,"['1910.06592.pdf', '1910.05154.pdf', '1908.07245.pdf', '1910.11235.pdf', '1606.00189.pdf', '1703.07090.pdf', '1806.07711.pdf', '1706.08032.pdf', '1611.02550.pdf', '1906.01081.pdf', '1810.06743.pdf', '2001.06286.pdf', '1603.00968.pdf', '1912.01673.pdf', '1901.02257.pdf', '1909.00279.pdf', '1909.03242.pdf']",Table TABREF13 presents statistics on our dataset.,FacTweet: Profiling Fake News Twitter Accounts,"We present an approach to detect fake news in Twitter at the account level
using a neural recurrent model and a variety of different semantic and
stylistic features. Our method extracts a set of features from the timelines of
news Twitter accounts by reading their posts as chunks, rather than dealing
with each tweet independently. We show the experimental benefits of modeling
latent stylistic signatures of mixed fake and real news with a sequential model
over a wide range of strong baselines."
84737d871bde8058d8033e496179f7daec31c2d3,1910.03467,Is the supervised morphological learner evaluated on Japanese data in the Japanese-Vietnamese translation experiments outlined in this paper?,"[{'answer': 'No', 'type': 'boolean'}]",1910.03467.pdf,"['1910.03467.pdf', '2003.03044.pdf', '1707.08559.pdf', '1904.03288.pdf', '1911.08976.pdf', '1910.04269.pdf', '1909.13375.pdf', '1907.09369.pdf', '2002.06424.pdf', '1909.00430.pdf', '1908.07816.pdf', '1910.06748.pdf', '1909.03544.pdf', '2003.12738.pdf', '1809.00540.pdf', '2002.04181.pdf', '1911.13066.pdf']",We conduct two out of the three proposed approaches for Japanese-Vietnamese translation systems and the results are given in the Table TABREF15.,Overcoming the Rare Word Problem for Low-Resource Language Pairs in Neural Machine Translation,"Among the six challenges of neural machine translation (NMT) coined by (Koehn
and Knowles, 2017), rare-word problem is considered the most severe one,
especially in translation of low-resource languages. In this paper, we propose
three solutions to address the rare words in neural machine translation
systems. First, we enhance source context to predict the target words by
connecting directly the source embeddings to the output of the attention
component in NMT. Second, we propose an algorithm to learn morphology of
unknown words for English in supervised way in order to minimize the adverse
effect of rare-word problem. Finally, we exploit synonymous relation from the
WordNet to overcome out-of-vocabulary (OOV) problem of NMT. We evaluate our
approaches on two low-resource language pairs: English-Vietnamese and
Japanese-Vietnamese. In our experiments, we have achieved significant
improvements of up to roughly +1.0 BLEU points in both language pairs."
2e1ededb7c8460169cf3c38e6cde6de402c1e720,1912.10806,What is the mean prediction accuracy achieved by the DP-LSTM model for S&P 500 stock predictions in the paper?,"[{'answer': 'mean prediction accuracy 0.99582651\nS&P 500 Accuracy 0.99582651', 'type': 'abstractive'}]",1912.10806.pdf,"['1912.10806.pdf', '2002.06675.pdf', '1809.01202.pdf', '2003.12218.pdf', '2003.05377.pdf', '2004.03788.pdf']",FLOAT SELECTED: Table 1: Predicted Mean MPA results.,DP-LSTM: Differential Privacy-inspired LSTM for Stock Prediction Using Financial News,"Stock price prediction is important for value investments in the stock
market. In particular, short-term prediction that exploits financial news
articles is promising in recent years. In this paper, we propose a novel deep
neural network DP-LSTM for stock price prediction, which incorporates the news
articles as hidden information and integrates difference news sources through
the differential privacy mechanism. First, based on the autoregressive moving
average model (ARMA), a sentiment-ARMA is formulated by taking into
consideration the information of financial news articles in the model. Then, an
LSTM-based deep neural network is designed, which consists of three components:
LSTM, VADER model and differential privacy (DP) mechanism. The proposed DP-LSTM
scheme can reduce prediction errors and increase the robustness. Extensive
experiments on S&P 500 stocks show that (i) the proposed DP-LSTM achieves 0.32%
improvement in mean MPA of prediction result, and (ii) for the prediction of
the market index S&P 500, we achieve up to 65.79% improvement in MSE."
2d47cdf2c1e0c64c73518aead1b94e0ee594b7a5,1911.0168,"What are the train, dev, and test set sizes for the Onsei Intent Slot dataset in the slot filling experiments?","[{'answer': 'Dataset has 1737 train, 497 dev and 559 test sentences.', 'type': 'abstractive'}]",1911.01680.pdf,"['1911.01680.pdf', '1806.07711.pdf', '1708.09609.pdf', '1908.05434.pdf', '1909.09270.pdf', '2002.01984.pdf']","In our experiments, we use Onsei Intent Slot dataset. Table TABREF21 shows the statics of this dataset.",,
ffa7f91d6406da11ddf415ef094aaf28f3c3872d,1906.01081,"Based on the data, how much higher is the correlation of the PARENT metric with human judgments compared to the best-performing baseline on both the WikiBio and WebNLG datasets, and what are the specific improvements in correlation values?","[{'answer': 'Best proposed metric has average correlation with human judgement of 0.913 and 0.846 compared to best compared metrics result of 0.758 and 0.829 on WikiBio and WebNLG challenge.', 'type': 'abstractive'}, {'answer': 'Their average correlation tops the best other model by 0.155 on WikiBio.', 'type': 'abstractive'}]",1906.01081.pdf,"['1906.01081.pdf', '1703.07090.pdf', '1806.07711.pdf', '1703.02507.pdf']",We report the average correlation across all bootstrap samples for each metric in Table TABREF37 .,Handling Divergent Reference Texts when Evaluating Table-to-Text Generation,"Automatically constructed datasets for generating text from semi-structured
data (tables), such as WikiBio, often contain reference texts that diverge from
the information in the corresponding semi-structured data. We show that metrics
which rely solely on the reference texts, such as BLEU and ROUGE, show poor
correlation with human judgments when those references diverge. We propose a
new metric, PARENT, which aligns n-grams from the reference and generated texts
to the semi-structured data before computing their precision and recall.
Through a large scale human evaluation study of table-to-text models for
WikiBio, we show that PARENT correlates with human judgments better than
existing text generation metrics. We also adapt and evaluate the information
extraction based evaluation proposed by Wiseman et al (2017), and show that
PARENT has comparable correlation to it, while being easier to use. We show
that PARENT is also applicable when the reference texts are elicited from
humans using the data from the WebNLG challenge."
53377f1c5eda961e438424d71d16150e669f7072,2004.0198,"In the human evaluation results, TitleStylist outperforms which summarization model by 9.68% in terms of attraction?","[{'answer': 'pure summarization model NHG', 'type': 'extractive'}]",2004.01980.pdf,"['2004.01980.pdf', '1712.03556.pdf', '1905.07464.pdf', '1710.06700.pdf', '1611.02550.pdf', '1911.01680.pdf', '1908.05828.pdf', '1905.11901.pdf', '2003.07723.pdf', '1704.00939.pdf']","FLOAT SELECTED: Table 2: Human evaluation on three aspects: relevance, attraction, and fluency. “None” represents the original headlines in the dataset.",,
f37ed011e7eb259360170de027c1e8557371f002,2004.0198,"What are the percentage improvements in relevance, attraction, and fluency for humor-styled headlines generated using the TitleStylist method compared to the multitask baseline, as reported in the human evaluation results?","[{'answer': 'Humor in headlines (TitleStylist vs Multitask baseline):\nRelevance: +6.53% (5.87 vs 5.51)\nAttraction: +3.72% (8.93 vs 8.61)\nFluency: 1,98% (9.29 vs 9.11)', 'type': 'abstractive'}]",2004.01980.pdf,"['2004.01980.pdf', '1909.01247.pdf', '2002.06675.pdf', '2002.04181.pdf', '1905.10810.pdf', '1910.02339.pdf', '1909.03135.pdf', '1701.06538.pdf', '1705.00108.pdf', '1912.10435.pdf', '1910.03467.pdf', '1901.02257.pdf', '2002.01359.pdf']","We summarize the human evaluation results on the first three criteria in Table TABREF51, and the last criteria in Table TABREF57.",,
08333e4dd1da7d6b5e9b645d40ec9d502823f5d7,1603.07044,"What are the exact MAP score differences between the neural attention-based RNN encoder model and the handcrafted feature-based approach for Tasks A, B, and C as reported in the SemEval-2016 cQA experiments?","[{'answer': '0.007 MAP on Task A, 0.032 MAP on Task B, 0.055 MAP on Task C', 'type': 'abstractive'}]",1603.07044.pdf,"['1603.07044.pdf', '2003.03044.pdf', '2002.08899.pdf', '1810.05241.pdf', '1907.09369.pdf', '1701.09123.pdf']",FLOAT SELECTED: Table 4: Compared with other systems (bold is best).,Recurrent Neural Network Encoder with Attention for Community Question Answering,"We apply a general recurrent neural network (RNN) encoder framework to
community question answering (cQA) tasks. Our approach does not rely on any
linguistic processing, and can be applied to different languages or domains.
Further improvements are observed when we extend the RNN encoders with a neural
attention mechanism that encourages reasoning over entire sequences. To deal
with practical issues such as data sparsity and imbalanced labels, we apply
various techniques such as transfer learning and multitask learning. Our
experiments on the SemEval-2016 cQA task show 10% improvement on a MAP score
compared to an information retrieval-based approach, and achieve comparable
performance to a strong handcrafted feature-based method."
cc9f0ac8ead575a9b485a51ddc06b9ecb2e2a44d,2002.00652,How much improvement in Query Matching and Interaction Matching does the Turn+SQL Attn+Action Copy model in this paper achieve over the previous state-of-the-art on the SParC dataset without using BERT?,"[{'answer': 'Compared with the previous SOTA without BERT on SParC, our model improves Ques.Match and Int.Match by $10.6$ and $5.4$ points, respectively.', 'type': 'extractive'}]",2002.00652.pdf,"['2002.00652.pdf', '1909.08041.pdf', '2003.03014.pdf', '2002.04181.pdf', '1804.08139.pdf', '2002.08899.pdf', '1909.11467.pdf', '1909.08824.pdf', '1812.06864.pdf', '1801.05147.pdf', '1702.03342.pdf', '1712.03547.pdf', '1809.05752.pdf', '1603.07044.pdf', '2003.03106.pdf']",EditSQL BIBREF5 is a STOA baseline which mainly makes use of SQL attention and token-level copy (analogous to our Turn+SQL Attn+Action Copy).,How Far are We from Effective Context Modeling? An Exploratory Study on Semantic Parsing in Context,"Recently semantic parsing in context has received considerable attention,
which is challenging since there are complex contextual phenomena. Previous
works verified their proposed methods in limited scenarios, which motivates us
to conduct an exploratory study on context modeling methods under real-world
semantic parsing in context. We present a grammar-based decoding semantic
parser and adapt typical context modeling methods on top of it. We evaluate 13
context modeling methods on two large complex cross-domain datasets, and our
best model achieves state-of-the-art performances on both datasets with
significant improvements. Furthermore, we summarize the most frequent
contextual phenomena, with a fine-grained analysis on representative models,
which may shed light on potential research directions. Our code is available at
https://github.com/microsoft/ContextualSP."
5b551ba47d582f2e6467b1b91a8d4d6a30c343ec,1909.00105,"What specific metrics are reported for evaluating distinctness, user matching accuracy, and preference alignment in generated personalized recipes within this study?","[{'answer': 'Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)', 'type': 'abstractive'}, {'answer': 'BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence', 'type': 'extractive'}, {'answer': ' Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)', 'type': 'abstractive'}]",1909.00105.pdf,"['1909.00105.pdf', '1901.02262.pdf', '1812.06864.pdf', '1910.12795.pdf', '1905.00563.pdf', '1908.06264.pdf', '1603.07044.pdf', '2003.04642.pdf', '1910.02339.pdf', '2001.06286.pdf', '1810.12085.pdf', '1812.10479.pdf', '1910.00458.pdf', '1902.09666.pdf', '1909.01383.pdf']","FLOAT SELECTED: Table 2: Metrics on generated recipes from test set. D-1/2 = Distinct-1/2, UMA = User Matching Accuracy, MRR = Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model).",Generating Personalized Recipes from Historical User Preferences,"Existing approaches to recipe generation are unable to create recipes for
users with culinary preferences but incomplete knowledge of ingredients in
specific dishes. We propose a new task of personalized recipe generation to
help these users: expanding a name and incomplete ingredient details into
complete natural-text instructions aligned with the user's historical
preferences. We attend on technique- and recipe-level representations of a
user's previously consumed recipes, fusing these 'user-aware' representations
in an attention fusion layer to control recipe text generation. Experiments on
a new dataset of 180K recipes and 700K interactions show our model's ability to
generate plausible and personalized recipes compared to non-personalized
baselines."
0682bf049f96fa603d50f0fdad0b79a5c55f6c97,2003.03014,"Does the paper specifically analyze how the term ""homosexual"" has come to be more strongly associated with dehumanizing language compared to ""gay and lesbian,"" particularly in the context of the Gallup survey's change in wording?","[{'answer': 'Yes', 'type': 'boolean'}, {'answer': 'Yes', 'type': 'boolean'}]",2003.03014.pdf,"['2003.03014.pdf', '1611.04642.pdf', '1909.06162.pdf', '1910.11769.pdf', '1909.03544.pdf', '1910.03467.pdf', '1912.10011.pdf', '1909.03242.pdf', '1905.12260.pdf', '1806.04330.pdf', '1912.03457.pdf', '2004.03744.pdf', '1808.09029.pdf', '1909.05855.pdf', '1711.11221.pdf', '1808.03430.pdf', '2003.08385.pdf']","The Gallup survey asked for opinions on legality of “homosexual relations"" until 2008, but then changed the wording to “gay and lesbian relations"". This was likely because many people who identify as gay and lesbian find the word homosexual to be outdated and derogatory.",A Framework for the Computational Linguistic Analysis of Dehumanization,"Dehumanization is a pernicious psychological process that often leads to
extreme intergroup bias, hate speech, and violence aimed at targeted social
groups. Despite these serious consequences and the wealth of available data,
dehumanization has not yet been computationally studied on a large scale.
Drawing upon social psychology research, we create a computational linguistic
framework for analyzing dehumanizing language by identifying linguistic
correlates of salient components of dehumanization. We then apply this
framework to analyze discussions of LGBTQ people in the New York Times from
1986 to 2015. Overall, we find increasingly humanizing descriptions of LGBTQ
people over time. However, we find that the label homosexual has emerged to be
much more strongly associated with dehumanizing attitudes than other labels,
such as gay. Our proposed techniques highlight processes of linguistic
variation and change in discourses surrounding marginalized groups.
Furthermore, the ability to analyze dehumanizing language at a large scale has
implications for automatically detecting and understanding media bias as well
as abusive language online."
71d59c36225b5ee80af11d3568bdad7425f17b0c,1911.07228,"According to the Vietnamese NER error analysis paper, how do the F1 scores of the BLSTM-CNN-CRF and BLSTM-CRF models compare when using the VLSP 2016 dataset with Kyubyong Park’s or Edouard Grave’s pre-trained word embeddings?","[{'answer': 'Best BLSTM-CNN-CRF had F1 score 86.87 vs 86.69 of best BLSTM-CRF ', 'type': 'abstractive'}]",1911.07228.pdf,"['1911.07228.pdf', '1909.00105.pdf', '1707.00110.pdf', '1909.00015.pdf', '1906.11180.pdf', '1712.05999.pdf', '1903.00172.pdf', '1912.00864.pdf', '2004.03788.pdf', '1810.05241.pdf', '1709.05413.pdf', '1904.10503.pdf', '1909.00252.pdf', '1910.12795.pdf', '1905.06566.pdf', '1909.02480.pdf']",Table 2 shows our experiments on two models with and without different pre-trained word embedding – KP means the Kyubyong Park’s pre-trained word embeddings and EG means Edouard Grave’s pre-trained word embeddings.,Error Analysis for Vietnamese Named Entity Recognition on Deep Neural Network Models,"In recent years, Vietnamese Named Entity Recognition (NER) systems have had a
great breakthrough when using Deep Neural Network methods. This paper describes
the primary errors of the state-of-the-art NER systems on Vietnamese language.
After conducting experiments on BLSTM-CNN-CRF and BLSTM-CRF models with
different word embeddings on the Vietnamese NER dataset. This dataset is
provided by VLSP in 2016 and used to evaluate most of the current Vietnamese
NER systems. We noticed that BLSTM-CNN-CRF gives better results, therefore, we
analyze the errors on this model in detail. Our error-analysis results provide
us thorough insights in order to increase the performance of NER for the
Vietnamese language and improve the quality of the corpus in the future works."
ab0fd94dfc291cf3e54e9b7a7f78b852ddc1a797,1903.09722,"Which prior work, using a task-specific summarization architecture, is compared against the pre-trained sequence-to-sequence model in this paper?","[{'answer': 'BIBREF26 ', 'type': 'extractive'}, {'answer': 'BIBREF26', 'type': 'extractive'}]",1903.09722.pdf,"['1903.09722.pdf', '1804.07789.pdf', '1909.00430.pdf', '1906.01081.pdf', '2002.06675.pdf', '1909.09270.pdf', '1905.06566.pdf', '1804.11346.pdf', '1611.00514.pdf', '1802.06024.pdf']","Following BIBREF11 , we experiment on the non-anonymized version of . When generating summaries, we follow standard practice of tuning the maximum output length and disallow repeating the same trigram BIBREF27 , BIBREF14 . For this task we train language model representations on the combination of newscrawl and the training data. Table TABREF16 shows that pre-trained embeddings can significantly improve on top of a strong baseline transformer. We also compare to BIBREF26 who use a task-specific architecture compared to our generic sequence to sequence baseline. Pre-trained representations are complementary to their method.",Pre-trained Language Model Representations for Language Generation,"Pre-trained language model representations have been successful in a wide
range of language understanding tasks. In this paper, we examine different
strategies to integrate pre-trained representations into sequence to sequence
models and apply it to neural machine translation and abstractive
summarization. We find that pre-trained representations are most effective when
added to the encoder network which slows inference by only 14%. Our experiments
in machine translation show gains of up to 5.3 BLEU in a simulated
resource-poor setup. While returns diminish with more labeled data, we still
observe improvements when millions of sentence-pairs are available. Finally, on
abstractive summarization we achieve a new state of the art on the full text
version of CNN/DailyMail."
6b91fe29175be8cd8f22abf27fb3460e43b9889a,2003.05377,"What are the 14 genres that categorize the 138,368 Brazilian song lyrics used for genre classification in the study?","[{'answer': 'Gospel, Sertanejo, MPB, Forró, Pagode, Rock, Samba, Pop, Axé, Funk-carioca, Infantil, Velha-guarda, Bossa-nova and Jovem-guarda', 'type': 'abstractive'}]",2003.05377.pdf,"['2003.05377.pdf', '1704.06194.pdf', '1910.10288.pdf', '1910.06592.pdf', '1911.02711.pdf', '2004.03788.pdf', '1908.06379.pdf', '1703.02507.pdf', '1906.05474.pdf']",FLOAT SELECTED: Table 1: The number of songs and artists by genre,Brazilian Lyrics-Based Music Genre Classification Using a BLSTM Network,"Organize songs, albums, and artists in groups with shared similarity could be
done with the help of genre labels. In this paper, we present a novel approach
for automatic classifying musical genre in Brazilian music using only the song
lyrics. This kind of classification remains a challenge in the field of Natural
Language Processing. We construct a dataset of 138,368 Brazilian song lyrics
distributed in 14 genres. We apply SVM, Random Forest and a Bidirectional Long
Short-Term Memory (BLSTM) network combined with different word embeddings
techniques to address this classification task. Our experiments show that the
BLSTM method outperforms the other models with an F1-score average of $0.48$.
Some genres like ""gospel"", ""funk-carioca"" and ""sertanejo"", which obtained 0.89,
0.70 and 0.69 of F1-score, respectively, can be defined as the most distinct
and easy to classify in the Brazilian musical genres context."
384d571e4017628ebb72f3debb2846efaf0cb0cb,1909.01958,"What corpus, consisting of a large Web-crawled dataset of $5 \times 10^{10}$ tokens from the University of Waterloo and targeted science content from platforms like Wikipedia and SimpleWikipedia, underpins the Aristo system's success on the New York Regents Science Exams?","[{'answer': 'Aristo Corpus\nRegents 4th\nRegents 8th\nRegents `12th\nARC-Easy\nARC-challenge ', 'type': 'abstractive'}]",1909.01958.pdf,"['1909.01958.pdf', '1912.01772.pdf', '1912.03457.pdf', '1812.01704.pdf', '2002.05829.pdf', '1605.07683.pdf']","Several methods make use of the Aristo Corpus, comprising a large Web-crawled corpus ($5 \times 10^{10}$ tokens (280GB)) originally from the University of Waterloo, combined with targeted science content from Wikipedia, SimpleWikipedia, and several smaller online science texts (BID25).",From 'F' to 'A' on the N.Y. Regents Science Exams: An Overview of the Aristo Project,"AI has achieved remarkable mastery over games such as Chess, Go, and Poker,
and even Jeopardy, but the rich variety of standardized exams has remained a
landmark challenge. Even in 2016, the best AI system achieved merely 59.3% on
an 8th Grade science exam challenge. This paper reports unprecedented success
on the Grade 8 New York Regents Science Exam, where for the first time a system
scores more than 90% on the exam's non-diagram, multiple choice (NDMC)
questions. In addition, our Aristo system, building upon the success of recent
language models, exceeded 83% on the corresponding Grade 12 Science Exam NDMC
questions. The results, on unseen test questions, are robust across different
test years and different variations of this kind of test. They demonstrate that
modern NLP methods can result in mastery on this task. While not a full
solution to general question-answering (the questions are multiple choice, and
the domain is restricted to 8th Grade science), it represents a significant
milestone for the field."
d0c79f4a5d5c45fe673d9fcb3cd0b7dd65df7636,1909.01013,"What are the P@1 accuracy results for EN-IT, IT-EN, and other language pairs on the Vecmap benchmark after applying the proposed duality regularization method?","[{'answer': 'New best results of accuracy (P@1) on Vecmap:\nOurs-GeoMMsemi: EN-IT 50.00 IT-EN 42.67 EN-DE 51.60 DE-EN 47.22 FI-EN 39.62 EN-ES 39.47 ES-EN 36.43', 'type': 'abstractive'}]",1909.01013.pdf,"['1909.01013.pdf', '1804.08139.pdf', '1709.10217.pdf', '1607.06025.pdf']",Table TABREF15 shows the final results on Vecmap.,Duality Regularization for Unsupervised Bilingual Lexicon Induction,"Unsupervised bilingual lexicon induction naturally exhibits duality, which
results from symmetry in back-translation. For example, EN-IT and IT-EN
induction can be mutually primal and dual problems. Current state-of-the-art
methods, however, consider the two tasks independently. In this paper, we
propose to train primal and dual models jointly, using regularizers to
encourage consistency in back translation cycles. Experiments across 6 language
pairs show that the proposed method significantly outperforms competitive
baselines, obtaining the best-published results on a standard benchmark."
54c7fc08598b8b91a8c0399f6ab018c45e259f79,1909.01013,"What are the P@1 accuracy results comparing the proposed duality regularization method and the best baseline method for each language pair (EN-IT, IT-EN, EN-DE, DE-EN, EN-FI, FI-EN, EN-ES, and ES-EN) on Vecmap?","[{'answer': 'Proposed method vs best baseline result on Vecmap (Accuracy P@1):\nEN-IT: 50 vs 50\nIT-EN: 42.67 vs 42.67\nEN-DE: 51.6 vs 51.47\nDE-EN: 47.22 vs 46.96\nEN-FI: 35.88 vs 36.24\nFI-EN: 39.62 vs 39.57\nEN-ES: 39.47 vs 39.30\nES-EN: 36.43 vs 36.06', 'type': 'abstractive'}]",1909.01013.pdf,"['1909.01013.pdf', '1810.00663.pdf', '1901.02262.pdf', '1912.10435.pdf', '1611.00514.pdf', '1701.03214.pdf', '1809.05752.pdf', '1902.09393.pdf', '2002.06675.pdf', '1701.02877.pdf', '2003.06044.pdf']","FLOAT SELECTED: Table 4: Accuracy (P@1) on Vecmap. The best results are bolded. †Results as reported in the original paper. For unsupervised methods, we report the average accuracy across 10 runs.",Duality Regularization for Unsupervised Bilingual Lexicon Induction,"Unsupervised bilingual lexicon induction naturally exhibits duality, which
results from symmetry in back-translation. For example, EN-IT and IT-EN
induction can be mutually primal and dual problems. Current state-of-the-art
methods, however, consider the two tasks independently. In this paper, we
propose to train primal and dual models jointly, using regularizers to
encourage consistency in back translation cycles. Experiments across 6 language
pairs show that the proposed method significantly outperforms competitive
baselines, obtaining the best-published results on a standard benchmark."
03ce42ff53aa3f1775bc57e50012f6eb1998c480,1909.01013,What are the six language pairs on which the duality regularization method significantly reduced back-translation inconsistencies compared to Adv-C?,"[{'answer': 'EN<->ES\nEN<->DE\nEN<->IT\nEN<->EO\nEN<->MS\nEN<->FI', 'type': 'abstractive'}]",1909.01013.pdf,"['1909.01013.pdf', '1604.00400.pdf', '1709.10367.pdf', '1904.09678.pdf', '1910.03467.pdf']","Compared with Adv-C, our model significantly reduces the inconsistency rates on all language pairs, which explains the overall improvement in Table TABREF12.",Duality Regularization for Unsupervised Bilingual Lexicon Induction,"Unsupervised bilingual lexicon induction naturally exhibits duality, which
results from symmetry in back-translation. For example, EN-IT and IT-EN
induction can be mutually primal and dual problems. Current state-of-the-art
methods, however, consider the two tasks independently. In this paper, we
propose to train primal and dual models jointly, using regularizers to
encourage consistency in back translation cycles. Experiments across 6 language
pairs show that the proposed method significantly outperforms competitive
baselines, obtaining the best-published results on a standard benchmark."
ff28d34d1aaa57e7ad553dba09fc924dc21dd728,1909.00578,"What are the Spearman’s ρ, Kendall’s τ, and Pearson’s r correlation values for the BEST-ROUGE versions across the DUC-05, DUC-06, and DUC-07 datasets, as evaluated by the SumQE model on questions Q1–Q5?","[{'answer': 'High correlation results range from 0.472 to 0.936', 'type': 'abstractive'}]",1909.00578.pdf,"['1909.00578.pdf', '1909.06937.pdf', '2002.02070.pdf', '1606.00189.pdf', '1911.01680.pdf', '1805.04033.pdf', '1703.06492.pdf']","FLOAT SELECTED: Table 1: Spearman’s ρ, Kendall’s τ and Pearson’s r correlations on DUC-05, DUC-06 and DUC-07 for Q1–Q5. BEST-ROUGE refers to the version that achieved best correlations and is different across years.",SumQE: a BERT-based Summary Quality Estimation Model,"We propose SumQE, a novel Quality Estimation model for summarization based on
BERT. The model addresses linguistic quality aspects that are only indirectly
captured by content-based approaches to summary evaluation, without involving
comparison with human references. SumQE achieves very high correlations with
human ratings, outperforming simpler models addressing these linguistic
aspects. Predictions of the SumQE model can be used for system development, and
to inform users of the quality of automatically produced summaries and other
types of generated text."
9fe4a2a5b9e5cf29310ab428922cc8e7b2fc1d11,1910.00458,Which baseline models were used for comparison against MMM on the DREAM dataset to demonstrate the accuracy improvements in the paper?,"[{'answer': 'FTLM++, BERT-large, XLNet', 'type': 'abstractive'}]",1910.00458.pdf,"['1910.00458.pdf', '1910.10288.pdf', '2003.01769.pdf', '1808.09920.pdf', '1910.12129.pdf', '1805.03710.pdf', '2003.03044.pdf', '2004.04721.pdf', '1908.07816.pdf', '1901.09755.pdf', '1912.08960.pdf', '2002.10361.pdf', '1707.08559.pdf', '2003.08385.pdf']",FLOAT SELECTED: Table 3: Accuracy on the DREAM dataset. Performance marked by ? is reported by (Sun et al. 2019). Numbers in parentheses indicate the accuracy increased by MMM compared to the baselines.,MMM: Multi-stage Multi-task Learning for Multi-choice Reading Comprehension,"Machine Reading Comprehension (MRC) for question answering (QA), which aims
to answer a question given the relevant context passages, is an important way
to test the ability of intelligence systems to understand human language.
Multiple-Choice QA (MCQA) is one of the most difficult tasks in MRC because it
often requires more advanced reading comprehension skills such as logical
reasoning, summarization, and arithmetic operations, compared to the extractive
counterpart where answers are usually spans of text within given passages.
Moreover, most existing MCQA datasets are small in size, making the learning
task even harder. We introduce MMM, a Multi-stage Multi-task learning framework
for Multi-choice reading comprehension. Our method involves two sequential
stages: coarse-tuning stage using out-of-domain datasets and multi-task
learning stage using a larger in-domain dataset to help model generalize better
with limited data. Furthermore, we propose a novel multi-step attention network
(MAN) as the top-level classifier for this task. We demonstrate MMM
significantly advances the state-of-the-art on four representative MCQA
datasets."
85abd60094c92eb16f39f861c6de8c2064807d02,1910.05154,What bilingual model specifically translates word-level bilingual data into unsegmented phonemic sequences during the language documentation process detailed in the paper?,"[{'answer': ' Neural Machine Translation (NMT) models are trained between language pairs, using as source language the translation (word-level) and as target', 'type': 'extractive'}]",1910.05154.pdf,"['1910.05154.pdf', '1902.10525.pdf', '2002.01207.pdf', '1812.06864.pdf', '1909.04002.pdf', '2003.07996.pdf', '1705.00108.pdf', '1911.03597.pdf', '1902.00672.pdf', '1804.11346.pdf']","In this approach, Neural Machine Translation (NMT) models are trained between language pairs, using as source language the translation (word-level) and as target, the language to document (unsegmented phonemic sequence).",How Does Language Influence Documentation Workflow? Unsupervised Word Discovery Using Translations in Multiple Languages,"For language documentation initiatives, transcription is an expensive
resource: one minute of audio is estimated to take one hour and a half on
average of a linguist's work (Austin and Sallabank, 2013). Recently, collecting
aligned translations in well-resourced languages became a popular solution for
ensuring posterior interpretability of the recordings (Adda et al. 2016). In
this paper we investigate language-related impact in automatic approaches for
computational language documentation. We translate the bilingual Mboshi-French
parallel corpus (Godard et al. 2017) into four other languages, and we perform
bilingual-rooted unsupervised word discovery. Our results hint towards an
impact of the well-resourced language in the quality of the output. However, by
combining the information learned by different bilingual models, we are only
able to marginally increase the quality of the segmentation."
a516b37ad9d977cb9d4da3897f942c1c494405fe,1810.12885,"What models are evaluated in the ReCoRD dataset paper, comparing machine reading comprehension methods to human performance on commonsense reasoning tasks?","[{'answer': 'DocQA, SAN, QANet, ASReader, LM, Random Guess', 'type': 'abstractive'}]",1810.12885.pdf,"['1810.12885.pdf', '2002.00652.pdf', '2003.05377.pdf', '1904.03288.pdf', '1909.08041.pdf', '1909.02480.pdf', '1806.11432.pdf', '1910.06748.pdf', '1809.00540.pdf', '1901.02257.pdf', '1911.07228.pdf', '1906.01081.pdf', '1908.05434.pdf', '1909.01383.pdf', '1909.00430.pdf', '1908.07245.pdf', '2002.11910.pdf', '1709.10217.pdf']",FLOAT SELECTED: Table 4: Performance of various methods and human.,ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension,"We present a large-scale dataset, ReCoRD, for machine reading comprehension
requiring commonsense reasoning. Experiments on this dataset demonstrate that
the performance of state-of-the-art MRC systems fall far behind human
performance. ReCoRD represents a challenge for future research to bridge the
gap between human and machine commonsense reading comprehension. ReCoRD is
available at http://nlp.jhu.edu/record."
6baf5d7739758bdd79326ce8f50731c785029802,2003.07996,Which four languages are included in the SER datasets presented in the *Cross Lingual Cross Corpus Speech Emotion Recognition* paper?,"[{'answer': 'German, English, Italian, Chinese', 'type': 'abstractive'}]",2003.07996.pdf,"['2003.07996.pdf', '2003.06044.pdf', '1609.00559.pdf', '1906.01081.pdf', '2002.02492.pdf', '1810.09774.pdf', '1906.11180.pdf', '2002.00652.pdf']",FLOAT SELECTED: Table 1: Datasets used for various SER experiments.,Cross Lingual Cross Corpus Speech Emotion Recognition,"The majority of existing speech emotion recognition models are trained and
evaluated on a single corpus and a single language setting. These systems do
not perform as well when applied in a cross-corpus and cross-language scenario.
This paper presents results for speech emotion recognition for 4 languages in
both single corpus and cross corpus setting. Additionally, since multi-task
learning (MTL) with gender, naturalness and arousal as auxiliary tasks has
shown to enhance the generalisation capabilities of the emotion models, this
paper introduces language ID as another auxiliary task in MTL framework to
explore the role of spoken language on emotion recognition which has not been
studied yet."
cd06d775f491b4a17c9d616a8729fd45aa2e79bf,2002.04181,Which sentiment class was predicted with the highest accuracy by the NLP tools in the ELS task in the study comparing named-entity recognition and entity-level sentiment analysis for political tweets?,"[{'answer': 'neutral sentiment', 'type': 'abstractive'}]",2002.04181.pdf,"['2002.04181.pdf', '1912.01673.pdf', '1901.02262.pdf', '2004.04721.pdf', '1910.11204.pdf']",FLOAT SELECTED: Table 1: Average Correct Classification Rate (CCR) for named-entity recognition (NER) of four presidential candidates and entity-level sentiment (ELS) analysis by NLP tools and crowdworkers,Performance Comparison of Crowdworkers and NLP Tools on Named-Entity Recognition and Sentiment Analysis of Political Tweets,"We report results of a comparison of the accuracy of crowdworkers and seven
Natural Language Processing (NLP) toolkits in solving two important NLP tasks,
named-entity recognition (NER) and entity-level sentiment (ELS) analysis. We
here focus on a challenging dataset, 1,000 political tweets that were collected
during the U.S. presidential primary election in February 2016. Each tweet
refers to at least one of four presidential candidates, i.e., four named
entities. The groundtruth, established by experts in political communication,
has entity-level sentiment information for each candidate mentioned in the
tweet. We tested several commercial and open-source tools. Our experiments show
that, for our dataset of political tweets, the most accurate NER system, Google
Cloud NL, performed almost on par with crowdworkers, but the most accurate ELS
analysis system, TensiStrength, did not match the accuracy of crowdworkers by a
large margin of more than 30 percent points."
9da1e124d28b488b0d94998d32aa2fa8a5ebec51,2002.1191,"What are the precision, recall, and F1 score improvements for both named entity recognition and nominal mention when applying the boundary assembling method compared to the models by He and Sun (2017) and Peng and Dredze (2017)?","[{'answer': 'Overall F1 score:\n- He and Sun (2017) 58.23\n- Peng and Dredze (2017) 58.99\n- Xu et al. (2018) 59.11', 'type': 'abstractive'}, {'answer': 'For Named entity the maximum precision was 66.67%, and the average 62.58%, same values for Recall was 55.97% and 50.33%, and for F1 57.14% and 55.64%. Where for Nominal Mention had maximum recall of 74.48% and average of 73.67%, Recall had values of 54.55% and 53.7%,  and F1 had values of  62.97% and 62.12%. Finally the Overall F1 score had maximum value of 59.11% and average of 58.77%', 'type': 'abstractive'}]",2002.11910.pdf,"['2002.11910.pdf', '2004.04721.pdf', '1904.10503.pdf', '1909.03405.pdf', '1809.01541.pdf', '1912.00864.pdf', '2003.05377.pdf']","FLOAT SELECTED: Table 1: The results of two previous models, and results of this study, in which we apply a boundary assembling method. Precision, recall, and F1 scores are shown for both named entity and nominal mention. For both tasks and their overall performance, we outperform the other two models.",,
e292676c8c75dd3711efd0e008423c11077938b1,1909.11297,Which attention-based models are compared as soft-selection approaches against the proposed hard-selection method in the paper's evaluation of opinion snippet detection for aspect-based sentiment analysis?,"[{'answer': 'LSTM and BERT ', 'type': 'abstractive'}]",1909.11297.pdf,"['1909.11297.pdf', '1912.01673.pdf', '1812.10479.pdf', '1912.03457.pdf', '1701.05574.pdf']",Previous attention-based methods can be categorized as soft-selection approaches since the attention weights scatter across the whole sentence and every word is taken into consideration with different weights. ,Learning to Detect Opinion Snippet for Aspect-Based Sentiment Analysis,"Aspect-based sentiment analysis (ABSA) is to predict the sentiment polarity
towards a particular aspect in a sentence. Recently, this task has been widely
addressed by the neural attention mechanism, which computes attention weights
to softly select words for generating aspect-specific sentence representations.
The attention is expected to concentrate on opinion words for accurate
sentiment prediction. However, attention is prone to be distracted by noisy or
misleading words, or opinion words from other aspects. In this paper, we
propose an alternative hard-selection approach, which determines the start and
end positions of the opinion snippet, and selects the words between these two
positions for sentiment prediction. Specifically, we learn deep associations
between the sentence and aspect, and the long-term dependencies within the
sentence by leveraging the pre-trained BERT model. We further detect the
opinion snippet by self-critical reinforcement learning. Especially,
experimental results demonstrate the effectiveness of our method and prove that
our hard-selection approach outperforms soft-selection approaches when handling
multi-aspect sentences."
657edbf39c500b2446edb9cca18de2912c628b7d,1810.10254,"What perplexity scores on the dev and test sets are reported for the language model trained using augmented code-switched sentences in the ""Learn to Code-Switch"" paper?","[{'answer': 'Perplexity score 142.84 on dev and 138.91 on test', 'type': 'abstractive'}]",1810.10254.pdf,"['1810.10254.pdf', '1912.01673.pdf', '1908.06151.pdf', '1905.11901.pdf', '2001.05970.pdf', '2002.11402.pdf', '1711.11221.pdf', '1909.06162.pdf', '2001.06888.pdf', '1904.10500.pdf', '1908.06267.pdf', '1810.06743.pdf', '1911.10049.pdf', '2004.01980.pdf', '2001.10161.pdf', '2002.05058.pdf', '1909.08824.pdf', '1610.00879.pdf']",FLOAT SELECTED: Table 3. Language Modeling Results (in perplexity).,Learn to Code-Switch: Data Augmentation using Copy Mechanism on Language Modeling,"Building large-scale datasets for training code-switching language models is
challenging and very expensive. To alleviate this problem using parallel corpus
has been a major workaround. However, existing solutions use linguistic
constraints which may not capture the real data distribution. In this work, we
propose a novel method for learning how to generate code-switching sentences
from parallel corpora. Our model uses a Seq2Seq model in combination with
pointer networks to align and choose words from the monolingual sentences and
form a grammatical code-switching sentence. In our experiment, we show that by
training a language model using the augmented sentences we improve the
perplexity score by 10% compared to the LSTM baseline."
680dc3e56d1dc4af46512284b9996a1056f89ded,2002.06644,What baseline models were used in the experiments on subjective bias detection in the Wiki Neutrality Corpus before comparing them to the proposed BERT-based ensembles?,"[{'answer': 'FastText, BiLSTM, BERT', 'type': 'extractive'}, {'answer': 'FastText, BERT , two-layer BiLSTM architecture with GloVe word embeddings', 'type': 'extractive'}]",2002.06644.pdf,"['2002.06644.pdf', '2002.01359.pdf', '1904.09678.pdf', '1705.01214.pdf', '1911.03310.pdf', '1910.03467.pdf', '1804.08139.pdf', '1610.00879.pdf', '2001.08868.pdf', '1904.03288.pdf', '1811.12254.pdf', '1911.11951.pdf', '2002.02070.pdf', '1707.03569.pdf', '1910.02339.pdf', '1911.08976.pdf', '1910.06036.pdf']","FastTextBIBREF4: It uses bag of words and bag of n-grams as features for text classification, capturing partial information about the local word order efficiently.",Towards Detection of Subjective Bias using Contextualized Word Embeddings,"Subjective bias detection is critical for applications like propaganda
detection, content recommendation, sentiment analysis, and bias neutralization.
This bias is introduced in natural language via inflammatory words and phrases,
casting doubt over facts, and presupposing the truth. In this work, we perform
comprehensive experiments for detecting subjective bias using BERT-based models
on the Wiki Neutrality Corpus(WNC). The dataset consists of $360k$ labeled
instances, from Wikipedia edits that remove various instances of the bias. We
further propose BERT-based ensembles that outperform state-of-the-art methods
like $BERT_{large}$ by a margin of $5.6$ F1 score."
b0a18628289146472aa42f992d0db85c200ec64b,2003.11563,"What metrics are reported that assess BERT's performance on the unseen portion of the training set and the development set, when applying different augmentation techniques to the Propaganda Techniques Corpus (PTC)?","[{'answer': 'precision, recall , F1 score', 'type': 'extractive'}]",2003.11563.pdf,"['2003.11563.pdf', '1809.01541.pdf', '1904.05584.pdf', '2003.11645.pdf', '1909.00361.pdf', '2004.03744.pdf', '1908.05434.pdf', '1911.08673.pdf', '1910.02339.pdf', '1610.07809.pdf', '2002.01984.pdf', '2003.03014.pdf', '1701.02877.pdf', '1911.07555.pdf', '1902.09314.pdf', '1810.06743.pdf', '1809.06537.pdf']",FLOAT SELECTED: Table 2: F1 scores on an unseen (not used for training) part of the training set and the development set on BERT using different augmentation techniques.,Cost-Sensitive BERT for Generalisable Sentence Classification with Imbalanced Data,"The automatic identification of propaganda has gained significance in recent
years due to technological and social changes in the way news is generated and
consumed. That this task can be addressed effectively using BERT, a powerful
new architecture which can be fine-tuned for text classification tasks, is not
surprising. However, propaganda detection, like other tasks that deal with news
documents and other forms of decontextualized social communication (e.g.
sentiment analysis), inherently deals with data whose categories are
simultaneously imbalanced and dissimilar. We show that BERT, while capable of
handling imbalanced classes with no additional data augmentation, does not
generalise well when the training and test data are sufficiently dissimilar (as
is often the case with news sources, whose topics evolve over time). We show
how to address this problem by providing a statistical measure of similarity
between datasets and a method of incorporating cost-weighting into BERT when
the training and test sets are dissimilar. We test these methods on the
Propaganda Techniques Corpus (PTC) and achieve the second-highest score on
sentence-level propaganda classification."
a48c6d968707bd79469527493a72bfb4ef217007,1810.09774,Which training dataset showed the highest test accuracy and best generalization across different natural language inference benchmarks?,"[{'answer': 'MultiNLI', 'type': 'abstractive'}]",1810.09774.pdf,"['1810.09774.pdf', '1908.11546.pdf', '1701.05574.pdf', '1903.09588.pdf', '1908.06083.pdf', '2003.07723.pdf', '1908.06151.pdf', '1911.11951.pdf', '1810.00663.pdf', '1905.07464.pdf']","FLOAT SELECTED: Table 4: Test accuracies (%). For the baseline results (highlighted in bold) the training data and test data have been drawn from the same benchmark corpus. ∆ is the difference between the test accuracy and the baseline accuracy for the same training set. Results marked with * are for the development set, as no annotated test set is openly available. Best scores with respect to accuracy and difference in accuracy are underlined.",Testing the Generalization Power of Neural Network Models Across NLI Benchmarks,"Neural network models have been very successful in natural language
inference, with the best models reaching 90% accuracy in some benchmarks.
However, the success of these models turns out to be largely benchmark
specific. We show that models trained on a natural language inference dataset
drawn from one benchmark fail to perform well in others, even if the notion of
inference assumed in these benchmarks is the same or similar. We train six high
performing neural network models on different datasets and show that each one
of these has problems of generalizing when we replace the original test set
with a test set taken from another corpus designed for the same task. In light
of these results, we argue that most of the current neural network models are
not able to generalize well in the task of natural language inference. We find
that using large pre-trained language models helps with transfer learning when
the datasets are similar enough. Our results also highlight that the current
NLI datasets do not cover the different nuances of inference extensively
enough."
a379c380ac9f67f824506951444c873713405eed,1911.08962,What baseline models are listed in the CAIL2019-SCM paper for the Similar Case Matching task on the valid and test datasets?,"[{'answer': 'CNN, LSTM, BERT', 'type': 'abstractive'}]",1911.08962.pdf,"['1911.08962.pdf', '1908.07195.pdf', '1808.03430.pdf', '1901.02257.pdf', '1707.00110.pdf', '2001.05467.pdf', '1806.07711.pdf', '2002.06424.pdf', '2001.06888.pdf', '1809.00530.pdf', '2003.11563.pdf', '1907.03060.pdf', '1909.01013.pdf', '1909.13714.pdf', '1902.10525.pdf', '1809.08298.pdf']",FLOAT SELECTED: Table 2: Results of baselines and scores of top 3 participants on valid and test datasets.,CAIL2019-SCM: A Dataset of Similar Case Matching in Legal Domain,"In this paper, we introduce CAIL2019-SCM, Chinese AI and Law 2019 Similar
Case Matching dataset. CAIL2019-SCM contains 8,964 triplets of cases published
by the Supreme People's Court of China. CAIL2019-SCM focuses on detecting
similar cases, and the participants are required to check which two cases are
more similar in the triplets. There are 711 teams who participated in this
year's competition, and the best team has reached a score of 71.88. We have
also implemented several baselines to help researchers better understand this
task. The dataset and more details can be found from
https://github.com/china-ai-law-challenge/CAIL2019/tree/master/scm."
2ceced87af4c8fdebf2dc959aa700a5c95bd518f,1804.11346,Does the NLI-PT dataset indicate that the distribution of essays among the various L1 groups is balanced?,"[{'answer': 'No', 'type': 'boolean'}, {'answer': 'No', 'type': 'boolean'}]",1804.11346.pdf,"['1804.11346.pdf', '1909.13695.pdf', '1908.06264.pdf', '1707.08559.pdf', '2004.04721.pdf', '1611.00514.pdf', '1910.06748.pdf', '1706.08032.pdf', '1812.06705.pdf', '1902.10525.pdf', '2002.10361.pdf', '1911.00069.pdf', '1909.11687.pdf', '1912.01214.pdf', '1902.09666.pdf', '1701.03214.pdf']",FLOAT SELECTED: Table 2: Distribution by L1s and source corpora.,A Portuguese Native Language Identification Dataset,"In this paper we present NLI-PT, the first Portuguese dataset compiled for
Native Language Identification (NLI), the task of identifying an author's first
language based on their second language writing. The dataset includes 1,868
student essays written by learners of European Portuguese, native speakers of
the following L1s: Chinese, English, Spanish, German, Russian, French,
Japanese, Italian, Dutch, Tetum, Arabic, Polish, Korean, Romanian, and Swedish.
NLI-PT includes the original student text and four different types of
annotation: POS, fine-grained POS, constituency parses, and dependency parses.
NLI-PT can be used not only in NLI but also in research on several topics in
the field of Second Language Acquisition and educational NLP. We discuss
possible applications of this dataset and present the results obtained for the
first lexical baseline system for Portuguese NLI."
8a871b136ccef78391922377f89491c923a77730,2001.06888,What baseline NER models were compared to the proposed multimodal deep learning approaches that incorporate image features?,"[{'answer': 'Stanford NER, BiLSTM+CRF, LSTM+CNN+CRF, T-NER and BiLSTM+CNN+Co-Attention', 'type': 'abstractive'}]",2001.06888.pdf,"['2001.06888.pdf', '1901.09755.pdf', '1708.09609.pdf', '1908.07816.pdf', '2003.07996.pdf', '1909.09587.pdf', '1904.01608.pdf', '1912.01772.pdf', '1804.00079.pdf', '1909.13714.pdf', '1612.05270.pdf', '1809.05752.pdf', '1910.00912.pdf', '1910.07481.pdf', '1905.07464.pdf', '1911.03597.pdf', '1909.00578.pdf']",FLOAT SELECTED: Table 3: Evaluation results of different approaches compared to ours,A multimodal deep learning approach for named entity recognition from social media,"Named Entity Recognition (NER) from social media posts is a challenging task.
User generated content that forms the nature of social media, is noisy and
contains grammatical and linguistic errors. This noisy content makes it much
harder for tasks such as named entity recognition. We propose two novel deep
learning approaches utilizing multimodal deep learning and Transformers. Both
of our approaches use image features from short social media posts to provide
better results on the NER task. On the first approach, we extract image
features using InceptionV3 and use fusion to combine textual and image
features. This presents more reliable name entity recognition when the images
related to the entities are provided by the user. On the second approach, we
use image features combined with text and feed it into a BERT like Transformer.
The experimental results, namely, the precision, recall and F1 score metrics
show the superiority of our work compared to other state-of-the-art NER
solutions."
323e100a6c92d3fe503f7a93b96d821408f92109,1904.05584,Which references are cited in the caption when evaluating 11 downstream sentence-level transfer tasks using word vectors fed into a BiLSTM with max-pooling?,"[{'answer': 'BIBREF13 , BIBREF18', 'type': 'extractive'}]",1904.05584.pdf,"['1904.05584.pdf', '1811.01088.pdf', '1906.03538.pdf', '1610.00879.pdf', '2003.08385.pdf', '1606.05320.pdf', '1809.02279.pdf', '1910.06592.pdf', '1909.08089.pdf', '1908.08345.pdf']","Finally, we fed these obtained word vectors to a BiLSTM with max-pooling and evaluated the final sentence representations in 11 downstream transfer tasks BIBREF13 , BIBREF18 .",Gating Mechanisms for Combining Character and Word-level Word Representations: An Empirical Study,"In this paper we study how different ways of combining character and
word-level representations affect the quality of both final word and sentence
representations. We provide strong empirical evidence that modeling characters
improves the learned representations at the word and sentence levels, and that
doing so is particularly useful when representing less frequent words. We
further show that a feature-wise sigmoid gating mechanism is a robust method
for creating representations that encode semantic similarity, as it performed
reasonably well in several word similarity datasets. Finally, our findings
suggest that properly capturing semantic similarity at the word level does not
consistently yield improved performance in downstream sentence-level tasks. Our
code is available at https://github.com/jabalazs/gating"
707db46938d16647bf4b6407b2da84b5c7ab4a81,1912.10435,"What were the exact F1 improvements reported after incorporating both Simple Skip and Transformer Skip connections into the coattention-enhanced base BERT model, in comparison to the baseline performance?","[{'answer': 'Simple Skip improves F1 from 74.34 to 74.81\nTransformer Skip improes F1 from 74.34 to 74.95 ', 'type': 'abstractive'}]",1912.10435.pdf,"['1912.10435.pdf', '2001.10161.pdf', '2002.01664.pdf', '1910.00458.pdf', '1610.07809.pdf', '1811.12254.pdf', '1701.09123.pdf', '2004.03744.pdf', '1901.08079.pdf', '2001.05493.pdf', '1607.06025.pdf', '1712.05999.pdf', '2001.08868.pdf', '2002.02070.pdf', '1910.03814.pdf']","Table TABREF20 reports the F1 and EM scores obtained for the experiments on the base model. The first column reports the base BERT baseline scores, while the second reports the results for the C2Q/Q2C attention addition. The two skip columns report scores for the skip connection connecting the BERT embedding layer to the coattention output (Simple Skip) and the scores for the same skip connection containing a Transformer block (Transformer Skip).",BERTQA -- Attention on Steroids,"In this work, we extend the Bidirectional Encoder Representations from
Transformers (BERT) with an emphasis on directed coattention to obtain an
improved F1 performance on the SQUAD2.0 dataset. The Transformer architecture
on which BERT is based places hierarchical global attention on the
concatenation of the context and query. Our additions to the BERT architecture
augment this attention with a more focused context to query (C2Q) and query to
context (Q2C) attention via a set of modified Transformer encoder units. In
addition, we explore adding convolution-based feature extraction within the
coattention architecture to add localized information to self-attention. We
found that coattention significantly improves the no answer F1 by 4 points in
the base and 1 point in the large architecture. After adding skip connections
the no answer F1 improved further without causing an additional loss in has
answer F1. The addition of localized feature extraction added to attention
produced an overall dev F1 of 77.03 in the base architecture. We applied our
findings to the large BERT model which contains twice as many layers and
further used our own augmented version of the SQUAD 2.0 dataset created by back
translation, which we have named SQUAD 2.Q. Finally, we performed
hyperparameter tuning and ensembled our best models for a final F1/EM of
82.317/79.442 (Attention on Steroids, PCE Test Leaderboard)."
fbee81a9d90ff23603ee4f5986f9e8c0eb035b52,1809.05752,"What are the domain-specific F1 scores for the topic extraction models, and how do the Substance, Interpersonal, and Mood domains compare in terms of model performance in predicting psychiatric readmissions?","[{'answer': 'Achieved the highest per-domain scores on Substance (F1 ≈ 0.8) and the lowest scores on Interpersonal and Mood (F1 ≈ 0.5), and show consistency in per-domain performance rankings between MLP and RBF models.', 'type': 'abstractive'}]",1809.05752.pdf,"['1809.05752.pdf', '1909.01013.pdf', '1901.01010.pdf', '1909.04002.pdf', '1910.12129.pdf', '1605.07333.pdf', '1901.03866.pdf', '1611.03382.pdf', '1909.03135.pdf', '1703.02507.pdf', '1712.03547.pdf', '1906.01081.pdf', '1712.05999.pdf']","FLOAT SELECTED: Table 5: Overall and domain-specific Precision, Recall, and F1 scores for our models. The first row computes similarity directly from the TF-IDF matrix, as in (McCoy et al., 2015). All other rows are classifier outputs.",Analysis of Risk Factor Domains in Psychosis Patient Health Records,"Readmission after discharge from a hospital is disruptive and costly,
regardless of the reason. However, it can be particularly problematic for
psychiatric patients, so predicting which patients may be readmitted is
critically important but also very difficult. Clinical narratives in
psychiatric electronic health records (EHRs) span a wide range of topics and
vocabulary; therefore, a psychiatric readmission prediction model must begin
with a robust and interpretable topic extraction component. We created a data
pipeline for using document vector similarity metrics to perform topic
extraction on psychiatric EHR data in service of our long-term goal of creating
a readmission risk classifier. We show initial results for our topic extraction
model and identify additional features we will be incorporating in the future."
8cc56fc44136498471754186cfa04056017b4e54,1809.0496,"In the retrieval and generative evaluation settings, by how much does the proposed model + IR2 outperform NVDM in terms of MRR, MR, Recall@10, BLEU, CIDEr, ROUGE, and METEOR in the ""Unsupervised Machine Commenting with Neural Variational Topic Model"" paper?","[{'answer': 'Under the retrieval evaluation setting, their proposed model + IR2 had better MRR than NVDM by 0.3769, better MR by 4.6, and better Recall@10 by  20 . \nUnder the generative evaluation setting the proposed model + IR2 had better BLEU by 0.044 , better CIDEr by 0.033, better ROUGE by 0.032, and better METEOR by 0.029', 'type': 'abstractive'}, {'answer': 'Proposed model is better than both lexical based models by significan margin in all metrics: BLEU 0.261 vs 0.250, ROUGLE 0.162 vs 0.155 etc.', 'type': 'abstractive'}]",1809.04960.pdf,"['1809.04960.pdf', '1911.12579.pdf', '2002.11910.pdf', '1804.11346.pdf', '1909.00754.pdf', '1909.00578.pdf', '1910.11235.pdf', '1912.01772.pdf', '1911.04952.pdf', '2003.07996.pdf', '1909.00175.pdf', '1707.05236.pdf', '1909.00694.pdf']","NVDM (Lexical, Neural) is a VAE-based approach for document modeling BIBREF10 . We compare our model with this baseline to demonstrate the effect of modeling topic.",,
bdc91d1283a82226aeeb7a2f79dbbc57d3e84a1a,1909.03405,"What is the reported absolute performance gain on the RTE dataset when applying the 3-class NSP with PSP method, and how does it compare to BERTBase?","[{'answer': ' improvement on the RTE dataset is significant, i.e., 4% absolute gain over the BERTBase', 'type': 'extractive'}, {'answer': 'The average score improved by 1.4 points over the previous best result.', 'type': 'abstractive'}]",1909.03405.pdf,"['1909.03405.pdf', '1911.13066.pdf', '1909.00361.pdf', '1912.10435.pdf', '1711.11221.pdf', '1910.04269.pdf', '1801.05147.pdf', '2001.00137.pdf', '1909.13695.pdf', '1911.10049.pdf', '1910.12129.pdf', '1709.05413.pdf', '2004.03744.pdf', '1912.10806.pdf']","Table TABREF21 illustrates the experimental results, showing that our method is beneficial for all of NLI tasks. The improvement on the RTE dataset is significant, i.e., 4% absolute gain over the BERTBase.",Symmetric Regularization based BERT for Pair-wise Semantic Reasoning,"The ability of semantic reasoning over the sentence pair is essential for
many natural language understanding tasks, e.g., natural language inference and
machine reading comprehension. A recent significant improvement in these tasks
comes from BERT. As reported, the next sentence prediction (NSP) in BERT, which
learns the contextual relationship between two sentences, is of great
significance for downstream problems with sentence-pair input. Despite the
effectiveness of NSP, we suggest that NSP still lacks the essential signal to
distinguish between entailment and shallow correlation. To remedy this, we
propose to augment the NSP task to a 3-class categorization task, which
includes a category for previous sentence prediction (PSP). The involvement of
PSP encourages the model to focus on the informative semantics to determine the
sentence order, thereby improves the ability of semantic understanding. This
simple modification yields remarkable improvement against vanilla BERT. To
further incorporate the document-level information, the scope of NSP and PSP is
expanded into a broader range, i.e., NSP and PSP also include close but
nonsuccessive sentences, the noise of which is mitigated by the label-smoothing
technique. Both qualitative and quantitative experimental results demonstrate
the effectiveness of the proposed method. Our method consistently improves the
performance on the NLI and MRC benchmarks, including the challenging HANS
dataset \cite{hans}, suggesting that the document-level task is still promising
for the pre-training."
cd2878c5a52542ddf080b20bec005d9a74f2d916,1612.08205,What are the 14 industry categories that were used for classifying social media users in the industry-detection system presented in this paper?,"[{'answer': 'technology, religion, fashion, publishing, sports or recreation, real estate, agriculture/environment, law, security/military, tourism, construction, museums or libraries, banking/investment banking, automotive', 'type': 'abstractive'}, {'answer': 'Technology, Religion, Fashion, Publishing, Sports coach, Real Estate, Law, Environment, Tourism, Construction, Museums, Banking, Security, Automotive.', 'type': 'abstractive'}]",1612.08205.pdf,"['1612.08205.pdf', '1905.07464.pdf', '1708.09609.pdf', '1904.09678.pdf', '1810.09774.pdf', '1810.12885.pdf', '1911.10049.pdf', '1711.02013.pdf']",FLOAT SELECTED: Table 1: Industry categories and number of users per category.,Predicting the Industry of Users on Social Media,"Automatic profiling of social media users is an important task for supporting
a multitude of downstream applications. While a number of studies have used
social media content to extract and study collective social attributes, there
is a lack of substantial research that addresses the detection of a user's
industry. We frame this task as classification using both feature engineering
and ensemble learning. Our industry-detection system uses both posted content
and profile information to detect a user's industry with 64.3% accuracy,
significantly outperforming the majority baseline in a taxonomy of fourteen
industry classes. Our qualitative analysis suggests that a person's industry
not only affects the words used and their perceived meanings, but also the
number and type of emotions being expressed."
df8cc1f395486a12db98df805248eb37c087458b,1812.06705,"On which specific datasets did the authors validate the performance of the conditional BERT contextual augmentation, particularly in the domains of sentiment analysis, subjectivity detection, opinion polarity, and question classification?","[{'answer': 'SST (Stanford Sentiment Treebank), Subj (Subjectivity dataset), MPQA Opinion Corpus, RT is another movie review sentiment dataset, TREC is a dataset for classification of the six question types', 'type': 'extractive'}]",1812.06705.pdf,"['1812.06705.pdf', '1912.10806.pdf', '1909.04002.pdf', '1812.01704.pdf', '1909.09270.pdf', '2001.08051.pdf', '1804.05918.pdf', '1909.08089.pdf']","SST BIBREF25 SST (Stanford Sentiment Treebank) is a dataset for sentiment classification on movie reviews, which are annotated with five labels (SST5: very positive, positive, neutral, negative, or very negative) or two labels (SST2: positive or negative).

Subj BIBREF26 Subj (Subjectivity dataset) is annotated with whether a sentence is subjective or objective.

MPQA BIBREF27 MPQA Opinion Corpus is an opinion polarity detection dataset of short phrases rather than sentences, which contains news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations, etc.).

RT BIBREF28 RT is another movie review sentiment dataset contains a collection of short review excerpts from Rotten Tomatoes collected by Bo Pang and Lillian Lee.

TREC BIBREF29 TREC is a dataset for classification of the six question types (whether the question is about person, location, numeric information, etc.).",Conditional BERT Contextual Augmentation,"We propose a novel data augmentation method for labeled sentences called
conditional BERT contextual augmentation. Data augmentation methods are often
applied to prevent overfitting and improve generalization of deep neural
network models. Recently proposed contextual augmentation augments labeled
sentences by randomly replacing words with more varied substitutions predicted
by language model. BERT demonstrates that a deep bidirectional language model
is more powerful than either an unidirectional language model or the shallow
concatenation of a forward and backward model. We retrofit BERT to conditional
BERT by introducing a new conditional masked language model\footnote{The term
""conditional masked language model"" appeared once in original BERT paper, which
indicates context-conditional, is equivalent to term ""masked language model"".
In our paper, ""conditional masked language model"" indicates we apply extra
label-conditional constraint to the ""masked language model"".} task. The well
trained conditional BERT can be applied to enhance contextual augmentation.
Experiments on six various different text classification tasks show that our
method can be easily applied to both convolutional or recurrent neural networks
classifier to obtain obvious improvement."
6e97c06f998f09256be752fa75c24ba853b0db24,1812.06705,What metric is used to evaluate CBERT's performance on six different datasets across two classifier architectures?,"[{'answer': 'Accuracy across six datasets', 'type': 'abstractive'}]",1812.06705.pdf,"['1812.06705.pdf', '1910.12129.pdf', '1702.03342.pdf', '1907.03060.pdf', '1808.09920.pdf', '1909.00175.pdf', '1910.06036.pdf', '1912.01673.pdf', '2003.04642.pdf', '1909.06162.pdf', '1810.06743.pdf', '2003.11645.pdf', '1605.08675.pdf', '1603.04513.pdf', '1911.02086.pdf', '1709.05413.pdf', '2001.05467.pdf']","FLOAT SELECTED: Table 2: Accuracies of different methods for various benchmarks on two classifier architectures. CBERT, which represents conditional BERT, performs best on two classifier structures over six datasets. “w/” represents “with”, lines marked with “*” are experiments results from Kobayashi(Kobayashi, 2018).",Conditional BERT Contextual Augmentation,"We propose a novel data augmentation method for labeled sentences called
conditional BERT contextual augmentation. Data augmentation methods are often
applied to prevent overfitting and improve generalization of deep neural
network models. Recently proposed contextual augmentation augments labeled
sentences by randomly replacing words with more varied substitutions predicted
by language model. BERT demonstrates that a deep bidirectional language model
is more powerful than either an unidirectional language model or the shallow
concatenation of a forward and backward model. We retrofit BERT to conditional
BERT by introducing a new conditional masked language model\footnote{The term
""conditional masked language model"" appeared once in original BERT paper, which
indicates context-conditional, is equivalent to term ""masked language model"".
In our paper, ""conditional masked language model"" indicates we apply extra
label-conditional constraint to the ""masked language model"".} task. The well
trained conditional BERT can be applied to enhance contextual augmentation.
Experiments on six various different text classification tasks show that our
method can be easily applied to both convolutional or recurrent neural networks
classifier to obtain obvious improvement."
63bb39fd098786a510147f8ebc02408de350cb7c,1812.06705,"In the experiments presented, are any other pretrained language models besides conditional BERT evaluated for contextual augmentation across all datasets?","[{'answer': 'No', 'type': 'boolean'}]",1812.06705.pdf,"['1812.06705.pdf', '1803.09230.pdf', '2002.05829.pdf', '1704.00939.pdf', '1912.10435.pdf', '1911.11951.pdf', '2002.11402.pdf', '1711.00106.pdf', '1909.11467.pdf', '1712.03556.pdf', '1910.06748.pdf', '1905.00563.pdf', '1804.00079.pdf', '1906.01081.pdf', '1911.03310.pdf', '2001.08051.pdf']","FLOAT SELECTED: Table 2: Accuracies of different methods for various benchmarks on two classifier architectures. CBERT, which represents conditional BERT, performs best on two classifier structures over six datasets. “w/” represents “with”, lines marked with “*” are experiments results from Kobayashi(Kobayashi, 2018).",Conditional BERT Contextual Augmentation,"We propose a novel data augmentation method for labeled sentences called
conditional BERT contextual augmentation. Data augmentation methods are often
applied to prevent overfitting and improve generalization of deep neural
network models. Recently proposed contextual augmentation augments labeled
sentences by randomly replacing words with more varied substitutions predicted
by language model. BERT demonstrates that a deep bidirectional language model
is more powerful than either an unidirectional language model or the shallow
concatenation of a forward and backward model. We retrofit BERT to conditional
BERT by introducing a new conditional masked language model\footnote{The term
""conditional masked language model"" appeared once in original BERT paper, which
indicates context-conditional, is equivalent to term ""masked language model"".
In our paper, ""conditional masked language model"" indicates we apply extra
label-conditional constraint to the ""masked language model"".} task. The well
trained conditional BERT can be applied to enhance contextual augmentation.
Experiments on six various different text classification tasks show that our
method can be easily applied to both convolutional or recurrent neural networks
classifier to obtain obvious improvement."
3aa7173612995223a904cc0f8eef4ff203cbb860,1901.02257,"Which baseline models, both in ensemble and single variations, are the MPFN model compared against in the experimental results?","[{'answer': 'SLQA, Rusalka, HMA Model (single), TriAN (single), jiangnan (ensemble), MITRE (ensemble), TriAN (ensemble), HMA Model (ensemble)', 'type': 'abstractive'}]",1901.02257.pdf,"['1901.02257.pdf', '1807.07961.pdf', '2002.11402.pdf', '1704.06194.pdf', '1909.11687.pdf', '1910.14497.pdf', '2003.06044.pdf', '1908.06264.pdf', '1904.03288.pdf', '1811.12254.pdf', '2003.01769.pdf', '1711.02013.pdf', '1909.11467.pdf', '1701.00185.pdf', '1911.03597.pdf', '2003.03044.pdf', '2004.03788.pdf']",FLOAT SELECTED: Table 2: Experimental Results of Models,Multi-Perspective Fusion Network for Commonsense Reading Comprehension,"Commonsense Reading Comprehension (CRC) is a significantly challenging task,
aiming at choosing the right answer for the question referring to a narrative
passage, which may require commonsense knowledge inference. Most of the
existing approaches only fuse the interaction information of choice, passage,
and question in a simple combination manner from a \emph{union} perspective,
which lacks the comparison information on a deeper level. Instead, we propose a
Multi-Perspective Fusion Network (MPFN), extending the single fusion method
with multiple perspectives by introducing the \emph{difference} and
\emph{similarity} fusion\deleted{along with the \emph{union}}. More
comprehensive and accurate information can be captured through the three types
of fusion. We design several groups of experiments on MCScript dataset
\cite{Ostermann:LREC18:MCScript} to evaluate the effectiveness of the three
types of fusion respectively. From the experimental results, we can conclude
that the difference fusion is comparable with union fusion, and the similarity
fusion needs to be activated by the union fusion. The experimental result also
shows that our MPFN model achieves the state-of-the-art with an accuracy of
83.52\% on the official test set."
e659ceb184777015c12db2da5ae396635192f0b0,1904.105,"Based on the dataset's intent annotation results, do the intent labels show an imbalance in the distribution for the utterance-level AV intent detection task?","[{'answer': 'Yes', 'type': 'boolean'}]",1904.10500.pdf,"['1904.10500.pdf', '1703.02507.pdf', '2003.07723.pdf', '1901.09755.pdf', '2002.01207.pdf', '1701.09123.pdf', '1911.00069.pdf', '2004.04721.pdf', '1604.00400.pdf', '1909.13695.pdf', '1908.06379.pdf', '1901.02257.pdf', '1611.02550.pdf', '1912.08960.pdf', '1808.09029.pdf', '1707.00110.pdf', '2002.06424.pdf']"," Those annotation results for utterance-level intent types, slots and intent keywords can be found in Table TABREF7 and Table TABREF8 as a summary of dataset statistics.",,
9ee07edc371e014df686ced4fb0c3a7b9ce3d5dc,1704.06194,"According to the performance results, on which QA benchmarks did their proposed KBQA system achieve either state-of-the-art or near-state-of-the-art accuracy?","[{'answer': 'SimpleQuestions, WebQSP', 'type': 'extractive'}, {'answer': 'WebQSP, SimpleQuestions', 'type': 'extractive'}]",1704.06194.pdf,"['1704.06194.pdf', '2001.08051.pdf', '1707.03569.pdf', '1812.10479.pdf', '2001.00137.pdf']","As shown on the last row of Table 3 , this gives a significant performance boost, resulting in a new state-of-the-art result on SimpleQuestions and a result comparable to the state-of-the-art on WebQSP",Improved Neural Relation Detection for Knowledge Base Question Answering,"Relation detection is a core component for many NLP applications including
Knowledge Base Question Answering (KBQA). In this paper, we propose a
hierarchical recurrent neural network enhanced by residual learning that
detects KB relations given an input question. Our method uses deep residual
bidirectional LSTMs to compare questions and relation names via different
hierarchies of abstraction. Additionally, we propose a simple KBQA system that
integrates entity linking and our proposed relation detector to enable one
enhance another. Experimental results evidence that our approach achieves not
only outstanding relation detection performance, but more importantly, it helps
our KBQA system to achieve state-of-the-art accuracy for both single-relation
(SimpleQuestions) and multi-relation (WebQSP) QA benchmarks."
a978a1ee73547ff3a80c66e6db3e6c3d3b6512f4,1701.03214,"How much does the ""mixed fine tuning"" method improve BLEU-4 scores compared to standard fine tuning for the IWSLT-CE domain on the 2011, 2012, and 2013 test sets using the NTCIR-CE data for domain adaptation?","[{'answer': '0.08 points on the 2011 test set, 0.44 points on the 2012 test set, 0.42 points on the 2013 test set for IWSLT-CE.', 'type': 'abstractive'}]",1701.03214.pdf,"['1701.03214.pdf', '1703.06492.pdf', '1809.01541.pdf', '1908.11546.pdf', '1909.02480.pdf', '2002.01664.pdf', '1611.02550.pdf', '1904.09678.pdf', '1911.03597.pdf']",FLOAT SELECTED: Table 1: Domain adaptation results (BLEU-4 scores) for IWSLT-CE using NTCIR-CE.,An Empirical Comparison of Simple Domain Adaptation Methods for Neural Machine Translation,"In this paper, we propose a novel domain adaptation method named ""mixed fine
tuning"" for neural machine translation (NMT). We combine two existing
approaches namely fine tuning and multi domain NMT. We first train an NMT model
on an out-of-domain parallel corpus, and then fine tune it on a parallel corpus
which is a mix of the in-domain and out-of-domain corpora. All corpora are
augmented with artificial tags to indicate specific domains. We empirically
compare our proposed method against fine tuning and multi domain methods and
discuss its benefits and shortcomings."
efe49829725cfe54de01405c76149a4fe4d18747,1901.03866,"How much does HAS-QA improve over traditional RC models like GA, BiDAF, and AQA on the QuasarT dataset in terms of EM and F1 scores?","[{'answer': 'For example, in QuasarT, it improves 16.8% in EM score and 20.4% in F1 score. , For example, in QuasarT, it improves 4.6% in EM score and 3.5% in F1 score.', 'type': 'extractive'}]",1901.03866.pdf,"['1901.03866.pdf', '1911.02821.pdf', '1904.05584.pdf', '2002.06675.pdf', '1707.08559.pdf', '1809.01541.pdf', '2002.10361.pdf', '1904.10503.pdf', '1908.07245.pdf', '1809.01202.pdf']","HAS-QA outperforms traditional RC baselines with a large gap, such as GA, BiDAF, AQA listed in the first part. For example, in QuasarT, it improves 16.8% in EM score and 20.4% in F1 score. As RC task is just a special case of OpenQA task. Some experiments on standard SQuAD dataset(dev-set) BIBREF9 show that HAS-QA yields EM/F1:0.719/0.798, which is comparable with the best released single model Reinforced Mnemonic Reader BIBREF25 in the leaderboard (dev-set) EM/F1:0.721/0.816. ",HAS-QA: Hierarchical Answer Spans Model for Open-domain Question Answering,"This paper is concerned with open-domain question answering (i.e., OpenQA).
Recently, some works have viewed this problem as a reading comprehension (RC)
task, and directly applied successful RC models to it. However, the
performances of such models are not so good as that in the RC task. In our
opinion, the perspective of RC ignores three characteristics in OpenQA task: 1)
many paragraphs without the answer span are included in the data collection; 2)
multiple answer spans may exist within one given paragraph; 3) the end position
of an answer span is dependent with the start position. In this paper, we first
propose a new probabilistic formulation of OpenQA, based on a three-level
hierarchical structure, i.e.,~the question level, the paragraph level and the
answer span level. Then a Hierarchical Answer Spans Model (HAS-QA) is designed
to capture each probability. HAS-QA has the ability to tackle the above three
problems, and experiments on public OpenQA datasets show that it significantly
outperforms traditional RC baselines and recent OpenQA baselines."
5dfa59c116e0ceb428efd99bab19731aa3df4bbd,2004.03744,How many corrected image-sentence pairs are there in the validation and test sets of the e-SNLI-VE-2.0 dataset with human-written natural language explanations?,"[{'answer': 'Totally 6980 validation and test image-sentence pairs have been corrected.', 'type': 'abstractive'}]",2004.03744.pdf,"['2004.03744.pdf', '2001.05493.pdf', '1904.10500.pdf', '1904.07904.pdf', '1902.00330.pdf', '1909.00361.pdf', '1904.09678.pdf', '1810.00663.pdf', '1912.10011.pdf', '1808.09920.pdf', '2004.03354.pdf', '2003.03106.pdf', '1810.05241.pdf', '1912.08960.pdf', '2002.05829.pdf']",The statistics of e-SNLI-VE-2.0 are shown in Table TABREF40.,e-SNLI-VE: Corrected Visual-Textual Entailment with Natural Language Explanations,"The recently proposed SNLI-VE corpus for recognising visual-textual
entailment is a large, real-world dataset for fine-grained multimodal
reasoning. However, the automatic way in which SNLI-VE has been assembled (via
combining parts of two related datasets) gives rise to a large number of errors
in the labels of this corpus. In this paper, we first present a data collection
effort to correct the class with the highest error rate in SNLI-VE. Secondly,
we re-evaluate an existing model on the corrected corpus, which we call
SNLI-VE-2.0, and provide a quantitative comparison with its performance on the
non-corrected corpus. Thirdly, we introduce e-SNLI-VE, which appends
human-written natural language explanations to SNLI-VE-2.0. Finally, we train
models that learn from these explanations at training time, and output such
explanations at testing time."
bf52c01bf82612d0c7bbf2e6a5bb2570c322936f,1604.004,"How do different variants of ROUGE scores (e.g., ROUGE-1-P and ROUGE-3-F) correlate with human Pyramid scores using Pearson, Spearman, and Kendall correlations in the context of scientific article summarization?","[{'answer': 'we observe that many variants of Rouge scores do not have high correlations with human pyramid scores', 'type': 'extractive'}, {'answer': 'Using Pearson corelation measure,  for example, ROUGE-1-P is 0.257 and ROUGE-3-F 0.878.', 'type': 'abstractive'}]",1604.00400.pdf,"['1604.00400.pdf', '2001.06888.pdf', '1909.00512.pdf', '1704.00939.pdf', '1904.01608.pdf', '1909.00430.pdf', '1806.07711.pdf', '1611.04642.pdf', '2003.04642.pdf', '1908.11365.pdf']","Table TABREF23 shows the Pearson, Spearman and Kendall correlation of Rouge and Sera, with pyramid scores.",,
50716cc7f589b9b9f3aca806214228b063e9695b,1912.03457,"What language technologies are listed as being particularly challenging to adopt for extremely-low resource languages, such as Gondi, due to data collection difficulties?","[{'answer': '- Font & Keyboard\n- Speech-to-Text\n- Text-to-Speech\n- Text Prediction\n- Spell Checker\n- Grammar Checker\n- Text Search\n- Machine Translation\n- Voice to Text Search\n- Voice to Speech Search', 'type': 'abstractive'}]",1912.03457.pdf,"['1912.03457.pdf', '1912.01772.pdf', '2003.03044.pdf', '1908.11365.pdf', '1909.00015.pdf', '1908.07816.pdf', '1809.03449.pdf', '1809.08298.pdf', '1707.03569.pdf', '1707.05236.pdf', '1904.07904.pdf']","Table TABREF6 describes the various technologies and their presence concerning languages with different levels of resource availability and the ease of data collection. We can observe that for low resource languages, there is considerable difficulty in adopting these tools.",Unsung Challenges of Building and Deploying Language Technologies for Low Resource Language Communities,"In this paper, we examine and analyze the challenges associated with
developing and introducing language technologies to low-resource language
communities. While doing so, we bring to light the successes and failures of
past work in this area, challenges being faced in doing so, and what they have
achieved. Throughout this paper, we take a problem-facing approach and describe
essential factors which the success of such technologies hinges upon. We
present the various aspects in a manner which clarify and lay out the different
tasks involved, which can aid organizations looking to make an impact in this
area. We take the example of Gondi, an extremely-low resource Indian language,
to reinforce and complement our discussion."
2fa0b9d0cb26e1be8eae7e782ada6820bc2c037f,1710.067,What was the lemmatization accuracy reported for the WikiNews test set in the paper that evaluated an Arabic lemmatizer?,"[{'answer': '97.32%', 'type': 'abstractive'}]",1710.06700.pdf,"['1710.06700.pdf', '1711.00106.pdf', '2003.05377.pdf', '1908.06264.pdf']",FLOAT SELECTED: Table 3: Lemmatization accuracy using WikiNews testset,,
9c4a4dfa7b0b977173e76e2d2f08fa984af86f0e,1910.02339,"What are the Full Testing Set and Cleaned Testing Set accuracies achieved by TP-N2F on the AlgoLisp dataset, and how does this performance compare to LSTM-based Seq2Seq models?","[{'answer': 'Full Testing Set accuracy: 84.02\nCleaned Testing Set accuracy: 93.48', 'type': 'abstractive'}]",1910.02339.pdf,"['1910.02339.pdf', '1804.08050.pdf', '1911.01799.pdf', '1804.05918.pdf', '1703.02507.pdf', '1910.03467.pdf', '1911.01680.pdf', '1910.03814.pdf', '1809.10644.pdf', '2004.01878.pdf', '1603.00968.pdf', '1705.01265.pdf']",FLOAT SELECTED: Table 2: Results of AlgoLisp dataset,Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations,"Generating formal-language programs represented by relational tuples, such as
Lisp programs or mathematical operations, to solve problems stated in natural
language is a challenging task because it requires explicitly capturing
discrete symbolic structural information implicit in the input. However, most
general neural sequence models do not explicitly capture such structural
information, limiting their performance on these tasks. In this paper, we
propose a new encoder-decoder model based on a structured neural
representation, Tensor Product Representations (TPRs), for mapping
Natural-language problems to Formal-language solutions, called TP-N2F. The
encoder of TP-N2F employs TPR `binding' to encode natural-language symbolic
structure in vector space and the decoder uses TPR `unbinding' to generate, in
symbolic space, a sequential program represented by relational tuples, each
consisting of a relation (or operation) and a number of arguments. TP-N2F
considerably outperforms LSTM-based seq2seq models on two benchmarks and
creates new state-of-the-art results. Ablation studies show that improvements
can be attributed to the use of structured TPRs explicitly in both the encoder
and decoder. Analysis of the learned structures shows how TPRs enhance the
interpretability of TP-N2F."
4c7ac51a66c15593082e248451e8f6896e476ffb,1910.02339,What are the Full Testing Set Accuracy and Cleaned Testing Set Accuracy of the TP-N2F model on the AlgoLisp dataset?,"[{'answer': 'Full Testing Set Accuracy: 84.02\nCleaned Testing Set Accuracy: 93.48', 'type': 'abstractive'}]",1910.02339.pdf,"['1910.02339.pdf', '1805.04033.pdf', '2003.03014.pdf', '2004.01878.pdf', '1909.01383.pdf', '1703.07090.pdf', '2003.05377.pdf', '1711.00106.pdf', '1612.08205.pdf', '1909.09484.pdf', '1909.11687.pdf', '1909.01013.pdf', '1909.00361.pdf', '1705.00108.pdf', '1909.00754.pdf']",FLOAT SELECTED: Table 2: Results of AlgoLisp dataset,Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations,"Generating formal-language programs represented by relational tuples, such as
Lisp programs or mathematical operations, to solve problems stated in natural
language is a challenging task because it requires explicitly capturing
discrete symbolic structural information implicit in the input. However, most
general neural sequence models do not explicitly capture such structural
information, limiting their performance on these tasks. In this paper, we
propose a new encoder-decoder model based on a structured neural
representation, Tensor Product Representations (TPRs), for mapping
Natural-language problems to Formal-language solutions, called TP-N2F. The
encoder of TP-N2F employs TPR `binding' to encode natural-language symbolic
structure in vector space and the decoder uses TPR `unbinding' to generate, in
symbolic space, a sequential program represented by relational tuples, each
consisting of a relation (or operation) and a number of arguments. TP-N2F
considerably outperforms LSTM-based seq2seq models on two benchmarks and
creates new state-of-the-art results. Ablation studies show that improvements
can be attributed to the use of structured TPRs explicitly in both the encoder
and decoder. Analysis of the learned structures shows how TPRs enhance the
interpretability of TP-N2F."
05671d068679be259493df638d27c106e7dd36d0,1910.02339,What operation and execution accuracies did the TP-N2F model achieve on the MathQA dataset in comparison to the SEQ2PROG model?,"[{'answer': 'Operation accuracy: 71.89\nExecution accuracy: 55.95', 'type': 'abstractive'}]",1910.02339.pdf,"['1910.02339.pdf', '1908.05828.pdf', '2004.03354.pdf', '1809.01202.pdf', '1911.08976.pdf', '1810.00663.pdf']","Our model outperforms both the original seq2prog, designated SEQ2PROG-orig, and the best reimplemented seq2prog after an extensive hyperparameter search, designated SEQ2PROG-best. Table TABREF16 presents the results.",Mapping Natural-language Problems to Formal-language Solutions Using Structured Neural Representations,"Generating formal-language programs represented by relational tuples, such as
Lisp programs or mathematical operations, to solve problems stated in natural
language is a challenging task because it requires explicitly capturing
discrete symbolic structural information implicit in the input. However, most
general neural sequence models do not explicitly capture such structural
information, limiting their performance on these tasks. In this paper, we
propose a new encoder-decoder model based on a structured neural
representation, Tensor Product Representations (TPRs), for mapping
Natural-language problems to Formal-language solutions, called TP-N2F. The
encoder of TP-N2F employs TPR `binding' to encode natural-language symbolic
structure in vector space and the decoder uses TPR `unbinding' to generate, in
symbolic space, a sequential program represented by relational tuples, each
consisting of a relation (or operation) and a number of arguments. TP-N2F
considerably outperforms LSTM-based seq2seq models on two benchmarks and
creates new state-of-the-art results. Ablation studies show that improvements
can be attributed to the use of structured TPRs explicitly in both the encoder
and decoder. Analysis of the learned structures shows how TPRs enhance the
interpretability of TP-N2F."
e8fcfb1412c3b30da6cbc0766152b6e11e17196c,1701.06538,What reduction in test perplexity does the MoE model achieve compared to the best published model after 10 epochs in the language modeling benchmark?,"[{'answer': 'Perpexity is improved from 34.7 to 28.0.', 'type': 'abstractive'}]",1701.06538.pdf,"['1701.06538.pdf', '1603.00968.pdf', '1908.05828.pdf', '1605.08675.pdf', '1911.13066.pdf', '1910.04269.pdf', '1912.10435.pdf', '1912.13109.pdf', '1909.06937.pdf', '1910.08987.pdf', '1701.03214.pdf', '1703.06492.pdf', '1605.07683.pdf', '1911.07228.pdf', '1901.03866.pdf', '1708.09609.pdf']","The two models achieved test perplexity of INLINEFORM0 and INLINEFORM1 respectively, showing that even in the presence of a large MoE, more computation is still useful. Results are reported at the bottom of Table TABREF76 . The larger of the two models has a similar computational budget to the best published model from the literature, and training times are similar. Comparing after 10 epochs, our model has a lower test perplexity by INLINEFORM2 .",Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer,"The capacity of a neural network to absorb information is limited by its
number of parameters. Conditional computation, where parts of the network are
active on a per-example basis, has been proposed in theory as a way of
dramatically increasing model capacity without a proportional increase in
computation. In practice, however, there are significant algorithmic and
performance challenges. In this work, we address these challenges and finally
realize the promise of conditional computation, achieving greater than 1000x
improvements in model capacity with only minor losses in computational
efficiency on modern GPU clusters. We introduce a Sparsely-Gated
Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward
sub-networks. A trainable gating network determines a sparse combination of
these experts to use for each example. We apply the MoE to the tasks of
language modeling and machine translation, where model capacity is critical for
absorbing the vast quantities of knowledge available in the training corpora.
We present model architectures in which a MoE with up to 137 billion parameters
is applied convolutionally between stacked LSTM layers. On large language
modeling and machine translation benchmarks, these models achieve significantly
better results than state-of-the-art at lower computational cost."
2815bac42db32d8f988b380fed997af31601f129,1909.00252,"How did the accuracy and F1 score of the Transformer model on the Short Jokes dataset, as reported in the ""Humor Detection: A Transformer Gets the Last Laugh"" paper, compare to previous models like CNNs, and what was the magnitude of improvement?","[{'answer': 'It had the highest accuracy comparing to all datasets 0.986% and It had the highest improvement comparing to previous methods on the same dataset by 8%', 'type': 'abstractive'}]",1909.00252.pdf,"['1909.00252.pdf', '1906.01081.pdf', '1908.07245.pdf', '1910.12795.pdf', '1912.10011.pdf', '1910.03814.pdf', '2003.05377.pdf', '2002.02492.pdf', '1809.01541.pdf', '1911.04952.pdf', '1909.03135.pdf']",Our experiment with the Short Jokes dataset found the Transformer model's accuracy and F1 score to be 0.986. This was a jump of 8 percent from the most recent work done with CNNs (Table 4).,Humor Detection: A Transformer Gets the Last Laugh,"Much previous work has been done in attempting to identify humor in text. In
this paper we extend that capability by proposing a new task: assessing whether
or not a joke is humorous. We present a novel way of approaching this problem
by building a model that learns to identify humorous jokes based on ratings
gleaned from Reddit pages, consisting of almost 16,000 labeled instances. Using
these ratings to determine the level of humor, we then employ a Transformer
architecture for its advantages in learning from sentence context. We
demonstrate the effectiveness of this approach and show results that are
comparable to human performance. We further demonstrate our model's increased
capabilities on humor identification problems, such as the previously created
datasets for short jokes and puns. These experiments show that this method
outperforms all previous work done on these tasks, with an F-measure of 93.1%
for the Puns dataset and 98.6% on the Short Jokes dataset."
f9c5799091e7e35a8133eee4d95004e1b35aea00,1908.06151,"In the experiment where you modified the layer configuration to 6-6-4 in the $N_{src}$-$N_{mt}$-$N_{pe}$ setup, which result demonstrated that reducing the number of layers in the final decoder ($N_{pe}$) does not notably affect APE performance, as detailed in Section 5.1?","[{'answer': 'Exp. 5.1', 'type': 'extractive'}]",1908.06151.pdf,"['1908.06151.pdf', '1810.12885.pdf', '1911.12579.pdf', '1810.05241.pdf', '1909.09270.pdf', '2002.05058.pdf', '1909.13375.pdf', '1703.07090.pdf', '1812.06705.pdf', '1912.01772.pdf', '1909.06937.pdf', '1912.01214.pdf']","Last, we analyze the importance of our second encoder ($enc_{src \rightarrow mt}$), compared to the source encoder ($enc_{src}$) and the decoder ($dec_{pe}$), by reducing and expanding the amount of layers in the encoders and the decoder. Our standard setup, which we use for fine-tuning, ensembling etc., is fixed to 6-6-6 for $N_{src}$-$N_{mt}$-$N_{pe}$ (cf. Figure FIGREF1), where 6 is the value that was proposed by Vaswani:NIPS2017 for the base model. We investigate what happens in terms of APE performance if we change this setting to 6-6-4 and 6-4-6. ",The Transference Architecture for Automatic Post-Editing,"In automatic post-editing (APE) it makes sense to condition post-editing (pe)
decisions on both the source (src) and the machine translated text (mt) as
input. This has led to multi-source encoder based APE approaches. A research
challenge now is the search for architectures that best support the capture,
preparation and provision of src and mt information and its integration with pe
decisions. In this paper we present a new multi-source APE model, called
transference. Unlike previous approaches, it (i) uses a transformer encoder
block for src, (ii) followed by a decoder block, but without masking for
self-attention on mt, which effectively acts as second encoder combining src ->
mt, and (iii) feeds this representation into a final decoder block generating
pe. Our model outperforms the state-of-the-art by 1 BLEU point on the WMT 2016,
2017, and 2018 English--German APE shared tasks (PBSMT and NMT). We further
investigate the importance of our newly introduced second encoder and find that
a too small amount of layers does hurt the performance, while reducing the
number of layers of the decoder does not matter much."
04012650a45d56c0013cf45fd9792f43916eaf83,1908.06151,"Based on the experimental evidence presented in the paper, how does reducing the number of layers in the $enc_{src \rightarrow mt}$ encoder block (Exp. 5.2) affect BLEU and TER scores, particularly for test2016 and test2017, and how do these results compare to the impact of reducing the number of decoder layers as discussed in Exp. 5.1?","[{'answer': 'comparing to the results from reducing the number of layers in the decoder, the BLEU score was 69.93 which is less than 1% in case of test2016 and in case of test2017 it was less by 0.2 %. In terms of TER it had higher score by 0.7 in case of test2016 and 0.1 in case of test2017. ', 'type': 'abstractive'}]",1908.06151.pdf,"['1908.06151.pdf', '1903.09588.pdf', '1809.01202.pdf', '1909.09484.pdf', '2002.00652.pdf', '1910.03467.pdf', '1810.00663.pdf']","Exp. 5.1 shows that decreasing the number of layers on the decoder side does not hurt the performance. In fact, in the case of test2016, we got some improvement, while for test2017, the scores got slightly worse. In contrast, reducing the $enc_{src \rightarrow mt}$ encoder block's depth (Exp. 5.2) does indeed reduce the performance for all four scores, showing the importance of this second encoder.",The Transference Architecture for Automatic Post-Editing,"In automatic post-editing (APE) it makes sense to condition post-editing (pe)
decisions on both the source (src) and the machine translated text (mt) as
input. This has led to multi-source encoder based APE approaches. A research
challenge now is the search for architectures that best support the capture,
preparation and provision of src and mt information and its integration with pe
decisions. In this paper we present a new multi-source APE model, called
transference. Unlike previous approaches, it (i) uses a transformer encoder
block for src, (ii) followed by a decoder block, but without masking for
self-attention on mt, which effectively acts as second encoder combining src ->
mt, and (iii) feeds this representation into a final decoder block generating
pe. Our model outperforms the state-of-the-art by 1 BLEU point on the WMT 2016,
2017, and 2018 English--German APE shared tasks (PBSMT and NMT). We further
investigate the importance of our newly introduced second encoder and find that
a too small amount of layers does hurt the performance, while reducing the
number of layers of the decoder does not matter much."
6cd8bad8a031ce6d802ded90f9754088e0c8d653,1605.07333,By how much does the combined voting-based model of ER-CNN and R-RNN surpass the top state-of-the-art performance on the SemEval 2010 relation classification task?,"[{'answer': '0.8% F1 better than the best state-of-the-art', 'type': 'abstractive'}, {'answer': 'Best proposed model achieves F1 score of 84.9 compared to best previous result of 84.1.', 'type': 'abstractive'}]",1605.07333.pdf,"['1605.07333.pdf', '1902.00330.pdf', '1911.08962.pdf', '1909.09587.pdf', '1604.00400.pdf', '2002.06675.pdf', '1912.03457.pdf', '1711.00106.pdf', '1603.04513.pdf', '1909.09270.pdf', '1611.03382.pdf']",Table TABREF16 shows the results of our models ER-CNN (extended ranking CNN) and R-RNN (ranking RNN) in the context of other state-of-the-art models.,Combining Recurrent and Convolutional Neural Networks for Relation Classification,"This paper investigates two different neural architectures for the task of
relation classification: convolutional neural networks and recurrent neural
networks. For both models, we demonstrate the effect of different architectural
choices. We present a new context representation for convolutional neural
networks for relation classification (extended middle context). Furthermore, we
propose connectionist bi-directional recurrent neural networks and introduce
ranking loss for their optimization. Finally, we show that combining
convolutional and recurrent neural networks using a simple voting scheme is
accurate enough to improve results. Our neural models achieve state-of-the-art
results on the SemEval 2010 relation classification task."
