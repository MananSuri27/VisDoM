q_id,doc_id,question,answer,doc_path,documents,image_file,caption,first_mention,title,abstract
scgqa_0,1305.1657v1,Why can't we draw broad conclusions from the results depicted in Fig. 5 of the research paper?,"The main limitation of the graph is that it only shows the results for a single test case. In order to make more general conclusions about the accuracy of the proposed data fusion algorithm, it would be necessary to run more tests under different conditions.",1305.1657v1.pdf,"['1305.1657v1.pdf', '2003.09700v4.pdf', '1811.00912v4.pdf', '1707.04476v5.pdf', '2006.16705v1.pdf', '1702.06270v2.pdf', '1804.04290v1.pdf', '2006.04002v2.pdf']",1305.1657v1-Figure5-1.png,Fig. 5. Localization results and relative errors using the proposed data fusion algorithm.,"The tracking path output is shown in Fig. 5. It stands out that the tracking algorithm follows the real path during almost all the test. The maximum positioning errors is equal to 2.23 m, while the RMS one is 0.88 m. These results demonstrate that our sub-optimal data fusion technique is both efficient and accurate enough to satisfy the large part of non-critical WSNs applications.","Low Complexity Indoor Localization in Wireless Sensor Networks by UWB
  and Inertial Data Fusion","Precise indoor localization of moving targets is a challenging activity which
cannot be easily accomplished without combining different sources of
information. In this sense, the combination of different data sources with an
appropriate filter might improve both positioning and tracking performance.
This work proposes an algorithm for hybrid positioning in Wireless Sensor
Networks based on data fusion of UWB and inertial information. A constant-gain
Steady State Kalman Filter is used to bound the complexity of the system,
simplifying its implementation on a typical low-power WSN node. The performance
of the presented data fusion algorithm has been evaluated in a realistic
scenario using both simulations and realistic datasets. The obtained results
prove the validity of this approach, which efficiently fuses different
positioning data sources, reducing the localization error."
scgqa_1,1804.10488v2,"In the context of less frequent queries, how do absolute errors of estimators behave as shown in the paper?","The graph shows that the absolute errors of all estimators increase as we consider less frequent queries. This is expected since less frequent queries provide less training data. Nevertheless, our estimators still improve the performance of the baselines.",1804.10488v2.pdf,"['1804.10488v2.pdf', '1909.03961v2.pdf', '1804.00243v2.pdf', '1603.04153v1.pdf', '1408.5389v1.pdf', '1805.01772v1.pdf', '1607.05970v2.pdf', '2005.09634v1.pdf', '1709.03329v1.pdf']",1804.10488v2-Figure3-1.png,Figure 3: Prediction errors on less frequent queries as a function of clipping parameterM .,"The errors of all estimators are reported in Figure 3. We observe that the absolute errors of all estimators increase as we consider less frequent queries. This is expected since less frequent queries provide less training data. Nevertheless, our estimators still improve",Offline Evaluation of Ranking Policies with Click Models,"Many web systems rank and present a list of items to users, from recommender
systems to search and advertising. An important problem in practice is to
evaluate new ranking policies offline and optimize them before they are
deployed. We address this problem by proposing evaluation algorithms for
estimating the expected number of clicks on ranked lists from historical logged
data. The existing algorithms are not guaranteed to be statistically efficient
in our problem because the number of recommended lists can grow exponentially
with their length. To overcome this challenge, we use models of user
interaction with the list of items, the so-called click models, to construct
estimators that learn statistically efficiently. We analyze our estimators and
prove that they are more efficient than the estimators that do not use the
structure of the click model, under the assumption that the click model holds.
We evaluate our estimators in a series of experiments on a real-world dataset
and show that they consistently outperform prior estimators."
scgqa_2,2002.01322v1,"What accuracy does the full model achieve with 1000 real speech examples and synthetic augmentation, according to the paper?","The graph shows that the full model benefits from the inclusion of synthetic speech examples, but the effect is more pronounced when the number of real speech examples is small. When trained on 1000 real examples per word, the full model achieves an accuracy of 94.8% with or without synthetic speech examples. However, when the number of real examples is reduced to 125 per word, the full model achieves an accuracy of 90.3% without synthetic speech examples, but 95.8% with synthetic speech examples. This suggests that the synthetic speech examples can help the full model to learn more effectively from a small dataset.",2002.01322v1.pdf,"['2002.01322v1.pdf', '2007.11446v1.pdf', '1712.03538v1.pdf', '1603.01185v2.pdf', '1903.10464v3.pdf', '1801.09097v2.pdf']",2002.01322v1-Figure3-1.png,"Fig. 3: Performance of different amounts of real speech examples, with and without augmentation using synthetic speech examples, optionally based on the speech embedding model. The standard deviation over 20 runs is shown for each data point.","Figure 3 shows these results. At the right of the plot we can see that when we have 1000 real examples per word (35k total), using the embedding model improves the accuracy noticeably from 94.8% to 96.8%. The inclusion of the synthetic speech data does not result in any further improvements. Decreasing the number of real examples to 125 per word reduces the accuracy of the full model by 4.5% (absolute) to 90.3%, while the head model, at an accuracy of 95.8%, only loses 1% (absolute). This is also the first point where adding the synthetic data produces a measurable improvement of 0.1%. Reducing the number of real examples further causes the performance of the full model to rapidly decrease.",Training Keyword Spotters with Limited and Synthesized Speech Data,"With the rise of low power speech-enabled devices, there is a growing demand
to quickly produce models for recognizing arbitrary sets of keywords. As with
many machine learning tasks, one of the most challenging parts in the model
creation process is obtaining a sufficient amount of training data. In this
paper, we explore the effectiveness of synthesized speech data in training
small, spoken term detection models of around 400k parameters. Instead of
training such models directly on the audio or low level features such as MFCCs,
we use a pre-trained speech embedding model trained to extract useful features
for keyword spotting models. Using this speech embedding, we show that a model
which detects 10 keywords when trained on only synthetic speech is equivalent
to a model trained on over 500 real examples. We also show that a model without
our speech embeddings would need to be trained on over 4000 real examples to
reach the same accuracy."
scgqa_3,2007.15404v1,"In the context of the rainfall prediction study, how does prediction accuracy change with increasing days ahead?","The graph shows that the accuracy of the predictions decreases as days ahead increases. This is consistent with the findings of other studies, which have shown that the accuracy of weather forecasting decreases as the time horizon increases.",2007.15404v1.pdf,"['2007.15404v1.pdf', '1711.06964v1.pdf', '1804.04818v1.pdf', '1707.02439v2.pdf', '1512.00843v3.pdf', '2005.14165v4.pdf', '1805.00184v1.pdf', '1603.04153v1.pdf', '1610.04213v4.pdf']",2007.15404v1-Figure5-1.png,"Fig. 5. Mean macro f1-scores as a function of days ahead for different input image sequence length, averaged over all image sizes and tiles.","Figures 5 and 6 isolate the effect of input sequence length and image size, respectively. In Figure 5 the macro f1 scores for all predictions for all tiles for each days-ahead were averaged, and the results plotted as a function of days-ahead. The figure shows that no particular input sequence length is superior to the others. Figure 6 similarly averages macro f1 scores separately for each image size. There appears to be a slight advantage of about 1 percentage point when using the larger image size (172 × 123) instead of the smaller size (87 × 61). Both figures show a clear decrease in accuracy as days-ahead increases. in contrast to the constant classification accuracy found in [13].","Regional Rainfall Prediction Using Support Vector Machine Classification
  of Large-Scale Precipitation Maps","Rainfall prediction helps planners anticipate potential social and economic
impacts produced by too much or too little rain. This research investigates a
class-based approach to rainfall prediction from 1-30 days in advance. The
study made regional predictions based on sequences of daily rainfall maps of
the continental US, with rainfall quantized at 3 levels: light or no rain;
moderate; and heavy rain. Three regions were selected, corresponding to three
squares from a $5\times5$ grid covering the map area. Rainfall predictions up
to 30 days ahead for these three regions were based on a support vector machine
(SVM) applied to consecutive sequences of prior daily rainfall map images. The
results show that predictions for corner squares in the grid were less accurate
than predictions obtained by a simple untrained classifier. However, SVM
predictions for a central region outperformed the other two regions, as well as
the untrained classifier. We conclude that there is some evidence that SVMs
applied to large-scale precipitation maps can under some conditions give useful
information for predicting regional rainfall, but care must be taken to avoid
pitfall"
scgqa_4,1405.7705v1,"According to the findings in Figure 20 of this study, what does DOFs indicate in kinematic models?","DOFs stands for degrees of freedom. In the context of kinematic chains, the number of DOFs refers to the number of independent motions that the chain can perform. For example, a simple pendulum has one DOF, while a double pendulum has two DOFs.",1405.7705v1.pdf,"['1405.7705v1.pdf', '1707.02342v1.pdf', '1903.10464v3.pdf', '1502.03556v1.pdf', '1906.09756v1.pdf', '1402.0635v3.pdf', '1902.05922v1.pdf']",1405.7705v1-Figure20-1.png,Figure 20: Estimated number of DOFs for the open and the closed kinematic chain object (see Fig. 19). Left: open kinematic chain. Right: closed kinematic chain.,"We also analyzed the progression of model selection while the training data is incorporated. The left plot of Fig. 20 shows the DOFs of the learned kinematic model for the open kinematic chain. Note that we opened the yardstick segment by segment, therefore the number of DOFs increases step-wise from zero to three. The right plot shows the estimated number of DOFs for the closed kinematic chain: our approach correctly estimates the number of DOFs to one already after the first few observations.","A Probabilistic Framework for Learning Kinematic Models of Articulated
  Objects","Robots operating in domestic environments generally need to interact with
articulated objects, such as doors, cabinets, dishwashers or fridges. In this
work, we present a novel, probabilistic framework for modeling articulated
objects as kinematic graphs. Vertices in this graph correspond to object parts,
while edges between them model their kinematic relationship. In particular, we
present a set of parametric and non-parametric edge models and how they can
robustly be estimated from noisy pose observations. We furthermore describe how
to estimate the kinematic structure and how to use the learned kinematic models
for pose prediction and for robotic manipulation tasks. We finally present how
the learned models can be generalized to new and previously unseen objects. In
various experiments using real robots with different camera systems as well as
in simulation, we show that our approach is valid, accurate and efficient.
Further, we demonstrate that our approach has a broad set of applications, in
particular for the emerging fields of mobile manipulation and service robotics."
scgqa_5,2008.01961v3,"What does the study reveal about the influence of node count on the computation time for the composed algorithms A4, A5, A7, and A8?","The graph does not show any clear relationship between the computation time of Algorithms A4, A5, A7, and A8 and the number of nodes in the graph. This is likely because the number of nodes in the graph does not have a significant impact on the computation time of these algorithms.",2008.01961v3.pdf,"['2008.01961v3.pdf', '2001.09043v3.pdf', '1603.02175v1.pdf', '2011.08042v1.pdf', '2009.08716v1.pdf', '1803.06598v1.pdf']",2008.01961v3-Figure14-1.png,"Figure 14a. Computation Time with Edge Number of Algorithms A4, A5, A7 and A8","Figure 13 and Figure 14 show how the computation time is changing with node number and edge number on Algorithms A4, A5, A7, and A8, respectively. Algorithms A4, A5, A7, and A8 are the composed algorithms based on Algorithm A1 structure with MWIS approximation algorithms. They are slower than the approximation algorithms utilized, but they are still much faster than the exact MWIS algorithms. The computation time is less than 45 seconds on the test graphs. Algorithm A5 and A8 are faster than Algorithm A4 and A7, respectively. This result of computational experiments matches the conjectures in Section 6 that is the Compare Set computation is based on a smaller subgraph. And the Algorithm A7 and A8 are faster than Algorithm A4 and A5, respectively. This result also justifies that Algorithms A6 is faster than Algorithms A3 when the graph is relatively small (less than 3500 edges and","An Algorithm Framework for the Exact Solution and Improved Approximation
  of the Maximum Weighted Independent Set Problem","The Maximum Weighted Independent Set (MWIS) problem, which considers a graph
with weights assigned to nodes and seeks to discover the ""heaviest"" independent
set, that is, a set of nodes with maximum total weight so that no two nodes in
the set are connected by an edge. The MWIS problem arises in many application
domains, including the resource-constrained scheduling, error-correcting
coding, complex system analysis and optimization, and communication networks.
Since solving the MWIS problem is the core function for finding the optimum
solution of our novel graph-based formulation of the resource-constrained
Process Planning and Scheduling (PPS) problem, it is essential to have
""good-performance"" algorithms to solve the MWIS problem. In this paper, we
propose a Novel Hybrid Heuristic Algorithm (NHHA) framework in a
divide-and-conquer structure that yields optimum feasible solutions to the MWIS
problem. The NHHA framework is optimized to minimize the recurrence. Using the
NHHA framework, we also solve the All Maximal Independent Sets Listing (AMISL)
problem, which can be seen as the subproblem of the MWIS problem. Moreover,
building composed MWIS algorithms that utilizing fast approximation algorithms
with the NHHA framework is an effective way to improve the accuracy of
approximation MWIS algorithms (e.g., GWMIN and GWMIN2 (Sakai et al., 2003)).
Eight algorithms for the MWIS problem, the exact MWIS algorithm, the AMISL
algorithm, two approximation algorithms from the literature, and four composed
algorithms, are applied and tested for solving the graph-based formulation of
the resource-constrained PPS problem to evaluate the scalability, accuracy, and
robustness."
scgqa_6,2005.14165v4,How does the performance of GPT-3 change with varying counts of in-context examples in the task illustrated in Figure 1.2?,"The graph shows that the number of examples in the model's context also improves model performance. This is likely because the more examples the model has to learn from, the better it can generalize to new situations.",2005.14165v4.pdf,"['2005.14165v4.pdf', '1910.08413v1.pdf', '1311.6183v1.pdf', '1909.03961v2.pdf', '1804.04818v1.pdf', '1807.06736v1.pdf']",2005.14165v4-Figure1.2-1.png,"Figure 1.2: Larger models make increasingly efficient use of in-context information. We show in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task description (see Sec. 3.9.2). The steeper “in-context learning curves” for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks.","Figure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model’s context, K. Few-shot learning also improves dramatically with model size. Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study. We emphasize that these “learning” curves involve no gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning.",Language Models are Few-Shot Learners,"Recent work has demonstrated substantial gains on many NLP tasks and
benchmarks by pre-training on a large corpus of text followed by fine-tuning on
a specific task. While typically task-agnostic in architecture, this method
still requires task-specific fine-tuning datasets of thousands or tens of
thousands of examples. By contrast, humans can generally perform a new language
task from only a few examples or from simple instructions - something which
current NLP systems still largely struggle to do. Here we show that scaling up
language models greatly improves task-agnostic, few-shot performance, sometimes
even reaching competitiveness with prior state-of-the-art fine-tuning
approaches. Specifically, we train GPT-3, an autoregressive language model with
175 billion parameters, 10x more than any previous non-sparse language model,
and test its performance in the few-shot setting. For all tasks, GPT-3 is
applied without any gradient updates or fine-tuning, with tasks and few-shot
demonstrations specified purely via text interaction with the model. GPT-3
achieves strong performance on many NLP datasets, including translation,
question-answering, and cloze tasks, as well as several tasks that require
on-the-fly reasoning or domain adaptation, such as unscrambling words, using a
novel word in a sentence, or performing 3-digit arithmetic. At the same time,
we also identify some datasets where GPT-3's few-shot learning still struggles,
as well as some datasets where GPT-3 faces methodological issues related to
training on large web corpora. Finally, we find that GPT-3 can generate samples
of news articles which human evaluators have difficulty distinguishing from
articles written by humans. We discuss broader societal impacts of this finding
and of GPT-3 in general."
scgqa_7,1707.04849v1,What relationship does Figure 2 illustrate between learning sample length and decision risk in the paper?,"The graph shows that the risk of a wrong decision decreases as the length of the learning sample increases. This is because the learning sample provides more information about the object, which helps the maximum likelihood strategy to better estimate the unknown parameter θ.",1707.04849v1.pdf,"['1707.04849v1.pdf', '1809.02337v2.pdf', '1910.08413v1.pdf', '1803.09990v2.pdf', '1712.03538v1.pdf', '2009.06124v1.pdf', '1910.10700v1.pdf']",1707.04849v1-Figure2-1.png,"Figure 2: Example 1. Probability of a wrong decision (risk) for different sizes n of the learning sample. The curve R(qML, θ) is the risk of a maximum likelihood strategy. The curve R(qminmax, θ) is the risk of a minimax strategy. The curve min","Now let us assume that there is a sample of signals generated by an object in the second state but with higher variance 16. A maximum likelihood strategy estimates the unknown parameter θ and then makes a decision about y as if the estimated value of the parameter is its true value. Fig. 2 shows how the probability of a wrong decision (called the risk) depends on parameter θ for different sizes of the learning sample. If the learning sample is sufficiently long, the risk of maximum likelihood strategy may become arbitrarily close to the minimum possible risk. Naturally, when the length of the sample decreases the risk becomes worse and worse. Furthermore, when it becomes as small as 3 or 2 elements the risk of the maximum likelihood strategy becomes worse than the risk of the minimax strategy that uses neither the learning sample nor the signal x2 at all. Hence, it is better to ignore available additional data about the recognized object than to try to make use of it in a conventional way. It demonstrates a serious theoretical flaw of commonly used methods, and definitely not that short samples are useless. Any learning sample, no mater how long or short it is, provides some, may be not a lot information about the recognized object and a reasonable method has to use it.","Minimax deviation strategies for machine learning and recognition with
  short learning samples","The article is devoted to the problem of small learning samples in machine
learning. The flaws of maximum likelihood learning and minimax learning are
looked into and the concept of minimax deviation learning is introduced that is
free of those flaws."
scgqa_8,2008.06431v1,What does Figure 4.1 reveal about test set MSE and predictor count in your PAC-Bayes bound minimization research?,"The graph shows that as the number of predictors selected increases, the test set MSE also increases. This is because as more predictors are added to the model, the model becomes more complex and less likely to generalize well to new data.",2008.06431v1.pdf,"['2008.06431v1.pdf', '1206.6850v1.pdf', '1912.00088v1.pdf']",2008.06431v1-Figure4.1-1.png,Figure 4.1. Comparing objectives to test set MSE for Freedman’s paradox with two true predictors.,"In both variations of the experiment, we do not utilize a gradient-based algorithm for hyperparameter optimization, and instead perform stepwise forward selection on the features of a linear model. Forward selection using Eq. 1 results in a model that includes an arbitrarily large set of predictors; spuriously correlated features improve validation set goodness-of-fit. In Figure C.1a (for the first experiment) and Figure 4.1a (for the second experiment), we compare the optimization objective to test set mean-squared-error (MSE) for models selected using Eq. 1. The supposed “best” model performs poorly, and, in fact, better model performance on the validation data (as measured by the hyperparameter objective, the right axis) correlates with worse performance on out-of-sample data (test set MSE, the left axis).","Efficient hyperparameter optimization by way of PAC-Bayes bound
  minimization","Identifying optimal values for a high-dimensional set of hyperparameters is a
problem that has received growing attention given its importance to large-scale
machine learning applications such as neural architecture search. Recently
developed optimization methods can be used to select thousands or even millions
of hyperparameters. Such methods often yield overfit models, however, leading
to poor performance on unseen data. We argue that this overfitting results from
using the standard hyperparameter optimization objective function. Here we
present an alternative objective that is equivalent to a Probably Approximately
Correct-Bayes (PAC-Bayes) bound on the expected out-of-sample error. We then
devise an efficient gradient-based algorithm to minimize this objective; the
proposed method has asymptotic space and time complexity equal to or better
than other gradient-based hyperparameter optimization methods. We show that
this new method significantly reduces out-of-sample error when applied to
hyperparameter optimization problems known to be prone to overfitting."
scgqa_9,2008.07011v1,"In the context of the paper's findings, what does Figure 1 suggest regarding β and link capacity (Cl)?","The graph shows that β increases with increasing link capacity (Cl). This is because as Cl increases, the amount of data that can be transmitted per unit time increases, which in turn reduces the amount of time required to transmit a given amount of data. This results in a decrease in the latency of the system, which is reflected in the increase in β.

The graph also shows that β decreases with increasing n and QoE. This is because as n increases, the number of users in the system increases, which in turn increases the amount of data that needs to be transmitted. This results in an increase in the latency of the system, which is reflected in the decrease in β.

Similarly, as QoE increases, the quality of the video that is being transmitted increases, which in turn increases the amount of data that needs to be transmitted. This also results in an increase in the latency of the system, which is reflected in the decrease in β.

Overall, the graph shows that β is inversely proportional to link capacity (Cl), n, and QoE. This means that as any of these parameters increases, β decreases.",2008.07011v1.pdf,"['2008.07011v1.pdf', '2008.13170v1.pdf', '1703.03892v5.pdf', '2005.14165v4.pdf', '1204.5592v1.pdf', '1201.3056v1.pdf', '1311.6183v1.pdf', '1407.6074v1.pdf', '1509.01310v1.pdf']",2008.07011v1-Figure1-1.png,Fig. 1. β - Link capacity relationship,"in Section VI-A. To understand the impact of any of these parameters on β, the values of the other two parameters (controlling parameter) were kept fixed. The values of the controlling parameters for both sequences are shown in Fig. 1, 2 and 3. These figures also show the relationship between β and each of Cl, n and QoE respectively.","A Novel Traffic Rate Measurement Algorithm for QoE-Aware Video Admission
  Control","With the inevitable dominance of video traffic on the Internet, providing
perceptually good video quality is becoming a challenging task. This is partly
due to the bursty nature of video traffic, changing network conditions and
limitations of network transport protocols. This growth of video traffic has
made Quality of Experience (QoE) of the end user the focus of the research
community. In contrast, Internet service providers are concerned about
maximizing revenue by accepting as many sessions as possible, as long as
customers remain satisfied. However, there is still no entirely satisfactory
admission algorithm for flows with variable rate. The trade-off between the
number of sessions and perceived QoE can be optimized by exploiting the bursty
nature of video traffic. This paper proposes a novel algorithm to determine the
upper limit of the aggregate video rate that can exceed the available bandwidth
without degrading the QoE of accepted video sessions. A parameter $\beta$ that
defines the exceedable limit is defined. The proposed algorithm results in
accepting more sessions without compromising the QoE of on-going video
sessions. Thus it contributes to the optimization of the QoE-Session trade-off
in support of the expected growth of video traffic on the Internet."
scgqa_10,1805.01892v1,"According to Figure 9 in the research, how does connectivity distribution influence mean opinion stabilization?","The graph shows that the mean opinion converges to a value that is higher for the power law connectivity distribution than for the exponential connectivity distribution. This is because the power law distribution has a higher probability of having nodes with a high connectivity, which leads to more information being shared and a faster convergence to consensus.",1805.01892v1.pdf,"['1805.01892v1.pdf', '1911.09804v2.pdf', '1808.00136v2.pdf', '1910.05107v2.pdf', '1803.06598v1.pdf']",1805.01892v1-Figure9-1.png,"Figure 9: Test 2, Section 4.2. Time evolution of the mean opinion in the time interval [0, 100] with the two connectivity distributions given in (30). The evolution of the mean opinion in the case of independent w, c, cf. Section (2.3), is also reported for duly comparison.","threshold connectivity c. This trend is even more evident in Figure 8, which shows the time evolution of the marginal opinion distribution f(t, w), and in Figure 9, which shows the time",Opinion modeling on social media and marketing aspects,"We introduce and discuss kinetic models of opinion formation on social
networks in which the distribution function depends on both the opinion and the
connectivity of the agents. The opinion formation model is subsequently coupled
with a kinetic model describing the spreading of popularity of a product on the
web through a social network. Numerical experiments on the underlying kinetic
models show a good qualitative agreement with some measured trends of hashtags
on social media websites and illustrate how companies can take advantage of the
network structure to obtain at best the advertisement of their products."
scgqa_11,1504.07495v1,"In the context of Figure 12, what relaying techniques maximize the rate region for wireless communication?","The results in Figure 12 suggest that for a wireless communication system with a large difference in channel gains between users, it is optimal to use DF from user 1 and DT from user 2. This is because DF from user 1 and DT from user 2 are able to achieve the full rate region.",1504.07495v1.pdf,"['1504.07495v1.pdf', '2007.11391v1.pdf', '2010.08182v3.pdf', '1403.2732v1.pdf']",1504.07495v1-Figure12-1.png,Fig. 12. Case2.,"(1 + g2r2P2), then γ2 = P2 and γ1 = 0 which locates point E in the upper corner as shown in Figure 12. Then, point E is also outside the DF-DT time-shared region. Point E and C collide and they are achieved by user 1 doing full DF and user 2 doing DT. Since the rate region for this case is a rectangle as shown in Figure 12, DF from user 1 and DT from user 2 are optimal to achieve the full rate region.",Decode-Forward Transmission for the Two-Way Relay Channels,"We propose composite decode-forward (DF) schemes for the two-way relay
channel in both the full- and half-duplex modes by combining coherent relaying,
independent relaying and partial relaying strategies. For the full-duplex mode,
the relay partially decodes each user's information in each block and forwards
this partial information coherently with the source user to the destination
user in the next block as in block Markov coding. In addition, the relay
independently broadcasts a binning index of both users' decoded information
parts in the next block as in independent network coding. Each technique has a
different impact on the relay power usage and the rate region. We further
analyze in detail the independent partial DF scheme and derive in closed-form
link regimes when this scheme achieves a strictly larger rate region than just
time-sharing between its constituent techniques, direct transmission and
independent DF relaying, and when it reduces to a simpler scheme. For the
half-duplex mode, we propose a 6-phase time-division scheme that incorporates
all considered relaying techniques and uses joint decoding simultaneously over
all receiving phases. Numerical results show significant rate gains over
existing DF schemes, obtained by performing link adaptation of the composite
scheme based on the identified link regimes."
scgqa_12,1905.05284v1,What conclusions can be drawn from Figure 5 regarding the credibility of regions with the Fisher approximation?,"The graph shows that the Fisher approximation has contours that reflect those of the true posterior distribution much more accurately than the other variational methods. This is because the Fisher approximation is based on the true posterior distribution, while the other variational methods are based on approximations of the posterior distribution. As a result, the Fisher approximation is able to capture the true posterior distribution more accurately, and thus produce more accurate credible regions.",1905.05284v1.pdf,"['1905.05284v1.pdf', '1910.09592v1.pdf', '1409.2897v1.pdf', '1603.08983v6.pdf', '1307.1204v1.pdf']",1905.05284v1-Figure5-1.png,"Figure 5: Rc is the 100c% credible region based on the Fisher (red), JJ (green), and DSVI (blue) approximation, and Πy(Rc) is the corresponding posterior probability.","regions/contours, Rc, as c varies in (0, 1). If the approximation is accurate, then Rc should have posterior probability close to c, so c 7→ Πy(Rc) should be close to the 45- degree line; otherwise, the curve will be above or below the 45-degree line, depending on whether the approximation over- or under-estimates the posterior dispersion. According to Figure 5, our proposed Fisher approximation has contours that reflect those of the true posterior distribution much more accurately than the other variational methods. And our high-quality approximation of the true posterior is achieved in just a matter of seconds— compared to minutes for MCMC—which is only slightly slower than the very-fast but less-accurate JJ method.",Variational approximations using Fisher divergence,"Modern applications of Bayesian inference involve models that are
sufficiently complex that the corresponding posterior distributions are
intractable and must be approximated. The most common approximation is based on
Markov chain Monte Carlo, but these can be expensive when the data set is large
and/or the model is complex, so more efficient variational approximations have
recently received considerable attention. The traditional variational methods,
that seek to minimize the Kullback--Leibler divergence between the posterior
and a relatively simple parametric family, provide accurate and efficient
estimation of the posterior mean, but often does not capture other moments, and
have limitations in terms of the models to which they can be applied. Here we
propose the construction of variational approximations based on minimizing the
Fisher divergence, and develop an efficient computational algorithm that can be
applied to a wide range of models without conjugacy or potentially unrealistic
mean-field assumptions. We demonstrate the superior performance of the proposed
method for the benchmark case of logistic regression."
scgqa_13,1907.06845v5,"In Figure 6, how does the EM algorithm perform with the continuous Bernoulli likelihood compared to the Bernoulli likelihood?","The graph shows that the EM algorithm performs best when using the correct continuous Bernoulli likelihood. When using the B likelihood, the EM algorithm performs worse, and this performance decreases as the number of mixture components K increases. When using the B likelihood plus a µ−1 correction, the EM algorithm performs better than when using the B likelihood alone, but still not as well as when using the correct continuous Bernoulli likelihood.",1907.06845v5.pdf,"['1907.06845v5.pdf', '1906.07610v2.pdf', '2001.09043v3.pdf', '2006.04002v2.pdf', '2010.08182v3.pdf', '1511.04338v2.pdf', '1612.03449v3.pdf', '1809.07412v2.pdf']",1907.06845v5-Figure6-1.png,"Figure 6: Bias of the EM algorithm to estimate CB parameters when using a CB likelihood (dark blue), B likelihood (light blue) and a B likelihood plus a µ−1 correction (light red).","The results of performing the procedure described above 10 times and averaging the KL values (over these 10 repetitions), along with standard errors, are shown in Figure 6. First, we can see that when using the correct continuous Bernoulli likelihood, the EM algorithm correctly recovers the true distribution. We can also see that, as the number of mixture components K gets larger, ignoring the normalizing constant results in worse performance, even after correcting with µ−1, which helps but does not fully remedy the situation (except at K = 1, as noted in §4.4).","The continuous Bernoulli: fixing a pervasive error in variational
  autoencoders","Variational autoencoders (VAE) have quickly become a central tool in machine
learning, applicable to a broad range of data types and latent variable models.
By far the most common first step, taken by seminal papers and by core software
libraries alike, is to model MNIST data using a deep network parameterizing a
Bernoulli likelihood. This practice contains what appears to be and what is
often set aside as a minor inconvenience: the pixel data is [0,1] valued, not
{0,1} as supported by the Bernoulli likelihood. Here we show that, far from
being a triviality or nuisance that is convenient to ignore, this error has
profound importance to VAE, both qualitative and quantitative. We introduce and
fully characterize a new [0,1]-supported, single parameter distribution: the
continuous Bernoulli, which patches this pervasive bug in VAE. This
distribution is not nitpicking; it produces meaningful performance improvements
across a range of metrics and datasets, including sharper image samples, and
suggests a broader class of performant VAE."
scgqa_14,2002.11440v1,"In Figure 2, how do ZRSG and ZRSQN algorithms perform with unbiased compared to biased gradient/Hessian during nonconvex SVM optimization?","The graph shows that the ZRSG and ZRSQN algorithms with unbiased gradient/Hessian information outperform the other algorithms. This is because unbiased gradient/Hessian information provides a more accurate estimate of the gradient and Hessian, which leads to better convergence.",2002.11440v1.pdf,"['2002.11440v1.pdf', '2001.07829v1.pdf', '1405.5364v2.pdf', '2005.09814v3.pdf', '1803.10225v1.pdf', '1504.07495v1.pdf']",2002.11440v1-Figure2-1.png,"Figure 2: Evolution of the SNG as the iteration limit is varied, for the ZRSG and ZRSQN algorithm under the non-convex SVM problem.","Figure 2a presents the SNG at xR for the ZRSG and ZRSQN algorithms with unbiased and biased gradients/Hessian for the nonconvex SVM problem (57) on the heart disease data set, while Figure 2b compares the same algorithms on the banknote authentication data set. As expected, ZRSG/ZRSQN algorithms with unbiased gradient/Hessian information outperform the other algorithms. Among the algorithms using both (biased) gradient/Hessian information, 2RDSA-Perm-DP performed best, while GS outperformed other algorithm that use gradients, on both datasets. For a given estimation method, for instance, Perm-DP, we observe that the quasi-Newton ZRSQN variant outperforms the gradient RSG variant.","Non-asymptotic bounds for stochastic optimization with biased noisy
  gradient oracles","We introduce biased gradient oracles to capture a setting where the function
measurements have an estimation error that can be controlled through a batch
size parameter. Our proposed oracles are appealing in several practical
contexts, for instance, risk measure estimation from a batch of independent and
identically distributed (i.i.d.) samples, or simulation optimization, where the
function measurements are `biased' due to computational constraints. In either
case, increasing the batch size reduces the estimation error. We highlight the
applicability of our biased gradient oracles in a risk-sensitive reinforcement
learning setting. In the stochastic non-convex optimization context, we analyze
a variant of the randomized stochastic gradient (RSG) algorithm with a biased
gradient oracle. We quantify the convergence rate of this algorithm by deriving
non-asymptotic bounds on its performance. Next, in the stochastic convex
optimization setting, we derive non-asymptotic bounds for the last iterate of a
stochastic gradient descent (SGD) algorithm with a biased gradient oracle."
scgqa_15,2005.14165v4,What does the data in Figure 1.2 suggest about the influence of model size and examples on performance?,The graph shows that the general trends with both model size and number of examples in-context hold for most tasks we study. This suggests that these factors are important for improving few-shot learning performance.,2005.14165v4.pdf,"['2005.14165v4.pdf', '2007.06852v1.pdf', '1805.00184v1.pdf', '2004.03870v1.pdf', '1706.03019v1.pdf']",2005.14165v4-Figure1.2-1.png,"Figure 1.2: Larger models make increasingly efficient use of in-context information. We show in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task description (see Sec. 3.9.2). The steeper “in-context learning curves” for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks.","Figure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model’s context, K. Few-shot learning also improves dramatically with model size. Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study. We emphasize that these “learning” curves involve no gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning.",Language Models are Few-Shot Learners,"Recent work has demonstrated substantial gains on many NLP tasks and
benchmarks by pre-training on a large corpus of text followed by fine-tuning on
a specific task. While typically task-agnostic in architecture, this method
still requires task-specific fine-tuning datasets of thousands or tens of
thousands of examples. By contrast, humans can generally perform a new language
task from only a few examples or from simple instructions - something which
current NLP systems still largely struggle to do. Here we show that scaling up
language models greatly improves task-agnostic, few-shot performance, sometimes
even reaching competitiveness with prior state-of-the-art fine-tuning
approaches. Specifically, we train GPT-3, an autoregressive language model with
175 billion parameters, 10x more than any previous non-sparse language model,
and test its performance in the few-shot setting. For all tasks, GPT-3 is
applied without any gradient updates or fine-tuning, with tasks and few-shot
demonstrations specified purely via text interaction with the model. GPT-3
achieves strong performance on many NLP datasets, including translation,
question-answering, and cloze tasks, as well as several tasks that require
on-the-fly reasoning or domain adaptation, such as unscrambling words, using a
novel word in a sentence, or performing 3-digit arithmetic. At the same time,
we also identify some datasets where GPT-3's few-shot learning still struggles,
as well as some datasets where GPT-3 faces methodological issues related to
training on large web corpora. Finally, we find that GPT-3 can generate samples
of news articles which human evaluators have difficulty distinguishing from
articles written by humans. We discuss broader societal impacts of this finding
and of GPT-3 in general."
scgqa_16,1804.03842v1,"In the context of OLCPM and SocioPatterns, why is a stable algorithm essential for understanding collaborations?","A stable algorithm for community detection is important in collaboration networks because it can help to identify groups of people who are working together on similar projects. This information can be used to improve collaboration and productivity within organizations. Additionally, a stable algorithm can help to identify potential conflicts or problems within a collaboration network.",1804.03842v1.pdf,"['1804.03842v1.pdf', '1212.3950v3.pdf', '1804.10488v2.pdf', '1908.09653v1.pdf', '1409.2897v1.pdf', '1408.5389v1.pdf', '1610.01283v4.pdf', '1608.06005v1.pdf']",1804.03842v1-Figure10-1.png,Figure 10: NMI values of OLCPM and CPM [13] for k = 3 and k = 4 in on SocioPatterns collaboration networks [31].,"(Communities yielded by DyCPM and OCPM are identical). Then, for each snapshot, we compute the NMI according to [30]. Results are displayed in Figure 10. We show results for k=3 and k=4, which yield the best results.","OLCPM: An Online Framework for Detecting Overlapping Communities in
  Dynamic Social Networks","Community structure is one of the most prominent features of complex
networks. Community structure detection is of great importance to provide
insights into the network structure and functionalities. Most proposals focus
on static networks. However, finding communities in a dynamic network is even
more challenging, especially when communities overlap with each other. In this
article , we present an online algorithm, called OLCPM, based on clique
percolation and label propagation methods. OLCPM can detect overlapping
communities and works on temporal networks with a fine granularity. By locally
updating the community structure, OLCPM delivers significant improvement in
running time compared with previous clique percolation techniques. The
experimental results on both synthetic and real-world networks illustrate the
effectiveness of the method."
scgqa_17,2008.06431v1,"In terms of model performance shown in Figure 4.1, what correlation exists between hyperparameter objective and test set MSE?","The graph shows that the hyperparameter objective is not a good predictor of test set MSE. This is because the hyperparameter objective is only based on the validation data, which is not representative of the true distribution of data. As a result, the hyperparameter objective can be misleading and can lead to the selection of models that perform poorly on out-of-sample data.",2008.06431v1.pdf,"['2008.06431v1.pdf', '1703.07020v4.pdf', '1809.01628v1.pdf', '1511.07907v2.pdf', '2002.06199v1.pdf', '1101.0235v1.pdf', '1609.06577v1.pdf', '1803.01118v2.pdf']",2008.06431v1-Figure4.1-1.png,Figure 4.1. Comparing objectives to test set MSE for Freedman’s paradox with two true predictors.,"In both variations of the experiment, we do not utilize a gradient-based algorithm for hyperparameter optimization, and instead perform stepwise forward selection on the features of a linear model. Forward selection using Eq. 1 results in a model that includes an arbitrarily large set of predictors; spuriously correlated features improve validation set goodness-of-fit. In Figure C.1a (for the first experiment) and Figure 4.1a (for the second experiment), we compare the optimization objective to test set mean-squared-error (MSE) for models selected using Eq. 1. The supposed “best” model performs poorly, and, in fact, better model performance on the validation data (as measured by the hyperparameter objective, the right axis) correlates with worse performance on out-of-sample data (test set MSE, the left axis).","Efficient hyperparameter optimization by way of PAC-Bayes bound
  minimization","Identifying optimal values for a high-dimensional set of hyperparameters is a
problem that has received growing attention given its importance to large-scale
machine learning applications such as neural architecture search. Recently
developed optimization methods can be used to select thousands or even millions
of hyperparameters. Such methods often yield overfit models, however, leading
to poor performance on unseen data. We argue that this overfitting results from
using the standard hyperparameter optimization objective function. Here we
present an alternative objective that is equivalent to a Probably Approximately
Correct-Bayes (PAC-Bayes) bound on the expected out-of-sample error. We then
devise an efficient gradient-based algorithm to minimize this objective; the
proposed method has asymptotic space and time complexity equal to or better
than other gradient-based hyperparameter optimization methods. We show that
this new method significantly reduces out-of-sample error when applied to
hyperparameter optimization problems known to be prone to overfitting."
scgqa_18,1511.07907v2,How does the graph in Fig. 10 reflect the interaction between best price offset and competitor's pricing strategy?,"The graph shows that the best price offset, i.e., Bi(pj)) − pj (i 6= j), is strictly decreasing with its competitor's price. This means that as the competitor's price increases, the best price offset decreases. This is because the charging station will want to offer a lower price than its competitor in order to attract more customers.",1511.07907v2.pdf,"['1511.07907v2.pdf', '1804.10488v2.pdf', '2011.03519v1.pdf', '1703.03892v5.pdf', '1707.04849v1.pdf', '1203.1203v2.pdf', '1610.01283v4.pdf', '1502.03556v1.pdf']",1511.07907v2-Figure10-1.png,"Fig. 10: Best price offsets with µ1 = 16, µ2 = 14 and pi ∈ [0.25, 0.3].","We first illustrate the conditions in Theorem 6. Let us consider the FULL-FULL scenario with µ1 = 16, µ2 = 14, and pi ∈ [0.25, 0.3]. Fig. 10 and Fig. 11 show the best price offset and the best response, respectively. According to Fig. 10, the best price offset, i.e., Bi(pj)) − pj (i 6= j), is strictly decreasing with its competitor’s price. As shown in Fig. 11, the best response curves of charging station 1 and 2 are both increasing.",Competitive Charging Station Pricing for Plug-in Electric Vehicles,"This paper considers the problem of charging station pricing and plug-in
electric vehicles (PEVs) station selection. When a PEV needs to be charged, it
selects a charging station by considering the charging prices, waiting times,
and travel distances. Each charging station optimizes its charging price based
on the prediction of the PEVs' charging station selection decisions and the
other station's pricing decision, in order to maximize its profit. To obtain
insights of such a highly coupled system, we consider a one-dimensional system
with two competing charging stations and Poisson arriving PEVs. We propose a
multi-leader-multi-follower Stackelberg game model, in which the charging
stations (leaders) announce their charging prices in Stage I, and the PEVs
(followers) make their charging station selections in Stage II. We show that
there always exists a unique charging station selection equilibrium in Stage
II, and such equilibrium depends on the charging stations' service capacities
and the price difference between them. We then characterize the sufficient
conditions for the existence and uniqueness of the pricing equilibrium in Stage
I. We also develop a low complexity algorithm that efficiently computes the
pricing equilibrium and the subgame perfect equilibrium of the two-stage
Stackelberg game."
scgqa_19,1911.11395v2,"In the context of the experiment's findings, how do different study conditions compare in generating unique ideas?","The graph suggests that the dynamic and static conditions are more effective than the solo condition in terms of generating non-redundant ideas and novelty ratings. This is likely because the dynamic and static conditions provide participants with more opportunities to interact with each other and share ideas, which can lead to more creative outcomes.",1911.11395v2.pdf,"['1911.11395v2.pdf', '1512.02567v1.pdf', '1810.04824v1.pdf', '1309.3959v1.pdf', '1906.02003v1.pdf', '1701.08947v1.pdf']",1911.11395v2-Figure7-1.png,"Figure 7: Individual-level comparisons of (A) cumulative non-redundant idea counts, (B) average novelty ratings and (C) cumulative creativity quotients among various study conditions. The static and dynamic conditions significantly outperformed the solo condition in the total number of nonredundant ideas (2-tailed test; dynamic vs solo: t(142) = 2.7, p < 0.03; static vs solo: t(142) = 2.898, p < 0.02). Whiskers denote 95% C.I. *Bonferroni-corrected p < 0.05.","Individual creative performance comparisons among various study conditions. We analyze the individual creative performances in various study conditions. The participants in both the dynamic and static conditions significantly outperformed the solo participants in the total number of nonredundant ideas (2-tailed test; dynamic nd = 108, static ns = 108, solo nc = 36; dynamic (d) vs solo (c): t(142) = 2.7, p < 0.03; static (s) vs solo (c): t(142) = 2.898, p < 0.02; all pvalues Bonferroni-corrected; Figure 7A, SI Table S7). However, there was no significant difference between the dynamic and static conditions (2-tailed test, p > 0.05).","Creativity in temporal social networks: How divergent thinking is
  impacted by one's choice of peers","Creativity is viewed as one of the most important skills in the context of
future-of-work. In this paper, we explore how the dynamic (self-organizing)
nature of social networks impacts the fostering of creative ideas. We run 6
trials (N=288) of a web-based experiment involving divergent ideation tasks. We
find that network connections gradually adapt to individual creative
performances, as the participants predominantly seek to follow high-performing
peers for creative inspirations. We unearth both opportunities and bottlenecks
afforded by such self-organization. While exposure to high-performing peers is
associated with better creative performances of the followers, we see a
counter-effect that choosing to follow the same peers introduces semantic
similarities in the followers' ideas. We formulate an agent-based simulation
model to capture these intuitions in a tractable manner, and experiment with
corner cases of various simulation parameters to assess the generality of the
findings. Our findings may help design large-scale interventions to improve the
creative aptitude of people interacting in a social network."
scgqa_20,1709.08441v4,How does the figure illustrate the relationship between uncertainty and Nash equilibrium quality in this research?,The graph shows that the price of anarchy decreases as the level of uncertainty increases. This means that uncertainty helps to improve the quality of the Nash equilibrium.,1709.08441v4.pdf,"['1709.08441v4.pdf', '1607.05970v2.pdf', '1906.02003v1.pdf']",1709.08441v4-Figure1-1.png,Fig. 1. Price of anarchy as a function of rmax = maxθ rθ for multicommodity selfish routing games for three different values of γ = minθ rθ,"Before proving Theorem 2, we remark on the price of anarchy and its dependence on the level of uncertainty. The price of anarchy in (14) is plotted in Fig. 1 as a function of rmax for three different values of γ. This result validates our message that uncertainty helps equilibrium quality when users over-estimate their costs and hurts equilibrium quality when users under-estimate their costs. To understand why, let us first consider the case of γ = 1—i.e. the uncertainty is the same across user types. We already know that in the absence of uncertainty, the price of anarchy of multi-commodity routing games with linear costs is given by 4/3 [22]; this can also be seen by substituting rmax = 1, γ = 1 in (14). We observe that the price of anarchy is strictly smaller than 4/3 for rmax ≤ 2 and reaches the optimum value of 1 at rmax = 2 thereby confirming Corollary 1. More interestingly, even when rmax > 2, the price of anarchy with uncertainty is smaller than that without uncertainty, affirming our previous statement that cautious behavior helps lower congestion (at least in the worst case).",Uncertainty in Multi-Commodity Routing Networks: When does it help?,"We study the equilibrium behavior in a multi-commodity selfish routing game
with many types of uncertain users where each user over- or under-estimates
their congestion costs by a multiplicative factor. Surprisingly, we find that
uncertainties in different directions have qualitatively distinct impacts on
equilibria. Namely, contrary to the usual notion that uncertainty increases
inefficiencies, network congestion actually decreases when users over-estimate
their costs. On the other hand, under-estimation of costs leads to increased
congestion. We apply these results to urban transportation networks, where
drivers have different estimates about the cost of congestion. In light of the
dynamic pricing policies aimed at tackling congestion, our results indicate
that users' perception of these prices can significantly impact the policy's
efficacy, and ""caution in the face of uncertainty"" leads to favorable network
conditions."
scgqa_21,1603.02175v1,"According to Figure 3, how does the number of communicating days impact interest similarity in the findings of this research?","The graph shows that there is a positive correlation between interest similarity and monthly qq message count and number of monthly communicating days. This means that users who interact more frequently and have more monthly communicating days are more likely to share similar interests. This is likely because people who interact more frequently have more opportunities to learn about each other's interests and hobbies, and people who have more monthly communicating days are more likely to have similar lifestyles and values.",1603.02175v1.pdf,"['1603.02175v1.pdf', '1603.01185v2.pdf', '1502.00588v1.pdf', '2003.00870v1.pdf', '1610.04213v4.pdf', '1908.04647v1.pdf', '2008.02777v1.pdf', '1805.00184v1.pdf', '1905.12868v5.pdf']",1603.02175v1-Figure3-1.png,Figure 3: Interest similarity vs. monthly qq message count and number of monthly communicating days.,"interaction frequency, as shown in Fig. 3. Although the fitted curves4 in the RTP case is not the same as that for PTP, they both show that users with higher intensity of interaction share more interests, particularly when the interaction intensity is very large. Also, interest similarity increases more sharply when number of communicating days is small. Interestingly, interaction frequency is more correlated with interest similarity than interaction intensity, implying that some casual or transactional interaction could also have a large intensity.","Who are Like-minded: Mining User Interest Similarity in Online Social
  Networks","In this paper, we mine and learn to predict how similar a pair of users'
interests towards videos are, based on demographic (age, gender and location)
and social (friendship, interaction and group membership) information of these
users. We use the video access patterns of active users as ground truth (a form
of benchmark). We adopt tag-based user profiling to establish this ground
truth, and justify why it is used instead of video-based methods, or many
latent topic models such as LDA and Collaborative Filtering approaches. We then
show the effectiveness of the different demographic and social features, and
their combinations and derivatives, in predicting user interest similarity,
based on different machine-learning methods for combining multiple features. We
propose a hybrid tree-encoded linear model for combining the features, and show
that it out-performs other linear and treebased models. Our methods can be used
to predict user interest similarity when the ground-truth is not available,
e.g. for new users, or inactive users whose interests may have changed from old
access data, and is useful for video recommendation. Our study is based on a
rich dataset from Tencent, a popular service provider of social networks, video
services, and various other services in China."
scgqa_22,1504.01124v3,What trend is observed in processing time relative to processor count in Figure 4 of the multi-object tracking paper?,"The graph shows that the processing time decreases as the number of processors increases. This is because the parallel implementation of the algorithm allows for the processing of multiple nodes simultaneously, which reduces the overall time required to complete the task.",1504.01124v3.pdf,"['1504.01124v3.pdf', '1909.00392v1.pdf', '1610.08332v1.pdf', '2004.05448v1.pdf']",1504.01124v3-Figure4-1.png,"Fig. 4. Processing time and speed-up factors for different number of processors (P = 1 to P = 4) of the TUD Stadtmitte (top row) and PETS (bottom row) datasets. For each case, we perform 10 runs of the algorithm which are drawn with the same color.","To assess the advantages offered by the parallel implementation, we consider a simple scheduling strategy, which directly follows the non-interference condition (see Section 3.2.2) and selects the nodes at random. For each number of processor, we ran the algorithm 10 times and noted the evolution of objective function. The results are depicted in Figure 4. The reported time is different from Figure 3 because of the fact that the parallel implementation is done in C++. Although the parallel implementation decreases the computational time, we observe that the reduction is not proportional to the degree of parallelism. This sub-optimal speed-up factor is due to the fact that we run the algorithm in batches of nodes. As a consequence, the time required to process a batch is governed by the longest time taken by one of its nodes. The algorithm for node-selection strategy and the distribution of time taken by nodes in the batch are presented in the supplementary material.","Discriminative and Efficient Label Propagation on Complementary Graphs
  for Multi-Object Tracking","Given a set of detections, detected at each time instant independently, we
investigate how to associate them across time. This is done by propagating
labels on a set of graphs, each graph capturing how either the spatio-temporal
or the appearance cues promote the assignment of identical or distinct labels
to a pair of detections. The graph construction is motivated by a locally
linear embedding of the detection features. Interestingly, the neighborhood of
a node in appearance graph is defined to include all the nodes for which the
appearance feature is available (even if they are temporally distant). This
gives our framework the uncommon ability to exploit the appearance features
that are available only sporadically. Once the graphs have been defined,
multi-object tracking is formulated as the problem of finding a label
assignment that is consistent with the constraints captured each graph, which
results into a difference of convex (DC) program. We propose to decompose the
global objective function into node-wise sub-problems. This not only allows a
computationally efficient solution, but also supports an incremental and
scalable construction of the graph, thereby making the framework applicable to
large graphs and practical tracking scenarios. Moreover, it opens the
possibility of parallel implementation."
scgqa_23,1802.02193v1,What trend is illustrated regarding BS density and distribution accuracy in the results shown in Fig. 3?,"The graph shows that the accuracy of the distributions increases as the BS density decreases. This is because as the BS density decreases, the probability that a Voronoi cell includes or is included by the achievable range increases. This means that fN1(n) and fN2(n) are more likely to approach the true distribution fN(n).",1802.02193v1.pdf,"['1802.02193v1.pdf', '1402.7063v1.pdf', '1803.11512v1.pdf']",1802.02193v1-Figure3-1.png,"Fig. 3: Probabilities indicating validity of distributions when Pu = 23 dBm, ρo = −70 dBm, and α = 4.","fN(n) in (5) is well approximated by fN2(n) if λBS is sufficiently small so that g2(λBS) ≃ 1. Two probabilities indicating the validity of the distributions are shown in Fig. 3. We show the range of the BS density in which either probability exceeds 0.9, which means more than 90% of the Voronoi cells includes or is included by the achievable range, and therefore that either fN1(n) or fN2(n) would approach the true distribution fN(n). In the simulation part, we show that the Monte Carlo simulation result is well approximated by the analytical result for the BS density at which the corresponding probability shown in Fig. 3 is approximately equal to 0.9. Obtaining the distribution","Asymptotic Analysis of Normalized SNR-Based Scheduling in Uplink
  Cellular Networks with Truncated Channel Inversion Power Control","This paper provides the signal-to-interference-plus-noise ratio (SINR)
complimentary cumulative distribution function (CCDF) and average data rate of
the normalized SNR-based scheduling in an uplink cellular network using
stochastic geometry. The uplink analysis is essentially different from the
downlink analysis in that the per-user transmit power control is performed and
that the interferers are composed of at most one transmitting user in each cell
other than the target cell. In addition, as the effect of multi-user diversity
varies from cell to cell depending on the number of users involved in the
scheduling, the distribution of the number of users is required to obtain the
averaged performance of the scheduling. This paper derives the SINR CCDF
relative to the typical scheduled user by focusing on two incompatible cases,
where the scheduler selects a user from all the users in the corresponding
Voronoi cell or does not select users near cell edges. In each case, the SINR
CCDF is marginalized over the distribution of the number of users involved in
the scheduling, which is asymptotically correct if the BS density is
sufficiently large or small. Through the simulations, the accuracies of the
analytical results are validated for both cases, and the scheduling gains are
evaluated to confirm the multi-user diversity gain."
scgqa_24,1803.03080v1,What insights does the cepstrum provide about fault detection in the continuous stirred tank reactor experiment?,"The graph shows that the cepstrum is able to detect hidden faults in a process, without having to model the process. This is because the cepstrum captures the dynamics of the processes and controller involved, and can therefore detect changes in the process that are not captured by the controller.",1803.03080v1.pdf,"['1803.03080v1.pdf', '1808.08442v1.pdf', '2008.13170v1.pdf', '2006.09358v2.pdf', '2010.13032v1.pdf']",1803.03080v1-Figure2-1.png,"Fig. 2: The cepstrum coefficients for input, output and process. We show both normal and faulty operating behaviour, with (CL) and without (OL) the controller. The cepstral coefficients for the process in (c) clearly show different fingerprints for the different cases. The normal operating behaviour changes only slightly when turning on the controller, capturing the extra dynamics of the controller. However, faulty operating behaviour results in a deviation from the normal operating behaviour, both in OL and, notably, in CL, capturing faults hidden by the controller. We can see the different contributions to this process cepstrum (see Equation (25)) in (a) and (b): (a) shows the change in the input dynamics in the faulty regime, when the controller starts compensating. (b) shows the change in the output in the faulty regime when the controller is turned off.","−6). We estimate the cepstra of input, output and of the process itself and show that the MIMO cepstrum indeed captures the dynamics of the processes and controller involved. The 200 data points around the fault are omitted. Results are shown and discussed in Fig. 2a for the input signals, Fig. 2b for the output signals and Fig. 2c for the process dynamics. Notice that the cepstrum of the process explicitly shows the change in the reactor dynamics, whether the controller is on or off, and detects hidden faults in a process, without having to model the process.",A Multiple-Input Multiple-Output Cepstrum,"This paper extends the concept of scalar cepstrum coefficients from
single-input single-output linear time invariant dynamical systems to
multiple-input multiple-output models, making use of the Smith-McMillan form of
the transfer function. These coefficients are interpreted in terms of poles and
transmission zeros of the underlying dynamical system. We present a method to
compute the MIMO cepstrum based on input/output signal data for systems with
square transfer function matrices (i.e. systems with as many inputs as
outputs). This allows us to do a model-free analysis. Two examples to
illustrate these results are included: a simple MIMO system with 3 inputs and 3
outputs, of which the poles and zeros are known exactly, that allows us to
directly verify the equivalences derived in the paper, and a case study on
realistic data. This case study analyses data coming from a (model of) a
non-isothermal continuous stirred tank reactor, which experiences linear
fouling. We analyse normal and faulty operating behaviour, both with and
without a controller present. We show that the cepstrum detects faulty
behaviour, even when hidden by controller compensation. The code for the
numerical analysis is available online."
scgqa_25,1306.4036v2,"In the context of Figure 4, what trend is observed regarding α and quantization levels as shown in the research?","The graph shows that as α increases, the network performs better with coarser quantization levels. This is because with higher α, the network is more likely to be attacked by a Byzantine attacker, so it needs to use coarser quantization levels to filter out the noise and make it more difficult for the attacker to succeed.",1306.4036v2.pdf,"['1306.4036v2.pdf', '1704.03458v1.pdf', '1710.09234v1.pdf', '1710.10733v4.pdf', '1208.2451v1.pdf', '1804.00243v2.pdf', '2004.01867v1.pdf', '1409.2897v1.pdf', '1502.03556v1.pdf']",1306.4036v2-Figure4-1.png,"Fig. 4: Contribution of a sensor to the overall conditional FI at the FC as a function ofα, for different number of quantization levels whenθ = 0 and A = 2. The pentagrams on the x-axis correspond to theαblind for 1-bit, 2-bit, 3-bit and 4-bit quantizations respectively from left to right.","Figure 4 plots the conditional FI corresponding to one sensor, for different values of α and M , when the uniform quantizer is centered around the true value of θ. Note that as SNR increases (σ → 0), we observe that it is better for the network to perform as much finer quantization as possible to mitigate the Byzantine attackers. On the other hand, if SNR is low, coarse quantization performs better for lower values of α. This phenomenon of coarse quantization performing better under low SNR scenarios, can be attributed to the fact that more noise gets filtered as the quantization gets coarser (decreasing M ) than the signal itself. On the other hand, in the case of high SNR, since the signal level is high, coarse quantization cancels out the signal component significantly, thereby resulting in a degradation in performance.","Distributed Inference with M-ary Quantized Data in the Presence of
  Byzantine Attacks","The problem of distributed inference with M-ary quantized data at the sensors
is investigated in the presence of Byzantine attacks. We assume that the
attacker does not have knowledge about either the true state of the phenomenon
of interest, or the quantization thresholds used at the sensors. Therefore, the
Byzantine nodes attack the inference network by modifying modifying the symbol
corresponding to the quantized data to one of the other M symbols in the
quantization alphabet-set and transmitting the false symbol to the fusion
center (FC). In this paper, we find the optimal Byzantine attack that blinds
any distributed inference network. As the quantization alphabet size increases,
a tremendous improvement in the security performance of the distributed
inference network is observed.
  We also investigate the problem of distributed inference in the presence of
resource-constrained Byzantine attacks. In particular, we focus our attention
on two problems: distributed detection and distributed estimation, when the
Byzantine attacker employs a highly-symmetric attack. For both the problems, we
find the optimal attack strategies employed by the attacker to maximally
degrade the performance of the inference network. A reputation-based scheme for
identifying malicious nodes is also presented as the network's strategy to
mitigate the impact of Byzantine threats on the inference performance of the
distributed sensor network."
scgqa_26,2004.14564v2,How does the average H(sys|ref) vary between Prism and ParaBank 2 regarding low sentBLEU outputs in WMT19?,"The graph shows that the Prism model has a higher average H(sys|ref) score than the ParaBank 2 model for system outputs with low lexical difference. This is likely because the Prism model is more likely to generate paraphrases that are syntactically similar to the input, while the ParaBank 2 model is more likely to generate paraphrases that are semantically similar to the input.",2004.14564v2.pdf,"['2004.14564v2.pdf', '1610.08534v1.pdf', '1006.3688v1.pdf']",2004.14564v2-Figure2-1.png,"Figure 2: Average H(sys|ref) as a function of average lexical difference (as measured by sentBLEU) for every English (sys, ref) pair submitted to WMT19, for both the Prism and ParaBank 2 paraphrasers. (sys, ref) pairs are split into 10 sentBLEU bins of uniform width. Fraction of total data in each bin is shown on x-axis (in parentheses).","We find H(sys|ref) increases monotonically with sentBLEU for the Prism model, but the model trained on ParaBank 2 has nearly the same scores for output with sentBLEU in the range of 60 to 100; however that range accounts for only about 8.5% of all system outputs (see Figure 2). We find that a copy of the input is almost as probable as beam search output for the Prism model. In contrast, the","Automatic Machine Translation Evaluation in Many Languages via Zero-Shot
  Paraphrasing","We frame the task of machine translation evaluation as one of scoring machine
translation output with a sequence-to-sequence paraphraser, conditioned on a
human reference. We propose training the paraphraser as a multilingual NMT
system, treating paraphrasing as a zero-shot translation task (e.g., Czech to
Czech). This results in the paraphraser's output mode being centered around a
copy of the input sequence, which represents the best case scenario where the
MT system output matches a human reference. Our method is simple and intuitive,
and does not require human judgements for training. Our single model (trained
in 39 languages) outperforms or statistically ties with all prior metrics on
the WMT 2019 segment-level shared metrics task in all languages (excluding
Gujarati where the model had no training data). We also explore using our model
for the task of quality estimation as a metric--conditioning on the source
instead of the reference--and find that it significantly outperforms every
submission to the WMT 2019 shared task on quality estimation in every language
pair."
scgqa_27,1311.1567v3,What parameter influences normalized transmit power in frequency-selective channels as shown in Fig. 9 of the paper?,"The graph shows the normalized per-BS transmit power required for TOA-based localization in frequency-selective channels. The normalized power is defined as the ratio of the transmit power required for TOA-based localization to the transmit power required for conventional TDD systems. The number of blocks NC is the number of blocks used for TOA-based localization. The constraints R and Q are the maximum number of reflections and the maximum angle spread, respectively. The values of R and Q are chosen to be 3 and (0.3δ)2, respectively. The values of M, NB , and NM are the number of BSs, the number of BS antennas, and the number of MSs, respectively. The value of N is the number of subcarriers. The graph shows that the normalized per-BS transmit power decreases as the number of blocks NC increases. This is because as the number of blocks NC increases, the number of reflections that need to be estimated decreases. This results in a reduction in the transmit power required for TOA-based localization.",1311.1567v3.pdf,"['1311.1567v3.pdf', '1006.3688v1.pdf', '1603.01185v2.pdf', '1207.5027v1.pdf', '1207.3107v3.pdf', '1910.04573v3.pdf', '1902.03993v2.pdf']",1311.1567v3-Figure9-1.png,"Fig. 9. Normalized per-BS transmit power with TOA-based loca ization for frequency-selective channels as a function of the number of blocksNC for (M,NB , NM ) = (4, 4, 2) with N = 32 and constraintsR = 3 andQ = (0.3δ)2.","For frequency-selective channels, we assume that all BS-MS pairs have three multi-paths, i.e., Li = 3. Moreover, we model the each lth path between BS j and MS i as hji,l = |hji,l|sji(φji + ǫφji,l), where hji,l for l = {0, 1, 2} are complex-valued zero-mean Gaussian random variables with exponential power decay and ǫφji,l is a random angle uniformly distributed over the interval [−10, 10] (degrees). Fig. 9 shows the required transmit power as a","Beamforming Design for Joint Localization and Data Transmission in
  Distributed Antenna System","A distributed antenna system is studied whose goal is to provide data
communication and positioning functionalities to Mobile Stations (MSs). Each MS
receives data from a number of Base Stations (BSs), and uses the received
signal not only to extract the information but also to determine its location.
This is done based on Time of Arrival (TOA) or Time Difference of Arrival
(TDOA) measurements, depending on the assumed synchronization conditions. The
problem of minimizing the overall power expenditure of the BSs under data
throughput and localization accuracy requirements is formulated with respect to
the beamforming vectors used at the BSs. The analysis covers both
frequency-flat and frequency-selective channels, and accounts also for
robustness constraints in the presence of parameter uncertainty. The proposed
algorithmic solutions are based on rank-relaxation and Difference-of-Convex
(DC) programming."
scgqa_28,1804.04818v1,What observation about aggregate gap in relation to DBS number is presented in the paper's Fig. 4?,"The graph shows that the aggregate gap decreases as the number of DBSs increases. This is because as more DBSs are used, the communication rates of the terminals are closer to the target rate. This is because the DBSs can provide more resources to the terminals, which allows them to communicate at higher rates.",1804.04818v1.pdf,"['1804.04818v1.pdf', '1402.1892v2.pdf', '1905.12868v5.pdf']",1804.04818v1-Figure4-1.png,Fig. 4. Plot of the aggregate gap achieved through the proposed scheme vs. the number of DBSs.,"Next, Fig. 4 illustrates the aggregate gap between the communication rates and the target rate of terminals with respect to the change in the number of utilized DBSs. One important observation we can make from Fig. 4 is that it identifies the minimum number of DBSs that is need for all the terminals to satisfy the target rate. This implies that through a parallel execution of the proposed algorithm with different number of DBSs, we can find the minimum number of DBSs for which all terminals satisfy the required target rate.","Cooperative Strategies for {UAV}-Enabled Small Cell Networks Sharing
  Unlicensed Spectrum","In this paper, we study an aerial drone base station (DBS) assisted cellular
network that consists of a single ground macro base station (MBS), multiple
DBSs, and multiple ground terminals (GT). We assume that the MBS transmits to
the DBSs and the GTs in the licensed band while the DBSs use a separate
unlicensed band (e.g. Wi-Fi) to transmit to the GTs. For the utilization of the
DBSs, we propose a cooperative decode--forward (DF) protocol in which multiple
DBSs assist the terminals simultaneously while maintaining a predetermined
interference level on the coexisting unlicensed band users. For our network
setup, we formulate a joint optimization problem for minimizing the aggregate
gap between the target rates and the throughputs of terminals by optimizing
over the 3D positions of the DBSs and the resources (power, time, bandwidth) of
the network. To solve the optimization problem, we propose an efficient nested
structured algorithm based on particle swarm optimization and convex
optimization methods. Extensive numerical evaluations of the proposed algorithm
is performed considering various aspects to demonstrate the performance of our
algorithm and the gain for utilizing DBSs."
scgqa_29,2003.06259v1,"According to Figure 8, what is the relationship between η values and algorithm performance in the experiments?","The graph shows that when η is selected within a certain range, the training performance does not change much. This suggests that the algorithm is robust with respect to η. However, when η takes extreme values, the performance degrades.",2003.06259v1.pdf,"['2003.06259v1.pdf', '1405.6408v2.pdf', '1303.1635v1.pdf', '1710.09234v1.pdf', '1304.7375v1.pdf', '1708.07888v3.pdf']",2003.06259v1-Figure8-1.png,"Figure 8. Ablation study on the effect of varying η. The x-axis shows the training frames (a total of 400M frames) and y-axis shows the mean human-normalized scores averaged across all Atari games. We select η ∈ {0.5, 1.0, 1.5}. In the legend, numbers in the brackets indicate the value of η.","In this part we study the impact of the hyper-parameter η on the performance of algorithms derived from second-order expansion. In particular, we study the effect of η in the near on-policy optimization as in the context of Section 5.1. In Figure 8, x-axis shows the training frames (400M in total) and y-axis shows the mean human-normalized scores across Atari games. We select η ∈ {0.5, 1.0, 1.5} and compare their training curves. We find that when η is selected within this range, the training performance does not change much, which hints on some robustness with respect to η. Inevitably, when η takes extreme values the performance degrades. When η = 0 the algorithm reduces to the first-order case and the performance gets marginally worse as discussed in the main text.",Taylor Expansion Policy Optimization,"In this work, we investigate the application of Taylor expansions in
reinforcement learning. In particular, we propose Taylor expansion policy
optimization, a policy optimization formalism that generalizes prior work
(e.g., TRPO) as a first-order special case. We also show that Taylor expansions
intimately relate to off-policy evaluation. Finally, we show that this new
formulation entails modifications which improve the performance of several
state-of-the-art distributed algorithms."
scgqa_30,2002.06090v1,"According to Fig. 3 in the paper, how does cache capability influence the performance of edge computing systems?","The graph suggests that cache-enabled mobile edge computing systems can achieve higher throughput by increasing the cache capability. This is important for applications that require low latency and high throughput, such as real-time video streaming and gaming.",2002.06090v1.pdf,"['2002.06090v1.pdf', '1805.01892v1.pdf', '1602.07579v1.pdf', '1910.09823v3.pdf', '1311.1567v3.pdf', '1703.01827v3.pdf', '1506.06213v1.pdf', '1608.08469v1.pdf']",2002.06090v1-Figure3-1.png,"Figure 3: Impact of cache capability capability. 2.5 3 3.5 4 Computation Capability, g(GHz)",Fig. 3 and Fig. 4 illustrate the impact of cache size and computing capability on the average,"Mobile Communications, Computing and Caching Resources Optimization for
  Coded Caching with Device Computing","Edge caching and computing have been regarded as an efficient approach to
tackle the wireless spectrum crunch problem. In this paper, we design a general
coded caching with device computing strategy for content computation, e.g.,
virtual reality (VR) rendering, to minimize the average transmission bandwidth
with the caching capacity and the energy constraints of each mobile device, and
the maximum tolerable delay constraint of each task. The key enabler is that
because both coded data and stored data can be the data before or after
computing, the proposed scheme has numerous edge computing and caching paths
corresponding to different bandwidth requirement. We thus formulate a joint
coded caching and computing optimization problem to decide whether the mobile
devices cache the input data or the output data, which tasks to be coded cached
and which tasks to compute locally. The optimization problem is shown to be 0-1
nonconvex nonsmooth programming and can be decomposed into the computation
programming and the coded caching programming. We prove the convergence of the
computation programming problem by utilizing the alternating direction method
of multipliers (ADMM), and a stationary point can be obtained. For the coded
cache programming, we design a low complexity algorithm to obtain an acceptable
solution. Numerical results demonstrate that the proposed scheme provides a
significant bandwidth saving by taking full advantage of the caching and
computing capability of mobile devices."
scgqa_31,1409.2897v1,How is the relationship between writing duration and mutual information depicted in Figure 4 of the handwriting recognition research?,"The graph shows that there is a positive correlation between writing duration and mutual information. This means that as the writing duration increases, the mutual information also increases. This is likely because the user has more time to think about what they are writing, and therefore is able to generate more information.",1409.2897v1.pdf,"['1409.2897v1.pdf', '1910.09823v3.pdf', '1501.07107v1.pdf', '1311.1567v3.pdf', '1407.7736v1.pdf', '2004.05448v1.pdf', '1909.03961v2.pdf', '2010.12427v3.pdf', '1804.06674v1.pdf']",1409.2897v1-Figure4-1.png,Figure 4: The average writing time per session and the average mutual information per session under the condition Rfixed.,"Furthermore, Figure 4a and Figure 4b reveal that the major contribution of user adaptation comes from the fact that the users write faster in the last 5 sessions compared to the first 5 sessions (p < 0.0001), and not because of the system received more information from the user (p = 0.9723). This result is as expected according to the law of practice [12].",Co-adaptation in a Handwriting Recognition System,"Handwriting is a natural and versatile method for human-computer interaction,
especially on small mobile devices such as smart phones. However, as
handwriting varies significantly from person to person, it is difficult to
design handwriting recognizers that perform well for all users. A natural
solution is to use machine learning to adapt the recognizer to the user. One
complicating factor is that, as the computer adapts to the user, the user also
adapts to the computer and probably changes their handwriting. This paper
investigates the dynamics of co-adaptation, a process in which both the
computer and the user are adapting their behaviors in order to improve the
speed and accuracy of the communication through handwriting. We devised an
information-theoretic framework for quantifying the efficiency of a handwriting
system where the system includes both the user and the computer. Using this
framework, we analyzed data collected from an adaptive handwriting recognition
system and characterized the impact of machine adaptation and of human
adaptation. We found that both machine adaptation and human adaptation have
significant impact on the input rate and must be considered together in order
to improve the efficiency of the system as a whole."
scgqa_32,1003.1655v1,"In the context of the multiple access attack discussed, what do inner and outer bounds imply for security?","The inner and outer regions represent the maximum and minimum rates at which the system can be secure, respectively. The fact that the inner and outer regions do not coincide means that there is a gap in the achievable rates, which implies that the system is not perfectly secure. However, the gap is not too large, which suggests that the system is still reasonably secure.",1003.1655v1.pdf,"['1003.1655v1.pdf', '2011.09375v1.pdf', '1708.07888v3.pdf', '1910.09823v3.pdf', '1902.06156v1.pdf', '1810.04824v1.pdf', '1706.03112v1.pdf']",1003.1655v1-Figure2-1.png,"Figure 2: The inner boundR∗in(0.45, 0.4) for the Example and two subsets ofR ∗ out(0.45, 0.4) obtained by setting|T1| = |T2| = 6 and|T1| = |T2| = 7. The obtained regions lie between the corresponding solid or dashed lines and the horizontal and vertical axes.","Example Let the covertexts be independent binary sources with U1 = U2 = {0, 1} and QU1(U1 = 0) = 0.05 and QU2(U2 = 0) = 0.1. Let the MAAC be a binary additive channel with X1 = X2 = Y = Z = {0, 1} and Y = X1 ⊕ X2 ⊕ Z , where Z is independent of (X1,X2) with Pr(Z = 1) = 0.02 and ⊕ denotes modulo 2 addition. Let D1 = 0.45 and D2 = 0.4. Fig. 2 illustrates the numerically computed inner and outer regions of Theorems 1 and 2 (which coincide with the regions of Theorem 3 since U1 and U2 are independent). To compute R∗in(0.45, 0.4), we only need to consider auxiliary RVs with alphabets |T1| = |T2| = 5. For comparison, we also plot two subsets of the region R∗out(0.45, 0.4) by setting |T1| = |T2| = 6 and |T1| = |T2| = 7, respectively (recall that Theorem 3 does not give an upper bound on the alphabet sizes for T2 and T2 for the outer bound). It is seen that there exist noticeable gaps between R∗in(0.45, 0.4) and the numerically obtained subsets of R ∗ out(0.45, 0.4). When computing the above regions, we quantized the unit interval using a step-size of resolution 0.1 to calculate the joint distributions. We can conclude that the obtained inner and outer bounds do not coincide, and furthermore, that in case there exists a finite upper bound on the auxiliary RV alphabet sizes for the outer region, this upper bound must be at least 7 for the binary problem.","Inner and Outer Bounds for the Public Information Embedding Capacity
  Region Under Multiple Access Attacks","We consider a public multi-user information embedding (watermarking) system
in which two messages (watermarks) are independently embedded into two
correlated covertexts and are transmitted through a multiple-access attack
channel. The tradeoff between the achievable embedding rates and the average
distortions for the two embedders is studied. For given distortion levels,
inner and outer bounds for the embedding capacity region are obtained in
single-letter form. Tighter bounds are also given for independent covertexts."
scgqa_33,1808.00136v2,"In the context of the paper, how quickly does cycle-WGAN converge compared to its baseline on various datasets?","The graph shows that the proposed cycle-WGAN model converges faster than the baseline for three out of four datasets. However, when the `CLS` loss is included in (7) to form the loss in (8) (transforming cycle-WGAN into cycle-CLSWGAN), then the convergence speed decreases. This suggests that the `CLS` loss may slow down the convergence of the model.",1808.00136v2.pdf,"['1808.00136v2.pdf', '1505.05173v6.pdf', '1610.01283v4.pdf', '1803.11512v1.pdf']",1808.00136v2-Figure4-1.png,"Fig. 4. Convergence of the top-1 accuracy in terms of the number of epochs for the generated training samples from the seen classes for CUB, FLO, SUN and AWA.","An important question about out approach is whether the regularisation succeeds in mapping the generated visual representations back to the semantic space. In order to answer this question, we show in Fig. 3 the evolution of the reconstruction loss `REG in (6) as a function of the number of epochs. In general, the reconstruction loss decreases steadily over training, showing that our model succeeds at such mapping. Another relevant question is if our proposed methods take more or less epochs to converge, compared to the Baseline – Fig. 4 shows the classification accuracy of the generated training samples from the seen classes for the proposed models cycle-WGAN and cycle-CLSWGAN, and also for the baseline (note that cycle-(U)WGAN is a fine-tuned model from the cycle-WGAN, so their loss functions are in fact identical for the seen classes shown in the graph). For three out of four datasets, our proposed cycle-WGAN converges faster. However, when the `CLS in included in (7) to form the loss in (8) (transforming cycle-WGAN into cycle-CLSWGAN), then the convergence",Multi-modal Cycle-consistent Generalized Zero-Shot Learning,"In generalized zero shot learning (GZSL), the set of classes are split into
seen and unseen classes, where training relies on the semantic features of the
seen and unseen classes and the visual representations of only the seen
classes, while testing uses the visual representations of the seen and unseen
classes. Current methods address GZSL by learning a transformation from the
visual to the semantic space, exploring the assumption that the distribution of
classes in the semantic and visual spaces is relatively similar. Such methods
tend to transform unseen testing visual representations into one of the seen
classes' semantic features instead of the semantic features of the correct
unseen class, resulting in low accuracy GZSL classification. Recently,
generative adversarial networks (GAN) have been explored to synthesize visual
representations of the unseen classes from their semantic features - the
synthesized representations of the seen and unseen classes are then used to
train the GZSL classifier. This approach has been shown to boost GZSL
classification accuracy, however, there is no guarantee that synthetic visual
representations can generate back their semantic feature in a multi-modal
cycle-consistent manner. This constraint can result in synthetic visual
representations that do not represent well their semantic features. In this
paper, we propose the use of such constraint based on a new regularization for
the GAN training that forces the generated visual features to reconstruct their
original semantic features. Once our model is trained with this multi-modal
cycle-consistent semantic compatibility, we can then synthesize more
representative visual representations for the seen and, more importantly, for
the unseen classes. Our proposed approach shows the best GZSL classification
results in the field in several publicly available datasets."
scgqa_34,1906.02003v1,"Referencing Figure 7.3, how does the regularization parameter affect prediction and model errors?","The data trends show that the prediction error is an increasing function of the regularization parameter, as expected. However, the model error, calculated as the sum of squared differences between the true system parameters and the estimated parameters, is minimized by the optimal value of λ. This suggests that the optimal value of λ is the best choice for minimizing the model error, while also keeping the prediction error under control.",1906.02003v1.pdf,"['1906.02003v1.pdf', '1512.00843v3.pdf', '1801.06867v1.pdf']",1906.02003v1-Figure7.3-1.png,"Figure 7.3 The top panes show (log) error distributions on training data when solving Eq. (7.3) on data generated by (7.1)-(7.2). The prediction error is a strictly increasing function of λ, whereas the model error is minimized by the optimal value for λ. The bottom four panes show quantile-quantile plots of prediction errors on the training data, for the different choices of λ.","using the Kalman-smoothing algorithm and 4 different values of λ, shown in color in Fig. 7.2. The optimal value of λ is approximately given by λ=σv/σw = 10. In Fig. 7.2 we see how a too small value of the regularization parameter leads to noisy estimates of the time-varying parameters, whereas a too high value leads to overly conservative changes. Arguably, the optimal value of λ = 10 performs best. In Fig. 7.3, we illustrate the log-distributions of prediction errors in the top-left pane, and model errors in the top-right pane, in both cases on the training data. It is clear that the prediction error is an increasing function of the regularization parameter, as expected. However, the model error, calculated as the sum of squared differences between the true system parameters and the estimated parameters, is minimized by the optimal value of λ. In reality, this optimal value is unknown and determining it is not always easy. In the bottom four panes, we show quantilequantile plots [Wilk and Gnanadesikan, 1968] of the prediction errors. The optimal","Machine Learning and System Identification for Estimation in Physical
  Systems","In this thesis, we draw inspiration from both classical system identification
and modern machine learning in order to solve estimation problems for
real-world, physical systems. The main approach to estimation and learning
adopted is optimization based. Concepts such as regularization will be utilized
for encoding of prior knowledge and basis-function expansions will be used to
add nonlinear modeling power while keeping data requirements practical. The
thesis covers a wide range of applications, many inspired by applications
within robotics, but also extending outside this already wide field. Usage of
the proposed methods and algorithms are in many cases illustrated in the
real-world applications that motivated the research. Topics covered include
dynamics modeling and estimation, model-based reinforcement learning, spectral
estimation, friction modeling and state estimation and calibration in robotic
machining. In the work on modeling and identification of dynamics, we develop
regularization strategies that allow us to incorporate prior domain knowledge
into flexible, overparameterized models. We make use of classical control
theory to gain insight into training and regularization while using flexible
tools from modern deep learning. A particular focus of the work is to allow use
of modern methods in scenarios where gathering data is associated with a high
cost. In the robotics-inspired parts of the thesis, we develop methods that are
practically motivated and ensure that they are implementable also outside the
research setting. We demonstrate this by performing experiments in realistic
settings and providing open-source implementations of all proposed methods and
algorithms."
scgqa_35,2002.01322v1,What does this paper's figure indicate about the head model's accuracy versus the full model's with limited real speech?,"The graph shows that the head model is more robust to the amount of real speech examples than the full model. When trained on 1000 real examples per word, the head model achieves an accuracy of 95.8%, while the full model achieves an accuracy of 94.8%. However, when the number of real examples is reduced to 125 per word, the head model only loses 1% accuracy, while the full model loses 4.5% accuracy. This suggests that the head model is able to learn more efficiently from a smaller dataset.",2002.01322v1.pdf,"['2002.01322v1.pdf', '1612.07141v3.pdf', '1106.3242v2.pdf', '1908.04655v1.pdf', '1610.08534v1.pdf', '1910.09592v1.pdf', '1804.04290v1.pdf']",2002.01322v1-Figure3-1.png,"Fig. 3: Performance of different amounts of real speech examples, with and without augmentation using synthetic speech examples, optionally based on the speech embedding model. The standard deviation over 20 runs is shown for each data point.","Figure 3 shows these results. At the right of the plot we can see that when we have 1000 real examples per word (35k total), using the embedding model improves the accuracy noticeably from 94.8% to 96.8%. The inclusion of the synthetic speech data does not result in any further improvements. Decreasing the number of real examples to 125 per word reduces the accuracy of the full model by 4.5% (absolute) to 90.3%, while the head model, at an accuracy of 95.8%, only loses 1% (absolute). This is also the first point where adding the synthetic data produces a measurable improvement of 0.1%. Reducing the number of real examples further causes the performance of the full model to rapidly decrease.",Training Keyword Spotters with Limited and Synthesized Speech Data,"With the rise of low power speech-enabled devices, there is a growing demand
to quickly produce models for recognizing arbitrary sets of keywords. As with
many machine learning tasks, one of the most challenging parts in the model
creation process is obtaining a sufficient amount of training data. In this
paper, we explore the effectiveness of synthesized speech data in training
small, spoken term detection models of around 400k parameters. Instead of
training such models directly on the audio or low level features such as MFCCs,
we use a pre-trained speech embedding model trained to extract useful features
for keyword spotting models. Using this speech embedding, we show that a model
which detects 10 keywords when trained on only synthetic speech is equivalent
to a model trained on over 500 real examples. We also show that a model without
our speech embeddings would need to be trained on over 4000 real examples to
reach the same accuracy."
scgqa_36,1804.04290v1,What trends in position errors are depicted in Fig. 5 for the teleoperation system in scenario 1?,"The graph shows that the position errors between the master and the slave manipulators in scenario 1 are relatively small. This is because the manipulators are able to track the master's position well, even when they are moving in free motion. The position errors are also shown to converge to the origin over time, which indicates that the manipulators are able to stabilize their positions.",1804.04290v1.pdf,"['1804.04290v1.pdf', '1909.00392v1.pdf', '1702.06270v2.pdf', '2008.07524v3.pdf']",1804.04290v1-Figure5-1.png,"Fig. 5: Scenario 1: joint position errors between the master and the slave manipulators. (a) joint 1, (b) joint 2.","We now analyze the simulation results of the teleoperation system in different simulation scenarios. Firstly, when the considered teleoperation system is in free motion, i.e., simulation scenario 1, the simulation results are given in Fig. 4 – Fig. 6. From Fig. 4, we know that the formation’s center of the slaves follows the master’s position at around 2s (the last row of Fig. 4) under the scheduling protocols. The tracking performance of each pair of qm and q̌si is also provided in Fig. 4. It is noted that in Fig. 4, the positions of the manipulators under TOD scheduling protocol are higher than the ones under RR scheduling protocol. Fig. 5 depicts the curves of position errors, and it is shown that the position errors under TOD scheduling protocol converge faster to the origin in this scenario. From Fig. 6, we observe that the velocities of each manipulator converge to the origin very fast (after about 3s).","Bilateral Teleoperation of Multiple Robots under Scheduling
  Communication","In this paper, bilateral teleoperation of multiple slaves coupled to a single
master under scheduling communication is investigated. The sampled-data
transmission between the master and the multiple slaves is fulfilled over a
delayed communication network, and at each sampling instant, only one slave is
allowed to transmit its current information to the master side according to
some scheduling protocols. To achieve the master-slave synchronization,
Round-Robin scheduling protocol and Try-Once-Discard scheduling protocol are
employed, respectively. By designing a scheduling-communication-based
controller, some sufficient stability criteria related to the controller gain
matrices, sampling intervals, and communication delays are obtained for the
closed-loop teleoperation system under Round-Robin and Try-Once-Discard
scheduling protocols, respectively. Finally, simulation studies are given to
validate the effectiveness of the proposed results."
scgqa_37,2007.15958v1,What relationship is illustrated in Figure 6 regarding aggregated frames and decision-making in the gait recognition systems?,"The graph shows that the systems based on feature learning become more robust with increasing the number of aggregated frame scores. This is because a system that makes decisions based on multiple frames essentially makes the final decision based on more data. As a result, the system is less likely to make a mistake, and the EER (the lower, the better) is reduced.",2007.15958v1.pdf,"['2007.15958v1.pdf', '2004.14564v2.pdf', '1902.07084v2.pdf', '2008.02777v1.pdf', '2007.11391v1.pdf', '1706.01341v1.pdf', '1912.00035v1.pdf', '1210.1356v2.pdf', '1110.6199v1.pdf']",2007.15958v1-Figure6-1.png,Figure 6: Comparison of performances of systems using different features across different number of aggregated frames. Same-day vs. cross-day settings.,"The systems based on feature learning become more robust with increasing the number of aggregated frame scores. A system that makes decisions based on multiple frames essentially makes the final decision based on more data. Figure 6 shows the performances in terms of EER (the lower, the better) in same-day",Feature Learning for Accelerometer based Gait Recognition,"Recent advances in pattern matching, such as speech or object recognition
support the viability of feature learning with deep learning solutions for gait
recognition. Past papers have evaluated deep neural networks trained in a
supervised manner for this task. In this work, we investigated both supervised
and unsupervised approaches. Feature extractors using similar architectures
incorporated into end-to-end models and autoencoders were compared based on
their ability of learning good representations for a gait verification system.
Both feature extractors were trained on the IDNet dataset then used for feature
extraction on the ZJU-GaitAccel dataset. Results show that autoencoders are
very close to discriminative end-to-end models with regards to their feature
learning ability and that fully convolutional models are able to learn good
feature representations, regardless of the training strategy."
scgqa_38,1301.5201v1,"In the context of the paper, what factors contribute to the rising number of stable groups among users over time?","There are a few possible reasons why the number of people belonging to one, two, or three stable groups increases over time. First, the popularity of the portal may be increasing, which would lead to more people participating in discussions and forming groups. Second, the significance of political events may be increasing, which would lead to more people being interested in discussing them. Finally, the portal may be changing in ways that make it easier for people to form groups, such as by providing more features for users to interact with each other.",1301.5201v1.pdf,"['1301.5201v1.pdf', '1708.05355v1.pdf', '1206.6850v1.pdf', '1712.03538v1.pdf', '1802.05945v1.pdf', '1611.04706v2.pdf', '2011.08042v1.pdf', '1909.03961v2.pdf']",1301.5201v1-Figure3-1.png,Fig. 3: Membership of people to groups for k=3 in comments model.,"In figs. 3a and 3b, the numbers of users belonging to one, two or three stable groups in each interval for k=3 are specified. The figure presents mentioned belongings only in the comments model, but in the post model diagram is very similar. We can notice that these numbers increase, mostly because of the increase of the popularity of the portal and the significance of political events taking place.","Models of Social Groups in Blogosphere Based on Information about
  Comment Addressees and Sentiments","This work concerns the analysis of number, sizes and other characteristics of
groups identified in the blogosphere using a set of models identifying social
relations. These models differ regarding identification of social relations,
influenced by methods of classifying the addressee of the comments (they are
either the post author or the author of a comment on which this comment is
directly addressing) and by a sentiment calculated for comments considering the
statistics of words present and connotation. The state of a selected blog
portal was analyzed in sequential, partly overlapping time intervals. Groups in
each interval were identified using a version of the CPM algorithm, on the
basis of them, stable groups, existing for at least a minimal assumed duration
of time, were identified."
scgqa_39,2007.11446v1,"According to Figure 6, how do different SSMF algorithms perform with increased noise in the experiments?","The graph shows that as the noise level increases, the performance of all algorithms decreases steadily. This is because noise makes it more difficult to learn the underlying structure of the data, which is important for all of the algorithms. However, GFPI is able to better handle noise than the other algorithms, which is why it performs better in all cases.",2007.11446v1.pdf,"['2007.11446v1.pdf', '1707.02342v1.pdf', '1206.5265v1.pdf', '1906.02003v1.pdf', '1606.01062v1.pdf', '1701.08947v1.pdf']",2007.11446v1-Figure6-1.png,"Figure 6: Average ERR metric for 10 randomly generated data sets depending on purity for the different SSMF algorithms, for different noise levels: SNR of 60 (top), 50 (middle) and 40 (bottom), and for m = r = 3 (left) and m = r = 4 (right).","In this section, we compare the behavior of the different algorithms in the presence of noise. We use three levels of noise (SNR = 60, 50 and 40) and investigate the effect of the purity for r = m = {3, 4}. Figure 6 reports the ERR metric, similarly as for Figure 5 (average of 10 randomly generated synthetic data sets). As the noise level increases (SNR decreases), the performance of all algorithms decreases steadily. However, in almost all cases, GFPI outperforms all other approaches, especially when the the purity p is low. As for the noiseless case, MVIE performs the second best.","Simplex-Structured Matrix Factorization: Sparsity-based Identifiability
  and Provably Correct Algorithms","In this paper, we provide novel algorithms with identifiability guarantees
for simplex-structured matrix factorization (SSMF), a generalization of
nonnegative matrix factorization. Current state-of-the-art algorithms that
provide identifiability results for SSMF rely on the sufficiently scattered
condition (SSC) which requires the data points to be well spread within the
convex hull of the basis vectors. The conditions under which our proposed
algorithms recover the unique decomposition is in most cases much weaker than
the SSC. We only require to have $d$ points on each facet of the convex hull of
the basis vectors whose dimension is $d-1$. The key idea is based on extracting
facets containing the largest number of points. We illustrate the effectiveness
of our approach on synthetic data sets and hyperspectral images, showing that
it outperforms state-of-the-art SSMF algorithms as it is able to handle higher
noise levels, rank deficient matrices, outliers, and input data that highly
violates the SSC."
scgqa_40,1702.06270v2,How does Figure 11 illustrate the relationship between external location count and trajectory uniqueness in the study's context?,"The graph shows that as the number of external locations increases, the uniqueness of recovered trajectories decreases. However, even when only Top-2 locations are provided, the uniquely distinguished rate is stable and remains above 85% with datasets of different scale. This suggests that even with limited external information, the attack system is still able to recover trajectories that are unique to individual users.",1702.06270v2.pdf,"['1702.06270v2.pdf', '1608.00887v1.pdf', '2001.09043v3.pdf', '1905.12729v2.pdf', '1809.09034v1.pdf']",1702.06270v2-Figure11-1.png,Figure 11: The impact of number of trajectories on privacy leakage.,"privacy breach. To evaluate this assumption, we randomly sample subsets of different number of individuals from the investigated datasets, and then utilize the proposed attack system to recover the trajectories from each sampled subset. The obtained results are presented in Figure 11, where #1, #2 and #3 represent the recovered results after step 1, step 2 and step 3, respectively. Observing Figure 11(a), we find out that when the dataset contains only 1,000 mobile users’ trajectories, the accuracy is around 0.99, which indicates that the attack system correctly recovers 99% of spatiotemporal points. In addition, the recovery accuracy indeed decreases as scale of dataset increases. However, when the number of users reaches to 100,000, the attack system is still able to recover the trajectories at accuracy around 50%. On the other hand, Figure 11(b) shows that the uniqueness of recovered trajectories decreases as the scale of datasets grows when only Top-1 location is provided. However, with more external information provided, such as Top-2 or Top-3 locations, the uniquely distinguished rate is stable and remains above 85% with datasets of different scale. These results reveal that increasing the size of datasets can indeed reduce the accuracy of recovered trajectories, but it cannot prevent attackers link the recovered trajectories with mobile users when more than Top-2 locations are provided. More importantly, the evaluation demonstrate that the attack system is valid at the scale of tens of thousands to hundreds of thousands individuals in terms of correctly recovering most of the trajectories.","Trajectory Recovery From Ash: User Privacy Is NOT Preserved in
  Aggregated Mobility Data","Human mobility data has been ubiquitously collected through cellular networks
and mobile applications, and publicly released for academic research and
commercial purposes for the last decade. Since releasing individual's mobility
records usually gives rise to privacy issues, datasets owners tend to only
publish aggregated mobility data, such as the number of users covered by a
cellular tower at a specific timestamp, which is believed to be sufficient for
preserving users' privacy. However, in this paper, we argue and prove that even
publishing aggregated mobility data could lead to privacy breach in
individuals' trajectories. We develop an attack system that is able to exploit
the uniqueness and regularity of human mobility to recover individual's
trajectories from the aggregated mobility data without any prior knowledge. By
conducting experiments on two real-world datasets collected from both mobile
application and cellular network, we reveal that the attack system is able to
recover users' trajectories with accuracy about 73%~91% at the scale of tens of
thousands to hundreds of thousands users, which indicates severe privacy
leakage in such datasets. Through the investigation on aggregated mobility
data, our work recognizes a novel privacy problem in publishing statistic data,
which appeals for immediate attentions from both academy and industry."
scgqa_41,1208.4662v2,"Based on Figure 6, how does spectral clustering's MSE behavior contrast with that of the MCD method in this research?","The results of the graph suggest that the spectral clustering method is not as effective as the MCD method in automatically segmenting FLIM images. This is because the MCD method consistently decreases the MSE in estimating average fluorescence lifetimes of the correct segments with increasing resolution, while the spectral clustering method does not.",1208.4662v2.pdf,"['1208.4662v2.pdf', '1407.7736v1.pdf', '1006.4386v1.pdf', '1703.07020v4.pdf', '2010.08182v3.pdf', '1709.03329v1.pdf', '2003.13216v1.pdf']",1208.4662v2-Figure6-1.png,"Figure 6: (A-B) The mean-square error (MSE) in estimating average fluorescence lifetimes of the correct segments using the spectral clustering method developed by Ng et al. for images shown in Fig. 1A and Fig. 1B, respectively. The MSE in estimating average FLTs of the correct segments for the image shown in Fig. 1A using the spectral clustering method does not consistently decrease with increasing resolution. For Fig. 1B, the MSE shows a decrease in increasing the resolution by decreasing α up to 0.0625. Decreasing α below 0.0625 increases the MSE to be very high, and thus, such MSEs are not depicted here for clarity.","The MSE in estimating average FLTs of the correct segments using the MCD method consistently decreases with increasing resolution; see Fig. 5. The MCD method offers lower MSE than the spectral clustering method in its all network resolution for the FLIM image shown in Fig. 1A, and in its high network resolution region (γ > 10) for the FLIM image shown in Fig. 1B; see Figs. 5-6. The MSE in estimating average FLTs of the correct segments using the spectral clustering method does not consistently decrease with increasing resolution for the FLIM image shown in Fig. 1A. For Fig. 1B, the MSE in estimating average FLTs of the correct segments using the spectral clustering method shows a decrease in increasing the resolution by decreasing α up to 0.0625. Decreasing α below this value introduces noisy segments in the output, and the MSE in estimating average FLTs of the correct segments becomes very high. Consequently, such MSEs at limiting resolutions are not shown in Fig. 6B for clarity. In summary, the proposed MCD method outperforms the spectral clustering method in MSE sense in automatically segmenting the FLIM images shown in Fig. 1.","Automatic Segmentation of Fluorescence Lifetime Microscopy Images of
  Cells Using Multi-Resolution Community Detection","We have developed an automatic method for segmenting fluorescence lifetime
(FLT) imaging microscopy (FLIM) images of cells inspired by a multi-resolution
community detection (MCD) based network segmentation method. The image
processing problem is framed as identifying segments with respective average
FLTs against a background in FLIM images. The proposed method segments a FLIM
image for a given resolution of the network composed using image pixels as the
nodes and similarity between the pixels as the edges. In the resulting
segmentation, low network resolution leads to larger segments and high network
resolution leads to smaller segments. Further, the mean-square error (MSE) in
estimating the FLT segments in a FLIM image using the proposed method was found
to be consistently decreasing with increasing resolution of the corresponding
network. The proposed MCD method outperformed a popular spectral clustering
based method in performing FLIM image segmentation. The spectral segmentation
method introduced noisy segments in its output at high resolution. It was
unable to offer a consistent decrease in MSE with increasing resolution."
scgqa_42,1908.09034v2,What relationship between wind direction and induction factors is illustrated in Fig. 3 of this study?,"The graph shows that the normalized induction factors are not constant with respect to the wind direction. This is because the induction factors are dependent on the velocity deficits in the far wake, which are in turn dependent on the wind direction. As the wind direction changes, the velocity deficits in the far wake will also change, which will in turn affect the induction factors.",1908.09034v2.pdf,"['1908.09034v2.pdf', '1905.08337v1.pdf', '1607.08438v1.pdf', '1206.6850v1.pdf', '1608.06005v1.pdf', '1304.7375v1.pdf', '1805.01772v1.pdf', '2007.11391v1.pdf']",1908.09034v2-Figure3-1.png,"Fig. 3. Normalized induction factors defined as ψk 1/3 for deterministic model (µa = 1, µb = −2) and stochastic model with various values of input-dependent multiplicative noise standard deviation (µa = 1, σa = 0, µb = −2, σb > 0, γa = 0 and γb = 0).","Fig. 3 illustrates the optimal induction factor sequence {ψ0, . . . , ψ9} and Fig. 4 depicts the optimal efficiency η∗` under different standard deviation values of the input-dependent multiplicative noise bk. The induction factors in Fig. 3 are normalized by 1/3, which is the value achieving the Betz limit for a single isolated turbine [28]. We set the mean value µa to 1, and the skewness to zero. Fig. 4 demonstrates that the optimal array efficiency improves with increasing variance on bk. This result is intuitively reasonable, in the sense that higher variability of the velocity deficits in the far wake may lead to increased power extraction. We speculate that this multiplicative stochastic perturbation on the velocity",Stochastic Dynamic Programming for Wind Farm Power Maximization,"Wind farms can increase annual energy production (AEP) with advanced control
algorithms by coordinating the set points of individual turbine controllers
across the farm. However, it remains a significant challenge to achieve
performance improvements in practice because of the difficulty of utilizing
models that capture pertinent complex aerodynamic phenomena while remaining
amenable to control design. We formulate a multi-stage stochastic optimal
control problem for wind farm power maximization and show that it can be solved
analytically via dynamic programming. In particular, our model incorporates
state- and input-dependent multiplicative noise whose distributions capture
stochastic wind fluctuations. The optimal control policies and value functions
explicitly incorporate the moments of these distributions, establishing a
connection between wind flow data and optimal feedback control. We illustrate
the results with numerical experiments that demonstrate the advantages of our
approach over existing methods based on deterministic models."
scgqa_43,1603.01185v2,Discuss the findings in Figure 4 regarding fitness reductions and delayed expression with varying B.,"The graph shows that there is a significant drop in fitness for B>2 compared to B<3, regardless of K. This is because, in these cases, evolution struggles to produce high fitness networks. The percentage of nodes with delayed expression is also higher for B>2, which is likely due to the fact that these networks are more complex and require more time to evolve.",1603.01185v2.pdf,"['1603.01185v2.pdf', '1604.06979v1.pdf', '1604.04026v1.pdf', '2002.11440v1.pdf', '2011.03519v1.pdf']",1603.01185v2-Figure4-1.png,"Fig. 4. Evolutionary behaviour after 5000 generations in the asynchronous RBNK model for R=100, N=10 and various B and K combinations. Left column shows fitnesses and the percentage of nodes with delayed expression, the right column shows the corresponding average delay. Error bars show min and max values.","Figure 4 (left column) shows how there is a significant (T-test, p<0.05) drop in fitness for B>2 compared to B<3 regardless of K, as in the synchronous case above (see also [2]). Again, it can be seen that around 3% of nodes, on average, have an expression time of longer than one update cycle for all K. In contrast to the synchronous case above, this is also true for B=1. As noted in Section 2, such RBN typically exhibit point attractors whereas this is not the case for asynchronous updating. Analysis in all cases shows that the delayed nodes form part the subset changing state within the attractor, which corresponds with the B=1 result for synchronous updating as they are not used. Figure 4 (right column) shows the average expression time for the delayed nodes. As can be seen, there again appears to be no selective pressure on T in the cases where evolution struggles to produce high fitness networks, ie, when B>2. For B=1, T≈1 which is significantly (T-test, p<0.05) less than for B=2 where T≈2. The latter value for T is the same as for synchronous updating above. Similar general findings were found for other ranges of T (not shown).",Evolving Boolean Regulatory Networks with Variable Gene Expression Times,"The time taken for gene expression varies not least because proteins vary in
length considerably. This paper uses an abstract, tuneable Boolean regulatory
network model to explore gene expression time variation. In particular, it is
shown how non-uniform expression times can emerge under certain conditions
through simulated evolution. That is, gene expression time variance appears
beneficial in the shaping of the dynamical behaviour of the regulatory network
without explicit consideration of protein function."
scgqa_44,1405.6298v2,What does the relationship between damping coefficient k and torque u indicate about the pendulum's behavior in Figure 10?,"The graph shows that the pendulum can exhibit a variety of behaviors, depending on the values of k and u. For example, when k is small and u is large, the pendulum will exhibit large oscillations. However, when k is large and u is small, the pendulum will exhibit small oscillations. The graph also shows that there is a critical value of k, kc, below which the pendulum can exhibit bistability. This means that the pendulum can exist in two stable states, one with small oscillations and one with large oscillations.",1405.6298v2.pdf,"['1405.6298v2.pdf', '1005.0416v1.pdf', '1908.04647v1.pdf']",1405.6298v2-Figure10-1.png,"Figure 10. The qualitative behavior of the pendulum for large/small values of torque and damping, as reported in [46, p. 272].","It is of interest to interpret differential positivity against the textbook analysis [46]. Following [46, Chapter 8], Figure 10 summarizes the qualitative behavior of the pendulum for different values of the damping coefficient k ≥ 0 and of the constant torque input u ≥ 0 (the behavior of the pendulum for u ≤ 0 is symmetric). The nonlinear pendulum cannot be differentially positive for arbitrary values of the torque when k ≤ kc. This is because the region of bistable behaviors (coexistence of small and large oscillations) is delineated by a homoclinic orbit, which is ruled out by differental positivity (Corollary 3). For instance, looking at Figure 10, for any k < kc there exists a value u = uc(k) for which the pendulum encounters a homoclinic bifurcation (see [46, Section 8.5] and",Differentially positive systems,"The paper introduces and studies differentially positive systems, that is,
systems whose linearization along an arbitrary trajectory is positive. A
generalization of Perron Frobenius theory is developed in this differential
framework to show that the property induces a (conal) order that strongly
constrains the asymptotic behavior of solutions. The results illustrate that
behaviors constrained by local order properties extend beyond the well-studied
class of linear positive systems and monotone systems, which both require a
constant cone field and a linear state space."
scgqa_45,1511.04338v2,What do the time series and phase-space trajectories in Figure 4 show regarding one-rod barrel modes?,"The time series and phase-space plots in Figure 4 illustrate the dominant modes of the one-rod barrel. The time series show the motion of the mass along the rod of the barrel over time, while the phase-space plots show the relationship between the mass's position and velocity. The different modes correspond to different patterns of motion, and the plots show how these patterns change over time.",1511.04338v2.pdf,"['1511.04338v2.pdf', '1802.02193v1.pdf', '1409.2897v1.pdf', '2005.13300v1.pdf', '1801.06867v1.pdf', '1604.04026v1.pdf', '1902.05922v1.pdf']",1511.04338v2-Figure4-1.png,"Figure 4. The motion x(t) of the mass along the rod of the one-rod barrel (top row), together with the corresponding phase-plane trajectories (x(t), vb(t)) (bottom row), compare Fig. 2. The gain and the adaption rate are a = 1.9 and ε = 0.25 respectively. Shown are the 0:1, 1:1 and 1:3 modes (left/middle/right column). Note, that the velocity vb(t) of the barrel vanishes for the 0:1 mode, oscillating but remaining otherwise positive for the 1:1 and the 1:3 mode. (click for movie).","In Fig. 4 we illustrate the time series and the corresponding phase-space plots of the dominant modes of the one-rod barrel shown in Fig. 1. The simulation parameters a = 1.9 for the gain, and the ε = 0.25 adaption rate are close to the Hopf bifurcation line shown in Fig. 3, but in the on mode. Which means, that the ball moves both for fixed horizontal or vertical rods.","The sensorimotor loop as a dynamical system: How regular motion
  primitives may emerge from self-organized limit cycles","We investigate the sensorimotor loop of simple robots simulated within the
LPZRobots environment from the point of view of dynamical systems theory. For a
robot with a cylindrical shaped body and an actuator controlled by a single
proprioceptual neuron we find various types of periodic motions in terms of
stable limit cycles. These are self-organized in the sense, that the dynamics
of the actuator kicks in only, for a certain range of parameters, when the
barrel is already rolling, stopping otherwise. The stability of the resulting
rolling motions terminates generally, as a function of the control parameters,
at points where fold bifurcations of limit cycles occur. We find that several
branches of motion types exist for the same parameters, in terms of the
relative frequencies of the barrel and of the actuator, having each their
respective basins of attractions in terms of initial conditions. For low
drivings stable limit cycles describing periodic and drifting back-and-forth
motions are found additionally. These modes allow to generate symmetry breaking
explorative behavior purely by the timing of an otherwise neutral signal with
respect to the cyclic back-and-forth motion of the robot."
scgqa_46,2009.08716v1,"In the context of Fig. 4, how does FedNAG perform against other algorithms regarding convergence rates?","The graph shows that FedNAG converges faster than other benchmark algorithms on both MNIST and CIFAR-10 datasets. This is likely due to the fact that FedNAG uses a more efficient update rule that takes into account the gradient information from all workers. This allows FedNAG to make more informed updates, which leads to faster convergence.",2009.08716v1.pdf,"['2009.08716v1.pdf', '1402.0808v1.pdf', '1106.3826v2.pdf']",2009.08716v1-Figure4-1.png,Fig. 4. Convergence performance with benchmark algorithms,"1) Convergence Performance: In Fig. 4, we compare the convergence performance of FedNAG with other three benchmarks. The experiment is performed on two datasets. MNIST is trained by linear regression, logistic regression, and CNN; and CIFAR-10 is trained by CNN. The setting in this experiment is τ = 4, γ = 0.9, N = 4. For MNIST, the total number of iterations T is 1000. For CIFAR-10, T is set to 10000.",Federated Learning with Nesterov Accelerated Gradient,"Federated learning (FL) is a fast-developing technique that allows multiple
workers to train a global model based on a distributed dataset. Conventional FL
(FedAvg) employs gradient descent algorithm, which may not be efficient enough.
Momentum is able to improve the situation by adding an additional momentum step
to accelerate the convergence and has demonstrated its benefits in both
centralized and FL environments. It is well-known that Nesterov Accelerated
Gradient (NAG) is a more advantageous form of momentum, but it is not clear how
to quantify the benefits of NAG in FL so far. This motives us to propose
FedNAG, which employs NAG in each worker as well as NAG momentum and model
aggregation in the aggregator. We provide a detailed convergence analysis of
FedNAG and compare it with FedAvg. Extensive experiments based on real-world
datasets and trace-driven simulation are conducted, demonstrating that FedNAG
increases the learning accuracy by 3-24% and decreases the total training time
by 11-70% compared with the benchmarks under a wide range of settings."
scgqa_47,1405.6408v2,"What does Figure 1 illustrate about the relationship between PFA, PD, and detection thresholds in this paper?","The probability of false alarm (PFA) is the probability of rejecting a true hypothesis, while the probability of detection (PD) is the probability of accepting a true hypothesis. In this context, the hypothesis is that the mean of the distribution is equal to zero, and the test statistic is the sample mean. The PFA and PD are plotted as a function of the detection threshold, which is the value of the sample mean at which the hypothesis is rejected.",1405.6408v2.pdf,"['1405.6408v2.pdf', '1808.06818v1.pdf', '1703.10422v2.pdf', '2002.12489v3.pdf', '1203.1203v2.pdf']",1405.6408v2-Figure1-1.png,"Fig. 1. Probability of false alarm vs. detection threshold, with k = n.","More terms can be added (L > 2) with an expected increase in accuracy; however, with L = 2, i.e., involving up to the fourth cumulant, the c.d.f. of W0 and of W1 are already approximated with high accuracy. This can be observed in Figs. 1 and 2, which plot respectively the probability of false alarm PFA(η) and the probability of detection PD(η), both as a function","Analysis and Design of Multiple-Antenna Cognitive Radios with Multiple
  Primary User Signals","We consider multiple-antenna signal detection of primary user transmission
signals by a secondary user receiver in cognitive radio networks. The optimal
detector is analyzed for the scenario where the number of primary user signals
is no less than the number of receive antennas at the secondary user. We first
derive exact expressions for the moments of the generalized likelihood ratio
test (GLRT) statistic, yielding approximations for the false alarm and
detection probabilities. We then show that the normalized GLRT statistic
converges in distribution to a Gaussian random variable when the number of
antennas and observations grow large at the same rate. Further, using results
from large random matrix theory, we derive expressions to compute the detection
probability without explicit knowledge of the channel, and then particularize
these expressions for two scenarios of practical interest: 1) a single primary
user sending spatially multiplexed signals, and 2) multiple spatially
distributed primary users. Our analytical results are finally used to obtain
simple design rules for the signal detection threshold."
scgqa_48,1603.04812v2,How does the fixed P_thresholde affect convergence iterations in Table II under varying SNR in Fig. 3?,"The number of iterations needed for the convergence of the algorithm in Table II decreases as SNR increases because P thresholde is set fixed at 10 −8 for all SNRs while the error probability decreases from about 10−1 to 10−5. This means that as SNR increases, the algorithm becomes more accurate and requires fewer iterations to converge.",1603.04812v2.pdf,"['1603.04812v2.pdf', '1902.05312v2.pdf', '1808.08442v1.pdf', '1504.01124v3.pdf', '1501.01582v1.pdf']",1603.04812v2-Figure3-1.png,Fig. 3. Average number of iterations for convergence of algorithms of Table I and II when M = 3 antennas and K = 3 users without user selection.,"In Fig. 3 we show the average number of iterations needed for the convergence of the algorithms in Tables I and II. In both algorithms the average number of iterations is less than 20. It is very interesting to observe that as SNR increases the number of iterations needed for the convergence of the algorithm in Table II decreases. This is because P thresholde is set fixed at 10 −8 for all SNRs while the error probability decreases from about 10−1 to 10−5. If it is assumed that the error probability is known in advance, it would be better to change P thresholde to a small fraction of the error probability to have a faster convergence with the same reliability.","Modulation-Specific Multiuser Transmit Precoding and User Selection for
  BPSK Signalling","Motivated by challenges to existing multiuser transmission methods in a low
signal to noise ratio (SNR) regime, and emergence of massive numbers of low
data rate ehealth and internet of things (IoT) devices, in this paper we show
that it is beneficial to incorporate knowledge of modulation type into
multiuser transmit precoder design. Particularly, we propose a transmit
precoding (beamforming) specific to BPSK modulation, which has maximum power
efficiency and capacity in poor channel conditions. To be more specific, in a
multiuser scenario, an objective function is formulated based on the weighted
sum of error probabilities of BPSK modulated users. Convex optimization is used
to transform and solve this ill-behaved non-convex minimum probability of error
(MPE) precoding problem. Numerical results confirm significant performance
improvement. We then develop a low-complexity user selection algorithm for MPE
precoding. Based on line packing principles in Grassmannian manifolds, the
number of supported users is able to exceed the number of transmit antennas,
and hence the proposed approach is able to support more simultaneous users
compared with existing multiuser transmit precoding methods."
scgqa_49,1403.2732v1,"According to Figure 7 in 'The Bursty Dynamics of the Twitter Information Network', how consistent is tweet similarity among users?","The graph shows that the distribution of follower tweet similarity is highly variable, even for users with comparable number of followers. This suggests that the number of followers is not a good predictor of the similarity of their tweets.",1403.2732v1.pdf,"['1403.2732v1.pdf', '2002.06090v1.pdf', '1809.07412v2.pdf', '1701.00365v2.pdf', '2009.08716v1.pdf', '1106.3826v2.pdf', '1909.03961v2.pdf', '1707.02342v1.pdf']",1403.2732v1-Figure7-1.png,"Figure 7: The distribution of follower tweet similarity for several different users. The number of followers for each user is listed in the legend. Even for users with comparable number of followers, the variability in the distribution of follow er tweet similarity is significant.",Figure 7 confirms this intuition by plotting the distribution of tweet similarities of followers for several different users. Notice a high variability in the distribution of follower tweets for individual users. In order to reliably model the probability of a new follow we now account for this variability.,The Bursty Dynamics of the Twitter Information Network,"In online social media systems users are not only posting, consuming, and
resharing content, but also creating new and destroying existing connections in
the underlying social network. While each of these two types of dynamics has
individually been studied in the past, much less is known about the connection
between the two. How does user information posting and seeking behavior
interact with the evolution of the underlying social network structure?
  Here, we study ways in which network structure reacts to users posting and
sharing content. We examine the complete dynamics of the Twitter information
network, where users post and reshare information while they also create and
destroy connections. We find that the dynamics of network structure can be
characterized by steady rates of change, interrupted by sudden bursts.
Information diffusion in the form of cascades of post re-sharing often creates
such sudden bursts of new connections, which significantly change users' local
network structure. These bursts transform users' networks of followers to
become structurally more cohesive as well as more homogenous in terms of
follower interests. We also explore the effect of the information content on
the dynamics of the network and find evidence that the appearance of new topics
and real-world events can lead to significant changes in edge creations and
deletions. Lastly, we develop a model that quantifies the dynamics of the
network and the occurrence of these bursts as a function of the information
spreading through the network. The model can successfully predict which
information diffusion events will lead to bursts in network dynamics."
scgqa_50,1912.00035v1,What does Figure 8 reveal about the scheduling of QAOA circuits based on different initial qubit placements?,"The graph shows that the initial qubit placement policy has a significant impact on the schedule of QAOA circuits with p = 1. The policy that places qubits randomly (SRO: random) results in the longest schedules, while the policy that places qubits on a subgraph (SRO: subgraph) results in the shortest schedules. The other policies, which are SR1 one-qubit-first and SR2 dynamical-pattern-improvement, fall in between these two extremes.",1912.00035v1.pdf,"['1912.00035v1.pdf', '1208.2451v1.pdf', '1809.08207v1.pdf', '1504.07495v1.pdf', '1502.03556v1.pdf', '1905.05284v1.pdf', '1708.01249v1.pdf', '2004.05579v1.pdf']",1912.00035v1-Figure8-1.png,"FIG. 8. Impact of the policy for initial qubit placement (SR0) on the schedule of QAOA circuits with p = 1. Schedules obtained for the 17-qubit chip described in [9]) after appropriate gate decomposition. Each point represents the average of 224 instances of 3-regular graphs, and each circuit has been scheduled 2000 times. The statistical fluctuations of each point are of the same magnitude as in Fig. 10, but are not included here to increase the readability. The other policies are: SR1 one-qubit-first, SR2 dynamical-pattern-improvement, SR3 always.","First of all, we consider the unmodified trasmon architecture that requires gate decomposition for ZZ rotations. We aim at quantifying the impact of the initial qubit placement and therefore schedule the shallowest QAOA circuits composed by a single layer, i.e. for p = 1. The results are shown in Fig. 8 in which we average 224 instances of MaxCut on 3-regular graphs. Each circuit has been scheduled 2000 times and we report the instance average of the","Scheduler of quantum circuits based on dynamical pattern improvement and
  its application to hardware design","As quantum hardware increases in complexity, successful algorithmic execution
relies more heavily on awareness of existing device constraints. In this work
we focus on the problem of routing quantum information across the machine to
overcome the limited connectivity of quantum hardware. Previous approaches
address the problem for each two-qubit gate separately and then impose the
compatibility of the different routes. Here we shift the focus onto the set of
all routing operations that are possible at any given time and favor those that
most benefit the global pattern of two-qubit gates. We benchmark our
optimization technique by scheduling variational algorithms for transmon chips.
Finally we apply our scheduler to the design problem of quantifying the impact
of manufacturing decisions. Specifically, we address the number of distinct
qubit frequencies in superconducting architectures and how they affect the
algorithmic performance of the quantum Fourier transform."
scgqa_51,1804.06674v1,"According to the measurements in the study, what does Fig. 1 reveal about the computational costs related to ring signatures?","The graph shows that the generation time of a ballot is linear to the ring size. This is because the generation time is mainly spent on signing the ballot with a ring signature, and the computation of a ring signature is linear to the ring size. The verification time is also almost the same as the generation time, since the bottleneck of both is the computation of the ring signature. However, the verification time is still acceptable for voters to keep their anonymity.",1804.06674v1.pdf,"['1804.06674v1.pdf', '1910.09592v1.pdf', '1804.10488v2.pdf', '1801.09097v2.pdf', '2009.08716v1.pdf', '1808.08442v1.pdf']",1804.06674v1-Figure1-1.png,Fig. 1. The generation and verification time of a ballot.,"Our measurements are performed on a MacBook Pro running OS X 10.12.5 equipped with 2 core, 2.7 GHz Intel Core i5 and 8 GB DDR3 RAM. For communication with the blockchain conveniently, we written our code in Javascript. As figure 1. shows, the time spent by voter is mainly to sign the ballot with ring signature, which is linear to ring size. And the verification time is almost the same as the time of genration, since the bottleneck of both is computation of ring signature, but it is still acceptable for voters to keep their anonymity.",An efficient and effective Decentralized Anonymous Voting System,"A trusted electronic election system requires that all the involved
information must go public, that is, it focuses not only on transparency but
also privacy issues. In other words, each ballot should be counted anonymously,
correctly, and efficiently. In this work, a lightweight E-voting system is
proposed for voters to minimize their trust in the authority or government. We
ensure the transparency of election by putting all message on the Ethereum
blockchain, in the meantime, the privacy of individual voter is protected via
an efficient and effective ring signature mechanism. Besides, the attractive
self-tallying feature is also built in our system, which guarantees that
everyone who can access the blockchain network is able to tally the result on
his own, no third party is required after voting phase. More importantly, we
ensure the correctness of voting results and keep the Ethereum gas cost of
individual participant as low as possible, at the same time. Clearly, the
pre-described characteristics make our system more suitable for large-scale
election."
scgqa_52,1301.5201v1,How does the graph in Fig. 3 illustrate user membership changes related to political events in the blog data?,"The graph shows that the number of people belonging to one, two, or three stable groups in each interval increases over time. This is likely due to the increase in the popularity of the portal and the significance of political events taking place.",1301.5201v1.pdf,"['1301.5201v1.pdf', '1303.1635v1.pdf', '1906.07255v3.pdf']",1301.5201v1-Figure3-1.png,Fig. 3: Membership of people to groups for k=3 in comments model.,"In figs. 3a and 3b, the numbers of users belonging to one, two or three stable groups in each interval for k=3 are specified. The figure presents mentioned belongings only in the comments model, but in the post model diagram is very similar. We can notice that these numbers increase, mostly because of the increase of the popularity of the portal and the significance of political events taking place.","Models of Social Groups in Blogosphere Based on Information about
  Comment Addressees and Sentiments","This work concerns the analysis of number, sizes and other characteristics of
groups identified in the blogosphere using a set of models identifying social
relations. These models differ regarding identification of social relations,
influenced by methods of classifying the addressee of the comments (they are
either the post author or the author of a comment on which this comment is
directly addressing) and by a sentiment calculated for comments considering the
statistics of words present and connotation. The state of a selected blog
portal was analyzed in sequential, partly overlapping time intervals. Groups in
each interval were identified using a version of the CPM algorithm, on the
basis of them, stable groups, existing for at least a minimal assumed duration
of time, were identified."
scgqa_53,1809.01628v1,What does the mean accuracy rate representation in Figure 6 tell us regarding kDN value assessment?,"The graph shows the mean accuracy rate of OLA and LCA for each group of kDN value. This means that the accuracy rate of both DCS techniques is calculated for each group of kDN value, and the mean of these accuracy rates is plotted. The kDN value is a measure of the hardness of a sample, and the groups are created by dividing the samples into different ranges of kDN value.",1809.01628v1.pdf,"['1809.01628v1.pdf', '1804.06674v1.pdf', '1701.00365v2.pdf', '1910.03072v1.pdf', '2005.13754v1.pdf', '1208.2451v1.pdf', '1407.6074v1.pdf', '2008.07011v1.pdf']",1809.01628v1-Figure6-1.png,"Figure 6: Mean accuracy rate of OLA and LCA for each group of kDN value, for all datasets from Table 1. The neighborhood sizes of the DCS techniques and the kDN measure are ks = kh = 7.","In order to further characterize the relationship between the kDN measure and the DCS techniques, all samples from each dataset were grouped by their true hardness value and the mean accuracy rate of both DCS techniques on each group was calculated. The results are summarized in Figure 6.","Online local pool generation for dynamic classifier selection: an
  extended version","Dynamic Classifier Selection (DCS) techniques have difficulty in selecting
the most competent classifier in a pool, even when its presence is assured.
Since the DCS techniques rely only on local data to estimate a classifier's
competence, the manner in which the pool is generated could affect the choice
of the best classifier for a given sample. That is, the global perspective in
which pools are generated may not help the DCS techniques in selecting a
competent classifier for samples that are likely to be mislabelled. Thus, we
propose in this work an online pool generation method that produces a locally
accurate pool for test samples in difficult regions of the feature space. The
difficulty of a given area is determined by the classification difficulty of
the samples in it. That way, by using classifiers that were generated in a
local scope, it could be easier for the DCS techniques to select the best one
for the difficult samples. For the query samples in easy regions, a simple
nearest neighbors rule is used. In the extended version of this work, a deep
analysis on the correlation between instance hardness and the performance of
DCS techniques is presented. An instance hardness measure that conveys the
degree of local class overlap is then used to decide when the local pool is
used in the proposed scheme. The proposed method yielded significantly greater
recognition rates in comparison to a Bagging-generated pool and two other
global pool generation schemes for all DCS techniques evaluated. The proposed
scheme's performance was also significantly superior to three state-of-the-art
classification models and statistically equivalent to five of them. Moreover,
an extended analysis on the computational complexity of the proposed method and
of several DS techniques is presented in this version. We also provide the
implementation of the proposed technique using the DESLib library on GitHub."
scgqa_54,1902.05922v1,"In the phase field modeling research, what relationship does the graph depict between load and displacement under shear and tension?","The graph shows that the load increases with displacement until it reaches a peak, after which it drops quickly. The peak load and the residual load (the load after the drop) increase with the increase in the critical energy release rate. This suggests that the plate is more resistant to fracture when the critical energy release rate is higher.",1902.05922v1.pdf,"['1902.05922v1.pdf', '1707.04476v5.pdf', '1905.12729v2.pdf', '1306.4036v2.pdf', '1408.5389v1.pdf', '1202.4232v2.pdf', '1701.00365v2.pdf']",1902.05922v1-Figure18-1.png,Figure 18: Load-displacement curves for the 2D notched plate under shear and tension,"Figure 17 shows the crack patterns of the 2D notched plate when the displacement is δ = 0.026 mm. The crack for lager Gc has a smaller inclination angle to the horizontal direction and is more curved. This results in a smaller distance between the two parallel cracks. Figure 18 presents the load-displacement curves of the 2D notched plated under shear and tension for different critical energy release rates. A quick drop of the load after a nearly linear increase is observed. The peak and residual values of the load increases with the increase in the critical energy release rate. Figure 19 shows the crack propagation in the 3D notched square plate for Gc = 75 and 100 J/m 2. The crack patterns of the 2D and 3D simulations are in good agreement. The load-displacement curves of the 3D plate are depicted in Fig. 20, which are less steep after the peak compared with the 2D plate. The reason is that the 2D plate section suffers from tension perpendicular to the section under the plane strain assumption, which accelerates the drop in the load bearing capacity of the plate.","Phase field modeling of quasi-static and dynamic crack propagation:
  COMSOL implementation and case studies","The phase-field model (PFM) represents the crack geometry in a diffusive way
without introducing sharp discontinuities. This feature enables PFM to
effectively model crack propagation compared with numerical methods based on
discrete crack model, especially for complex crack patterns. Due to the
involvement of \phased field"", phase-field method can be essentially treated a
multifield problem even for pure mechanical problem. Therefore, it is supposed
that the implementation of PFM based on a software developer that especially
supports the solution of multifield problems should be more effective, simpler
and more efficient than PFM implemented on a general finite element software.
In this work, the authors aim to devise a simple and efficient implementation
of phase-field model for the modelling of quasi-static and dynamic fracture in
the general purpose commercial software developer, COMSOL Multiphysics. Notably
only the tensile stress induced crack is accounted for crack evolution by using
the decomposition of elastic strain energy. The width of the diffusive crack is
controlled by a length-scale parameter. Equations that govern body motion and
phase-field evolution are written into different modules in COMSOL, which are
then coupled to a whole system to be solved. A staggered scheme is adopted to
solve the coupled system and each module is solved sequentially during one time
step. A number of 2D and 3D examples are tested to investigate the performance
of the present implementation. Our simulations show good agreement with
previous works, indicating the feasibility and validity of the COMSOL
implementation of PFM."
scgqa_55,1606.04646v1,"Referring to Figure 3, what conclusions can be drawn about the nature of optimal solutions in the study?","The key takeaways from the graph are as follows:

* The global optimal solution to (2) (i.e., the first term in (4)) coincides with the global optimal solution of the supervised learning problem.
* There is a local optimal solution, which the algorithm could easily get stuck in.
* The cost function of the local optimal solution seems to be very close to that of the global optimal solution.
* The local optimal solution is a trivial solution which totally ignores the inputs, although its cost is close to that of the global optimal solution.",1606.04646v1.pdf,"['1606.04646v1.pdf', '1910.08413v1.pdf', '1810.04824v1.pdf', '1911.02623v1.pdf', '1707.02342v1.pdf', '2004.05448v1.pdf']",1606.04646v1-Figure3-1.png,Figure 3: The landscape of supervised cost function and unsupervised cost functions (with different levels of regularizations) along random lines that pass through the ground truth solution.,"we can make from Figure 2(a) is that the global optimal solution to (2) (i.e., the first term in (4)) coincides with the global optimal solution of the supervised learning problem. On the other hand, there is a local optimal solution, which the algorithm could easily get stuck in, as shown in the figure. We also note that the cost function of the local optimal solution seems to be very close to that of the global optimal solution. There are two important questions to ask: (i) how good is this local optimal solution in compare with the global optimal solution, and (ii) how does the regularization term (second term in (4)) help the algorithm escape from local optima. To answer the first question, we visualize the weight matrix Wd in the middle part of Figure 2(c). We observe that the columns of the matrix are linearly dependent and the matrix is almost rank one by computing its singular values. With Wd being rank-1 (e.g., Wd ≈ abT ), the probability p(yt|xt,Wd) = softmax(γabTxt) = softmax(a), which is independent of xt. Therefore, this local optimal solution is a trivial solution which totally ignores the inputs, although its cost is close to that of the global optimal solution. We repeated the experiments many times and all the local optimal solutions end up with rank-1. In Figures 3(a) and 3(b), we plot more landscapes of the supervised and unsupervised cost functions along other random lines that pass through the ground truth solution. From the figures, we note similar behaviors as in Figure 2.",Unsupervised Learning of Predictors from Unpaired Input-Output Samples,"Unsupervised learning is the most challenging problem in machine learning and
especially in deep learning. Among many scenarios, we study an unsupervised
learning problem of high economic value --- learning to predict without costly
pairing of input data and corresponding labels. Part of the difficulty in this
problem is a lack of solid evaluation measures. In this paper, we take a
practical approach to grounding unsupervised learning by using the same success
criterion as for supervised learning in prediction tasks but we do not require
the presence of paired input-output training data. In particular, we propose an
objective function that aims to make the predicted outputs fit well the
structure of the output while preserving the correlation between the input and
the predicted output. We experiment with a synthetic structural prediction
problem and show that even with simple linear classifiers, the objective
function is already highly non-convex. We further demonstrate the nature of
this non-convex optimization problem as well as potential solutions. In
particular, we show that with regularization via a generative model, learning
with the proposed unsupervised objective function converges to an optimal
solution."
scgqa_56,1905.12868v5,What does Figure 11 illustrate about GP and CGAN's performance on the sinus and multi-modal datasets when training data is decreased?,"The graph shows that GP performs well with small datasets, while CGAN suffers from a reduced training set size. This is because GP averages the predictions using the posterior in the weight space, while CGAN uses only a single set of weights. On the multi-modal dataset, GP performs as well as CGAN when the training set size is small. This is because CGAN is unable to model the multi-modal noise well with very small amounts of data.",1905.12868v5.pdf,"['1905.12868v5.pdf', '1612.07141v3.pdf', '1610.00017v2.pdf', '1804.00243v2.pdf', '1812.09355v1.pdf', '1512.02567v1.pdf', '1609.06577v1.pdf', '1706.03112v1.pdf']",1905.12868v5-Figure11-1.png,Figure 11: The effect of the size of training data for a) sinus b) multi-modal.,"GP, like other Bayesian methods, is known to perform well with small datasets. This is because these methods average the predictions using the posterior in the weight space. CGAN does not enjoy this property since it uses only a single set of weights. Given this, it is worthwhile to investigate how CGAN behaves compared to GP when the training sample size becomes small. We report the results in Figure 11 on sinus and multi-modal as the training size N decreases. Here we take N to be in {300, 500, 750, 1000, 1500}. We notice that the performance of GP is stable over the range of training set sizes. CGAN suffers from a reduced training set size. Interestingly, on the multi-modal dataset we find that below 300 instances, GP is performing as well as CGAN. This is because, with very small amount of data, CGAN is unable to mdel the multi-modal noise well.",Benchmarking Regression Methods: A comparison with CGAN,"In recent years, impressive progress has been made in the design of implicit
probabilistic models via Generative Adversarial Networks (GAN) and its
extension, the Conditional GAN (CGAN). Excellent solutions have been
demonstrated mostly in image processing applications which involve large,
continuous output spaces. There is almost no application of these powerful
tools to problems having small dimensional output spaces. Regression problems
involving the inductive learning of a map, $y=f(x,z)$, $z$ denoting noise,
$f:\mathbb{R}^n\times \mathbb{R}^k \rightarrow \mathbb{R}^m$, with $m$ small
(e.g., $m=1$ or just a few) is one good case in point. The standard approach to
solve regression problems is to probabilistically model the output $y$ as the
sum of a mean function $m(x)$ and a noise term $z$; it is also usual to take
the noise to be a Gaussian. These are done for convenience sake so that the
likelihood of observed data is expressible in closed form. In the real world,
on the other hand, stochasticity of the output is usually caused by missing or
noisy input variables. Such a real world situation is best represented using an
implicit model in which an extra noise vector, $z$ is included with $x$ as
input. CGAN is naturally suited to design such implicit models. This paper
makes the first step in this direction and compares the existing regression
methods with CGAN.
  We notice however, that the existing methods like mixture density networks
(MDN) and XGBoost do quite well compared to CGAN in terms of likelihood and
mean absolute error, respectively. Both these methods are comparatively easier
to train than CGANs. CGANs need more innovation to have a comparable modeling
and ease-of-training with respect to the existing regression solvers. In
summary, for modeling uncertainty MDNs are better while XGBoost is better for
the cases where accurate prediction is more important."
scgqa_57,1110.6199v1,"According to Figure 1, how do the error rates of C4 correlate with those of the binary image codes?","The bit error rate (BER) and frame error rate (FER) are two important metrics for evaluating the performance of a communication system. The BER is the probability of a bit being incorrectly received, while the FER is the probability of a frame being incorrectly received. In general, the BER is a more stringent metric than the FER, as it takes into account the fact that multiple bits can be in error within a single frame.

The graph shows the BER and FER for the random non-binary code C4 with parameters q = 22, nq = 96, kq = 32, dl = 2, dr = 3 and its binary images considered in Table II. As can be seen, the BER and FER for the non-binary code are significantly lower than those for the binary images. This is because the non-binary code has a higher degree of freedom, which allows it to better correct errors.

The graph also shows that the BER and FER for the non-binary code decrease as the signal-to-noise ratio (SNR) increases. This is expected, as a higher SNR means that there is less noise in the signal, which makes it easier to correctly decode the message.

Overall, the graph shows that the random non-binary code C4 with parameters q = 22, nq = 96, kq = 32, dl = 2, dr = 3 and its binary images considered in Table II have good BER and FER performance. This makes them suitable for use in communication systems that require high reliability.",1110.6199v1.pdf,"['1110.6199v1.pdf', '1509.00374v2.pdf', '2010.13691v1.pdf', '1804.04290v1.pdf', '1808.06304v2.pdf', '1207.3107v3.pdf']",1110.6199v1-Figure1-1.png,"Fig. 1. Bit (dashed curves) and frame (solid curves) error rates for the random non-binary code C4 with parameters q = 22, nq = 96, kq = 32, dl = 2, dr = 3 and its binary images considered in Table II.","For the codes under consideration, Figures 1 and 2 plot the BP performance of the non-binary codes, their basic binary",Enhancing Binary Images of Non-Binary LDPC Codes,"We investigate the reasons behind the superior performance of belief
propagation decoding of non-binary LDPC codes over their binary images when the
transmission occurs over the binary erasure channel. We show that although
decoding over the binary image has lower complexity, it has worse performance
owing to its larger number of stopping sets relative to the original non-binary
code. We propose a method to find redundant parity-checks of the binary image
that eliminate these additional stopping sets, so that we achieve performance
comparable to that of the original non-binary LDPC code with lower decoding
complexity."
scgqa_58,1801.08825v1,"According to Figure 1, how does social media activity correlate with key political milestones during the 2013 campaign?","The graph shows that social media activity by politicians and audiences over time is closely related to political events. For example, the vertical lines in the graph represent the TV debate between the party leaders Angela Merkel and Peer Steinbrück (1 September 2013) and election day (22 September 2013). These events are clearly visible in the graph as spikes in social media activity. This indicates that social media users are particularly active during these high-attention periods.",1801.08825v1.pdf,"['1801.08825v1.pdf', '1209.5833v2.pdf', '1810.04915v1.pdf', '1910.00110v2.pdf', '1207.3107v3.pdf', '2010.13032v1.pdf']",1801.08825v1-Figure1-1.png,Figure 1: Social media messages over time.,"We start our analysis with a description of social media activity by politicians and audiences over time using the raw counts of messages from the unpreprocessed dataset. This gives us first indications about specific focus points and the topics predominantly addressed on social media. The vertical lines in Figure 1 represent the TV debate between the party leaders Angela Merkel and Peer Steinbrück (1 September 2013) and election day (22 September 2013). Especially the televised debate drew attention (see also Jungherr et al., 2016; Lietz, Wagner, Bleier, & Strohmaier, 2014), since social media users are particularly active during these high-attention periods (Diaz et al., 2016; Kreiss, 2016). Interestingly, the Facebook politicians time series barely reacts to the TV debate and not nearly in the order of magnitude like the other three time series. This indicates","Election campaigning on social media: Politicians, audiences and the
  mediation of political communication on Facebook and Twitter","Although considerable research has concentrated on online campaigning, it is
still unclear how politicians use different social media platforms in political
communication. Focusing on the German federal election campaign 2013, this
article investigates whether election candidates address the topics most
important to the mass audience and to which extent their communication is
shaped by the characteristics of Facebook and Twitter. Based on open-ended
responses from a representative survey conducted during the election campaign,
we train a human-interpretable Bayesian language model to identify political
topics. Applying the model to social media messages of candidates and their
direct audiences, we find that both prioritize different topics than the mass
audience. The analysis also shows that politicians use Facebook and Twitter for
different purposes. We relate the various findings to the mediation of
political communication on social media induced by the particular
characteristics of audiences and sociotechnical environments."
scgqa_59,1607.08438v1,Can you explain the significance of negative edge pruning for the results shown in the paper's Figure 13?,"Negative edge pruning is a technique that removes edges from a graph that are not considered to be important. This can be done by setting a threshold on the edge weights, and removing any edges that have a weight below this threshold. In the context of this graph, the edge weights represent the match score between two events. By pruning edges with low match scores, we can reduce the number of edges in the graph, which can improve the performance of the inference algorithm.",1607.08438v1.pdf,"['1607.08438v1.pdf', '1803.11512v1.pdf', '1301.5201v1.pdf', '1701.00365v2.pdf', '1403.5617v1.pdf', '1808.06304v2.pdf', '1809.09034v1.pdf', '1402.0635v3.pdf', '1906.07255v3.pdf']",1607.08438v1-Figure13-1.png,Fig. 13: Effect of negative edge pruning on the inference performance.,"Negative edge pruning We prune edges below certain match score (ψθ̃(Xi,Xj) < β), as simply adding pairwise terms can hurt the performance (figure 7 in main paper). In figure 13, we show the performance of the Faceless Recognition system on the validation set at different pruning thresholds β ∈ [0, 1]. Again for obfuscation scenarios (S2 and S3), black obfuscation is used. As mentioned in the main paper, we observe that any threshold in the range [0.4, 0.7] works equally fine, and thus use β = 0.5 in all the experiments.",Faceless Person Recognition; Privacy Implications in Social Media,"As we shift more of our lives into the virtual domain, the volume of data
shared on the web keeps increasing and presents a threat to our privacy. This
works contributes to the understanding of privacy implications of such data
sharing by analysing how well people are recognisable in social media data. To
facilitate a systematic study we define a number of scenarios considering
factors such as how many heads of a person are tagged and if those heads are
obfuscated or not. We propose a robust person recognition system that can
handle large variations in pose and clothing, and can be trained with few
training samples. Our results indicate that a handful of images is enough to
threaten users' privacy, even in the presence of obfuscation. We show detailed
experimental results, and discuss their implications."
scgqa_60,1809.08207v1,"In the context of the IoBT research, what relationship does Figure 3 illustrate between sensor compromise and energy efficiency?","The graph shows that the percentage decrease in energy consumed decreases as the probability that a sensor is compromised increases. This is because the equilibrium solution is less efficient when there are more compromised sensors, as the sensors cannot share information with each other as effectively.",1809.08207v1.pdf,"['1809.08207v1.pdf', '1710.10733v4.pdf', '1701.06190v1.pdf', '1611.04706v2.pdf', '1910.03072v1.pdf', '1904.06587v1.pdf']",1809.08207v1-Figure3-1.png,Figure 3: Percentage decrease of energy consumed versus the probability that a sensor is compromised for different numbers of sensors.,Fig. 3 shows the percentage decrease of energy consumed using the equilibrium solution compared to the baseline versus the probability that a sensor,"A Graphical Bayesian Game for Secure Sensor Activation in Internet of
  Battlefield Things","In this paper, the problem of secure sensor activation is studied for an
Internet of Battlefield Things (IoBT) system in which an attacker compromises a
set of the IoBT sensors for the purpose of eavesdropping and acquiring
information about the battlefield. In the considered model, each IoBT sensor
seeks to decide whether to transmit or not based on its utility. The utility of
each sensor is expressed in terms of the redundancy of the data transmitted,
the secrecy capacity and the energy consumed. Due to the limited communication
range of the IoBT sensors and their uncertainty about the location of the
eavesdroppers, the problem is formulated as a graphical Bayesian game in which
the IoBT sensors are the players. Then, the utilities of the IoBT sensors are
modified to take into account the effect of activating each sensor on the
utilities of its neighbors, in order to achieve better system performance. The
modified game is shown to be a Bayesian potential game, and a best response
algorithm that is guaranteed to find a Nash equilibrium of the game is
proposed. Simulation results show the tradeoff between the information
transmitted by the IoBT sensors and the desired secrecy level. Simulation
results also demonstrate the effectiveness of the proposed approach in reducing
the energy consumed compared to the baseline in which all the IoBT sensors are
activated. The reduction in energy consumption reaches up to 98% compared to
the baseline, when the number of sensors is 5000."
scgqa_61,1607.06988v1,"According to Figure 2, how does the mistake bound of interactive perceptron compare to the standard perceptron across different εs?","The graph shows that the mistake bound of interactive perceptron is improved over standard perceptron for all values of the margin γ̂. This improvement is more pronounced for smaller values of εs. For example, for εs = 0.0001, the mistake bound of interactive perceptron is improved by a factor of 100 for γ̂ = 10.",1607.06988v1.pdf,"['1607.06988v1.pdf', '1912.00035v1.pdf', '1606.04646v1.pdf', '2007.11446v1.pdf', '1505.02851v1.pdf']",1607.06988v1-Figure2-1.png,Figure 2: Illustration of the improvement in the mistake bound of interactive perceptron when compared to standard perceptron. The dashed line is y = 1.,"clearly improving the mistake bound of the standard perceptron algorithm. However, εs = 0 is not a realistic assumption. We therefore plot x-fold improvement of the mistake bound as a function of εs for a range of margins γ̂ in Figure 2. The y-axis is the ratio of mistake bounds of interactive perceptron to standard perceptron with all examples scaled to have unit Euclidean length (R = 1) and ‖u‖ = 1. From the figure, it is clear that even when εs > 0, it is possible to get non-trivial improvements in the mistake bound.",Interactive Learning from Multiple Noisy Labels,"Interactive learning is a process in which a machine learning algorithm is
provided with meaningful, well-chosen examples as opposed to randomly chosen
examples typical in standard supervised learning. In this paper, we propose a
new method for interactive learning from multiple noisy labels where we exploit
the disagreement among annotators to quantify the easiness (or meaningfulness)
of an example. We demonstrate the usefulness of this method in estimating the
parameters of a latent variable classification model, and conduct experimental
analyses on a range of synthetic and benchmark datasets. Furthermore, we
theoretically analyze the performance of perceptron in this interactive
learning framework."
scgqa_62,1906.07610v2,How does the MTL negation model's accuracy relate to the number of negation examples in the experiment?,"The graph shows that the MTL negation model improves over the baseline with as few as ten negation examples and plateaus somewhere near 600. This suggests that the model is able to learn from a relatively small number of examples, and that there is a point of diminishing returns when it comes to adding more data.",1906.07610v2.pdf,"['1906.07610v2.pdf', '1811.01194v1.pdf', '2002.11440v1.pdf', '1608.08469v1.pdf']",1906.07610v2-Figure4-1.png,Fig. 4: Mean accuracy on the SST-binary task when training MTL negation model with differing amounts of negation data from the SFU dataset (left) and sentiment data (right).,"the SFU dataset (from 10 to 800 in intervals of 100) and accuracy is calculated for each number of examples. Figure 4 (left) shows that the MTL model improves over the baseline with as few as ten negation examples and plateaus somewhere near 600. An analysis on the SST-fine setup showed a similar pattern. There is nearly always an effect of diminishing returns when it comes to adding training examples, but if we were to instead plot this learning curve with a log-scale on the x-axis, i.e. doubling the amount data for each increment, it would seem to indicate that having more data could indeed still prove useful, as long as there were sufficient amounts. In any case, regardless of the amount of data, exposing the model to a larger variety of negation examples could also prove beneficial – we follow up on this point in the next subsection.",Improving Sentiment Analysis with Multi-task Learning of Negation,"Sentiment analysis is directly affected by compositional phenomena in
language that act on the prior polarity of the words and phrases found in the
text. Negation is the most prevalent of these phenomena and in order to
correctly predict sentiment, a classifier must be able to identify negation and
disentangle the effect that its scope has on the final polarity of a text. This
paper proposes a multi-task approach to explicitly incorporate information
about negation in sentiment analysis, which we show outperforms learning
negation implicitly in a data-driven manner. We describe our approach, a
cascading neural architecture with selective sharing of LSTM layers, and show
that explicitly training the model with negation as an auxiliary task helps
improve the main task of sentiment analysis. The effect is demonstrated across
several different standard English-language data sets for both tasks and we
analyze several aspects of our system related to its performance, varying types
and amounts of input data and different multi-task setups."
scgqa_63,1509.08992v2,What does the center graph in Figure 2 reveal about the behavior of parameter estimates during optimization?,"The graph shows that the distance between the current estimated parameters and the optimal parameters also decreases as the number of iterations increases. This is again to be expected, as the algorithm is designed to converge to the optimal solution.",1509.08992v2.pdf,"['1509.08992v2.pdf', '1707.04849v1.pdf', '2008.01961v3.pdf', '1905.00569v2.pdf', '2006.04002v2.pdf', '1805.05887v1.pdf', '1905.12729v2.pdf']",1509.08992v2-Figure2-1.png,"Figure 2: Ising Model Example. Left: The difference of the current test log-likelihood from the optimal log-likelihood on 5 random runs. Center: The distance of the current estimated parameters from the optimal parameters on 5 random runs. Right: The current estimated parameters on one run, as compared to the optimal parameters (far right).","24× (2× .2)2, C = log(16) and α = exp(−(1 − 4 tanh .2)/16). Applying Corollary 9 with β1 = .01, β2 = .9 and β3 = .1 gives K = 46, M = 1533 and v = 561. Fig. 2 shows the results. In practice, the algorithm finds a solution tighter than the specified ǫθ, indicating a degree of conservatism in the theoretical bound.","Maximum Likelihood Learning With Arbitrary Treewidth via Fast-Mixing
  Parameter Sets","Inference is typically intractable in high-treewidth undirected graphical
models, making maximum likelihood learning a challenge. One way to overcome
this is to restrict parameters to a tractable set, most typically the set of
tree-structured parameters. This paper explores an alternative notion of a
tractable set, namely a set of ""fast-mixing parameters"" where Markov chain
Monte Carlo (MCMC) inference can be guaranteed to quickly converge to the
stationary distribution. While it is common in practice to approximate the
likelihood gradient using samples obtained from MCMC, such procedures lack
theoretical guarantees. This paper proves that for any exponential family with
bounded sufficient statistics, (not just graphical models) when parameters are
constrained to a fast-mixing set, gradient descent with gradients approximated
by sampling will approximate the maximum likelihood solution inside the set
with high-probability. When unregularized, to find a solution epsilon-accurate
in log-likelihood requires a total amount of effort cubic in 1/epsilon,
disregarding logarithmic factors. When ridge-regularized, strong convexity
allows a solution epsilon-accurate in parameter distance with effort quadratic
in 1/epsilon. Both of these provide of a fully-polynomial time randomized
approximation scheme."
scgqa_64,2005.09814v3,What trade-off between m and performance is illustrated in Figure 1 for the Walker2d task using on-policy MDPO?,"The graph shows that there is a clear trade-off between m and performance. In general, increasing m leads to better performance, but at the cost of increased computational cost. However, m = 10 seems to be the best value for most tasks, as it provides a good balance between performance and computational cost.",2005.09814v3.pdf,"['2005.09814v3.pdf', '1206.6850v1.pdf', '1509.01310v1.pdf']",2005.09814v3-Figure1-1.png,Figure 1: Performance of on-policy (top) and off-policy (bottom) MDPO (code level optimizations included) for different values of m on the Walker2d task. X-axis represents time steps in millions.,"In on-policy MDPO, we implement the multi-step update at each MD iteration of the algorithm, by sampling M trajectories from the current policy, generating estimates of the advantage function, and performing m gradient steps using the same set of trajectories. We evaluated on-policy MDPO for different values of m in all tasks. We show the results for Walker2d in Figure 1 (top). The results for all tasks show a clear trade-off between m and the performance. Moreover, m = 10 seems to be the best value for most tasks. This is why we use m = 10 in all our on-policy MDPO experiments. Our results clearly indicate that using m = 1 leads to inferior performance as compared to m = 10, reaffirming the theory that suggests solving the trust-region problem in RL requires taking several gradient steps at each MD iteration. Finally, we ran a similar experiment for TRPO which shows that performing multiple gradient steps at each iteration of TRPO does not lead to any improvement. In",Mirror Descent Policy Optimization,"Mirror descent (MD), a well-known first-order method in constrained convex
optimization, has recently been shown as an important tool to analyze
trust-region algorithms in reinforcement learning (RL). However, there remains
a considerable gap between such theoretically analyzed algorithms and the ones
used in practice. Inspired by this, we propose an efficient RL algorithm,
called {\em mirror descent policy optimization} (MDPO). MDPO iteratively
updates the policy by {\em approximately} solving a trust-region problem, whose
objective function consists of two terms: a linearization of the standard RL
objective and a proximity term that restricts two consecutive policies to be
close to each other. Each update performs this approximation by taking multiple
gradient steps on this objective function. We derive {\em on-policy} and {\em
off-policy} variants of MDPO, while emphasizing important design choices
motivated by the existing theory of MD in RL. We highlight the connections
between on-policy MDPO and two popular trust-region RL algorithms: TRPO and
PPO, and show that explicitly enforcing the trust-region constraint is in fact
{\em not} a necessity for high performance gains in TRPO. We then show how the
popular soft actor-critic (SAC) algorithm can be derived by slight
modifications of off-policy MDPO. Overall, MDPO is derived from the MD
principles, offers a unified approach to viewing a number of popular RL
algorithms, and performs better than or on-par with TRPO, PPO, and SAC in a
number of continuous control tasks. Code is available at
\url{https://github.com/manantomar/Mirror-Descent-Policy-Optimization}."
scgqa_65,1212.3950v3,What insights does the error value in Fig. 9 offer regarding the performance of the localization method?,"The error measure in the graph is a key indicator of the performance of the localization procedure. It represents the average distance in meters between each node's estimated location and its real position. This measure is important because it provides a direct comparison between the estimated location and the actual location of the node. A low error value indicates that the localization procedure is accurate, while a high error value indicates that the localization procedure is inaccurate.",1212.3950v3.pdf,"['1212.3950v3.pdf', '1505.02851v1.pdf', '1404.7045v3.pdf']",1212.3950v3-Figure9-1.png,Fig. 9. Associated error after protocol selection,3) Error: This measure illustrates the average distance in meters between each node’s estimated location and its real position. The prefix Loc. Proc. in Figure 9 highlights the fact that these are results gathered from the execution of the localization procedure.,Localization Procedure for Randomly Deployed Wireless Sensor Networks,"Wireless Sensor Networks (WSNs) are composed of nodes that gather metrics
like temperature, pollution or pressure from events generated by external
entities. Localization in WSNs is paramount, given that the collected metrics
must be related to the place of occurrence. This document presents an
alternative way towards localization in randomly deployed WSNs based on the
composability of localization protocols. Results show a totally distributed
localization procedure that achieves a higher number of located nodes than the
conventional, individual execution of localization protocols while maintaining
the same low levels of battery consumption."
scgqa_66,1904.03292v2,How does the ImageNet experiment in Figure 3 illustrate the relationship between dataset complexity and total loss?,"The graph shows that as the complexity of a dataset increases, the total loss for different values of β drops to zero with different rates. This reflects the fact that MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 are in increasing order of difficulty.",1904.03292v2.pdf,"['1904.03292v2.pdf', '1710.06548v1.pdf', '1907.10906v1.pdf', '1505.05173v6.pdf', '1905.05538v1.pdf', '1701.06190v1.pdf', '2008.06134v1.pdf', '1209.3394v5.pdf']",1904.03292v2-Figure3-1.png,"Figure 3: The same experiment as in Figure 2, on the ImageNet dataset [7] and using a pretrained ResNet-34 [9].","a simple dataset such as MNIST. Notice that, in this experiment, the critical value for random labels is not β∗ = 1: This is because the complexity is computed using an uninformative prior (see Section 5.1), and not the universal prior as in Proposition 3.6. In Figure 2 (left), as the complexity increases, we see that the total loss for different datasets drops to zero with different rates. This reflects the fact that MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 are in increasing order of difficulty. Figure 3 illustrates the results of the same experiment on the (much larger) ImageNet dataset [7].","The Information Complexity of Learning Tasks, their Structure and their
  Distance","We introduce an asymmetric distance in the space of learning tasks, and a
framework to compute their complexity. These concepts are foundational for the
practice of transfer learning, whereby a parametric model is pre-trained for a
task, and then fine-tuned for another. The framework we develop is
non-asymptotic, captures the finite nature of the training dataset, and allows
distinguishing learning from memorization. It encompasses, as special cases,
classical notions from Kolmogorov complexity, Shannon, and Fisher Information.
However, unlike some of those frameworks, it can be applied to large-scale
models and real-world datasets. Our framework is the first to measure
complexity in a way that accounts for the effect of the optimization scheme,
which is critical in Deep Learning."
scgqa_67,1701.00365v2,"In the context of Fig. 10, how does SNR impact the probability of completing channel estimation swiftly?",The graph shows that users at a larger SNR are more likely to estimate their channel before those with low SNR. This is because users with a larger SNR have a higher probability of receiving a sufficient number of channel measurements within a given time frame.,1701.00365v2.pdf,"['1701.00365v2.pdf', '1106.3826v2.pdf', '1305.1657v1.pdf', '1603.04153v1.pdf', '2005.14165v4.pdf', '1506.06213v1.pdf', '1210.1356v2.pdf']",1701.00365v2-Figure10-1.png,Fig. 10. Cumulative distribution functions for users with different SNR completing their channel estimation before TE .,"In order to gain an insight into when a user is likely to complete its channel estimation, we plot in Fig. 10 the cumulative distribution function (CDF) that a user completes its channel estimation before a given duration TE for both SWIFT approaches with various SNR values. From Fig. 10, we can first see that users at a larger SNR are more likely to estimate their channel before those with low SNR. With a large number of users distributed across all SNRs, we can infer that the occurrence of channel estimation feedback events would be spread over a large number of different times. As a result, this would alleviate pressure on the feedback channel (used by each user to feedback beamforming directions to the BS) as the number of users needing to communicate at any given time would be significantly reduced. It is also interesting to compare the differences in CDFs for both SWIFT variations. It can be seen that at high SNR, both approaches have similar CDFs which is consistent with the observation in Fig. 8 where both approaches are seen to have the same average number of measurements. At low-to-medium SNR, it can be seen that, by adopting SWIFT-PEPA, users having a greater probability of completing their estimation with a shorter duration. This is again consistent with Fig. 8, where SWIFT-PEPA has a lower average number of measurements in the low-to-medium SNR ranges. To validate our convergence analysis, Fig. 10 also shows the upper bound for the probability of convergence in (45). As can seen, both SWIFT-FPA and SWIFT-PEPA satisfy the bound. At larger values of TE , the numerical CDFs do not approach the convergence bound due to the non-zero probability of the channel either being in a deep fade or having no paths, which were not considered in this convergence bound.","Beam-On-Graph: Simultaneous Channel Estimation for mmWave MIMO Systems
  with Multiple Users","This paper is concerned with the channel estimation problem in multi-user
millimeter wave (mmWave) wireless systems with large antenna arrays. We develop
a novel simultaneous-estimation with iterative fountain training (SWIFT)
framework, in which multiple users estimate their channels at the same time and
the required number of channel measurements is adapted to various channel
conditions of different users. To achieve this, we represent the beam direction
estimation process by a graph, referred to as the beam-on-graph, and associate
the channel estimation process with a code-on-graph decoding problem.
Specifically, the base station (BS) and each user measure the channel with a
series of random combinations of transmit/receive beamforming vectors until the
channel estimate converges. As the proposed SWIFT does not adapt the BS's beams
to any single user, we are able to estimate all user channels simultaneously.
Simulation results show that SWIFT can significantly outperform the existing
random beamforming-based approaches, which use a predetermined number of
measurements, over a wide range of signal-to-noise ratios and channel coherence
time. Furthermore, by utilizing the users' order in terms of completing their
channel estimation, our SWIFT framework can infer the sequence of users'
channel quality and perform effective user scheduling to achieve superior
performance."
scgqa_68,1402.1892v2,What trends are observed in experimentally chosen thresholds with varying base rates in the study's simulations?,"The graph shows that as the base rate decreases, the distribution of experimentally chosen thresholds shifts from predicting almost all positives to almost all negatives. This is because the optimal decision in all cases is to predict all positive, i.e. to use a threshold of 0. However, as the base rate decreases, the probability of a positive example becomes smaller, and so the threshold that maximizes F1 on the training set must be increased to avoid predicting too many negatives. This results in a shift in the distribution of experimentally chosen thresholds to the right.",1402.1892v2.pdf,"['1402.1892v2.pdf', '1101.0235v1.pdf', '1304.7375v1.pdf']",1402.1892v2-Figure8-1.png,"Fig. 8: The distribution of experimentally chosen thresholds changes with varying b. For small b, a small fraction of examples are predicted positive even though the optimal thresholding is to predict all positive.","We simulate this behavior, executing 10,000 runs for each setting of the base rate, with n = 106 samples for each run to set the threshold (Figure 8). Scores are chosen using variance σ2 = 1. True labels are assigned at the base rate, independent of the scores. The threshold that maximizes F1 on the training set is selected. We plot a histogram of the fraction predicted positive as a function of the empirically chosen threshold. There is a shift from predicting almost all positives to almost all negatives as base rate is decreased. In particular, for low base rate b, even with a large number of samples, a small fraction of examples are predicted positive. The analytically derived optimal decision in all cases is to predict all positive, i.e. to use a threshold of 0.",Thresholding Classifiers to Maximize F1 Score,"This paper provides new insight into maximizing F1 scores in the context of
binary classification and also in the context of multilabel classification. The
harmonic mean of precision and recall, F1 score is widely used to measure the
success of a binary classifier when one class is rare. Micro average, macro
average, and per instance average F1 scores are used in multilabel
classification. For any classifier that produces a real-valued output, we
derive the relationship between the best achievable F1 score and the
decision-making threshold that achieves this optimum. As a special case, if the
classifier outputs are well-calibrated conditional probabilities, then the
optimal threshold is half the optimal F1 score. As another special case, if the
classifier is completely uninformative, then the optimal behavior is to
classify all examples as positive. Since the actual prevalence of positive
examples typically is low, this behavior can be considered undesirable. As a
case study, we discuss the results, which can be surprising, of applying this
procedure when predicting 26,853 labels for Medline documents."
scgqa_69,1306.4036v2,"According to Fig. 4 of the paper, what is the effect of increasing SNR on quantization level efficiency?","The graph shows that as the SNR increases, the network performs better with finer quantization levels. This is because with higher SNR, the noise is less of a problem, so the network can afford to use finer quantization levels without sacrificing performance.",1306.4036v2.pdf,"['1306.4036v2.pdf', '1706.03112v1.pdf', '1710.07771v1.pdf', '1409.2897v1.pdf', '1707.02327v1.pdf', '1509.00374v2.pdf', '1604.06979v1.pdf', '1906.11938v3.pdf', '1803.03080v1.pdf']",1306.4036v2-Figure4-1.png,"Fig. 4: Contribution of a sensor to the overall conditional FI at the FC as a function ofα, for different number of quantization levels whenθ = 0 and A = 2. The pentagrams on the x-axis correspond to theαblind for 1-bit, 2-bit, 3-bit and 4-bit quantizations respectively from left to right.","Figure 4 plots the conditional FI corresponding to one sensor, for different values of α and M , when the uniform quantizer is centered around the true value of θ. Note that as SNR increases (σ → 0), we observe that it is better for the network to perform as much finer quantization as possible to mitigate the Byzantine attackers. On the other hand, if SNR is low, coarse quantization performs better for lower values of α. This phenomenon of coarse quantization performing better under low SNR scenarios, can be attributed to the fact that more noise gets filtered as the quantization gets coarser (decreasing M ) than the signal itself. On the other hand, in the case of high SNR, since the signal level is high, coarse quantization cancels out the signal component significantly, thereby resulting in a degradation in performance.","Distributed Inference with M-ary Quantized Data in the Presence of
  Byzantine Attacks","The problem of distributed inference with M-ary quantized data at the sensors
is investigated in the presence of Byzantine attacks. We assume that the
attacker does not have knowledge about either the true state of the phenomenon
of interest, or the quantization thresholds used at the sensors. Therefore, the
Byzantine nodes attack the inference network by modifying modifying the symbol
corresponding to the quantized data to one of the other M symbols in the
quantization alphabet-set and transmitting the false symbol to the fusion
center (FC). In this paper, we find the optimal Byzantine attack that blinds
any distributed inference network. As the quantization alphabet size increases,
a tremendous improvement in the security performance of the distributed
inference network is observed.
  We also investigate the problem of distributed inference in the presence of
resource-constrained Byzantine attacks. In particular, we focus our attention
on two problems: distributed detection and distributed estimation, when the
Byzantine attacker employs a highly-symmetric attack. For both the problems, we
find the optimal attack strategies employed by the attacker to maximally
degrade the performance of the inference network. A reputation-based scheme for
identifying malicious nodes is also presented as the network's strategy to
mitigate the impact of Byzantine threats on the inference performance of the
distributed sensor network."
scgqa_70,1509.08992v2,What does Figure 2 reveal about the relationship between current estimates and optimal parameters in the study?,The graph shows that the current estimated parameters on one run are close to the optimal parameters. This suggests that the algorithm is able to find a good solution even with a small number of iterations.,1509.08992v2.pdf,"['1509.08992v2.pdf', '2002.06199v1.pdf', '1902.05312v2.pdf', '1809.09034v1.pdf', '1206.5265v1.pdf', '1802.03830v1.pdf', '1902.07084v2.pdf']",1509.08992v2-Figure2-1.png,"Figure 2: Ising Model Example. Left: The difference of the current test log-likelihood from the optimal log-likelihood on 5 random runs. Center: The distance of the current estimated parameters from the optimal parameters on 5 random runs. Right: The current estimated parameters on one run, as compared to the optimal parameters (far right).","24× (2× .2)2, C = log(16) and α = exp(−(1 − 4 tanh .2)/16). Applying Corollary 9 with β1 = .01, β2 = .9 and β3 = .1 gives K = 46, M = 1533 and v = 561. Fig. 2 shows the results. In practice, the algorithm finds a solution tighter than the specified ǫθ, indicating a degree of conservatism in the theoretical bound.","Maximum Likelihood Learning With Arbitrary Treewidth via Fast-Mixing
  Parameter Sets","Inference is typically intractable in high-treewidth undirected graphical
models, making maximum likelihood learning a challenge. One way to overcome
this is to restrict parameters to a tractable set, most typically the set of
tree-structured parameters. This paper explores an alternative notion of a
tractable set, namely a set of ""fast-mixing parameters"" where Markov chain
Monte Carlo (MCMC) inference can be guaranteed to quickly converge to the
stationary distribution. While it is common in practice to approximate the
likelihood gradient using samples obtained from MCMC, such procedures lack
theoretical guarantees. This paper proves that for any exponential family with
bounded sufficient statistics, (not just graphical models) when parameters are
constrained to a fast-mixing set, gradient descent with gradients approximated
by sampling will approximate the maximum likelihood solution inside the set
with high-probability. When unregularized, to find a solution epsilon-accurate
in log-likelihood requires a total amount of effort cubic in 1/epsilon,
disregarding logarithmic factors. When ridge-regularized, strong convexity
allows a solution epsilon-accurate in parameter distance with effort quadratic
in 1/epsilon. Both of these provide of a fully-polynomial time randomized
approximation scheme."
scgqa_71,1802.03830v1,"According to Figure 3, what is the impact of minibatch size on stochastic algorithm performance in multi-task learning?","The graph shows that the performance of stochastic algorithms improves with increasing minibatch size. This is because larger minibatches allow for more accurate gradient estimates, which in turn leads to faster convergence. However, the benefits of increasing the minibatch size are offset by the increased communication cost. Therefore, it is important to choose a minibatch size that strikes a balance between accuracy and communication cost.",1802.03830v1.pdf,"['1802.03830v1.pdf', '1905.12729v2.pdf', '1512.02567v1.pdf', '1405.7705v1.pdf', '1606.01062v1.pdf', '1906.11938v3.pdf', '1912.00088v1.pdf']",1802.03830v1-Figure3-1.png,Figure 3: Performance of stochastic algorithms with various minibatch sizes. Here C = 10.,"Figure 3 shows for each method the estimated F (W) over iterations (or rounds of communication) in the left plot, and over the amount of fresh samples processed (or total computation cost) in the right plot. As a reference, the error of Local and Centralized (using n = 500 samples per machine) are also given in the plots. We observe that with fresh samples, stochastic algorithms are",Distributed Stochastic Multi-Task Learning with Graph Regularization,"We propose methods for distributed graph-based multi-task learning that are
based on weighted averaging of messages from other machines. Uniform averaging
or diminishing stepsize in these methods would yield consensus (single task)
learning. We show how simply skewing the averaging weights or controlling the
stepsize allows learning different, but related, tasks on the different
machines."
scgqa_72,1902.07084v2,"According to the findings of the paper, how does average polarization relate to zealot presence in dense networks?","The graph shows that the average polarization of a network is not affected by the number of zealots or contrarians when the network is not too dense. However, when the network is denser, the average polarization increases with the number of zealots or contrarians. This is because in a denser network, there are more opportunities for zealots or contrarians to influence their neighbors, which leads to a higher level of polarization.",1902.07084v2.pdf,"['1902.07084v2.pdf', '1512.02567v1.pdf', '1405.6298v2.pdf', '1206.5265v1.pdf', '1309.3959v1.pdf', '1805.06370v2.pdf']",1902.07084v2-Figure2-1.png,"FIG. 2. Variation of the average value of the polarization 〈φ〉 with pz for the Erdős-Rényi graph with SICs (top panel) and RICs (bottom panel) for different values of the average degree c. The network size in each case is N = 5000, and averages are obtained using 1000 random realizations. Error bars show the range containing middle 90% of the data.","values of c, are shown in Fig. 2. As can be seen from the figure, average polarization 〈φ〉 is almost unaffected for SICs when the network is not too dense, i.e. when c is small. When c is large, for small values of pz, there is a modest increase in the polarization, and it saturates for larger pz values. For RICs, when the network is sparse, φ steadily decreases as the number of zealots is increased, but for denser networks it first increases, and then goes to zero for large values of pz. A deeper insight into these",Do zealots increase or decrease the polarization in social networks?,"Zealots are the vertices in a social network who do not change their opinions
under social pressure, and are crucial to the study of opinion dynamics on
complex networks. In this paper, we study the effect of zealots on the
polarization dynamics of a deterministic majority-rule model using the
configuration model as a substrate. To this end, we propose a novel quantifier,
called `correlated polarization', for measuring the amount of polarization in
the network when vertices can exists in two opposite states. The quantifier
takes into account not only the fraction of vertices with each opinion, but
also how they are connected to each other. We then show that the presence of
zealots does not have a fixed effect on the polarization, and can change it in
positive, negative or neutral way depending upon their topological
characteristics like degree, their total fraction in the network, density and
degree heterogeneity of the network, and the type of initial conditions of the
dynamics. Our results particularly highlight the importance of the role played
by the initial conditions in drifting the polarization towards lower or higher
values as the total number of zealots is increased."
scgqa_73,1909.05034v1,"In the context of the paper, what is indicated about streamline uniformity with increasing ν−1 in Figure 2?","The graph shows that as the value of ν−1 increases, the streamlines of the steady state solution become more uniform. This is because as ν−1 increases, the flow becomes more laminar and the streamlines become more parallel to each other. This is consistent with the results of [12], where it was shown that the iterative least-squares method is more robust for small values of ν.",1909.05034v1.pdf,"['1909.05034v1.pdf', '1509.01310v1.pdf', '1904.06587v1.pdf', '1906.03859v1.pdf']",1909.05034v1-Figure2-1.png,"Figure 2. Streamlines of the steady state solution for ν−1 = 500, 1000, 2000, 3000, 4000, 5000, 6000 and ν−1 = 7000.","This example has been used in [12] to solve the corresponding steady problem (for which the weak solution is not unique), using again an iterative least-squares strategy. There, the method proved to be robust enough for small values of ν of the order 10−4, while standard Newton method failed. Figures 2 depicts the streamlines of steady state solutions corresponding to ν−1 = 500 and to ν−1 = i×103 for i = 1, · · · , 7. The figures are in very good agreements with those depicted in [6]. When the Reynolds number (here equal to ν−1) is small, the final steady","A fully space-time least-squares method for the unsteady Navier-Stokes
  system","We introduce and analyze a space-time least-squares method associated to the
unsteady Navier-Stokes system. Weak solution in the two dimensional case and
regular solution in the three dimensional case are considered. From any initial
guess, we construct a minimizing sequence for the least-squares functional
which converges strongly to a solution of the Navier-Stokes system. After a
finite number of iterates related to the value of the viscosity constant, the
convergence is quadratic. Numerical experiments within the two dimensional case
support our analysis. This globally convergent least-squares approach is
related to the damped Newton method when used to solve the Navier-Stokes system
through a variational formulation."
scgqa_74,1708.05355v1,How do the sigmoid and Geman-McClure functions compare in terms of accuracy for image segmentation tasks presented in the paper?,"The sigmoid function is a popular choice for image segmentation because it is a smooth function that is easy to optimize. However, the Geman-McClure function can be a better choice for image segmentation in some cases. For example, the Geman-McClure function can be more robust to noise than the sigmoid function. Additionally, the Geman-McClure function can be more accurate in some cases, because it is not monotonically increasing.",1708.05355v1.pdf,"['1708.05355v1.pdf', '2008.13170v1.pdf', '1611.02955v1.pdf', '1711.06964v1.pdf', '1311.1567v3.pdf']",1708.05355v1-Figure6-1.png,Figure 6. (a) Sigmoid function. (b) Geman-McClure function.,"As shown in Figs. 6a and 6b, these continuous functions approximate the conventional discrete setting, but they assess subtle brightness variations more naturally when their input is near zero. In other words, they are still robust, but less brittle than the original Hamming-based definition.","MirrorFlow: Exploiting Symmetries in Joint Optical Flow and Occlusion
  Estimation","Optical flow estimation is one of the most studied problems in computer
vision, yet recent benchmark datasets continue to reveal problem areas of
today's approaches. Occlusions have remained one of the key challenges. In this
paper, we propose a symmetric optical flow method to address the well-known
chicken-and-egg relation between optical flow and occlusions. In contrast to
many state-of-the-art methods that consider occlusions as outliers, possibly
filtered out during post-processing, we highlight the importance of joint
occlusion reasoning in the optimization and show how to utilize occlusion as an
important cue for estimating optical flow. The key feature of our model is to
fully exploit the symmetry properties that characterize optical flow and
occlusions in the two consecutive images. Specifically through utilizing
forward-backward consistency and occlusion-disocclusion symmetry in the energy,
our model jointly estimates optical flow in both forward and backward
direction, as well as consistent occlusion maps in both views. We demonstrate
significant performance benefits on standard benchmarks, especially from the
occlusion-disocclusion symmetry. On the challenging KITTI dataset we report the
most accurate two-frame results to date."
scgqa_75,2010.13691v1,"In the study's context, how does the train-net's k-shell index indicate patterns of account influence shown in Figure 3?","The graph suggests that the structure of the train-net network is more conducive to the emergence of influential accounts. This is because the densely connected core of the train-net network provides a platform for the spread of information and influence. In contrast, the more homogeneous structure of the tagging-net network makes it more difficult for information and influence to spread.",2010.13691v1.pdf,"['2010.13691v1.pdf', '1908.04655v1.pdf', '1603.01793v2.pdf', '1202.4232v2.pdf', '2006.03632v1.pdf', '1804.06674v1.pdf']",2010.13691v1-Figure3-1.png,Figure 3: Complementary cumulative distribution of the 𝑘- shell indexes for accounts in the train-net network and tagging-net baseline. The categorization of conductors and riders in train-net reflects the account statuses during the data collection period. Mann-Whitney U tests show that the distributions are significantly different (𝑝 < 0.01) with large effect sizes.,"Comparing the two mention networks in Figure 2, we find that each community in the train-net network has a densely connected core and many peripheral nodes, while the tagging-net network displays a more homogeneous structure. To confirm our observation, we calculate the 𝑘-shell index [26], which measures node centrality and influence, of the nodes in both networks and plot the distributions in Figure 3. For the tagging-net baseline","The Manufacture of Partisan Echo Chambers by Follow Train Abuse on
  Twitter","A growing body of evidence points to critical vulnerabilities of social
media, such as the emergence of partisan echo chambers and the viral spread of
misinformation. We show that these vulnerabilities are amplified by abusive
behaviors associated with so-called ""follow trains"" on Twitter, in which long
lists of like-minded accounts are mentioned for others to follow. We present
the first systematic analysis of a large U.S. hyper-partisan train network. We
observe an artificial inflation of influence: accounts heavily promoted by
follow trains profit from a median six-fold increase in daily follower growth.
This catalyzes the formation of highly clustered echo chambers, hierarchically
organized around a dense core of active accounts. Train accounts also engage in
other behaviors that violate platform policies: we find evidence of activity by
inauthentic automated accounts and abnormal content deletion, as well as
amplification of toxic content from low-credibility and conspiratorial sources.
Some train accounts have been active for years, suggesting that platforms need
to pay greater attention to this kind of abuse."
scgqa_76,1610.04213v4,"In the context of damaged robot simulations, what insights does Figure 11 provide about RTE versus GP-TEXPLORE?","The graph shows that RTE outperforms GP-TEXPLORE and the re-planning baseline. The robot with RTE reaches the target in about 10 episodes, whereas with MCTS it needs more than 20 episodes and with GP-TEXPLORE is not able to reach the target even after 100 episodes. This indicates that RTE is a more effective algorithm for planning in damaged environments.",1610.04213v4.pdf,"['1610.04213v4.pdf', '2004.05448v1.pdf', '2011.08042v1.pdf', '1908.05243v1.pdf', '1804.04818v1.pdf']",1610.04213v4-Figure11-1.png,"Figure 11: Comparison between RTE, GP-TEXPLORE and MCTSbased planning — Hexapod robot simulation results. We measure the distance to the 5th target of RTE and GP-TEXPLORE as the number of episodes increases for the damage in Fig. 10C, environment #1 (Fig. 10D) and the second repertoire. RTE clearly outperforms GP-TEXPLORE and the re-planning baseline; the robot with RTE reaches the target in about 10 episodes, whereas with MCTS it needs more than 20 episodes and with GP-TEXPLORE is not able to reach the target even after 100 episodes. The lines represent medians over 50 runs and the shaded regions the 25th and 75th percentiles.","The results show that as the number of episodes increases, the robot that uses GP-TEXPLORE gets closer to the target, but cannot reach it when provided with a budget of 100 episodes (Fig. 11). On the contrary, the robot with RTE reaches the target in a small number of episodes (around 10 episodes in Fig. 11). Moreover, the robot that uses MCTS (the re-planning baseline) is",Reset-free Trial-and-Error Learning for Robot Damage Recovery,"The high probability of hardware failures prevents many advanced robots
(e.g., legged robots) from being confidently deployed in real-world situations
(e.g., post-disaster rescue). Instead of attempting to diagnose the failures,
robots could adapt by trial-and-error in order to be able to complete their
tasks. In this situation, damage recovery can be seen as a Reinforcement
Learning (RL) problem. However, the best RL algorithms for robotics require the
robot and the environment to be reset to an initial state after each episode,
that is, the robot is not learning autonomously. In addition, most of the RL
methods for robotics do not scale well with complex robots (e.g., walking
robots) and either cannot be used at all or take too long to converge to a
solution (e.g., hours of learning). In this paper, we introduce a novel
learning algorithm called ""Reset-free Trial-and-Error"" (RTE) that (1) breaks
the complexity by pre-generating hundreds of possible behaviors with a dynamics
simulator of the intact robot, and (2) allows complex robots to quickly recover
from damage while completing their tasks and taking the environment into
account. We evaluate our algorithm on a simulated wheeled robot, a simulated
six-legged robot, and a real six-legged walking robot that are damaged in
several ways (e.g., a missing leg, a shortened leg, faulty motor, etc.) and
whose objective is to reach a sequence of targets in an arena. Our experiments
show that the robots can recover most of their locomotion abilities in an
environment with obstacles, and without any human intervention."
scgqa_77,1809.08207v1,How does the number of sensors influence energy reduction percentage in the results presented in Figure 3?,"The graph shows that the percentage decrease in energy consumed increases as the number of sensors increases. This is because the equilibrium solution is more efficient when there are more sensors, as the sensors can share information with each other and coordinate their actions more effectively.",1809.08207v1.pdf,"['1809.08207v1.pdf', '1607.06988v1.pdf', '1902.06156v1.pdf']",1809.08207v1-Figure3-1.png,Figure 3: Percentage decrease of energy consumed versus the probability that a sensor is compromised for different numbers of sensors.,Fig. 3 shows the percentage decrease of energy consumed using the equilibrium solution compared to the baseline versus the probability that a sensor,"A Graphical Bayesian Game for Secure Sensor Activation in Internet of
  Battlefield Things","In this paper, the problem of secure sensor activation is studied for an
Internet of Battlefield Things (IoBT) system in which an attacker compromises a
set of the IoBT sensors for the purpose of eavesdropping and acquiring
information about the battlefield. In the considered model, each IoBT sensor
seeks to decide whether to transmit or not based on its utility. The utility of
each sensor is expressed in terms of the redundancy of the data transmitted,
the secrecy capacity and the energy consumed. Due to the limited communication
range of the IoBT sensors and their uncertainty about the location of the
eavesdroppers, the problem is formulated as a graphical Bayesian game in which
the IoBT sensors are the players. Then, the utilities of the IoBT sensors are
modified to take into account the effect of activating each sensor on the
utilities of its neighbors, in order to achieve better system performance. The
modified game is shown to be a Bayesian potential game, and a best response
algorithm that is guaranteed to find a Nash equilibrium of the game is
proposed. Simulation results show the tradeoff between the information
transmitted by the IoBT sensors and the desired secrecy level. Simulation
results also demonstrate the effectiveness of the proposed approach in reducing
the energy consumed compared to the baseline in which all the IoBT sensors are
activated. The reduction in energy consumption reaches up to 98% compared to
the baseline, when the number of sensors is 5000."
scgqa_78,1403.5617v1,What does Figure 1 illustrate regarding the number of strong ties as the overall graph size increases in this research?,"The graph shows that the number of users having at least 25 strong ties in G? increases as the size of the graph increases. This is because as the graph becomes larger, there are more opportunities for users to form strong ties with each other. However, the graph also shows that this number eventually reaches a plateau, after which it begins to decrease. This is because as the graph becomes even larger, it becomes more difficult for users to maintain strong ties with all of their friends.",1403.5617v1.pdf,"['1403.5617v1.pdf', '1905.11471v1.pdf', '2002.06199v1.pdf', '1805.00184v1.pdf', '1403.5801v2.pdf', '1707.02327v1.pdf']",1403.5617v1-Figure1-1.png,"Figure 1: Number of users having at least 25 friends in the strong friendship graph vs. the size of the graph, for ǫ = 0.01.","We run our experiments with three different values of ǫ, the threshold for the strong friendship between a pair of users. In each of the cases, we start with a complete graph with m0 = m vertices, where m is the the number of nodes every new node is connected to. In Figures 1, 2 and 3, we plot the number of the users having at least 25 (25 and 10, respectively) strong friends in Gst against the size of the graph Gst for a given m and ǫ. In each of these plots, we observe that this number increases till a point before starting to decrease.",On the Rise and Fall of Online Social Networks,"The rise and fall of online social networks recently generated an enormous
amount of interest among people, both inside and outside of academia. Gillette
[Businessweek magazine, 2011] did a detailed analysis of MySpace, which started
losing its popularity since 2008. Cannarella and Spechler [ArXiv, 2014] used a
model of disease spread to explain the rise and fall of MySpace. In this paper,
we present a graph theoretical model that may be able to provide an alternative
explanation for the rise and fall of online social networks. Our model is
motivated by the well-known Barabasi-Albert model of generating random
scale-free networks using preferential attachment or `rich-gets-richer'
phenomenon. As shown by our empirical analysis, we conjecture that such an
online social network growth model is inherently flawed as it fails to maintain
the stability of such networks while ensuring their growth. In the process, we
also conjecture that our model of preferential attachment also exhibits
scale-free phenomenon."
scgqa_79,1402.0635v3,What does Figure 14 indicate about the advantages of RLSVI in rapid learning within the mini-tetris environment?,The results presented in Figure 14 suggest that RLSVI is a promising reinforcement learning algorithm. It is able to learn faster and reach a higher convergent policy than LSVI with dithering. This suggests that RLSVI may be a good choice for applications where it is important to learn quickly and to avoid getting stuck in local minima.,1402.0635v3.pdf,"['1402.0635v3.pdf', '1908.05243v1.pdf', '1108.4475v4.pdf', '2007.15958v1.pdf', '1908.04647v1.pdf', '1812.09355v1.pdf']",1402.0635v3-Figure14-1.png,Figure 14. Reduced 4-row tetris with only S and Z pieces.,"In Figure 14 we present the results for this mini-tetris environment. As expected, this example highlights the benefits of RLSVI over LSVI with dithering. RLSVI greatly outperforms LSVI even with a tuned schedule. RLSVI learns faster and reaches a higher convergent policy.",Generalization and Exploration via Randomized Value Functions,"We propose randomized least-squares value iteration (RLSVI) -- a new
reinforcement learning algorithm designed to explore and generalize efficiently
via linearly parameterized value functions. We explain why versions of
least-squares value iteration that use Boltzmann or epsilon-greedy exploration
can be highly inefficient, and we present computational results that
demonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish
an upper bound on the expected regret of RLSVI that demonstrates
near-optimality in a tabula rasa learning context. More broadly, our results
suggest that randomized value functions offer a promising approach to tackling
a critical challenge in reinforcement learning: synthesizing efficient
exploration and effective generalization."
scgqa_80,2008.11326v4,How does the roofline model assist in understanding performance issues in the Si-214 experiments detailed in the paper?,"The hierarchical roofline analysis is a performance analysis tool that helps to identify the bottlenecks in a program. It is based on the roofline model, which is a theoretical model that describes the maximum performance of a program as a function of its arithmetic intensity and memory bandwidth. The hierarchical roofline analysis extends the roofline model by taking into account the different levels of memory hierarchy in a computer system. This allows for a more detailed analysis of the performance bottlenecks in a program.

In the graph, the x-axis shows the arithmetic intensity of the program, which is measured in FLOPS/Byte. The y-axis shows the performance of the program, which is measured in GFLOP/sec. The different lines in the graph represent the different levels of memory hierarchy in the system. The blue line represents the L1 cache, the green line represents the L2 cache, and the red line represents the HBM.

The graph shows that the performance of v3, v4 and v5 is limited by the L2 cache. This means that the programs are not able to access the data in the L2 cache as quickly as they need to. This could be due to a number of factors, such as the size of the L2 cache, the number of threads accessing the cache, or the latency of the cache.

The hierarchical roofline analysis can be used to identify the bottlenecks in a program and to make improvements to the performance. For example, the results of this analysis could be used to increase the size of the L2 cache, or to reduce the number of threads accessing the cache.",2008.11326v4.pdf,"['2008.11326v4.pdf', '1808.09050v2.pdf', '1709.08441v4.pdf', '1710.10733v4.pdf', '1511.07907v2.pdf', '2004.03870v1.pdf']",2008.11326v4-Figure5-1.png,"Fig. 5. Hierarchical Roofline analysis of v3, v4 and v5 for Si-214","Some computational characteristics of the GPP kernel are, 1) there is abundant parallelism, enough to saturate the GPU for efficient utilization, 2) most arrays in this kernel are in double precision, either floats or complex numbers, 3) dimensions of the arrays encompass almost all combinations of indices band, igp and ig, possibly posing a challenge in ensuring memory coalescence between threads in a warp, and the effectiveness of cache blocking, 4) among many multiplications, additions, and fused-multiplication and additions (FMAs), there are a few divides and abs() instructions, which present a longer latency and could potentially hurt the performance, 5) the reduction runs across all three loops band, igp, and ig, and accumulates results to the iw-th element of arrays achtemp and asxtemp. At the time of writing, OpenACC does not support array reductions, so some alternative needs to be sought. Luckily the loop count nw is very small and fixed so scalar reductions can be used for each iw explicitly. In PGI’s implementation of OpenACC, scalar reductions are usually done in a separate kernel after the main computational kernel, with much shorter computation and much fewer threads. There will be two kernels appearing in the Nsight Compute profile, however, we will only focus on the compute kernel for the bulk of this performance study. The runtime and throughput calculation, though, (Tab. I, and Fig. 1, Fig. 3, Fig. 5 and Fig. 6), will still be for the entire calculation, as indicated by the timers in the pseudo code above.","8 Steps to 3.7 TFLOP/s on NVIDIA V100 GPU: Roofline Analysis and Other
  Tricks","Performance optimization can be a daunting task especially as the hardware
architecture becomes more and more complex. This paper takes a kernel from the
Materials Science code BerkeleyGW, and demonstrates a few performance analysis
and optimization techniques. Despite challenges such as high register usage,
low occupancy, complex data access patterns, and the existence of several
long-latency instructions, we have achieved 3.7 TFLOP/s of double-precision
performance on an NVIDIA V100 GPU, with 8 optimization steps. This is 55% of
the theoretical peak, 6.7 TFLOP/s, at nominal frequency 1312 MHz, and 70% of
the more customized peak based on our 58% FMA ratio, 5.3 TFLOP/s. An array of
techniques used to analyze this OpenACC kernel and optimize its performance are
shown, including the use of hierarchical Roofline performance model and the
performance tool Nsight Compute. This kernel exhibits computational
characteristics that are commonly seen in many high-performance computing (HPC)
applications, and are expected to be very helpful to a general audience of HPC
developers and computational scientists, as they pursue more performance on
NVIDIA GPUs."
scgqa_81,1208.4662v2,"In the context of Figure 6, what trends are observed in the MSE with the spectral clustering method for FLIM images?","The graph shows that the spectral clustering method does not consistently decrease the mean-square error (MSE) in estimating average fluorescence lifetimes of the correct segments with increasing resolution. This is evident in the case of the FLIM image shown in Figure 1A, where the MSE does not decrease with increasing resolution. For the FLIM image shown in Figure 1B, the MSE decreases with increasing resolution by decreasing α up to 0.0625, but it increases again when α is decreased below this value. This is likely due to the fact that decreasing α below 0.0625 introduces noisy segments in the output, which increases the MSE.",1208.4662v2.pdf,"['1208.4662v2.pdf', '1901.10423v1.pdf', '2007.15404v1.pdf', '1905.12729v2.pdf', '1603.01185v2.pdf', '1603.04153v1.pdf', '1607.05970v2.pdf', '1307.3687v1.pdf', '2004.05448v1.pdf']",1208.4662v2-Figure6-1.png,"Figure 6: (A-B) The mean-square error (MSE) in estimating average fluorescence lifetimes of the correct segments using the spectral clustering method developed by Ng et al. for images shown in Fig. 1A and Fig. 1B, respectively. The MSE in estimating average FLTs of the correct segments for the image shown in Fig. 1A using the spectral clustering method does not consistently decrease with increasing resolution. For Fig. 1B, the MSE shows a decrease in increasing the resolution by decreasing α up to 0.0625. Decreasing α below 0.0625 increases the MSE to be very high, and thus, such MSEs are not depicted here for clarity.","The MSE in estimating average FLTs of the correct segments using the MCD method consistently decreases with increasing resolution; see Fig. 5. The MCD method offers lower MSE than the spectral clustering method in its all network resolution for the FLIM image shown in Fig. 1A, and in its high network resolution region (γ > 10) for the FLIM image shown in Fig. 1B; see Figs. 5-6. The MSE in estimating average FLTs of the correct segments using the spectral clustering method does not consistently decrease with increasing resolution for the FLIM image shown in Fig. 1A. For Fig. 1B, the MSE in estimating average FLTs of the correct segments using the spectral clustering method shows a decrease in increasing the resolution by decreasing α up to 0.0625. Decreasing α below this value introduces noisy segments in the output, and the MSE in estimating average FLTs of the correct segments becomes very high. Consequently, such MSEs at limiting resolutions are not shown in Fig. 6B for clarity. In summary, the proposed MCD method outperforms the spectral clustering method in MSE sense in automatically segmenting the FLIM images shown in Fig. 1.","Automatic Segmentation of Fluorescence Lifetime Microscopy Images of
  Cells Using Multi-Resolution Community Detection","We have developed an automatic method for segmenting fluorescence lifetime
(FLT) imaging microscopy (FLIM) images of cells inspired by a multi-resolution
community detection (MCD) based network segmentation method. The image
processing problem is framed as identifying segments with respective average
FLTs against a background in FLIM images. The proposed method segments a FLIM
image for a given resolution of the network composed using image pixels as the
nodes and similarity between the pixels as the edges. In the resulting
segmentation, low network resolution leads to larger segments and high network
resolution leads to smaller segments. Further, the mean-square error (MSE) in
estimating the FLT segments in a FLIM image using the proposed method was found
to be consistently decreasing with increasing resolution of the corresponding
network. The proposed MCD method outperformed a popular spectral clustering
based method in performing FLIM image segmentation. The spectral segmentation
method introduced noisy segments in its output at high resolution. It was
unable to offer a consistent decrease in MSE with increasing resolution."
scgqa_82,1908.05243v1,What does Fig. 6 indicate about the density of interfering DBSs near the serving DBS for UDM with RW and RWP?,"The graph shows that the density of the network of interfering DBSs for the UDM with the RW and RWP mobility models is relatively high in the vicinity of the serving DBS, and decreases as the distance from the serving DBS increases. This is because the RW and RWP mobility models both assume that the DBSs move randomly, and so the probability of finding a DBS in a particular location is higher near the serving DBS, where the DBSs are more likely to be located.",1908.05243v1.pdf,"['1908.05243v1.pdf', '1808.09050v2.pdf', '1706.03019v1.pdf', '1405.5329v4.pdf', '1708.07888v3.pdf', '1007.0328v1.pdf', '1901.10423v1.pdf']",1908.05243v1-Figure6-1.png,Fig. 6. Density of the network of interfering DBSs for the UDM with the RW and RWP mobility models. Serving distance is u0 = 500 m and the flights are Rayleigh distributed with mean 500 m.,"In Figs. 5 and 6, we plot the density of the network of interfering DBSs for all the mobility models considered in this paper at t ∈ {20, 40, 50, 200} s. We assume that the serving DBS follows the UDM and the exclusion zone radius is u0 = 500 m. In the SL mobility model, as also highlighted in Remark 4, the density will be divided into two homogeneous parts and one bowl-shaped inhomogeneous part after t = u0 v . Furthermore, the inhomogeneous part will become homogeneous as t→∞, which ultimately makes the point process of interferers homogeneous. This fact can also be directly inferred from Corollary 1 by taking the limit of (6) as t → ∞ and ux → vt. According to Fig. 6, this homogenization happens for the RW and RWP mobility","Performance Characterization of Canonical Mobility Models in Drone
  Cellular Networks","In this paper, we characterize the performance of several canonical mobility
models in a drone cellular network in which drone base stations (DBSs) serve
user equipments (UEs) on the ground. In particular, we consider the following
four mobility models: (i) straight line (SL), (ii) random stop (RS), (iii)
random walk (RW), and (iv) random waypoint (RWP), among which the SL mobility
model is inspired by the simulation models used by the third generation
partnership project (3GPP) for the placement and trajectory of drones, while
the other three are well-known canonical models (or their variants) that offer
a useful balance between realism and tractability. Assuming the
nearest-neighbor association policy, we consider two service models for the
UEs: (i) UE independent model (UIM), and (ii) UE dependent model (UDM). While
the serving DBS follows the same mobility model as the other DBSs in the UIM,
it is assumed to fly towards the UE of interest in the UDM and hover above its
location after reaching there. The main contribution of this paper is a unified
approach to characterize the point process of DBSs for all the mobility and
service models. Using this, we provide exact mathematical expressions for the
average received rate and the session rate as seen by the typical UE. Further,
using tools from calculus of variations, we concretely demonstrate that the
simple SL mobility model provides a lower bound on the performance of other
general mobility models (including the ones in which drones follow curved
trajectories) as long as the movement of each drone in these models is
independent and identically distributed (i.i.d.). To the best of our knowledge,
this is the first work that provides a rigorous analysis of key canonical
mobility models for an infinite drone cellular network and establishes useful
connections between them."
scgqa_83,2003.14319v2,"In the context of Figure 5, how do the estimators ∆1(s) and ∆ pr 1 (s) behave relative to true error?","The graph shows that the RCLtree method is able to accurately estimate the true error, with both ∆1(s) and ∆ pr 1 (s) closely following the true error curve. This is consistent with the results in Table 8, which showed that the RCLtree method had the lowest RMSE and MAE values.",2003.14319v2.pdf,"['2003.14319v2.pdf', '1712.02030v2.pdf', '1801.09097v2.pdf', '1006.3688v1.pdf', '1909.03961v2.pdf', '1804.06674v1.pdf', '1809.07412v2.pdf', '1910.05107v2.pdf']",2003.14319v2-Figure5-1.png,Figure 5: RCLtree: ∆1(s) and ∆ pr 1 (s) vs. the respective true errors at 900 frequency samples.,"Figures 5-7 further show the behaviors of the error estimators over the sample set Ξver including 900 samples, which are in agreement with the above analysis for the data in Table 8.","On Error Estimation for Reduced-order Modeling of Linear Non-parametric
  and Parametric Systems","Motivated by a recently proposed error estimator for the transfer function of
the reduced-order model of a given linear dynamical system, we further develop
more theoretical results in this work. Furthermore, we propose several variants
of the error estimator, and compare those variants with the existing ones both
theoretically and numerically. It has been shown that some of the proposed
error estimators perform better than or equally well as the existing ones. All
the error estimators considered can be easily extended to estimate output error
of reduced-order modeling for steady linear parametric systems."
scgqa_84,1208.2451v1,What insights does the experiment in Figure 5 provide about CALU PRRP's performance with different matrix types?,"The graph's findings suggest that CALU PRRP is a good choice for use with random matrices, but may not be as stable for special matrices. This is important to keep in mind when choosing an algorithm for LU factorization.",1208.2451v1.pdf,"['1208.2451v1.pdf', '2008.07011v1.pdf', '1811.01194v1.pdf', '2007.06852v1.pdf', '2011.09375v1.pdf', '1803.03080v1.pdf']",1208.2451v1-Figure5-1.png,"Figure 5: A summary of all our experimental data, showing the ratio of max(CALU PRRP’s backward error, machine epsilon) to max(GEPP’s backward error, machine epsilon) for all the test matrices in our set. Each vertical bar represents such a ratio for one test matrix. Bars above 100 = 1 mean that CALU PRRP’s backward error is larger, and bars below 1 mean that GEPP’s backward error is larger. For each matrix and algorithm, the backward error is measured using three different metrics. For the last third of the bars, labeled “componentwise backward error”, the metric is w in equation (8). The test matrices are further labeled either as “randn”, which are randomly generated, or “special”, listed in Table 15. Finally, each test matrix is factored using CALU PRRP with a binary reduction tree (labeled BCALU for BCALU PRRP) and with a flat reduction tree (labeled FCALU for FCALU PRRP).","As for the LU PRRP factorization, we discuss the stability of CALU PRRP using three metrics. For the matrices in our set, the relative error is at most 9.14 × 10−14, the normwise backward error is at most 1.37 × 10−14, and the componentwise backward error is at most 1.14×10−8 for Demmel matrix. Figure 5 displays the ratios of the errors with respect to those of GEPP, obtained by dividing the maximum of the backward errors of CALU PRRP and the machine epsilon by the maximum of those of GEPP and the machine epsilon. For random matrices, all the backward error ratios are at most 2.4. For the set of special matrices, the ratios of the relative error are at most 1 in over 62% of cases, and always smaller than 2, except for 8% of cases, where the ratios are between 2.4","LU factorization with panel rank revealing pivoting and its
  communication avoiding version","We present the LU decomposition with panel rank revealing pivoting (LU_PRRP),
an LU factorization algorithm based on strong rank revealing QR panel
factorization. LU_PRRP is more stable than Gaussian elimination with partial
pivoting (GEPP). Our extensive numerical experiments show that the new
factorization scheme is as numerically stable as GEPP in practice, but it is
more resistant to pathological cases and easily solves the Wilkinson matrix and
the Foster matrix. We also present CALU_PRRP, a communication avoiding version
of LU_PRRP that minimizes communication. CALU_PRRP is based on tournament
pivoting, with the selection of the pivots at each step of the tournament being
performed via strong rank revealing QR factorization. CALU_PRRP is more stable
than CALU, the communication avoiding version of GEPP. CALU_PRRP is also more
stable in practice and is resistant to pathological cases on which GEPP and
CALU fail."
scgqa_85,1910.08413v1,How are the combinations of distributions distinguished in the five evaluation scenarios of the paper?,What is the difference between the five scenarios?,1910.08413v1.pdf,"['1910.08413v1.pdf', '1710.07771v1.pdf', '1808.08442v1.pdf', '1209.3394v5.pdf', '1603.04812v2.pdf', '1409.3924v1.pdf', '1604.04026v1.pdf']",1910.08413v1-Figure3-1.png,Figure 3: Left: Five scenarios of two random variables with different distributions introduced for evaluating different comparison operators. Right: The absolute error of comparison operators for each scenario and for different numbers N of samples. The comparison operators in the legend are ranked according to their absolute error bound with the maximal number of samples N = 106.,"In the following, we evaluate the investigated comparison operators in terms of approximation error and execution time. For the evaluation, five scenarios of two random variables (X1, X2) with different distributions are selected, see the left column of Figure 3. These scenarios combine instances of various distributions including uniform U(X,X), Gaussian N (E [X ],Var [X ]) and beta B(α, β) distributions. The first four scenarios are adopted from [10]. These scenarios vary in their statistical properties and pose different challenges to the comparison operators.","Efficient Computation of Probabilistic Dominance in Robust
  Multi-Objective Optimization","Real-world problems typically require the simultaneous optimization of
several, often conflicting objectives. Many of these multi-objective
optimization problems are characterized by wide ranges of uncertainties in
their decision variables or objective functions, which further increases the
complexity of optimization. To cope with such uncertainties, robust
optimization is widely studied aiming to distinguish candidate solutions with
uncertain objectives specified by confidence intervals, probability
distributions or sampled data. However, existing techniques mostly either fail
to consider the actual distributions or assume uncertainty as instances of
uniform or Gaussian distributions. This paper introduces an empirical approach
that enables an efficient comparison of candidate solutions with uncertain
objectives that can follow arbitrary distributions. Given two candidate
solutions under comparison, this operator calculates the probability that one
solution dominates the other in terms of each uncertain objective. It can
substitute for the standard comparison operator of existing optimization
techniques such as evolutionary algorithms to enable discovering robust
solutions to problems with multiple uncertain objectives. This paper also
proposes to incorporate various uncertainties in well-known multi-objective
problems to provide a benchmark for evaluating uncertainty-aware optimization
techniques. The proposed comparison operator and benchmark suite are integrated
into an existing optimization tool that features a selection of multi-objective
optimization problems and algorithms. Experiments show that in comparison with
existing techniques, the proposed approach achieves higher optimization quality
at lower overheads."
scgqa_86,1402.0808v1,"In the context of Fig. 6, describe the relationship between deletion rate and MER for the architectures presented.","The graph shows that the deletion rate has a significant impact on the MER for all three architectures. With the exception of Architecture I, which has higher MERs than the others at zero deletion rate, the other two architectures have similar error rates. Architecture II and III have similar error rates up to deletion rates around 0.3, from where Architecture II performs better at higher MERs than Architecture III afterwards.",1402.0808v1.pdf,"['1402.0808v1.pdf', '2001.07829v1.pdf', '1405.5329v4.pdf', '1402.7063v1.pdf', '1902.06156v1.pdf', '1207.5027v1.pdf', '1910.10700v1.pdf']",1402.0808v1-Figure6-1.png,"Fig. 6. Comparison of the MER for Architectures I, II, and III as the deletion rate is increased for both binary-weighted and Multiple-valued connections (it=1).","Fig. 6 and Fig. 7 show a comparison of the effect of the deletion rate on the MER for the three architectures after one and four iterations respectively. With the exception of Architecture I that has higher MERs than the others at zero deletion rate, the other two architectures have similar error rates. Architecture II and III have similar error rates up to deletion rates around 0.3 from where Architecture II performs better at higher MERs than Architecture III afterwards.",Associative Memories Based on Multiple-Valued Sparse Clustered Networks,"Associative memories are structures that store data patterns and retrieve
them given partial inputs. Sparse Clustered Networks (SCNs) are
recently-introduced binary-weighted associative memories that significantly
improve the storage and retrieval capabilities over the prior state-of-the art.
However, deleting or updating the data patterns result in a significant
increase in the data retrieval error probability. In this paper, we propose an
algorithm to address this problem by incorporating multiple-valued weights for
the interconnections used in the network. The proposed algorithm lowers the
error rate by an order of magnitude for our sample network with 60% deleted
contents. We then investigate the advantages of the proposed algorithm for
hardware implementations."
scgqa_87,2010.13691v1,How does the k-shell index distribution of train-net accounts reflect their network structure in the paper?,"The graph shows that the distribution of k-shell indexes for accounts in the train-net network is significantly different from the baseline. This is because the train-net network has a more heterogeneous structure, with a densely connected core and many peripheral nodes. This results in a higher concentration of nodes with high k-shell indexes, as shown in the graph.",2010.13691v1.pdf,"['2010.13691v1.pdf', '1902.05922v1.pdf', '1703.03892v5.pdf', '1708.09328v1.pdf', '1302.3123v1.pdf', '1510.01155v1.pdf', '1805.06370v2.pdf', '1804.04290v1.pdf']",2010.13691v1-Figure3-1.png,Figure 3: Complementary cumulative distribution of the 𝑘- shell indexes for accounts in the train-net network and tagging-net baseline. The categorization of conductors and riders in train-net reflects the account statuses during the data collection period. Mann-Whitney U tests show that the distributions are significantly different (𝑝 < 0.01) with large effect sizes.,"Comparing the two mention networks in Figure 2, we find that each community in the train-net network has a densely connected core and many peripheral nodes, while the tagging-net network displays a more homogeneous structure. To confirm our observation, we calculate the 𝑘-shell index [26], which measures node centrality and influence, of the nodes in both networks and plot the distributions in Figure 3. For the tagging-net baseline","The Manufacture of Partisan Echo Chambers by Follow Train Abuse on
  Twitter","A growing body of evidence points to critical vulnerabilities of social
media, such as the emergence of partisan echo chambers and the viral spread of
misinformation. We show that these vulnerabilities are amplified by abusive
behaviors associated with so-called ""follow trains"" on Twitter, in which long
lists of like-minded accounts are mentioned for others to follow. We present
the first systematic analysis of a large U.S. hyper-partisan train network. We
observe an artificial inflation of influence: accounts heavily promoted by
follow trains profit from a median six-fold increase in daily follower growth.
This catalyzes the formation of highly clustered echo chambers, hierarchically
organized around a dense core of active accounts. Train accounts also engage in
other behaviors that violate platform policies: we find evidence of activity by
inauthentic automated accounts and abnormal content deletion, as well as
amplification of toxic content from low-credibility and conspiratorial sources.
Some train accounts have been active for years, suggesting that platforms need
to pay greater attention to this kind of abuse."
scgqa_88,1606.06377v1,"Regarding the experiments with the Kernel-Distortion classifier, what does Figure 5 indicate about variance ratio and classification error?","The graph shows that there is a negative correlation between variance ratio and total classification error. This means that as the variance ratio increases, the total classification error decreases. This is likely because a higher variance ratio means that the data is more spread out, which makes it easier to distinguish between different classes.",1606.06377v1.pdf,"['1606.06377v1.pdf', '1804.04818v1.pdf', '2003.00870v1.pdf', '1409.2897v1.pdf', '1611.04706v2.pdf', '1402.7063v1.pdf']",1606.06377v1-Figure5-1.png,Figure 5: Total classification error as a function of variance ratio σ2d/σ 2 o .,"mance improvement and the high storage requirement, we do not use a higher value. For p = 3 and q = 40, the performance as a function of γ is shown in Fig. 5, indicating that γ = 30 is a reasonable value. Notice that our approach is the Euclidean distance metric classifier when γ = 1. We also evaluate the performance as a function of p. The result is shown in Fig. 6, which shows no advantage for p larger than 3. We note that it is still possible to achieve better parameter settings, and different classification tasks would have different optimal parameter settings.",Kernel-based Generative Learning in Distortion Feature Space,"This paper presents a novel kernel-based generative classifier which is
defined in a distortion subspace using polynomial series expansion, named
Kernel-Distortion (KD) classifier. An iterative kernel selection algorithm is
developed to steadily improve classification performance by repeatedly removing
and adding kernels. The experimental results on character recognition
application not only show that the proposed generative classifier performs
better than many existing classifiers, but also illustrate that it has
different recognition capability compared to the state-of-the-art
discriminative classifier - deep belief network. The recognition diversity
indicates that a hybrid combination of the proposed generative classifier and
the discriminative classifier could further improve the classification
performance. Two hybrid combination methods, cascading and stacking, have been
implemented to verify the diversity and the improvement of the proposed
classifier."
scgqa_89,1201.3056v1,"According to Figure 5, what effect does increasing the variance of fi have on optimal relay pricing in the study?","The graph shows that as the variance of fi increases, the optimal relay price increases. This is because a higher variance means a higher average value of |fi|2, which on average means a higher power demand from the users. The relay price is set to be the maximum value of the users' willingness to pay, so as the demand increases, the price must also increase.",1201.3056v1.pdf,"['1201.3056v1.pdf', '1003.1655v1.pdf', '2002.06090v1.pdf', '1603.01793v2.pdf', '2008.11326v4.pdf', '2005.09814v3.pdf', '1402.7063v1.pdf', '1101.0235v1.pdf', '1511.07907v2.pdf']",1201.3056v1-Figure5-1.png,Fig. 5. Three-user relay network with Rayleigh fading channels and different variances offi.,"Next, we examine the trend of the optimal relay price with an increasing demand. From Lemma 2, P Ii (λ) is a non-decreasing function of |fi|2. So, we can use an increasing |fi|2 to simulate the increasing user demand. In this numerical experiment, we again consider a threeuser network and model all channels as independent circularly symmetric complex Gaussian random variables with zero-mean, that is, they are independent Rayleigh flat-fading channels. The variances of all gi’s and hi’s are 1, while the variance of all fi’s ranges from 1 to 20. A larger variance means a higher average value of |fi|2, which on average means a higher power demand from the users. The transmit power of the users is set to be 10 dB and relay power is set to be 20 dB. Figure 5 shows the optimal relay power price, the actual relay power sold, and the maximum relay revenue with different variances of fi. We can see that as the variance of fi increases, the optimal relay price increases, more relay power is sold, and the maximum relay revenue increases. This fits one of the laws of supply and demand, which says, if the supply is unchanged and demand increases, it leads to higher equilibrium price and quantity.","Power Allocation and Pricing in Multi-User Relay Networks Using
  Stackelberg and Bargaining Games","This paper considers a multi-user single-relay wireless network, where the
relay gets paid for helping the users forward signals, and the users pay to
receive the relay service. We study the relay power allocation and pricing
problems, and model the interaction between the users and the relay as a
two-level Stackelberg game. In this game, the relay, modeled as the service
provider and the leader of the game, sets the relay price to maximize its
revenue; while the users are modeled as customers and the followers who buy
power from the relay for higher transmission rates. We use a bargaining game to
model the negotiation among users to achieve a fair allocation of the relay
power. Based on the proposed fair relay power allocation rule, the optimal
relay power price that maximizes the relay revenue is derived analytically.
Simulation shows that the proposed power allocation scheme achieves higher
network sum-rate and relay revenue than the even power allocation. Furthermore,
compared with the sum-rate-optimal solution, simulation shows that the proposed
scheme achieves better fairness with comparable network sum-rate for a wide
range of network scenarios. The proposed pricing and power allocation solutions
are also shown to be consistent with the laws of supply and demand."
scgqa_90,1805.06370v2,"In Figure 3 of the paper, how is the model's ability to transfer skills illustrated during maze navigation?",The graph shows how well the model performs on the final task after it has been trained on a sequence of previous tasks. This is a measure of the model's ability to transfer knowledge from previous tasks to new tasks.,1805.06370v2.pdf,"['1805.06370v2.pdf', '1809.08207v1.pdf', '1106.3826v2.pdf', '1801.09097v2.pdf', '1803.09990v2.pdf', '1309.3959v1.pdf', '1611.02955v1.pdf', '2006.11769v1.pdf', '1910.09823v3.pdf']",1805.06370v2-Figure3-1.png,Figure 3. Positive transfer on random mazes. Shown is the learning progress on the final task after sequential training. Results averaged over 4 different final mazes. All rewards are normalised by the performance a dedicated model achieves on each task when training from scratch. Best viewed in colour.,"Positive transfer in the context of transfer- and continual learning is typically understood as either an improvement in generalisation performance or more data-efficient learning. The latter is of great importance in problems where data acquisition can be costly, such as robotics (Rusu et al., 2016b). In order to assess the capability of P&C to obtain positive transfer we show results for the navigation task in random mazes in Figure 3. Specifically, we train on a held-out maze after having visited all 7 previous mazes. As the similarity between the tasks is high, we would expect significant positive transfer for a suitable method. Indeed, we observe both forms of transfer for all methods including online EWC (although to a lesser extent). P&C performs on par with Finetuning, which in turn suffers from catastrophic forgetting. While online EWC does show positive transfer, the method underperforms when compared with Finetuning and P&C.",Progress & Compress: A scalable framework for continual learning,"We introduce a conceptually simple and scalable framework for continual
learning domains where tasks are learned sequentially. Our method is constant
in the number of parameters and is designed to preserve performance on
previously encountered tasks while accelerating learning progress on subsequent
problems. This is achieved by training a network with two components: A
knowledge base, capable of solving previously encountered problems, which is
connected to an active column that is employed to efficiently learn the current
task. After learning a new task, the active column is distilled into the
knowledge base, taking care to protect any previously acquired skills. This
cycle of active learning (progression) followed by consolidation (compression)
requires no architecture growth, no access to or storing of previous data or
tasks, and no task-specific parameters. We demonstrate the progress & compress
approach on sequential classification of handwritten alphabets as well as two
reinforcement learning domains: Atari games and 3D maze navigation."
scgqa_91,1803.11512v1,What do the multiple lines on the graph in Figure 6 signify about the throughput metrics in the study?,"The different lines on the graph represent the network throughput achieved by different coordinate selection rules in the distributed optimization control algorithm. The three coordinate selection rules are Cyclic, Gauss-Southwell, and Randomized. The Douglas-Rachford splitting method is also shown for comparison.",1803.11512v1.pdf,"['1803.11512v1.pdf', '1808.06818v1.pdf', '1804.10488v2.pdf', '1911.09804v2.pdf', '1708.07972v1.pdf', '2005.09814v3.pdf', '1608.06005v1.pdf', '1703.10422v2.pdf']",1803.11512v1-Figure6-1.png,Figure 6: Network throughput within collaboration space.,"In terms of network throughput, Fig. 6 shows that the throughput increases up to 22.48 Mbps. In this figure, the coordinate selection rules (Cyclic, Gauss-Southwell, Randomized) in our distributed optimization control algorithm and the Douglas-Rachford splitting method have almost the same performance.","Joint Communication, Computation, Caching, and Control in Big Data
  Multi-access Edge Computing","The concept of multi-access edge computing (MEC) has been recently introduced
to supplement cloud computing by deploying MEC servers to the network edge so
as to reduce the network delay and alleviate the load on cloud data centers.
However, compared to a resourceful cloud, an MEC server has limited resources.
When each MEC server operates independently, it cannot handle all of the
computational and big data demands stemming from the users devices.
Consequently, the MEC server cannot provide significant gains in overhead
reduction due to data exchange between users devices and remote cloud.
Therefore, joint computing, caching, communication, and control (4C) at the
edge with MEC server collaboration is strongly needed for big data
applications. In order to address these challenges, in this paper, the problem
of joint 4C in big data MEC is formulated as an optimization problem whose goal
is to maximize the bandwidth saving while minimizing delay, subject to the
local computation capability of user devices, computation deadline, and MEC
resource constraints. However, the formulated problem is shown to be
non-convex. To make this problem convex, a proximal upper bound problem of the
original formulated problem that guarantees descent to the original problem is
proposed. To solve the proximal upper bound problem, a block successive upper
bound minimization (BSUM) method is applied. Simulation results show that the
proposed approach increases bandwidth-saving and minimizes delay while
satisfying the computation deadlines."
scgqa_92,1809.01093v3,How does Figure 7 represent the relationship between singular values and upper bounds across various datasets?,"The graph compares the singular values of S with the upper bound d−1min| ∑T r=1 λ r i | ≥ σi(S) for different graphs. The singular values of S are plotted on the x-axis, and the upper bound d−1min| ∑T r=1 λ r i | ≥ σi(S) is plotted on the y-axis. The graph shows that the gap between the singular values of S and the upper bound is different across the different graphs. However, the gap is relatively small overall. This suggests that the upper bound is a good approximation of the singular values of S.",1809.01093v3.pdf,"['1809.01093v3.pdf', '1707.04849v1.pdf', '1911.02623v1.pdf', '1509.02054v1.pdf', '2006.16705v1.pdf', '1911.04231v2.pdf']",1809.01093v3-Figure7-1.png,Figure 7: The singular value of S and our upper bound d−1min| ∑T r=1 λ r i | ≥ σi(S) for different graphs.,"Upper bound on the singular values. Lemma 3 shows that LDW3 is an upper bound on LDW1 (excluding the elementwise logarithm). For a better understanding of the tightness of the bound we visualize the true singular values σi(S) of the matrix S and their respective upper bounds d−1min| ∑T r=1 λ r i | ≥ σi(S) obtained by applying Lemma 3 for all datasets. As we can see in Fig. 7, the gap is different across the different graphs and it is relatively small overall.",Adversarial Attacks on Node Embeddings via Graph Poisoning,"The goal of network representation learning is to learn low-dimensional node
embeddings that capture the graph structure and are useful for solving
downstream tasks. However, despite the proliferation of such methods, there is
currently no study of their robustness to adversarial attacks. We provide the
first adversarial vulnerability analysis on the widely used family of methods
based on random walks. We derive efficient adversarial perturbations that
poison the network structure and have a negative effect on both the quality of
the embeddings and the downstream tasks. We further show that our attacks are
transferable since they generalize to many models and are successful even when
the attacker is restricted."
scgqa_93,2007.15404v1,"In the study on rainfall prediction using SVM, what conclusion can be drawn from the macro f1-scores in Figure 5 regarding input sequence length?",The graph shows that there is no significant difference in the mean macro f1-scores for different input image sequence lengths. This suggests that the length of the input sequence does not have a significant impact on the accuracy of the predictions.,2007.15404v1.pdf,"['2007.15404v1.pdf', '1906.07610v2.pdf', '2011.07119v1.pdf', '1402.7063v1.pdf', '1110.6199v1.pdf']",2007.15404v1-Figure5-1.png,"Fig. 5. Mean macro f1-scores as a function of days ahead for different input image sequence length, averaged over all image sizes and tiles.","Figures 5 and 6 isolate the effect of input sequence length and image size, respectively. In Figure 5 the macro f1 scores for all predictions for all tiles for each days-ahead were averaged, and the results plotted as a function of days-ahead. The figure shows that no particular input sequence length is superior to the others. Figure 6 similarly averages macro f1 scores separately for each image size. There appears to be a slight advantage of about 1 percentage point when using the larger image size (172 × 123) instead of the smaller size (87 × 61). Both figures show a clear decrease in accuracy as days-ahead increases. in contrast to the constant classification accuracy found in [13].","Regional Rainfall Prediction Using Support Vector Machine Classification
  of Large-Scale Precipitation Maps","Rainfall prediction helps planners anticipate potential social and economic
impacts produced by too much or too little rain. This research investigates a
class-based approach to rainfall prediction from 1-30 days in advance. The
study made regional predictions based on sequences of daily rainfall maps of
the continental US, with rainfall quantized at 3 levels: light or no rain;
moderate; and heavy rain. Three regions were selected, corresponding to three
squares from a $5\times5$ grid covering the map area. Rainfall predictions up
to 30 days ahead for these three regions were based on a support vector machine
(SVM) applied to consecutive sequences of prior daily rainfall map images. The
results show that predictions for corner squares in the grid were less accurate
than predictions obtained by a simple untrained classifier. However, SVM
predictions for a central region outperformed the other two regions, as well as
the untrained classifier. We conclude that there is some evidence that SVMs
applied to large-scale precipitation maps can under some conditions give useful
information for predicting regional rainfall, but care must be taken to avoid
pitfall"
scgqa_94,1304.7375v1,"In the context of Figure 3, what is the outcome for the deterministic input after passing through the DT FRESH properizer?","The graph shows that the DT FRESH properizer can perfectly recover the input signal from its output. This is because the input signal is processed by the FD-RSW filter to generate the first term S1(f) of the DTFT of the DT FRESH properizer output, while S(−f)∗ is processed by the FD-RSW filter and shifted in the frequency domain to generate the second term S2(f). Thus, S1(f) contains all the frequency components of s[n] on the support G of the FD-RSW filter, while S2(f) contains all the remaining frequency components. Since the supports of S1(f) and S2(f) do not overlap, the DTFT of the output T (f) of the DT FRESH properizer contains all the frequency components of the input signal S(f) without any distortion.",1304.7375v1.pdf,"['1304.7375v1.pdf', '2007.11446v1.pdf', '2005.09814v3.pdf', '1604.06979v1.pdf', '1806.05387v1.pdf']",1304.7375v1-Figure3-1.png,"Fig. 3. Example that shows how the DT FRESH properizer with reference frequency1/M works in the frequency domain, when the input is a deterministic signals[n].","Fig. 3 shows how the DT FRESH properizer works in the frequency domain, especially when the input is a deterministic signal s[n] with the DTFT S(f) , F{s[n]}, the outputs of the upper and the lower branches in Fig. 2 are denoted by s1[n] with the DTFT S1(f) , F{s1[n]} and s2[n] with the DTFT S2(f) , F{s2[n]}, respectively, and the output signal is denoted by t[n] with the DTFT T (f) , F{t[n]}. Note that S(f) is processed by the FD-RSW filter to generate the first term S1(f) of the DTFT of the DT FRESH properizer output, while S(−f)∗ is processed by the FD-RSW filter and shifted in the frequency domain to generate the second term S2(f). Thus, S1(f) contains all the frequency components of s[n] on the support G of the FD-RSW filter, while S2(f) contains all the remaining frequency components. Since the supports of S1(f) and S2(f) do not overlap, the DTFT of the output T (f) of the DT FRESH properizer contains all the frequency components of the input signal S(f) without any distortion. Note also that, due to the periodicity of the DTFT with period 1, the frequency shift of S2(f) by any k/M for k ∈ Z generates the output signal that contains the same information as the input does. The following lemma makes this invertibility argument more precise.","Asymptotic FRESH Properizer for Block Processing of Improper-Complex
  Second-Order Cyclostationary Random Processes","In this paper, the block processing of a discrete-time (DT) improper-complex
second-order cyclostationary (SOCS) random process is considered. In
particular, it is of interest to find a pre-processing operation that enables
computationally efficient near-optimal post-processing. An invertible
linear-conjugate linear (LCL) operator named the DT FREquency Shift (FRESH)
properizer is first proposed. It is shown that the DT FRESH properizer converts
a DT improper-complex SOCS random process input to an equivalent DT
proper-complex SOCS random process output by utilizing the information only
about the cycle period of the input. An invertible LCL block processing
operator named the asymptotic FRESH properizer is then proposed that mimics the
operation of the DT FRESH properizer but processes a finite number of
consecutive samples of a DT improper-complex SOCS random process. It is shown
that the output of the asymptotic FRESH properizer is not proper but
asymptotically proper and that its frequency-domain covariance matrix converges
to a highly-structured block matrix with diagonal blocks as the block size
tends to infinity. Two representative estimation and detection problems are
presented to demonstrate that asymptotically optimal low-complexity
post-processors can be easily designed by exploiting these asymptotic
second-order properties of the output of the asymptotic FRESH properizer."
scgqa_95,2009.07756v1,What does Figure 5 reveal about the relationship between postdictive surprise and training termination in NILM?,"The graph suggests that postdictive surprise is an unreliable metric for terminating training in the general case. This is because postdictive surprise does not take into account the Markovian dynamics between super-states of the user's home. As a result, it can give misleading signals about when training should be terminated.",2009.07756v1.pdf,"['2009.07756v1.pdf', '1603.08981v2.pdf', '1803.06598v1.pdf', '2010.13691v1.pdf', '1804.04290v1.pdf', '2011.07119v1.pdf']",2009.07756v1-Figure5-1.png,Figure 5: Effectiveness measure averaged over 7 appliances as a function of surprise-based training cutoff,"Figure 5 shows the Van Rijsbergen’s effectiveness measure (defined simply as 1− F1-score) as a function of cutoff point during training. This measure decays slightly faster than that of the transitional surprise, but significantly after the postdictive surprise had converged. This lends credence to the claim that postdictive surprise is an unreliable metric for terminating training in the general case. The difference in decay rate between transitional surprise and the effectiveness measure is understandable given that the SSHMM by definition encodes the Markovian dynamics between super-states of the user’s home. The super-state of the home at a given instant in time can be thought of as the complete description of the home, denoting the operational mode of each appliance in the house. Each instant in time increments the underlying transition distributions between super-states of the home, rather than individual appliance states. This will in general encode the state dynamics more efficiently since there is more information used per time-step. Nevertheless, the basic notion of transitional surprise introduced here allows a useful overestimate of the learning rate of the system dynamics. Notably, the behaviour of the effectiveness measure in this case calls into question the specific values given for the joint threshold in equation 17. Here, a threshold on the transitional surprise of ≈ 0.4 seems adequate to predict stagnant performance improvements for this dataset. Significant exploration with","Exploring Bayesian Surprise to Prevent Overfitting and to Predict Model
  Performance in Non-Intrusive Load Monitoring","Non-Intrusive Load Monitoring (NILM) is a field of research focused on
segregating constituent electrical loads in a system based only on their
aggregated signal. Significant computational resources and research time are
spent training models, often using as much data as possible, perhaps driven by
the preconception that more data equates to more accurate models and better
performing algorithms. When has enough prior training been done? When has a
NILM algorithm encountered new, unseen data? This work applies the notion of
Bayesian surprise to answer these questions which are important for both
supervised and unsupervised algorithms. We quantify the degree of surprise
between the predictive distribution (termed postdictive surprise), as well as
the transitional probabilities (termed transitional surprise), before and after
a window of observations. We compare the performance of several benchmark NILM
algorithms supported by NILMTK, in order to establish a useful threshold on the
two combined measures of surprise. We validate the use of transitional surprise
by exploring the performance of a popular Hidden Markov Model as a function of
surprise threshold. Finally, we explore the use of a surprise threshold as a
regularization technique to avoid overfitting in cross-dataset performance.
Although the generality of the specific surprise threshold discussed herein may
be suspect without further testing, this work provides clear evidence that a
point of diminishing returns of model performance with respect to dataset size
exists. This has implications for future model development, dataset
acquisition, as well as aiding in model flexibility during deployment."
scgqa_96,1307.3687v1,"Based on the results displayed in Fig. 2, what relationship exists between edges and item inference accuracy?","The graph shows that the accuracy of inferring items increases as the number of edges increases. This is because as the number of edges increases, the graph becomes more connected and the items are more likely to be connected to each other. This makes it easier for the MAPE estimator to infer the items' ratings.",1307.3687v1.pdf,"['1307.3687v1.pdf', '1910.11127v1.pdf', '1804.10488v2.pdf', '1610.04213v4.pdf', '1405.5329v4.pdf', '1407.7736v1.pdf', '1902.05312v2.pdf']",1307.3687v1-Figure2-1.png,Fig. 2. Items classification accuracy comparison.,"The results are shown in Fig. 2. We first generated graphs with number of nodes |V | = 500 and varying number of edges (|E| = 1000, 2000, 3000, 4000, 5000) using different graph models. In each figure, we generated review samples of different sizes (500 ≤ |R| ≤ 5000), and show accuracy of inferring items averaged over 100 experiments respectively. We observe that when |R| increases, the accuracy also increases and approaches 1. This confirms that the MAPE estimator is asymptotically unbiased. For different graph models, we observe that the accuracy on Grnd is larger than the other two models. This indicates that constrained connections will make the inference performance poor. However, the accuracy curves","On Analyzing Estimation Errors due to Constrained Connections in Online
  Review Systems","Constrained connection is the phenomenon that a reviewer can only review a
subset of products/services due to narrow range of interests or limited
attention capacity. In this work, we study how constrained connections can
affect estimation performance in online review systems (ORS). We find that
reviewers' constrained connections will cause poor estimation performance, both
from the measurements of estimation accuracy and Bayesian Cramer Rao lower
bound."
scgqa_97,1803.06598v1,"In the context of the experiments, how does Gaussian sampling's standard deviation influence accuracy in facial landmark detection?",The findings in Figure 8 suggest that the standard deviation of Gaussian sampling is an important factor to consider in the design of facial landmark detection algorithms. Appropriate values of the standard deviation can lead to improved accuracy in facial landmark detection.,1803.06598v1.pdf,"['1803.06598v1.pdf', '1608.08469v1.pdf', '2005.11699v2.pdf', '1006.4386v1.pdf', '1208.2451v1.pdf', '1512.02567v1.pdf', '1910.00110v2.pdf', '1511.04338v2.pdf', '1809.01628v1.pdf']",1803.06598v1-Figure8-1.png,Figure 8: Performances of different Gaussian sampling in the 300-W public testing set.,"Effect of Gaussian sampling space parameters. As one of the most important processes, random sampling space significantly affects the final robustness and accuracy. As shown in Figure 8, the NME results are presented by varying the standard deviation σ of Gaussian sampling. Appropriate values lead to promising performance so that we set σ = 0.2 in our method.","Facial Landmarks Detection by Self-Iterative Regression based
  Landmarks-Attention Network","Cascaded Regression (CR) based methods have been proposed to solve facial
landmarks detection problem, which learn a series of descent directions by
multiple cascaded regressors separately trained in coarse and fine stages. They
outperform the traditional gradient descent based methods in both accuracy and
running speed. However, cascaded regression is not robust enough because each
regressor's training data comes from the output of previous regressor.
Moreover, training multiple regressors requires lots of computing resources,
especially for deep learning based methods. In this paper, we develop a
Self-Iterative Regression (SIR) framework to improve the model efficiency. Only
one self-iterative regressor is trained to learn the descent directions for
samples from coarse stages to fine stages, and parameters are iteratively
updated by the same regressor. Specifically, we proposed Landmarks-Attention
Network (LAN) as our regressor, which concurrently learns features around each
landmark and obtains the holistic location increment. By doing so, not only the
rest of regressors are removed to simplify the training process, but the number
of model parameters is significantly decreased. The experiments demonstrate
that with only 3.72M model parameters, our proposed method achieves the
state-of-the-art performance."
scgqa_98,1807.06736v1,How do the findings in this paper illustrate the relationship between bias adjustment and speech speed variation?,"The graph suggests that the proposed forward attention with transition agent is effective for speed control. The average duration of sentences can be increased or decreased by more than 10% by controlling the bias value. This means that the agent can be used to generate sentences at a variety of speeds, which could be useful for applications such as speech synthesis and text-to-speech.",1807.06736v1.pdf,"['1807.06736v1.pdf', '2006.04002v2.pdf', '2008.11326v4.pdf']",1807.06736v1-Figure3-1.png,Fig. 3. Average ratios of sentence duration modification achieved by controlling the bias value in the FA-TA system. Error bars represent the standard deviations.,"In the proposed forward attention with transition agent, as we adding positive or negative bias to the sigmoid output units of the DNN transition agent during generation, the transition probability gets increased or decreased. This leads to a faster or slower of attention results. An experiment was conducted using the plain FA-TA system to evaluate the effectiveness of speed control using this property. We used the same test set of the 20 utterances in Section 4.3. We increased or decreased the bias value from 0 with a step of 0.2, and synthesized all sentences in the test set. We stopped once one of the generated samples had the problem of missing phones, repeating phones, or making any perceivable mistakes. Then we calculated the average ratios between the lengths of synthetic sentences using modified bias and the lengths of synthetic sentences without bias modification. Fig. 3 show the results in a range of bias modification where all samples were generated correctly. From this figure, we can see that more than 10% speed control can be achieved using the proposed forward attention with transition agent. Informal listening test showed that such modification did not degrade the naturalness of synthetic speech.","Forward Attention in Sequence-to-sequence Acoustic Modelling for Speech
  Synthesis","This paper proposes a forward attention method for the sequenceto- sequence
acoustic modeling of speech synthesis. This method is motivated by the nature
of the monotonic alignment from phone sequences to acoustic sequences. Only the
alignment paths that satisfy the monotonic condition are taken into
consideration at each decoder timestep. The modified attention probabilities at
each timestep are computed recursively using a forward algorithm. A transition
agent for forward attention is further proposed, which helps the attention
mechanism to make decisions whether to move forward or stay at each decoder
timestep. Experimental results show that the proposed forward attention method
achieves faster convergence speed and higher stability than the baseline
attention method. Besides, the method of forward attention with transition
agent can also help improve the naturalness of synthetic speech and control the
speed of synthetic speech effectively."
scgqa_99,1501.01582v1,"According to the results in Fig. 4, what trend is observed between potential passenger numbers and mechanism efficiency?","The graph shows that the expected efficiency of the mechanism increases as the number of potential passengers increases. This is because as the number of potential passengers increases, the probability of finding a match for the driver increases, which in turn increases the expected efficiency of the mechanism.",1501.01582v1.pdf,"['1501.01582v1.pdf', '1804.06674v1.pdf', '1602.07579v1.pdf']",1501.01582v1-Figure4-1.png,"Fig. 4: Plot of the expected efficiency for our mechanism with optimized price-rate and the standard fixed price-rate approach, for varying number of potential passengers, N . High demand corresponds to αr = 3, βr = 1, medium demand corresponds to αr = βr = 1, and low demand corresponds to αr = 1, βr = 3. Simulation parameters: K = 5; average velocity of 30 km/hr; cost/km of 0.4 euros/km; maximum price-rate of 3 euros/km; and hard feasibility constraints are enforced.","Next, we turn to the expected efficiency (see (2)) of our mechanism demonstrated in Fig. 4.",Market Mechanism Design for Profitable On-Demand Transport Services,"On-demand transport services in the form of dial-a-ride and taxis are crucial
parts of the transport infrastructure in all major cities. However, not all
on-demand transport services are equal. In particular, not-for-profit
dial-a-ride services with coordinated drivers significantly differ from
profit-motivated taxi services with uncoordinated drivers. As such, there are
two key threads of research for efficient scheduling, routing, and pricing for
passengers: dial-a-ride services (first thread); and taxi services (second
thread). Unfortunately, there has been only limited development of algorithms
for joint optimization of scheduling, routing, and pricing; largely due to the
widespread assumption of fixed pricing. In this paper, we introduce another
thread: profit-motivated on-demand transport services with coordinated drivers.
To maximize provider profits and the efficiency of the service, we propose a
new market mechanism for this new thread of on-demand transport services, where
passengers negotiate with the service provider. In contrast to previous work,
our mechanism jointly optimizes scheduling, routing, and pricing. Ultimately,
we demonstrate that our approach can lead to higher profits, compared with
standard fixed price approaches, while maintaining comparable efficiency."
scgqa_100,1407.6074v1,"According to the research on cattle networks, how do direct contacts compare to indirect water contacts in the graph?",The graph shows that there is no significant relationship between the time series of direct contact and the time series of indirect contact with water. This suggests that drinking is not a key factor for network changes.,1407.6074v1.pdf,"['1407.6074v1.pdf', '1602.07579v1.pdf', '1805.01358v2.pdf', '2006.03632v1.pdf', '1302.2824v2.pdf', '1905.05538v1.pdf', '1804.04818v1.pdf', '1910.00110v2.pdf']",1407.6074v1-Figure1-1.png,"Fig 1. Time Series of Indirect Contact (Grain, Water, Hay), Periodogram, and Coherency Plot","The time series of cattle social network density (proportional to number of contacts) for the complete observation period (192h in total) is plotted against the time series of indirect contact between cattle and grain, water, and hay, respectively (Fig. 1). The time series of direct contact show substantial diurnal cycle, and the time series between direct and indirect contacts (grain and hay) are significantly coupled, which is supported by the coherency plot (since the solid black coherency line is usually above the red significance level). However the time series between direct and indirect contact with water are not significantly correlated (the coherency line hardly exceeds the significance level). These results show that cattle social network density changes significantly within a day, and feeding activity promotes clustering (thus more dense social networks) around grain and hay, while drinking is not a key factor for network changes. These results indicate that there is neither temporal stationarity nor spatial homogeneity in this high-resolution cattle social network (number of contacts).","Spatial-Temporal Dynamics of High-Resolution Animal Social Networks:
  What Can We Learn from Domestic Animals?","Recent studies of animal social networks have significantly increased our
understanding of animal behavior, social interactions, and many important
ecological and epidemiological processes. However, most of the studies are at
low temporal and spatial resolution due to the difficulty in recording accurate
contact information. Domestic animals such as cattle have social behavior and
serve as an excellent study system because their position can be explicitly and
continuously tracked, allowing their social networks to be accurately
constructed. We used radio-frequency tags to accurately track cattle position
and analyze high-resolution cattle social networks. We tested the hypothesis of
temporal stationarity and spatial homogeneity in these high-resolution networks
and demonstrated substantial spatial-temporal heterogeneity during different
daily time periods (feeding and non-feeding) and in different areas of the pen
(grain bunk, water trough, hay bunk, and other general pen area). The social
network structure is analyzed using global network characteristics (network
density, exponential random graph model structure), subgroup clustering
(modularity), triadic property (transitivity), and dyadic interactions
(correlation coefficient from a quadratic assignment procedure). Cattle tend to
have the strongest and most consistent contacts with others around the hay bunk
during the feeding time. These results cannot be determined from data at lower
spatial (aggregated at entire pen level) or temporal (aggregated at daily
level) resolution. These results reveal new insights for real-time animal
social network structure dynamics, providing more accurate descriptions that
allow more accurate modeling of multiple (both direct and indirect) disease
transmission pathways."
scgqa_101,1910.03072v1,"According to Figure 8, how does malicious input affect the performance of the TF-IDF + Gradient Boosting approach?",The graph shows that the model's performance is significantly affected by the corruption of inputs. The ROC and PR AUC curves for the model with corrupted inputs are significantly lower than those for the model with uncorrupted inputs. This suggests that the model is not robust to noise and is susceptible to malicious attacks.,1910.03072v1.pdf,"['1910.03072v1.pdf', '1803.11512v1.pdf', '1110.6199v1.pdf']",1910.03072v1-Figure8-1.png,Figure 8: Comparison of ROC and PR AUC curves before and after corruption of inputs of the “TF-IDF + Gradient Boosting” model for the test data. Random addition of a treatment results in a small drop in the quality. Malicious attack results in a significant drop in the quality of the model.,The ROC and PR curves before and after the corruption of labels are shown in Figure 8 for TF-IDF and gradient boosting approach and in Figure 9 for the SWEM-max approach. We see that changing of inputs to the model leads to a drop in its quality especially of the SWEM model. To make the model more stable one should augment the training data with more cases and possible distortions of the initial data (so-called data augmentation) and keep the model secret to avoid a malicious attack.,"Sequence embeddings help to identify fraudulent cases in healthcare
  insurance","Fraud causes substantial costs and losses for companies and clients in the
finance and insurance industries. Examples are fraudulent credit card
transactions or fraudulent claims. It has been estimated that roughly $10$
percent of the insurance industry's incurred losses and loss adjustment
expenses each year stem from fraudulent claims. The rise and proliferation of
digitization in finance and insurance have lead to big data sets, consisting in
particular of text data, which can be used for fraud detection. In this paper,
we propose architectures for text embeddings via deep learning, which help to
improve the detection of fraudulent claims compared to other machine learning
methods. We illustrate our methods using a data set from a large international
health insurance company. The empirical results show that our approach
outperforms other state-of-the-art methods and can help make the claims
management process more efficient. As (unstructured) text data become
increasingly available to economists and econometricians, our proposed methods
will be valuable for many similar applications, particularly when variables
have a large number of categories as is typical for example of the
International Classification of Disease (ICD) codes in health economics and
health services."
scgqa_102,2011.03519v1,What does Fig. 2 indicate about the relationship between energy consumption and its marginal utility?,"The marginal utility is the change in utility that results from a change in the quantity of a good or service consumed. In the context of the graph, the marginal utility is the change in utility that results from a change in the amount of energy consumed. The marginal utility is also known as the unit cost of energy.",2011.03519v1.pdf,"['2011.03519v1.pdf', '1909.03961v2.pdf', '1602.07579v1.pdf']",2011.03519v1-Figure2-1.png,"Fig. 2. A typical utility function (top) and its derivative, the marginal utility (bottom). The marginal utility is also the unit cost of energy.","?̂?𝑘 𝑠 = 𝑐𝑘 −1𝑎𝑘 + 𝑏𝑘 + 𝑑𝑘𝜃𝑘. (3) The above expression splits the model shiftable load ?̂?𝑘 𝑠 into cost dependent, constant, and temperature dependent terms. Fig. 2. illustrates how the utility and its derivative, the unit cost, vary with energy use. The utility is strictly concave in accordance with the law of diminishing returns. Although the unit cost is shown as a function in Fig. 2., in energy auctions the unit cost is usually determined by the DSO or aggregator, with each consumer agent responding by using the corresponding amount of energy [26],[27],[28],[29].","A Data-Driven Machine Learning Approach for Consumer Modeling with Load
  Disaggregation","While non-parametric models, such as neural networks, are sufficient in the
load forecasting, separate estimates of fixed and shiftable loads are
beneficial to a wide range of applications such as distribution system
operational planning, load scheduling, energy trading, and utility demand
response programs. A semi-parametric estimation model is usually required,
where cost sensitivities of demands must be known. Existing research work
consistently uses somewhat arbitrary parameters that seem to work best. In this
paper, we propose a generic class of data-driven semiparametric models derived
from consumption data of residential consumers. A two-stage machine learning
approach is developed. In the first stage, disaggregation of the load into
fixed and shiftable components is accomplished by means of a hybrid algorithm
consisting of non-negative matrix factorization (NMF) and Gaussian mixture
models (GMM), with the latter trained by an expectation-maximization (EM)
algorithm. The fixed and shiftable loads are subject to analytic treatment with
economic considerations. In the second stage, the model parameters are
estimated using an L2-norm, epsilon-insensitive regression approach. Actual
energy usage data of two residential customers show the validity of the
proposed method."
scgqa_103,1607.05970v2,"In the Bernoulli MAB experiment, how does the GIBL policy's performance compare to KGI at varying γ levels?","The graph shows that the GIBL policy performs the best overall, with the lowest mean percentage of lost reward. This is especially true for higher values of γ, where the GIBL policy is able to outperform the other policies by a significant margin. The KGI policy performs well for lower values of γ, but its performance degrades as γ increases. This is likely due to the fact that the KGI policy is more myopic than the GIBL policy, and as γ increases, the myopic nature of the KGI policy becomes more detrimental. The NKG and PKG policies perform similarly to the KG policy, with the NKG policy performing slightly better. The greedy policy performs the worst overall, with the highest mean percentage of lost reward.",1607.05970v2.pdf,"['1607.05970v2.pdf', '1603.02175v1.pdf', '2001.07829v1.pdf', '2002.11440v1.pdf']",1607.05970v2-Figure1-1.png,"Figure 1: Mean percentage of lost reward compared to the GI policy for five policies for the Bernoulli MAB with uniform priors and γ ∈ [0.9, 0.99]. The left plot shows k = 2 while on the right k = 10.","The first experiment tests performance over a range of γ for k ∈ {2, 10} arms, each with uniform Beta(1, 1) priors. The mean percentage lost reward for five policies are given in Figure 1. The results for the greedy policy are not plotted as they are clearly worse than the other policies (percentage loss going from 0.64 to 1.77 for k = 2). The overall behaviour of the policies is similar for k = 2 and k = 10. KGI is strong for lower γ but is weaker for higher γ while GIBL is strongest as γ increases. The sharp change in performance for GIBL at γ ≈ 0.975 occurs because the GIBL index is a piecewise function. Both NKG and PKG improve on KG for k = 2 but the three KG variants are almost identical for k = 10. The difference between KG and NKG gives the cost for the KG policy of dominated actions. These make up a large proportion of the lost reward for KG for lower γ but, as γ increases, over-greedy errors due to the myopic nature of the KG policy become more significant and these are not corrected by NKG. These errors are also the cause of the deteriorating performance of KGI at higher γ. At k = 10 the states given in Section 3.3 where KG was shown to take dominated actions occur infrequently. This is because, for larger numbers of arms there will more often be an arm with µ ≥ 0.5 and such arms are chosen in preference to dominated arms.","On the Identification and Mitigation of Weaknesses in the Knowledge
  Gradient Policy for Multi-Armed Bandits","The Knowledge Gradient (KG) policy was originally proposed for online ranking
and selection problems but has recently been adapted for use in online decision
making in general and multi-armed bandit problems (MABs) in particular. We
study its use in a class of exponential family MABs and identify weaknesses,
including a propensity to take actions which are dominated with respect to both
exploitation and exploration. We propose variants of KG which avoid such
errors. These new policies include an index heuristic which deploys a KG
approach to develop an approximation to the Gittins index. A numerical study
shows this policy to perform well over a range of MABs including those for
which index policies are not optimal. While KG does not make dominated actions
when bandits are Gaussian, it fails to be index consistent and appears not to
enjoy a performance advantage over competitor policies when arms are correlated
to compensate for its greater computational demands."
scgqa_104,1905.00569v2,What performance variations of fairness criteria does Fig. 11 illustrate based on user dynamics in the study?,"The graph shows that the performance of the four fairness criteria varies depending on the dynamic model. Under the model where the user departure is driven by false negative rate, EqOpt is better at maintaining representation. However, under the model where the users from each sub-group Gjk are driven by their own perceived loss, none of the four criteria can maintain group representation.",1905.00569v2.pdf,"['1905.00569v2.pdf', '1311.1567v3.pdf', '1106.3242v2.pdf']",1905.00569v2-Figure11-1.png,"Fig. 11: Sample paths under different dynamic models: Three cases are demonstrated including βa = βb (solid curves); βa = 3βb (dashed curves); βa = βb/3 (dotted dash curves). Fig. 11(a) illustrates the model where the user departure is driven by false negative rate: Nk(t + 1) = Nk(t)ν(FNk(θk(t))) + βk , with FNk(θk(t)) = ∫∞ θk(t) f0k (x)dx. Under this model EqOpt is better at maintaining representation. Fig. 11(b) illustrates the model where the users from each sub-group Gjk are driven by their own perceived loss: N j k(t + 1) = N jk(t)ν(L j k(θk(t))) + g j kβk, with L j k(θk) being false positives for j = 0 and false negatives for j = 1. Under this model none of the four criteria can maintain group representation.","To sustain the group representation, the key point is that the fairness definition should match the factors that drive user departure and arrival. If adopt different dynamic models, different fairness criteria should be adopted. Two examples with different dynamics and the performance of four fairness criteria are demonstrated in Fig. 11.","Group Retention when Using Machine Learning in Sequential Decision
  Making: the Interplay between User Dynamics and Fairness","Machine Learning (ML) models trained on data from multiple demographic groups
can inherit representation disparity (Hashimoto et al., 2018) that may exist in
the data: the model may be less favorable to groups contributing less to the
training process; this in turn can degrade population retention in these groups
over time, and exacerbate representation disparity in the long run. In this
study, we seek to understand the interplay between ML decisions and the
underlying group representation, how they evolve in a sequential framework, and
how the use of fairness criteria plays a role in this process. We show that the
representation disparity can easily worsen over time under a natural user
dynamics (arrival and departure) model when decisions are made based on a
commonly used objective and fairness criteria, resulting in some groups
diminishing entirely from the sample pool in the long run. It highlights the
fact that fairness criteria have to be defined while taking into consideration
the impact of decisions on user dynamics. Toward this end, we explain how a
proper fairness criterion can be selected based on a general user dynamics
model."
scgqa_105,1602.07579v1,"In the context of the Cognitive Radio Networks paper, how is the power-throughput curve affected by the RSI factor?","The graph shows that as the RSI factor increases, the power-throughput curve becomes flatter. This means that there is less of a tradeoff between the transmit power and the secondary throughput. This is because when the RSI factor is high, the primary user's interference is more significant, so the secondary user needs to transmit at a lower power in order to avoid interference.",1602.07579v1.pdf,"['1602.07579v1.pdf', '1909.01868v3.pdf', '1712.02030v2.pdf', '1309.3959v1.pdf', '2008.11326v4.pdf', '1805.00184v1.pdf', '1304.2109v1.pdf', '1811.01194v1.pdf']",1602.07579v1-Figure4-1.png,"Fig. 4. Power-Throughput Curve in terms of different RSI factor χ2, where the probability of the PU’s arrivalµ = 1/500, departureν = 6/500, the collision ratioPc = 0.1, the sample number of a slotNs is 300, sensing SNRγs = −5dB.","As an example, we plot the curves of the maximum of the left side and the corresponding value of the right side in Fig. 4(b). It is shown that when χ2 is smaller than 0.86, the maximum of the left is larger than the corresponding value of the right, and (29) will have solutions and power-throughput is likely to exist. When χ2 is greater than 0.85, there may be no tradeoff between the transmit power and secondary throughput, which is verified by the thick solid line in Fig. 4(a).","Listen-and-Talk: Protocol Design and Analysis for Full-duplex Cognitive
  Radio Networks","In traditional cognitive radio networks, secondary users (SUs) typically
access the spectrum of primary users (PUs) by a two-stage ""listen-before-talk""
(LBT) protocol, i.e., SUs sense the spectrum holes in the first stage before
transmitting in the second. However, there exist two major problems: 1)
transmission time reduction due to sensing, and 2) sensing accuracy impairment
due to data transmission. In this paper, we propose a ""listen-and-talk"" (LAT)
protocol with the help of full-duplex (FD) technique that allows SUs to
simultaneously sense and access the vacant spectrum. Spectrum utilization
performance is carefully analyzed, with the closed-form spectrum waste ratio
and collision ratio with the PU provided. Also, regarding the secondary
throughput, we report the existence of a tradeoff between the secondary
transmit power and throughput. Based on the power-throughput tradeoff, we
derive the analytical local optimal transmit power for SUs to achieve both high
throughput and satisfying sensing accuracy. Numerical results are given to
verify the proposed protocol and the theoretical results."
scgqa_106,1809.09034v1,What insight does Figure 15 provide about the accuracy of the coupled simulation for heat flow and electric currents?,The graph shows that the 1D-3D coupling method converges with a convergence order of around two. This means that the error decreases by a factor of four when the mesh size is halved. This is a good indication of the accuracy of the method.,1809.09034v1.pdf,"['1809.09034v1.pdf', '1904.06587v1.pdf', '1805.01772v1.pdf', '1912.03417v1.pdf', '1906.03859v1.pdf', '1810.03742v1.pdf']",1809.09034v1-Figure15-1.png,Figure 15: Convergence of (a) ∆1DL2 and (b) ∆ 3D L2 with respect to h for the bent wire 1D-3D coupling and rcpl = 1× 10−2/κ.,"We apply 0 V at the PEC electrode at x0, 1 V at the PEC electrode at x1, solve (48a) and consider the 1D solution as quantity of interest. We use a coupling radius of rcpl = 1× 10−2/κ, where κ ≈ 6.08 m−1 is the maximum Frenet-Serret curvature [33, 34] along the curve. In the following, we refer to the 1D and 3D reference solutions ϕ and ϕ as the solutions computed using h ≈ 1.884× 10−2 m and h = 1.563× 10−2. In Figure 14a, ϕ is shown with respect to the wire parametrization s and Figure 14b shows the solution ϕh on Λ and ϕh on D \Λ using a 3D visualization. Investigating the convergence of the error, we plot ∆1DL2 and ∆ 3D L2 with respect to h in Figure 15 using ϕ and ϕ as the reference. For both ∆1DL2 and ∆ 3D L2 , we observe a convergence order of around two.","Coupled Simulation of Transient Heat Flow and Electric Currents in Thin
  Wires: Application to Bond Wires in Microelectronic Chip Packaging","This work addresses the simulation of heat flow and electric currents in thin
wires. An important application is the use of bond wires in microelectronic
chip packaging. The heat distribution is modeled by an electrothermal coupled
problem, which poses numerical challenges due to the presence of different
geometric scales. The necessity of very fine grids is relaxed by solving and
embedding a 1D sub-problem along the wire into the surrounding 3D geometry. The
arising singularities are described using de Rham currents. It is shown that
the problem is related to fluid flow in porous 3D media with 1D fractures [C.
D'Angelo, SIAM Journal on Numerical Analysis 50.1, pp. 194-215, 2012]. A
careful formulation of the 1D-3D coupling condition is essential to obtain a
stable scheme that yields a physical solution. Elliptic model problems are used
to investigate the numerical errors and the corresponding convergence rates.
Additionally, the transient electrothermal simulation of a simplified
microelectronic chip package as used in industrial applications is presented."
scgqa_107,2010.13691v1,"In the context of the paper, how does the tagging-net's k-shell index distribution manifest compared to the train-net?","The graph shows that the distribution of k-shell indexes for accounts in the tagging-net network is more homogeneous. This is because the tagging-net network has a more uniform structure, with all nodes having a similar degree of connectivity. This results in a lower concentration of nodes with high k-shell indexes, as shown in the graph.",2010.13691v1.pdf,"['2010.13691v1.pdf', '1409.2897v1.pdf', '1405.7705v1.pdf', '1707.02439v2.pdf', '1311.6183v1.pdf', '1801.06867v1.pdf', '1405.5329v4.pdf']",2010.13691v1-Figure3-1.png,Figure 3: Complementary cumulative distribution of the 𝑘- shell indexes for accounts in the train-net network and tagging-net baseline. The categorization of conductors and riders in train-net reflects the account statuses during the data collection period. Mann-Whitney U tests show that the distributions are significantly different (𝑝 < 0.01) with large effect sizes.,"Comparing the two mention networks in Figure 2, we find that each community in the train-net network has a densely connected core and many peripheral nodes, while the tagging-net network displays a more homogeneous structure. To confirm our observation, we calculate the 𝑘-shell index [26], which measures node centrality and influence, of the nodes in both networks and plot the distributions in Figure 3. For the tagging-net baseline","The Manufacture of Partisan Echo Chambers by Follow Train Abuse on
  Twitter","A growing body of evidence points to critical vulnerabilities of social
media, such as the emergence of partisan echo chambers and the viral spread of
misinformation. We show that these vulnerabilities are amplified by abusive
behaviors associated with so-called ""follow trains"" on Twitter, in which long
lists of like-minded accounts are mentioned for others to follow. We present
the first systematic analysis of a large U.S. hyper-partisan train network. We
observe an artificial inflation of influence: accounts heavily promoted by
follow trains profit from a median six-fold increase in daily follower growth.
This catalyzes the formation of highly clustered echo chambers, hierarchically
organized around a dense core of active accounts. Train accounts also engage in
other behaviors that violate platform policies: we find evidence of activity by
inauthentic automated accounts and abnormal content deletion, as well as
amplification of toxic content from low-credibility and conspiratorial sources.
Some train accounts have been active for years, suggesting that platforms need
to pay greater attention to this kind of abuse."
scgqa_108,1405.6408v2,What effect does increasing terms in the cumulant expansion have on graph accuracy in this research?,"The number of terms in the cumulant expansion affects the accuracy of the approximation of the c.d.f. of the test statistic. With more terms, the approximation is more accurate. This can be seen in the graph, which shows that the probability of false alarm and the probability of detection are more accurate when the number of terms is greater.",1405.6408v2.pdf,"['1405.6408v2.pdf', '1909.05034v1.pdf', '1905.00569v2.pdf', '1811.00416v5.pdf', '1509.08992v2.pdf', '1402.7063v1.pdf', '1607.05970v2.pdf', '1804.04290v1.pdf', '1907.05050v3.pdf']",1405.6408v2-Figure1-1.png,"Fig. 1. Probability of false alarm vs. detection threshold, with k = n.","More terms can be added (L > 2) with an expected increase in accuracy; however, with L = 2, i.e., involving up to the fourth cumulant, the c.d.f. of W0 and of W1 are already approximated with high accuracy. This can be observed in Figs. 1 and 2, which plot respectively the probability of false alarm PFA(η) and the probability of detection PD(η), both as a function","Analysis and Design of Multiple-Antenna Cognitive Radios with Multiple
  Primary User Signals","We consider multiple-antenna signal detection of primary user transmission
signals by a secondary user receiver in cognitive radio networks. The optimal
detector is analyzed for the scenario where the number of primary user signals
is no less than the number of receive antennas at the secondary user. We first
derive exact expressions for the moments of the generalized likelihood ratio
test (GLRT) statistic, yielding approximations for the false alarm and
detection probabilities. We then show that the normalized GLRT statistic
converges in distribution to a Gaussian random variable when the number of
antennas and observations grow large at the same rate. Further, using results
from large random matrix theory, we derive expressions to compute the detection
probability without explicit knowledge of the channel, and then particularize
these expressions for two scenarios of practical interest: 1) a single primary
user sending spatially multiplexed signals, and 2) multiple spatially
distributed primary users. Our analytical results are finally used to obtain
simple design rules for the signal detection threshold."
scgqa_109,1610.01283v4,"According to Figure 9, how does the TRPO method affect EPOpt(e = 1)'s performance relative to REINFORCE?","The graph shows that EPOpt(e = 1) performs significantly better when using TRPO than REINFORCE. This is likely because TRPO uses a natural gradient, which is better suited for optimizing over probability distributions than the ""vanilla"" gradient used by REINFORCE. This observation is consistent with the findings of Kakade (2001), Schulman et al. (2015), and Duan et al. (2016).",1610.01283v4.pdf,"['1610.01283v4.pdf', '1810.04824v1.pdf', '2008.01961v3.pdf', '1910.09823v3.pdf', '2004.05448v1.pdf', '1906.02003v1.pdf', '1006.4386v1.pdf', '1007.0328v1.pdf', '2010.07597v2.pdf']",1610.01283v4-Figure9-1.png,Figure 9: Learning curves for EPOpt( = 1) when using the TRPO and REINFORCE methods for the BatchPolOpt step.,"reported results, we have used TRPO as the policy gradient method. Here, we compare the results to the case when using the classic REINFORCE algorithm. For this comparison, we use the same value function baseline parametrization for both TRPO and REINFORCE. Figure 9 depicts the learning curve when using the two policy gradient methods. We observe that performance with TRPO is significantly better. When optimizing over probability distributions, the natural gradient can navigate the warped parameter space better than the “vanilla” gradient. This observation is consistent with the findings of Kakade (2001), Schulman et al. (2015), and Duan et al. (2016).",EPOpt: Learning Robust Neural Network Policies Using Model Ensembles,"Sample complexity and safety are major challenges when learning policies with
reinforcement learning for real-world tasks, especially when the policies are
represented using rich function approximators like deep neural networks.
Model-based methods where the real-world target domain is approximated using a
simulated source domain provide an avenue to tackle the above challenges by
augmenting real data with simulated data. However, discrepancies between the
simulated source domain and the target domain pose a challenge for simulated
training. We introduce the EPOpt algorithm, which uses an ensemble of simulated
source domains and a form of adversarial training to learn policies that are
robust and generalize to a broad range of possible target domains, including
unmodeled effects. Further, the probability distribution over source domains in
the ensemble can be adapted using data from target domain and approximate
Bayesian methods, to progressively make it a better approximation. Thus,
learning on a model ensemble, along with source domain adaptation, provides the
benefit of both robustness and learning/adaptation."
scgqa_110,1512.02567v1,How does the MSD metric reflect the estimation capabilities of the proposed algorithms in the context of Gaussian mixture noise?,"The mean square deviation (MSD) is a measure of the average squared difference between the estimated and actual values of a signal. It is a common metric for evaluating the performance of signal processing algorithms, as it provides a quantitative measure of how well the algorithm is able to track the signal. In the context of the proposed adaptive sparse NLMF algorithms, the MSD is a measure of how well the algorithms are able to estimate the sparse representation of the input signal. A low MSD indicates that the algorithms are able to accurately estimate the sparse representation, while a high MSD indicates that the algorithms are not able to accurately estimate the sparse representation.",1512.02567v1.pdf,"['1512.02567v1.pdf', '1706.03112v1.pdf', '1707.02439v2.pdf', '1709.03329v1.pdf', '1608.08469v1.pdf', '1108.4475v4.pdf', '1207.5027v1.pdf']",1512.02567v1-Figure6-1.png,"Fig. 6. MSD performances of Local Sparse NLMS, Local Sparse NLMF and Distributed Sparse NLMF algorithms.","The performance of the proposed adaptive sparse NLMF algorithms evaluated by computer simulations are shown in Fig. 5, and Fig. 6, for local and distributed scenario, respectively.","Distributed Adaptive LMF Algorithm for Sparse Parameter Estimation in
  Gaussian Mixture Noise","A distributed adaptive algorithm for estimation of sparse unknown parameters
in the presence of nonGaussian noise is proposed in this paper based on
normalized least mean fourth (NLMF) criterion. At the first step, local
adaptive NLMF algorithm is modified by zero norm in order to speed up the
convergence rate and also to reduce the steady state error power in sparse
conditions. Then, the proposed algorithm is extended for distributed scenario
in which more improvement in estimation performance is achieved due to
cooperation of local adaptive filters. Simulation results show the superiority
of the proposed algorithm in comparison with conventional NLMF algorithms."
scgqa_111,1603.08981v2,"In your study, what does AUC signify in relation to the binary classification performance illustrated in Fig. 7?","AUC stands for Area Under the Curve. In this context, AUC represents the area under the Receiver Operating Characteristic (ROC) curve. The ROC curve is a graphical representation of the performance of a binary classifier system. It plots the true positive rate (TPR) against the false positive rate (FPR) at different threshold settings. The TPR is the proportion of true positives that are correctly identified by the classifier, while the FPR is the proportion of false positives that are incorrectly identified by the classifier. The higher the AUC, the better the performance of the classifier.",1603.08981v2.pdf,"['1603.08981v2.pdf', '1910.05107v2.pdf', '1610.00017v2.pdf', '1207.5027v1.pdf', '1905.07512v3.pdf', '1706.03112v1.pdf']",1603.08981v2-Figure7-1.png,Fig. 7. AUC for Twitter dataset on 116 important real world events.,of these accounts and obtain a star topology graph centered around each handle. We collect tweets of all users in all these networks for a window of time before and after the real life event. For each network we compute the statistics. The AUC curves in Fig. 7 are obtained by varying the threshold. A threshold value is said to correctly identify the true changepoint if the statistic value to the right of the change-point is greater than the threshold. This demonstrates the good performance of our algorithm against two baseline algorithms.,Detecting weak changes in dynamic events over networks,"Large volume of networked streaming event data are becoming increasingly
available in a wide variety of applications, such as social network analysis,
Internet traffic monitoring and healthcare analytics. Streaming event data are
discrete observation occurred in continuous time, and the precise time interval
between two events carries a great deal of information about the dynamics of
the underlying systems. How to promptly detect changes in these dynamic systems
using these streaming event data? In this paper, we propose a novel
change-point detection framework for multi-dimensional event data over
networks. We cast the problem into sequential hypothesis test, and derive the
likelihood ratios for point processes, which are computed efficiently via an
EM-like algorithm that is parameter-free and can be computed in a distributed
fashion. We derive a highly accurate theoretical characterization of the
false-alarm-rate, and show that it can achieve weak signal detection by
aggregating local statistics over time and networks. Finally, we demonstrate
the good performance of our algorithm on numerical examples and real-world
datasets from twitter and Memetracker."
scgqa_112,2006.16705v1,"Based on the results presented in Figure 3, which transformation yielded higher confidence estimates for CIFAR-100 and STL-10?","The image shift transformation performs better than the Gamma correction transformation on the CIFAR-100 and STL-10 datasets. However, the Gamma correction transformation performs better on the SVHN dataset.",2006.16705v1.pdf,"['2006.16705v1.pdf', '1610.04213v4.pdf', '1502.03556v1.pdf', '1806.05387v1.pdf', '1803.11512v1.pdf', '1710.10733v4.pdf', '1909.05034v1.pdf']",2006.16705v1-Figure3-1.png,"Figure 3. AORC vs. transformation intensity. Confidence estimation performance corresponding to different γ (top plot) or image shifting magnitude (bottom plot) values, when estimating by applying MSR on a single Gamma transformed or shifting transformed image version, respectively. For the shift transformation (bottom), per-magnitude values are averaged over different shifting directions. For the CIFAR-100 and STL-10 datasets, each experiment is repeated with a preliminary horizontal shift, and AORC values corresponding to the two versions are averaged.","Some transformation types constitute an infinite set of possible transformations, that can be characterized using one or more parameters. The effect of these parameter on confidence estimation performance is depicted in Fig. 3 for the image shift and Gamma correction3 transformation cases. We followed the same procedure conducted for Tab. 1 and computed per-parameter AORC values, each corresponding to confidence estimates utilizing a single image transformation. Similarly to color channel swapping, Gamma transformation seems to be useful for confidence estimation in the SVHN case, but less so for natural images (in which",Classification Confidence Estimation with Test-Time Data-Augmentation,"Machine learning plays an increasingly significant role in many aspects of
our lives (including medicine, transportation, security, justice and other
domains), making the potential consequences of false predictions increasingly
devastating. These consequences may be mitigated if we can automatically flag
such false predictions and potentially assign them to alternative, more
reliable mechanisms, that are possibly more costly and involve human attention.
This suggests the task of detecting errors, which we tackle in this paper for
the case of visual classification. To this end, we propose a novel approach for
classification confidence estimation. We apply a set of semantics-preserving
image transformations to the input image, and show how the resulting image sets
can be used to estimate confidence in the classifier's prediction. We
demonstrate the potential of our approach by extensively evaluating it on a
wide variety of classifier architectures and datasets, including
ResNext/ImageNet, achieving state of the art performance. This paper
constitutes a significant revision of our earlier work in this direction (Bahat
& Shakhnarovich, 2018)."
scgqa_113,1901.03808v4,"In the ECGadv paper, how are line curves and gray-scale images perceived differently according to Figure 1?","The graph shows that humans have different perceptual sensitivities to colors and line curves. When two data arrays are visualized as line curves, their differences are more prominent rather than those visualized as gray-scale images. This is because humans are more sensitive to changes in line curves than changes in colors.",1901.03808v4.pdf,"['1901.03808v4.pdf', '1212.3950v3.pdf', '1207.3107v3.pdf', '1903.10464v3.pdf', '1910.05107v2.pdf', '1808.06304v2.pdf', '1805.06370v2.pdf']",1901.03808v4-Figure1-1.png,"Figure 1: Perception test. There are two data arrays in the range of [−1, 1], and the second one is obtained by adding a few perturbations with 0.1 amplitude to the first one. Both of them are visualized as line curves and gray-scale images.","natural and not representative of an attack. We found that simply applying existing image-targeted attacks on ECG recordings generates suspicious adversarial instances, because commonly-used Lp norm in image domain to encourage visual imperceptibility is unsuitable for ECGs (see Figure 3). In visualization, each value in an ECG represents the voltage of a sampling point which is visualized as a line curve. Meanwhile, each value in a image represents the grayscale or RGB value of a pixel which is visualized as the corresponding color. Humans have different perceptual sensitivities to colors and line curves. As shown in Fig. 1, when two data arrays are visualized as line curves, their differences are more prominent rather than those visualized as gray-scale images. In this paper, we propose smoothness metrics to quantify perceptual similarities of line curves, and leverages them to generate unsuspicious adversarial ECG instances.","ECGadv: Generating Adversarial Electrocardiogram to Misguide Arrhythmia
  Classification System","Deep neural networks (DNNs)-powered Electrocardiogram (ECG) diagnosis systems
recently achieve promising progress to take over tedious examinations by
cardiologists. However, their vulnerability to adversarial attacks still lack
comprehensive investigation. The existing attacks in image domain could not be
directly applicable due to the distinct properties of ECGs in visualization and
dynamic properties. Thus, this paper takes a step to thoroughly explore
adversarial attacks on the DNN-powered ECG diagnosis system. We analyze the
properties of ECGs to design effective attacks schemes under two attacks models
respectively. Our results demonstrate the blind spots of DNN-powered diagnosis
systems under adversarial attacks, which calls attention to adequate
countermeasures."
scgqa_114,1405.7705v1,"According to the results depicted in Fig. 20, what is the performance of the approach on DOFs estimation?","The graph shows that the approach correctly estimates the number of DOFs for both the open and closed kinematic chain objects. This suggests that the approach is able to learn the correct kinematic model for a given object, even if the object is not fully observed.",1405.7705v1.pdf,"['1405.7705v1.pdf', '2007.11391v1.pdf', '1906.09756v1.pdf', '2003.14319v2.pdf', '2008.01961v3.pdf']",1405.7705v1-Figure20-1.png,Figure 20: Estimated number of DOFs for the open and the closed kinematic chain object (see Fig. 19). Left: open kinematic chain. Right: closed kinematic chain.,"We also analyzed the progression of model selection while the training data is incorporated. The left plot of Fig. 20 shows the DOFs of the learned kinematic model for the open kinematic chain. Note that we opened the yardstick segment by segment, therefore the number of DOFs increases step-wise from zero to three. The right plot shows the estimated number of DOFs for the closed kinematic chain: our approach correctly estimates the number of DOFs to one already after the first few observations.","A Probabilistic Framework for Learning Kinematic Models of Articulated
  Objects","Robots operating in domestic environments generally need to interact with
articulated objects, such as doors, cabinets, dishwashers or fridges. In this
work, we present a novel, probabilistic framework for modeling articulated
objects as kinematic graphs. Vertices in this graph correspond to object parts,
while edges between them model their kinematic relationship. In particular, we
present a set of parametric and non-parametric edge models and how they can
robustly be estimated from noisy pose observations. We furthermore describe how
to estimate the kinematic structure and how to use the learned kinematic models
for pose prediction and for robotic manipulation tasks. We finally present how
the learned models can be generalized to new and previously unseen objects. In
various experiments using real robots with different camera systems as well as
in simulation, we show that our approach is valid, accurate and efficient.
Further, we demonstrate that our approach has a broad set of applications, in
particular for the emerging fields of mobile manipulation and service robotics."
scgqa_115,1612.07141v3,How does noise level affect the classification accuracy for low versus high inter-cluster connectivity in Fig. 5?,"The low inter-cluster connectivity graph (left column of Fig. 5) is a simpler problem than the high inter-cluster connectivity graph (right column of Fig. 5). This is because the clusters in the low inter-cluster connectivity graph are more distinct, making it easier for the models to classify the points correctly. As a result, RobustGC is able to almost perfectly classify all the points in the low inter-cluster connectivity graph, even when the noise level is high. However, in the high inter-cluster connectivity graph, the clusters are more overlapping, making it more difficult for the models to classify the points correctly. As a result, RobustGC is not able to achieve perfect accuracy in the high inter-cluster connectivity graph, even when the noise level is low.",1612.07141v3.pdf,"['1612.07141v3.pdf', '2002.11440v1.pdf', '2010.08182v3.pdf']",1612.07141v3-Figure5-1.png,Fig. 5 Robust comparison for the low inter-cluster connectivity graph (left column) and the high inter-cluster connectivity graph (right column).,"The results are included in Fig. 5, where the solid lines represent the average accuracy, and the striped regions the areas between the minimum and maximum accuracies. In the case of the low inter-cluster connectivity dataset (left column of Fig. 5), RobustGC is able to almost perfectly classify all the points independently of the noise level (hence the striped region only appears when the number of labels is small and the noise is maximum). Moreover, PF-RobustGC is almost as good as RobustGC, and only slightly worse when the noise is the highest and the number of labels is small. These two models outperform BelkGC, and also ZhouGC, which is clearly the worse of the four approaches. Regarding the high inter-cluster connectivity dataset (right column of Fig. 5), for this more difficult problem RobustGC still gets a perfect classification except when the noise level is very high, where the accuracy drops a little when the number of labels is small. BelkGC is again worse than RobustGC, and the difference is more noticeable when the noise increases. On the other side, the heuristic PF-RobustGC is in this case worse than BelkGC (the selection of γ is clearly not optimal) but it still outperforms ZhouGC.",Robust Classification of Graph-Based Data,"A graph-based classification method is proposed for semi-supervised learning
in the case of Euclidean data and for classification in the case of graph data.
Our manifold learning technique is based on a convex optimization problem
involving a convex quadratic regularization term and a concave quadratic loss
function with a trade-off parameter carefully chosen so that the objective
function remains convex. As shown empirically, the advantage of considering a
concave loss function is that the learning problem becomes more robust in the
presence of noisy labels. Furthermore, the loss function considered here is
then more similar to a classification loss while several other methods treat
graph-based classification problems as regression problems."
scgqa_116,2010.12427v3,What does the progression of accuracy in Figure 5 indicate about the efficacy of the two classifiers on target data?,"The graph suggests that the bait classifier and the splitting mechanism are effective in improving the performance of domain adaptation. The bait classifier helps to learn a more robust representation of the target data, and the splitting mechanism helps to prevent overfitting to the source data.",2010.12427v3.pdf,"['2010.12427v3.pdf', '1208.4662v2.pdf', '1201.3056v1.pdf', '1905.12729v2.pdf', '1912.03417v1.pdf']",2010.12427v3-Figure5-1.png,"Figure 5: Accuracy curves when training on target data, on three subtasks of Office-Home: Cl→Ar, Pr→Ar and Rw→Ar. The three curves correspond to the accuracy with C1, C2 in BAIT and only one classifier with class balance loss respectively.","by 3.1%, and MA [17] by 1.1% on the more challenging VisDA dataset (Tab. 2). Note that MA highly relies on the extra synthesized data. On Office-31 (Tab. 3), the proposed BAIT achieves better result than SFDA and SHOT, and competitive result to MA. Our BAIT surpasses SFDA by a large margin, and is on par with SHOT on Office-Home (Tab. 4). The reported results clearly demonstrate the efficacy of the proposed method without access to the source data during adaptation. Ablation Study. We conduct an ablation study to isolate the validity of the key components of our method: the bait classifier and the splitting mechanism. As reported in the second row of Tab. 5 on the Office-Home dataset, directly testing the model on target domain shows the worst performance. Adapting the model trained on the source domain to the target domain with one classifier (the third row of Tab. 5), we are able to improve the result, but still obtain relatively low accuracy. Adding the second classifier and applying the splitting mechanism results in much better results. Also leveraging the Lb loss improves results significantly. Finally, combining both the Lb loss and the splitting mechanism, we achieve the best score. Evolution of the accuracy over time. Fig. 5 shows the evolution of the accuracy during adaptation to the target domain of Office-Home for a single classifier (only minimizing the class balance loss Lb), and the two classifiers of BAIT (C1 and C2). The starting point is the accuracy after training on the source data. The single classifier has an ephemeral im-",Casting a BAIT for Offline and Online Source-free Domain Adaptation,"We address the source-free domain adaptation (SFDA) problem, where only the
source model is available during adaptation to the target domain. We consider
two settings: the offline setting where all target data can be visited multiple
times (epochs) to arrive at a prediction for each target sample, and the online
setting where the target data needs to be directly classified upon arrival.
Inspired by diverse classifier based domain adaptation methods, in this paper
we introduce a second classifier, but with another classifier head fixed. When
adapting to the target domain, the additional classifier initialized from
source classifier is expected to find misclassified features. Next, when
updating the feature extractor, those features will be pushed towards the right
side of the source decision boundary, thus achieving source-free domain
adaptation. Experimental results show that the proposed method achieves
competitive results for offline SFDA on several benchmark datasets compared
with existing DA and SFDA methods, and our method surpasses by a large margin
other SFDA methods under online source-free domain adaptation setting."
scgqa_117,1710.09234v1,"According to the results presented, what is the meaning of capacity under unit bandwidth in this context?","The capacity of a communication channel is the maximum amount of information that can be transmitted through the channel per unit time. In this case, the capacity is expressed in bits per second per hertz (bps/Hz). The average SNR γ̄ is a measure of the average signal power to noise power ratio at the receiver. The values of m and K are parameters that characterize the FTR fading channel.",1710.09234v1.pdf,"['1710.09234v1.pdf', '1512.02567v1.pdf', '1608.06005v1.pdf', '1611.02955v1.pdf', '1809.09034v1.pdf', '1902.03993v2.pdf', '1704.03458v1.pdf', '2004.05579v1.pdf']",1710.09234v1-Figure2-1.png,Fig. 2. Capacity under unit bandwidth of the FTR fading channel against the average SNR γ̄ for different values of m (K = 10 and ∆ = 0.5).,"Figs. 2-3 depict the analytical (12) and simulation channel capacity against the average SNR γ̄ for different values of m and K. By varying one parameter while keeping other parameters fixed, we find that increasing the values of m helps overcome the effect of the FTR channel fading. As expected, the capacity that corresponds to light fluctuations (m = 10.3) is larger than the capacity that corresponds to heavy fluctuations (m = 0.3). Moreover, this increase is more pronounced for smaller values of m. When ∆ = 0.9, it is clear from Fig. 3.(a) that the capacity that corresponds to high power of the dominant waves (K = 10) is larger than the capacity that corresponds to high power of the scattered waves (K = 1). For ∆ = 1 in Fig. 3.(b), however, the capacity increases as K decreases, which is consistent with [8].","New Results on the Fluctuating Two-Ray Model with Arbitrary Fading
  Parameters and Its Applications","The fluctuating two-ray (FTR) fading model provides a much better fit than
other fading models for small-scale fading measurements in millimeter wave
communications. In this paper, using a mixture of gamma distributions, new
exact analytical expressions for the probability density and cumulative
distribution functions of the FTR distribution with arbitrary fading parameters
are presented. Moreover, the performance of digital communication systems over
the FTR fading channel is evaluated in terms of the channel capacity and the
bit error rate. The interaction between channel fading parameters and system
performance is further investigated. Our newly derived results extend and
complement previous knowledge of the FTR fading model."
scgqa_118,1409.2897v1,"In the context of the adaptive handwriting recognition system, what does Figure 4 indicate about user writing speed and information received?",The graph shows that there is no significant relationship between the system receiving more information from the user and the user writing faster. This means that the system does not necessarily receive more information from the user when the user writes faster.,1409.2897v1.pdf,"['1409.2897v1.pdf', '1603.08983v6.pdf', '1808.07801v3.pdf']",1409.2897v1-Figure4-1.png,Figure 4: The average writing time per session and the average mutual information per session under the condition Rfixed.,"Furthermore, Figure 4a and Figure 4b reveal that the major contribution of user adaptation comes from the fact that the users write faster in the last 5 sessions compared to the first 5 sessions (p < 0.0001), and not because of the system received more information from the user (p = 0.9723). This result is as expected according to the law of practice [12].",Co-adaptation in a Handwriting Recognition System,"Handwriting is a natural and versatile method for human-computer interaction,
especially on small mobile devices such as smart phones. However, as
handwriting varies significantly from person to person, it is difficult to
design handwriting recognizers that perform well for all users. A natural
solution is to use machine learning to adapt the recognizer to the user. One
complicating factor is that, as the computer adapts to the user, the user also
adapts to the computer and probably changes their handwriting. This paper
investigates the dynamics of co-adaptation, a process in which both the
computer and the user are adapting their behaviors in order to improve the
speed and accuracy of the communication through handwriting. We devised an
information-theoretic framework for quantifying the efficiency of a handwriting
system where the system includes both the user and the computer. Using this
framework, we analyzed data collected from an adaptive handwriting recognition
system and characterized the impact of machine adaptation and of human
adaptation. We found that both machine adaptation and human adaptation have
significant impact on the input rate and must be considered together in order
to improve the efficiency of the system as a whole."
scgqa_119,2006.03632v1,What does the mean cumulative reward indicate about the performance of the master versus base algorithms in this study?,"The mean cumulative reward is a measure of the overall performance of an algorithm. It is calculated by taking the average of the cumulative rewards over a number of runs. In this case, the number of runs is 100. The cumulative reward is the total reward an algorithm has earned over time. It is calculated by adding up the rewards the algorithm has earned at each time step. The mean cumulative reward is a useful metric for comparing the performance of different algorithms.",2006.03632v1.pdf,"['2006.03632v1.pdf', '2005.11699v2.pdf', '1703.10422v2.pdf', '1710.09234v1.pdf', '1203.1203v2.pdf', '1908.04647v1.pdf', '1107.4161v1.pdf']",2006.03632v1-Figure1-1.png,"Figure 1: Mean cumulative reward of the master and base algorithms over 100 runs, with (10%,90%) quantile bands",We present the mean cumulative reward results in figure 1. We demonstrate the behavior of the algorithm on a single run in figure 2 in appendix E. We provide additional details about the experimental setting in appendix E.,"Rate-adaptive model selection over a collection of black-box contextual
  bandit algorithms","We consider the model selection task in the stochastic contextual bandit
setting. Suppose we are given a collection of base contextual bandit
algorithms. We provide a master algorithm that combines them and achieves the
same performance, up to constants, as the best base algorithm would, if it had
been run on its own. Our approach only requires that each algorithm satisfy a
high probability regret bound.
  Our procedure is very simple and essentially does the following: for a well
chosen sequence of probabilities $(p_{t})_{t\geq 1}$, at each round $t$, it
either chooses at random which candidate to follow (with probability $p_{t}$)
or compares, at the same internal sample size for each candidate, the
cumulative reward of each, and selects the one that wins the comparison (with
probability $1-p_{t}$).
  To the best of our knowledge, our proposal is the first one to be
rate-adaptive for a collection of general black-box contextual bandit
algorithms: it achieves the same regret rate as the best candidate.
  We demonstrate the effectiveness of our method with simulation studies."
scgqa_120,2010.07597v2,"Regarding the lightweight end-to-end ASR system, what does Figure 4 indicate about the utilization of raw audio data?","The graph shows that one of the Sinc filters has converged to pass through the entire raw audio signal. This suggests that the network is learning to process the raw audio signal directly, rather than using a filter bank. This is likely due to the fact that the raw audio signal contains a lot of information that is not captured by a filter bank, and the network is able to learn how to extract this information.",2010.07597v2.pdf,"['2010.07597v2.pdf', '1304.7375v1.pdf', '1908.04655v1.pdf']",2010.07597v2-Figure4-1.png,Figure 4: Learned Sinc-convolution filters visualized by plotting the lower and upper bounds of the filters.,"Fig. 3 shows four learned Sinc-convolution filters of the LSC, and Fig. 4 visualizes the learned filters by sorting them and plotting their upper and lower bounds. In those filters, a trend towards a higher amplitude is noticeable, also towards a wider band pass in the spectral domain. Notably, one of the Sinc filters converged to pass through the entire raw audio signal, indicating an inclination of the network to directly process the raw data.","Lightweight End-to-End Speech Recognition from Raw Audio Data Using
  Sinc-Convolutions","Many end-to-end Automatic Speech Recognition (ASR) systems still rely on
pre-processed frequency-domain features that are handcrafted to emulate the
human hearing. Our work is motivated by recent advances in integrated learnable
feature extraction. For this, we propose Lightweight Sinc-Convolutions (LSC)
that integrate Sinc-convolutions with depthwise convolutions as a low-parameter
machine-learnable feature extraction for end-to-end ASR systems.
  We integrated LSC into the hybrid CTC/attention architecture for evaluation.
The resulting end-to-end model shows smooth convergence behaviour that is
further improved by applying SpecAugment in time-domain. We also discuss
filter-level improvements, such as using log-compression as activation
function. Our model achieves a word error rate of 10.7% on the TEDlium v2 test
dataset, surpassing the corresponding architecture with log-mel filterbank
features by an absolute 1.9%, but only has 21% of its model size."
scgqa_121,1708.09328v1,What are the broader implications of the algorithm in Figure 2 for optimization tasks as discussed in this paper?,"The algorithm that is used to generate the graph in Figure 2 can be used to solve a variety of problems in distributed optimization. For example, the algorithm could be used to solve problems in machine learning, control, and signal processing.",1708.09328v1.pdf,"['1708.09328v1.pdf', '1803.01118v2.pdf', '1305.1657v1.pdf', '1907.10906v1.pdf']",1708.09328v1-Figure2-1.png,Figure 2: Convergence of mean-field to the fixed-point,"In Figure 2, we plot d2E(x(t,u), π) as a function of t where dE is the euclidean distance defined by","Insensitivity of the mean-field Limit of Loss Systems Under Power-of-d
  Routing","In this paper, we study large multi-server loss models under power-of-$d$
routing scheme when service time distributions are general with finite mean.
Previous works have addressed the exponential service time case when the number
of servers goes to infinity giving rise to a mean field model. The fixed point
of limiting mean field equations (MFE) was shown to be insensitive to the
service time distribution through simulation. Showing insensitivity to general
service time distributions has remained an open problem. Obtaining the MFE in
this case poses a challenge due to the resulting Markov description of the
system being in positive orthant as opposed to a finite chain in the
exponential case. In this paper, we first obtain the MFE and then show that the
MFE has a unique fixed point that coincides with the fixed point in the
exponential case thus establishing insensitivity. The approach is via a
measure-valued Markov process representation and the martingale problem to
establish the mean-field limit. The techniques can be applied to other queueing
models."
scgqa_122,2001.11086v3,"How does Figure 9 illustrate seasonal accuracy differences between RNNEC,p and GLM-calib in lake temperature modeling?","The results in Figure 9 suggest that RNNEC,p is more accurate than GLM-calib in spring and winter, but less accurate in summer and fall. This is because spring and winter are less affected by stratification and rapid changes in temperature, which makes it easier for RNNEC,p to accurately estimate the temperature profile.",2001.11086v3.pdf,"['2001.11086v3.pdf', '1304.7375v1.pdf', '1608.00887v1.pdf']",2001.11086v3-Figure9-1.png,"Fig. 9. The error-depth relationship in (a) spring, (b) summer, (c) fall, and (d) winter.","To be er understand the di erence between our proposed method and GLM across seasons, we separately plot the error-depth relation for di erent seasons (see Fig. 9). We can observe the error-depth pro le in summer and fall are similar to that in Fig. 7. e di erence between RNNEC,p and calibrated GLM performance is especially worse in summer and fall because these two seasons are dominated by a stronger strati cation and/or rapid changes in","Physics-Guided Machine Learning for Scientific Discovery: An Application
  in Simulating Lake Temperature Profiles","Physics-based models of dynamical systems are often used to study engineering
and environmental systems. Despite their extensive use, these models have
several well-known limitations due to simplified representations of the
physical processes being modeled or challenges in selecting appropriate
parameters. While-state-of-the-art machine learning models can sometimes
outperform physics-based models given ample amount of training data, they can
produce results that are physically inconsistent. This paper proposes a
physics-guided recurrent neural network model (PGRNN) that combines RNNs and
physics-based models to leverage their complementary strengths and improves the
modeling of physical processes. Specifically, we show that a PGRNN can improve
prediction accuracy over that of physics-based models, while generating outputs
consistent with physical laws. An important aspect of our PGRNN approach lies
in its ability to incorporate the knowledge encoded in physics-based models.
This allows training the PGRNN model using very few true observed data while
also ensuring high prediction accuracy. Although we present and evaluate this
methodology in the context of modeling the dynamics of temperature in lakes, it
is applicable more widely to a range of scientific and engineering disciplines
where physics-based (also known as mechanistic) models are used, e.g., climate
science, materials science, computational chemistry, and biomedicine."
scgqa_123,2006.09358v2,What do the left and center graphs in Figure 3 reveal about the training versus testing accuracy for ResNet50 with SGD and gRDA?,"The two learning curves in the left and center of the graph show the training and testing accuracy of the ResNet50 model trained with SGD and gRDA, respectively. The main difference between the two curves is that the training accuracy of the model trained with gRDA is slightly lower than that of the model trained with SGD. This is likely due to the fact that gRDA uses a soft thresholding function to prune the weights, which can lead to a slight decrease in accuracy. However, the testing accuracy of the model trained with gRDA is higher than that of the model trained with SGD, which suggests that gRDA is able to achieve better generalization performance.",2006.09358v2.pdf,"['2006.09358v2.pdf', '2004.04276v1.pdf', '1906.11938v3.pdf', '1409.3924v1.pdf', '1403.5801v2.pdf', '2008.06431v1.pdf', '2002.01322v1.pdf', '1911.11395v2.pdf', '1803.10225v1.pdf']",2006.09358v2-Figure3-1.png,"Figure 3: Learning trajectories of (SGD) and (gRDA) for ResNet50 [31] on ImageNet image recognition task. Left: top 1 training accuracy. Center: top 1 testing accuracy. Right: the ratio between the number of nonzero parameters and the total number of parameters. The number of nonzero weights slightly increases, contradicting with Theorem 3.1. This could be because that Assumption (A5) fails due to the large learning rate. γ = 0.1 for both SGD and gRDA. Minibatch size is 256.","We use (gRDA) to simultaneously prune and train the ResNet50 [31] on the ImageNet dataset without any post-processing like retraining. The learning rate schedule usually applied jointly with the SGD with momentum does not work well for (gRDA), so we use either a constant learning rate or dropping the learning rate only once in the later training stage. Please find more implementation details in Section C.1 in the appendix. The results are shown in Figure 3, where µ is the increasing rate of the soft thresholding in the tuning function (5) of (gRDA).",Directional Pruning of Deep Neural Networks,"In the light of the fact that the stochastic gradient descent (SGD) often
finds a flat minimum valley in the training loss, we propose a novel
directional pruning method which searches for a sparse minimizer in or close to
that flat region. The proposed pruning method does not require retraining or
the expert knowledge on the sparsity level. To overcome the computational
formidability of estimating the flat directions, we propose to use a carefully
tuned $\ell_1$ proximal gradient algorithm which can provably achieve the
directional pruning with a small learning rate after sufficient training. The
empirical results demonstrate the promising results of our solution in highly
sparse regime (92% sparsity) among many existing pruning methods on the
ResNet50 with the ImageNet, while using only a slightly higher wall time and
memory footprint than the SGD. Using the VGG16 and the wide ResNet 28x10 on the
CIFAR-10 and CIFAR-100, we demonstrate that our solution reaches the same
minima valley as the SGD, and the minima found by our solution and the SGD do
not deviate in directions that impact the training loss. The code that
reproduces the results of this paper is available at
https://github.com/donlan2710/gRDA-Optimizer/tree/master/directional_pruning."
scgqa_124,2007.06852v1,"In the context of the experiments, how does the noise level interact with the convergence of trajectories in the paper's results?","The graph shows that the stochastic heavy ball method converges closer to the global minimum when the noise level is larger. This is consistent with Theorem 4, which states that the stochastic heavy ball method converges to the global minimum at a rate of O(1/k2) when the noise level is β. The results for the noiseless heavy ball method and Nesterov's method suggest that convergence may occur for a broader class of second-order dynamics than the setting of our analysis.",2007.06852v1.pdf,"['2007.06852v1.pdf', '1910.08413v1.pdf', '2010.13032v1.pdf', '2008.07524v3.pdf', '1710.09234v1.pdf', '1808.07801v3.pdf', '1911.04231v2.pdf']",2007.06852v1-Figure1-1.png,"Figure 1: Final loss value as the width n of the network increases for several second-order dynamics (left), and sample trajectories for n = 100 (right).","In a first set of experiments, we set the dimension to d = 100, and vary the width n of the model, while keeping the width of the ground truth network fixed to n0 = 20. No regularization is used in this experiment, so that the model can theoretically achieve zero loss whenever n ≥ n0. The results are reported in Figure 1. In the left subplot, each method is run for 105 iterations, and we measure the loss at the last iteration. We repeat the experiment 20 times and plot the average (represented by the lines) and the individual numbers (scatter plot). The right subplot shows the full trajectory for one realization, for the width n = 100. The results suggest that the dynamics converge to the global infimum even with a reasonably small width n. The results also highlight the effect of noise: the stochastic heavy ball method converges closer to the global minimum when β is larger, consistent with Theorem 4. Finally, the results for the noiseless heavy ball method and Nesterov’s method suggest that convergence may occur for a broader class of second-order dynamics than the setting of our analysis.",Global Convergence of Second-order Dynamics in Two-layer Neural Networks,"Recent results have shown that for two-layer fully connected neural networks,
gradient flow converges to a global optimum in the infinite width limit, by
making a connection between the mean field dynamics and the Wasserstein
gradient flow. These results were derived for first-order gradient flow, and a
natural question is whether second-order dynamics, i.e., dynamics with
momentum, exhibit a similar guarantee. We show that the answer is positive for
the heavy ball method. In this case, the resulting integro-PDE is a nonlinear
kinetic Fokker Planck equation, and unlike the first-order case, it has no
apparent connection with the Wasserstein gradient flow. Instead, we study the
variations of a Lyapunov functional along the solution trajectories to
characterize the stationary points and to prove convergence. While our results
are asymptotic in the mean field limit, numerical simulations indicate that
global convergence may already occur for reasonably small networks."
scgqa_125,1302.3123v1,How does Penalized Fuzzy C-Means compare to K-Means in identifying gene expression patterns from the dataset?,"The graph shows that the proposed algorithm, Penalized Fuzzy C-Means, produces better results in identification of differences between data sets. This is because the value of validity measures for the proposed algorithm is lower than the value of validity measures for other algorithms. This helps to correlate the samples according to the level of gene expression.",1302.3123v1.pdf,"['1302.3123v1.pdf', '2003.13216v1.pdf', '2010.07597v2.pdf', '2005.11699v2.pdf', '1304.7375v1.pdf', '1407.6074v1.pdf', '1809.01628v1.pdf']",1302.3123v1-Figure1-1.png,"Figure 1: Validity Measure for Data set size = 7129, k=7","above mentioned validity measures and the effectiveness of the proposed algorithm is well understood. We tested our method for the Brain Tumour gene expression dataset to cluster the highly suppressed and highly expressed genes and are depicted for various dataset sizes. It is observed that for each set of genes taken, the value of validity measures for the proposed algorithm is lower than the value of validity measures for other algorithms and it is graphically illustrated from Figure 1 to Figure 12. Among these clustering algorithms Penalized Fuzzy C-Means produces better results in identification of differences between data sets. This helps to correlate the samples according to the level of gene expression.","An Analysis of Gene Expression Data using Penalized Fuzzy C-Means
  Approach","With the rapid advances of microarray technologies, large amounts of
high-dimensional gene expression data are being generated, which poses
significant computational challenges. A first step towards addressing this
challenge is the use of clustering techniques, which is essential in the data
mining process to reveal natural structures and identify interesting patterns
in the underlying data. A robust gene expression clustering approach to
minimize undesirable clustering is proposed. In this paper, Penalized Fuzzy
C-Means (PFCM) Clustering algorithm is described and compared with the most
representative off-line clustering techniques: K-Means Clustering, Rough
K-Means Clustering and Fuzzy C-Means clustering. These techniques are
implemented and tested for a Brain Tumor gene expression Dataset. Analysis of
the performance of the proposed approach is presented through qualitative
validation experiments. From experimental results, it can be observed that
Penalized Fuzzy C-Means algorithm shows a much higher usability than the other
projected clustering algorithms used in our comparison study. Significant and
promising clustering results are presented using Brain Tumor Gene expression
dataset. Thus patterns seen in genome-wide expression experiments can be
interpreted as indications of the status of cellular processes. In these
clustering results, we find that Penalized Fuzzy C-Means algorithm provides
useful information as an aid to diagnosis in oncology."
scgqa_126,1707.02327v1,What insights does Figure 3 provide regarding the correlation between forks and star counts in deprecated projects?,"The graph shows that there is no clear relationship between the number of forks and the number of stars. This suggests that the number of forks is not a good indicator of the popularity of a project. In fact, some of the projects with the highest number of forks have very few stars, while some of the projects with the fewest forks have a lot of stars. This suggests that other factors, such as the quality of the project and the activity of the community, are more important in determining the popularity of a project.",1707.02327v1.pdf,"['1707.02327v1.pdf', '2003.13216v1.pdf', '1610.00017v2.pdf', '2007.15958v1.pdf', '1511.07907v2.pdf', '1707.02342v1.pdf', '1608.00887v1.pdf', '2010.08182v3.pdf']",1707.02327v1-Figure3-1.png,"Figure 3: Distribution of the (a) number of forks of the failed projects and (b) number of stars of the fork with the highest number of stars, for each failed project; both violin plots without outliers","Figure 3a shows the distribution of the number of forks of the failed projects. They usually have a relevant number of forks, since it is very simple to fork projects on GitHub. The first, median, and third quartile measures are 244, 400, and 638 forks, respectively. The violin plot in Figure 3b aims to reveal the relevance of these forks. For each project, we computed the fork with the highest number of stars. The violin plot shows the distribution of the number of stars of these most successful forks. As we can see, most forks are not popular at all. They are probably only used to submit pull requests or to create a copy of a repository, for backup purposes [20]. For example, the third quartile measure is 13 stars. However, there are two systems with an outlier behavior. The first one is an audio player, whose fork has 1,080 stars. In our survey, the developer of the original project answered that he abandoned the project due to other interests. However, his code was used to fork a new project, whose README acknowledges that this version “is a substantial rewrite of the fantastic work done in version 1.0 by [Projet-Owner] and others”. Besides 1,080 stars, the forked project has 70 contributors (as February, 2017). The second outlier is a dependency injector for Android, whose fork was made by Google and has 6,178 stars. The forked project’s README mentions that it “is currently in active development, primarily internally at Google, with regular pushes to the open source community"".",Why Modern Open Source Projects Fail,"Open source is experiencing a renaissance period, due to the appearance of
modern platforms and workflows for developing and maintaining public code. As a
result, developers are creating open source software at speeds never seen
before. Consequently, these projects are also facing unprecedented mortality
rates. To better understand the reasons for the failure of modern open source
projects, this paper describes the results of a survey with the maintainers of
104 popular GitHub systems that have been deprecated. We provide a set of nine
reasons for the failure of these open source projects. We also show that some
maintenance practices -- specifically the adoption of contributing guidelines
and continuous integration -- have an important association with a project
failure or success. Finally, we discuss and reveal the principal strategies
developers have tried to overcome the failure of the studied projects."
scgqa_127,1407.7736v1,"In the context of the lift chart, how does the churn-prediction model compare to the baseline?","The key takeaways from the lift chart are that the proposed churn-prediction model achieves higher lift factors than the baseline model. This means that the model is able to identify a higher percentage of true churners from a smaller subset of editors. This is important because it allows the model to be used to identify potential churners early on, which can help to prevent them from leaving the platform.",1407.7736v1.pdf,"['1407.7736v1.pdf', '2003.09700v4.pdf', '1908.04647v1.pdf']",1407.7736v1-Figure5-1.png,Fig. 5: Lift chart obtained by the proposed churn-prediction model. Different curves represent the lift curves for different sliding windows.,"Cumulative gains for churn prediction. The lift factors are widely used by researchers to evaluate the performance of churn-prediction models (e.g. [12]). The lift factors achieved by our model are shown in Figure 5. In lift chart, the diagonal line represents a baseline which randomly selects a subset of editors as potential churners, i.e., it selects s% of the editors that will contain s% of the true churners, resulting in a lift factor of 1. In Figure 5, on average, our model was capable of identifying 10% of editors that contained 21.2% of true churners (i.e. a lift factor of 2.12), 20% of editors that contained 39.3% of true churners (i.e. a lift factor of 1.97), and 30% of editors that contained 54.7% of true churners (i.e. a lift factor of 1.82). Evidently, our model achieved higher lift factors than the baseline. Thus if the objective of the lift analysis is to identify a",A Latent Space Analysis of Editor Lifecycles in Wikipedia,"Collaborations such as Wikipedia are a key part of the value of the modern
Internet. At the same time there is concern that these collaborations are
threatened by high levels of member turnover. In this paper we borrow ideas
from topic analysis to editor activity on Wikipedia over time into a latent
space that offers an insight into the evolving patterns of editor behavior.
This latent space representation reveals a number of different categories of
editor (e.g. content experts, social networkers) and we show that it does
provide a signal that predicts an editor's departure from the community. We
also show that long term editors gradually diversify their participation by
shifting edit preference from one or two namespaces to multiple namespaces and
experience relatively soft evolution in their editor profiles, while short term
editors generally distribute their contribution randomly among the namespaces
and experience considerably fluctuated evolution in their editor profiles."
scgqa_128,2003.13216v1,"Regarding the results presented in Fig. 6, what do K and β indicate about the sample generation process?",The number of augmented domains K is a hyper-parameter that controls the number of adversarial samples generated from the source domain. The coefficient of relaxation β is a hyper-parameter that controls the distance between the generated adversarial samples and the source domain.,2003.13216v1.pdf,"['2003.13216v1.pdf', '1906.11938v3.pdf', '1509.02054v1.pdf', '1501.07107v1.pdf', '1801.06867v1.pdf', '1208.2451v1.pdf', '1910.08413v1.pdf', '1306.4036v2.pdf', '1611.03254v1.pdf']",2003.13216v1-Figure6-1.png,Figure 6. Hyper-parameter tuning of K and β. We set K = 3 and β = 2.0× 103 according to the best classification accuracy.,"the accuracy curve under different K and β in Fig. 6. In Fig. 6 (left), we find that the accuracy reaches the summit when K = 3 and keeps falling with K increasing. This is due to the fact that excessive adversarial samples above a certain threshold will increase the instability and degrade the robustness of the model. In Fig. 6 (right), we observe that the accuracy reaches the summit when β = 2.0 × 103 and drops slightly when β increases. This is because large β will produce domains too far way from the source S and even reach out of the manifold in the embedding space.",Learning to Learn Single Domain Generalization,"We are concerned with a worst-case scenario in model generalization, in the
sense that a model aims to perform well on many unseen domains while there is
only one single domain available for training. We propose a new method named
adversarial domain augmentation to solve this Out-of-Distribution (OOD)
generalization problem. The key idea is to leverage adversarial training to
create ""fictitious"" yet ""challenging"" populations, from which a model can learn
to generalize with theoretical guarantees. To facilitate fast and desirable
domain augmentation, we cast the model training in a meta-learning scheme and
use a Wasserstein Auto-Encoder (WAE) to relax the widely used worst-case
constraint. Detailed theoretical analysis is provided to testify our
formulation, while extensive experiments on multiple benchmark datasets
indicate its superior performance in tackling single domain generalization."
scgqa_129,1910.11127v1,"According to Figure 13, how do reversible blocks affect SNR across layers in the hybrid architecture?","The graph shows that the SNR quickly degrades within reversible blocks, but almost raises back to its original level at the input of each reversible block. This is because the signal propagated to the input of each reversible block is recomputed using the reversible block inverse, which is much more stable.",1910.11127v1.pdf,"['1910.11127v1.pdf', '1609.06577v1.pdf', '1710.06548v1.pdf', '1906.11938v3.pdf', '1101.0235v1.pdf', '1708.09328v1.pdf', '2004.03870v1.pdf', '2005.09634v1.pdf']",1910.11127v1-Figure13-1.png,"Figure 13: Evolution of the SNR through the layers of a hybrid architecture model. The span of two consecutive reversible blocks are shown with color boxes. Within reversible blocks, the SNR quickly degrades due to the numerical errors introduced by invertible layers. However, the signal propagated to the input of each reversible block is recomputed using the reversible block inverse, which is much more stable. Hence, we can see a sharp decline of the SNR within the reversible blocks, but the SNR almost raises back to its original level at the input of each reversible block.","In section 4.2, we introduced a hybrid architecture, illustrated in Figure 8, to prevent the impact of numerical errors on accuracy. Figure 13 shows the propagation of the signal to noise ratio through the layers of such hybrid architecture. As can be seen in this figure, the hybrid architecture is much more robust to numerical errors as activations are propagated from one reversible block to the other using the reversible block inverse computations instead of layer-wise inversions.",Reversible designs for extreme memory cost reduction of CNN training,"Training Convolutional Neural Networks (CNN) is a resource intensive task
that requires specialized hardware for efficient computation. One of the most
limiting bottleneck of CNN training is the memory cost associated with storing
the activation values of hidden layers needed for the computation of the
weights gradient during the backward pass of the backpropagation algorithm.
Recently, reversible architectures have been proposed to reduce the memory cost
of training large CNN by reconstructing the input activation values of hidden
layers from their output during the backward pass, circumventing the need to
accumulate these activations in memory during the forward pass. In this paper,
we push this idea to the extreme and analyze reversible network designs
yielding minimal training memory footprint. We investigate the propagation of
numerical errors in long chains of invertible operations and analyze their
effect on training. We introduce the notion of pixel-wise memory cost to
characterize the memory footprint of model training, and propose a new model
architecture able to efficiently train arbitrarily deep neural networks with a
minimum memory cost of 352 bytes per input pixel. This new kind of architecture
enables training large neural networks on very limited memory, opening the door
for neural network training on embedded devices or non-specialized hardware.
For instance, we demonstrate training of our model to 93.3% accuracy on the
CIFAR10 dataset within 67 minutes on a low-end Nvidia GTX750 GPU with only 1GB
of memory."
scgqa_130,1502.03556v1,What implications can be drawn from the recall-precision graph in Figure 5 regarding weight factors?,"The key takeaway from this experiment is that the proposed automatic weight generation factor can improve the performance of instance matching. This is evident from the fact that the AFlood(PW+) system, which uses the proposed weight factor, outperforms the AFlood(PW-) system, which does not use the weight factor. The improvement in performance is most evident in the case of the IIMB2010 large dataset, where the AFlood(PW+) system achieves a higher recall and precision than the other methods.",1502.03556v1.pdf,"['1502.03556v1.pdf', '1710.09234v1.pdf', '1706.01341v1.pdf', '1703.10422v2.pdf', '1802.03830v1.pdf', '1606.01062v1.pdf']",1502.03556v1-Figure5-1.png,"Figure 5: Instance matching results against IIMB 2010 datasets. In the figure, our instance matcher, called as AFlood (PW+), shows the effectiveness of considering our proposed automatic weight generation factors","In the instance matching track of OAEI-20104, the participants for IIMB2010 large dataset were Combinatorial Optimization for Data Integration (CODI) [20], Automated Semantic Mapping of Ontologies with Validation (ASMOV) [35] and RiMOM [19]. We experimented with our core instance matching system without property weight, we called as AFlood(PW-), and our augmented instance matching system with proposed automatic weight factor, we call as AFlood(PW+). Fig.5 shows the recall-precision graph of the participants [34] and the curves of our instance matching systems. Our proposed method outperforms other methods in several cases although CODI shows better result in some cases.","An Efficient Metric of Automatic Weight Generation for Properties in
  Instance Matching Technique","The proliferation of heterogeneous data sources of semantic knowledge base
intensifies the need of an automatic instance matching technique. However, the
efficiency of instance matching is often influenced by the weight of a property
associated to instances. Automatic weight generation is a non-trivial, however
an important task in instance matching technique. Therefore, identifying an
appropriate metric for generating weight for a property automatically is
nevertheless a formidable task. In this paper, we investigate an approach of
generating weights automatically by considering hypotheses: (1) the weight of a
property is directly proportional to the ratio of the number of its distinct
values to the number of instances contain the property, and (2) the weight is
also proportional to the ratio of the number of distinct values of a property
to the number of instances in a training dataset. The basic intuition behind
the use of our approach is the classical theory of information content that
infrequent words are more informative than frequent ones. Our mathematical
model derives a metric for generating property weights automatically, which is
applied in instance matching system to produce re-conciliated instances
efficiently. Our experiments and evaluations show the effectiveness of our
proposed metric of automatic weight generation for properties in an instance
matching technique."
scgqa_131,1808.10082v4,"According to Figure 10, which privacy metric is most effective for privacy-preserving inference in this research?",The results in Figure 10 show that the choice of privacy metric can have a significant impact on the performance of privacy-preserving statistical inference algorithms. The proposed approach using information privacy as the privacy metric yields the minimum Bayes error for detecting H. This suggests that information privacy is a more appropriate privacy metric for privacy-preserving statistical inference algorithms than average information leakage or local differential privacy.,1808.10082v4.pdf,"['1808.10082v4.pdf', '1702.06270v2.pdf', '1607.08112v1.pdf', '2006.03632v1.pdf', '1302.2824v2.pdf', '1412.4318v1.pdf', '1710.09234v1.pdf', '2003.09700v4.pdf']",1808.10082v4-Figure10-1.png,Fig. 10: Bayes error for detecting H and G◦ with varying conditional mutual information I(X;H | G◦).,"In Fig. 10, we choose the privacy thresholds and budgets for the different metrics so that the Bayes error for detecting the private hypothesis G◦ is the same for all metrics when δ = 0. Similarly, when δ > 0, we set the Bayes error for detecting GMF to be the same across the metrics under comparison. We see that the Bayes error for detecting H decreases as I(X;H | G◦) increases. As expected, our proposed approach using information privacy as the privacy metric yields the minimum Bayes error for detecting H . By comparison, the approach using average information leakage has a slightly higher Bayes error for detecting H . The approach using local differential privacy has a significantly larger Bayes error for detecting H , since local differential privacy protects the data privacy of the sensor observations X and does not distinguish between statistical inferences for H and G◦. For the δ > 0 comparison, the maximal leakage privacy formulation (3) in [37] achieves the worst performance because it considers the strictest privacy criterion using the worst-case private hypothesis G. This serves as a benchmark upper bound for our utility performance if we let δ → 1 in our uncertainty set GX .",Decentralized Detection with Robust Information Privacy Protection,"We consider a decentralized detection network whose aim is to infer a public
hypothesis of interest. However, the raw sensor observations also allow the
fusion center to infer private hypotheses that we wish to protect. We consider
the case where there are an uncountable number of private hypotheses belonging
to an uncertainty set, and develop local privacy mappings at every sensor so
that the sanitized sensor information minimizes the Bayes error of detecting
the public hypothesis at the fusion center, while achieving information privacy
for all private hypotheses. We introduce the concept of a most favorable
hypothesis (MFH) and show how to find a MFH in the set of private hypotheses.
By protecting the information privacy of the MFH, information privacy for every
other private hypothesis is also achieved. We provide an iterative algorithm to
find the optimal local privacy mappings, and derive some theoretical properties
of these privacy mappings. Simulation results demonstrate that our proposed
approach allows the fusion center to infer the public hypothesis with low error
while protecting information privacy of all the private hypotheses."
scgqa_132,1911.04231v2,What relationship does Figure 4 illustrate between occlusion levels and the performance of our method compared to others in the YCB-Video dataset?,"The graph shows that our approach is more robust to occlusion than DenseFusion and PoseCNN+ICP. This is because our approach uses 3D keypoints, which are less affected by occlusion than 2D keypoints. As the percentage of invisible points increases, DenseFusion and PoseCNN+ICP fall faster than ours. This shows that our approach is more robust to occlusion and can still perform well even when objects are heavily occluded.",1911.04231v2.pdf,"['1911.04231v2.pdf', '1603.01185v2.pdf', '1810.04915v1.pdf', '1906.09756v1.pdf', '1805.07914v3.pdf', '1910.04573v3.pdf', '1701.00365v2.pdf', '1505.02851v1.pdf', '1811.00416v5.pdf']",1911.04231v2-Figure4-1.png,Figure 4. Performance of different approaches under increasing levels of occlusion on the YCB-Video dataset.,"Robust to Occlusion Scenes. One of the biggest advantages of our 3D-keypoint-based method is that it’s robust to occlusion naturally. To explored how different methods are influenced by different degrees of occlusion, we follow [50] and calculate the percentage of invisible points on the object surface. Accuracy of ADD-S < 2cm under different invisible surface percentage is shown in Figure 4. The performance of different approaches is very close when 50% of points are invisible. However, with the percentage of invisible part increase, DenseFusion and PoseCNN+ICP fall faster comparing with ours. Figure 3 shows that our model performs well even when objects are heavily occluded.","PVN3D: A Deep Point-wise 3D Keypoints Voting Network for 6DoF Pose
  Estimation","In this work, we present a novel data-driven method for robust 6DoF object
pose estimation from a single RGBD image. Unlike previous methods that directly
regressing pose parameters, we tackle this challenging task with a
keypoint-based approach. Specifically, we propose a deep Hough voting network
to detect 3D keypoints of objects and then estimate the 6D pose parameters
within a least-squares fitting manner. Our method is a natural extension of
2D-keypoint approaches that successfully work on RGB based 6DoF estimation. It
allows us to fully utilize the geometric constraint of rigid objects with the
extra depth information and is easy for a network to learn and optimize.
Extensive experiments were conducted to demonstrate the effectiveness of
3D-keypoint detection in the 6D pose estimation task. Experimental results also
show our method outperforms the state-of-the-art methods by large margins on
several benchmarks. Code and video are available at
https://github.com/ethnhe/PVN3D.git."
scgqa_133,1307.3687v1,What relationship does Fig. 2 illustrate regarding review sample size and item inference accuracy?,"The graph shows that the accuracy of inferring items also increases as the number of review samples increases. This is because as the number of review samples increases, the MAPE estimator has more data to work with and is therefore able to make more accurate predictions.",1307.3687v1.pdf,"['1307.3687v1.pdf', '2011.09375v1.pdf', '2002.12489v3.pdf', '2009.07756v1.pdf', '2005.13754v1.pdf', '1907.10906v1.pdf', '1712.02030v2.pdf']",1307.3687v1-Figure2-1.png,Fig. 2. Items classification accuracy comparison.,"The results are shown in Fig. 2. We first generated graphs with number of nodes |V | = 500 and varying number of edges (|E| = 1000, 2000, 3000, 4000, 5000) using different graph models. In each figure, we generated review samples of different sizes (500 ≤ |R| ≤ 5000), and show accuracy of inferring items averaged over 100 experiments respectively. We observe that when |R| increases, the accuracy also increases and approaches 1. This confirms that the MAPE estimator is asymptotically unbiased. For different graph models, we observe that the accuracy on Grnd is larger than the other two models. This indicates that constrained connections will make the inference performance poor. However, the accuracy curves","On Analyzing Estimation Errors due to Constrained Connections in Online
  Review Systems","Constrained connection is the phenomenon that a reviewer can only review a
subset of products/services due to narrow range of interests or limited
attention capacity. In this work, we study how constrained connections can
affect estimation performance in online review systems (ORS). We find that
reviewers' constrained connections will cause poor estimation performance, both
from the measurements of estimation accuracy and Bayesian Cramer Rao lower
bound."
scgqa_134,2003.00870v1,"In the context of black hole attack detection, how does AIS-DSR differ from the standard DSR protocol outlined in this research?",AIS-DSR is a protocol that uses an adaptive interval selection (AIS) mechanism to detect black hole attacks. DSR is a traditional routing protocol that does not have any built-in mechanism to detect black hole attacks.,2003.00870v1.pdf,"['2003.00870v1.pdf', '2006.16705v1.pdf', '2007.06852v1.pdf', '1207.5027v1.pdf', '1907.11314v1.pdf', '2004.05579v1.pdf', '2005.09634v1.pdf']",2003.00870v1-Figure9-1.png,Fig. 9. Throughput vs. pause time,"Fig. 9 compares the performance of AIS-DSR with that of DSR under black hole attacks for detection of the black hole attacks. As shown in the figure, AIS-DSR increases the throughput by more than 20% over than DSR under attacks, respectively.","An Artificial Immune Based Approach for Detection and Isolation
  Misbehavior Attacks in Wireless Networks","MANETs (Mobile Ad-hoc Networks) is a temporal network, which is managed by
autonomous nodes, which have the ability to communicate with each other without
having fixed network infrastructure or any central base station. Due to some
reasons such as dynamic changes of the network topology, trusting the nodes to
each other, lack of fixed substructure for the analysis of nodes behaviors and
loss of specific offensive lines, this type of networks is not supportive
against malicious nodes attacks. One of these attacks is black hole attack. In
this attack, the malicious nodes absorb data packets and destroy them. Thus, it
is essential to present an algorithm against the black hole attacks. This paper
proposed a new approach, which improvement the security of DSR routing protocol
to encounter the black hole attacks. This schema tries to identify malicious
nodes according to nodes behaviors in a MANETs and isolate them from routing.
The proposed protocol, called AIS-DSR (Artificial Immune System DSR) employ AIS
(Artificial Immune System) to defend against black hole attacks. AIS-DSR is
evaluated through extensive simulations in the ns-2 environment. The results
show that AIS-DSR outperforms other existing solutions in terms of throughput,
end-to-end delay, packets loss ratio and packets drop ratio."
scgqa_135,1906.03859v1,"Referencing the results in the Few-Shot Learning research, what trend does the graph show with training samples?","The graph shows that the performance of the different methods improves as the number of training samples increases. This is expected, as more training data allows the models to learn more about the underlying distribution of the data and to make more accurate predictions.",1906.03859v1.pdf,"['1906.03859v1.pdf', '1402.7063v1.pdf', '1809.01093v3.pdf', '1003.1655v1.pdf', '1905.08337v1.pdf']",1906.03859v1-Figure4-1.png,Figure 4: Percent error on SUN with 5 training samples as a function of the number of classes. Error bars denote the standard error of the mean over 5 repeats.,We further repeated the experiments for various number of classes. Figure 4 shows that our results are consistent across various number of classes. Classes were selected by setting upper and lower bonds on the,Few-Shot Learning with Per-Sample Rich Supervision,"Learning with few samples is a major challenge for parameter-rich models like
deep networks. In contrast, people learn complex new concepts even from very
few examples, suggesting that the sample complexity of learning can often be
reduced. Many approaches to few-shot learning build on transferring a
representation from well-sampled classes, or using meta learning to favor
architectures that can learn with few samples. Unfortunately, such approaches
often struggle when learning in an online way or with non-stationary data
streams. Here we describe a new approach to learn with fewer samples, by using
additional information that is provided per sample. Specifically, we show how
the sample complexity can be reduced by providing semantic information about
the relevance of features per sample, like information about the presence of
objects in a scene or confidence of detecting attributes in an image. We
provide an improved generalization error bound for this case. We cast the
problem of using per-sample feature relevance by using a new ellipsoid-margin
loss, and develop an online algorithm that minimizes this loss effectively.
Empirical evaluation on two machine vision benchmarks for scene classification
and fine-grain bird classification demonstrate the benefits of this approach
for few-shot learning."
scgqa_136,1704.04828v1,"In the context of the experiments shown in Fig. 14, how does the amount of control messages relate to CR node count?","The graph shows that as the number of CR nodes increases, the number of transmitting control messages also increases. This is because each CR node must send a control message to its neighboring nodes in order to coordinate the transmission of data. As the number of CR nodes increases, the number of neighboring nodes for each CR node also increases, which means that more control messages must be sent.",1704.04828v1.pdf,"['1704.04828v1.pdf', '1807.06736v1.pdf', '1608.08469v1.pdf', '1202.4232v2.pdf', '1707.02342v1.pdf', '1905.11471v1.pdf', '1509.08992v2.pdf']",1704.04828v1-Figure14-1.png,Fig. 14: Quantitative amount of control messages.,"The number of control messages which are involved in ROSS variants and the centralized scheme is related with the number of debatable nodes. Figure 13 shows the percentage of debatable nodes with different network densities, from which we can obtain the value of m. Table 2 shows the message complexity, quantitative amount of the control messages, and the size of control messages. Figure 14 shows the analytical result of the amount of transmissions involved in different schemes.",Versatile Robust Clustering of Ad Hoc Cognitive Radio Network,"Cluster structure in cognitive radio networks facilitates cooperative
spectrum sensing, routing and other functionalities. The unlicensed channels,
which are available for every member of a group of cognitive radio users,
consolidate the group into a cluster, and the availability of unlicensed
channels decides the robustness of that cluster against the licensed users'
influence. This paper analyses the problem that how to form robust clusters in
cognitive radio network, so that more cognitive radio users can get benefits
from cluster structure even when the primary users' operation are intense. We
provide a formal description of robust clustering problem, prove it to be
NP-hard and propose a centralized solution, besides, a distributed solution is
proposed to suit the dynamics in the ad hoc cognitive radio network. Congestion
game model is adopted to analyze the process of cluster formation, which not
only contributes designing the distributed clustering scheme directly, but also
provides the guarantee of convergence into Nash Equilibrium and convergence
speed. Our proposed clustering solution is versatile to fulfill some other
requirements such as faster convergence and cluster size control. The proposed
distributed clustering scheme outperforms the related work in terms of cluster
robustness, convergence speed and overhead. The extensive simulation supports
our claims."
scgqa_137,1709.03329v1,"In the context of weed classification, how do loss and average accuracy change with iterations in Fig. 6?","The graph shows that the loss and average class accuracy both decrease as the number of iterations increases. This is to be expected, as the model is learning from the training data and becoming more accurate at predicting the labels. However, the graph also shows that the rate of improvement slows down as the number of iterations increases. This suggests that there is a point of diminishing returns, where the model is no longer learning as much from the training data.",1709.03329v1.pdf,"['1709.03329v1.pdf', '1804.06161v2.pdf', '2002.06090v1.pdf', '1908.05243v1.pdf', '1809.02337v2.pdf', '1206.6850v1.pdf']",1709.03329v1-Figure6-1.png,"Fig. 6: Loss and average class accuracy of three input channels with fine-tuning over various iterations. The maximum number of iterations is set to 40,000, which takes 12hrs.","We use MATLAB to convert the collected datasets to the SegNet data format and annotate the images. A modified version of Caffe [35] with cuDNN processes input data using CUDA, C++ and Python 2.7. For model training, we set the following parameters: learning rate = 0.001, maximum iterations = 40,000 (640 epochs), batch size = 6, weight delay rate = 0.005 and Stochastic Gradient Descent (SGD) solver [36] is used for the optimization. The average model training time given the maximum number of iterations is 12hrs. Fig. 6 shows the loss and average class accuracy over 40,000 iterations. This figure suggests that 10,000-20,000 maximum iterations are sufficient since there is a very subtle performance improvement beyond this.","weedNet: Dense Semantic Weed Classification Using Multispectral Images
  and MAV for Smart Farming","Selective weed treatment is a critical step in autonomous crop management as
related to crop health and yield. However, a key challenge is reliable, and
accurate weed detection to minimize damage to surrounding plants. In this
paper, we present an approach for dense semantic weed classification with
multispectral images collected by a micro aerial vehicle (MAV). We use the
recently developed encoder-decoder cascaded Convolutional Neural Network (CNN),
Segnet, that infers dense semantic classes while allowing any number of input
image channels and class balancing with our sugar beet and weed datasets. To
obtain training datasets, we established an experimental field with varying
herbicide levels resulting in field plots containing only either crop or weed,
enabling us to use the Normalized Difference Vegetation Index (NDVI) as a
distinguishable feature for automatic ground truth generation. We train 6
models with different numbers of input channels and condition (fine-tune) it to
achieve about 0.8 F1-score and 0.78 Area Under the Curve (AUC) classification
metrics. For model deployment, an embedded GPU system (Jetson TX2) is tested
for MAV integration. Dataset used in this paper is released to support the
community and future work."
scgqa_138,1301.5201v1,"According to the findings in Fig. 3, what are the social consequences of rising stable group memberships?","The increasing number of people belonging to one, two, or three stable groups could have a number of implications. First, it could lead to more people being exposed to different viewpoints and ideas, which could help to promote tolerance and understanding. Second, it could lead to more people being involved in political discussions, which could help to improve the quality of democracy. Finally, it could lead to more people being able to find support and resources from others who share their interests, which could help to improve their quality of life.",1301.5201v1.pdf,"['1301.5201v1.pdf', '1905.11471v1.pdf', '1811.01194v1.pdf', '1305.1657v1.pdf', '2009.08716v1.pdf']",1301.5201v1-Figure3-1.png,Fig. 3: Membership of people to groups for k=3 in comments model.,"In figs. 3a and 3b, the numbers of users belonging to one, two or three stable groups in each interval for k=3 are specified. The figure presents mentioned belongings only in the comments model, but in the post model diagram is very similar. We can notice that these numbers increase, mostly because of the increase of the popularity of the portal and the significance of political events taking place.","Models of Social Groups in Blogosphere Based on Information about
  Comment Addressees and Sentiments","This work concerns the analysis of number, sizes and other characteristics of
groups identified in the blogosphere using a set of models identifying social
relations. These models differ regarding identification of social relations,
influenced by methods of classifying the addressee of the comments (they are
either the post author or the author of a comment on which this comment is
directly addressing) and by a sentiment calculated for comments considering the
statistics of words present and connotation. The state of a selected blog
portal was analyzed in sequential, partly overlapping time intervals. Groups in
each interval were identified using a version of the CPM algorithm, on the
basis of them, stable groups, existing for at least a minimal assumed duration
of time, were identified."
scgqa_139,1808.00136v2,"In the study on multi-modal cycle-consistent GZSL, what is revealed about reconstruction loss in relation to epochs?","The graph shows that the reconstruction loss decreases steadily over training, showing that the model succeeds at mapping the generated visual representations back to the semantic space. This is an important question about the proposed approach, as it ensures that the regularisation is effective in achieving this mapping.",1808.00136v2.pdf,"['1808.00136v2.pdf', '1708.07888v3.pdf', '1710.10571v5.pdf', '1803.09990v2.pdf', '1701.00365v2.pdf', '2003.00870v1.pdf', '1610.08332v1.pdf']",1808.00136v2-Figure4-1.png,"Fig. 4. Convergence of the top-1 accuracy in terms of the number of epochs for the generated training samples from the seen classes for CUB, FLO, SUN and AWA.","An important question about out approach is whether the regularisation succeeds in mapping the generated visual representations back to the semantic space. In order to answer this question, we show in Fig. 3 the evolution of the reconstruction loss `REG in (6) as a function of the number of epochs. In general, the reconstruction loss decreases steadily over training, showing that our model succeeds at such mapping. Another relevant question is if our proposed methods take more or less epochs to converge, compared to the Baseline – Fig. 4 shows the classification accuracy of the generated training samples from the seen classes for the proposed models cycle-WGAN and cycle-CLSWGAN, and also for the baseline (note that cycle-(U)WGAN is a fine-tuned model from the cycle-WGAN, so their loss functions are in fact identical for the seen classes shown in the graph). For three out of four datasets, our proposed cycle-WGAN converges faster. However, when the `CLS in included in (7) to form the loss in (8) (transforming cycle-WGAN into cycle-CLSWGAN), then the convergence",Multi-modal Cycle-consistent Generalized Zero-Shot Learning,"In generalized zero shot learning (GZSL), the set of classes are split into
seen and unseen classes, where training relies on the semantic features of the
seen and unseen classes and the visual representations of only the seen
classes, while testing uses the visual representations of the seen and unseen
classes. Current methods address GZSL by learning a transformation from the
visual to the semantic space, exploring the assumption that the distribution of
classes in the semantic and visual spaces is relatively similar. Such methods
tend to transform unseen testing visual representations into one of the seen
classes' semantic features instead of the semantic features of the correct
unseen class, resulting in low accuracy GZSL classification. Recently,
generative adversarial networks (GAN) have been explored to synthesize visual
representations of the unseen classes from their semantic features - the
synthesized representations of the seen and unseen classes are then used to
train the GZSL classifier. This approach has been shown to boost GZSL
classification accuracy, however, there is no guarantee that synthetic visual
representations can generate back their semantic feature in a multi-modal
cycle-consistent manner. This constraint can result in synthetic visual
representations that do not represent well their semantic features. In this
paper, we propose the use of such constraint based on a new regularization for
the GAN training that forces the generated visual features to reconstruct their
original semantic features. Once our model is trained with this multi-modal
cycle-consistent semantic compatibility, we can then synthesize more
representative visual representations for the seen and, more importantly, for
the unseen classes. Our proposed approach shows the best GZSL classification
results in the field in several publicly available datasets."
scgqa_140,1801.06867v1,What training-related factors contribute to the ImageNet-CNN's slight accuracy advantage over Places-CNN in SUN397?,"The ImageNet-CNN model was trained with the less related categories found in ILSVRC2012, which includes objects that are larger and take up a larger portion of the image. This may be why the ImageNet-CNN model performs slightly better than the Places-CNN model when the object is near full size.",1801.06867v1.pdf,"['1801.06867v1.pdf', '1909.01868v3.pdf', '2007.15404v1.pdf', '1703.01827v3.pdf']",1801.06867v1-Figure3-1.png,Figure 3. Object recognition accuracy on SUN397 (75 categories).,"We trained a SVM classifier with 50 images per class, and tested on the remaining 50 images. The input feature was the output of the fc7 activation. The results are shown in Fig. 3. We use two variants: objects masked and objects with background (see Fig. 2). Regarding objects masked, where the background is removed, we can see that in general the performance is optimal when the object is near full size, above 70-80%. This is actually the most interesting region, with ImageNet-CNN performing slightly better than Places-CNN. This is interesting, since Places-CNN was trained with scenes containing more similar objects to the ones in the test set, while ImageNet-CNN was trained with the less related categories found in ILSVRC2012 (e.g. dogs, cats). However, as we saw in Fig. 1a, objects in ILSVRC2012 cover a large portion of the image in contrast to smaller objects in SUN397, suggesting that a more similar scale in the training data may be more important than more similar object categories. As the object becomes smaller, the performance of both models degrades similarly, again showing a limited robustness to scale changes.","Scene recognition with CNNs: objects, scales and dataset bias","Since scenes are composed in part of objects, accurate recognition of scenes
requires knowledge about both scenes and objects. In this paper we address two
related problems: 1) scale induced dataset bias in multi-scale convolutional
neural network (CNN) architectures, and 2) how to combine effectively
scene-centric and object-centric knowledge (i.e. Places and ImageNet) in CNNs.
An earlier attempt, Hybrid-CNN, showed that incorporating ImageNet did not help
much. Here we propose an alternative method taking the scale into account,
resulting in significant recognition gains. By analyzing the response of
ImageNet-CNNs and Places-CNNs at different scales we find that both operate in
different scale ranges, so using the same network for all the scales induces
dataset bias resulting in limited performance. Thus, adapting the feature
extractor to each particular scale (i.e. scale-specific CNNs) is crucial to
improve recognition, since the objects in the scenes have their specific range
of scales. Experimental results show that the recognition accuracy highly
depends on the scale, and that simple yet carefully chosen multi-scale
combinations of ImageNet-CNNs and Places-CNNs, can push the state-of-the-art
recognition accuracy in SUN397 up to 66.26% (and even 70.17% with deeper
architectures, comparable to human performance)."
scgqa_141,1207.5027v1,"In the analysis presented in Figure 5, what do the x-axis and y-axis indicate about application tokens?","The x-axis represents the number of tokens in the application, and the y-axis represents the number of applications with that number of tokens.",1207.5027v1.pdf,"['1207.5027v1.pdf', '1610.08534v1.pdf', '2007.15958v1.pdf', '1511.07907v2.pdf', '1803.11512v1.pdf', '1710.06548v1.pdf']",1207.5027v1-Figure5-1.png,"Figure 5: Medium C applications in the range 150,000 - 400,000 SLOC.","As can be seen by studying the animation at http://www.leshatton.org/wp-content/uploads/2012/01/animate.gif the generic shape of (15) appears fairly early on, certainly within the first 1% of the total data represented by Figure 3. To give some of idea of medium and small systems, Figures 5 and 6 show a collage of individual systems in","Power-Laws and the Conservation of Information in discrete token
  systems: Part 1 General Theory","The Conservation of Energy plays a pivotal part in the development of the
physical sciences. With the growth of computation and the study of other
discrete token based systems such as the genome, it is useful to ask if there
are conservation principles which apply to such systems and what kind of
functional behaviour they imply for such systems.
  Here I propose that the Conservation of Hartley-Shannon Information plays the
same over-arching role in discrete token based systems as the Conservation of
Energy does in physical systems. I will go on to prove that this implies
power-law behaviour in component sizes in software systems no matter what they
do or how they were built, and also implies the constancy of average gene
length in biological systems as reported for example by Lin Xu et al
(10.1093/molbev/msk019).
  These propositions are supported by very large amounts of experimental data
extending the first presentation of these ideas in Hatton (2011, IFIP / SIAM /
NIST Working Conference on Uncertainty Quantification in Scientific Computing,
Boulder, August 2011)."
scgqa_142,1106.3826v2,What connection exists between the Greedy NPPTS algorithm and distributed computations for maximum independent sets in power-law networks?,"The implications of this result are that the Greedy NPPTS algorithm can be used as a building block for distributed algorithms for finding the maximum independent set in power-law graphs. This is because the Greedy NPPTS algorithm is a local algorithm, which means that it only needs to know the local neighborhood of each node in the graph. This makes it well-suited for distributed implementation.",1106.3826v2.pdf,"['1106.3826v2.pdf', '2002.06199v1.pdf', '2011.07119v1.pdf', '1809.07412v2.pdf']",1106.3826v2-Figure1-1.png,Fig. 1. Values of upper bound and lower bound in power-law graphs,"The estimated values of lower bound for 2 ≤ γ ≤ 2.8 is shown in Figure 1. Upper bound Suppose that one has run Greedy NPPTS algorithm under strict majority threshold on a graph with power-law degree distribution. The following theorem shows that unlike general graphs, the Greedy NPPTS algorithm guarantees a constant factor upper bound on power-law graphs.",On the Non-Progressive Spread of Influence through Social Networks,"The spread of influence in social networks is studied in two main categories:
the progressive model and the non-progressive model (see e.g. the seminal work
of Kempe, Kleinberg, and Tardos in KDD 2003). While the progressive models are
suitable for modeling the spread of influence in monopolistic settings,
non-progressive are more appropriate for modeling non-monopolistic settings,
e.g., modeling diffusion of two competing technologies over a social network.
Despite the extensive work on the progressive model, non-progressive models
have not been studied well. In this paper, we study the spread of influence in
the non-progressive model under the strict majority threshold: given a graph
$G$ with a set of initially infected nodes, each node gets infected at time
$\tau$ iff a majority of its neighbors are infected at time $\tau-1$. Our goal
in the \textit{MinPTS} problem is to find a minimum-cardinality initial set of
infected nodes that would eventually converge to a steady state where all nodes
of $G$ are infected.
  We prove that while the MinPTS is NP-hard for a restricted family of graphs,
it admits an improved constant-factor approximation algorithm for power-law
graphs. We do so by proving lower and upper bounds in terms of the minimum and
maximum degree of nodes in the graph. The upper bound is achieved in turn by
applying a natural greedy algorithm. Our experimental evaluation of the greedy
algorithm also shows its superior performance compared to other algorithms for
a set of real-world graphs as well as the random power-law graphs. Finally, we
study the convergence properties of these algorithms and show that the
non-progressive model converges in at most $O(|E(G)|)$ steps."
scgqa_143,2005.09634v1,How does Figure 44 illustrate performance variation during the evaluation of the CNN model in the study?,The graph shows that there is more variation between the tests Run within the k-Fold than there is between the k-Folds. This means that the model performs better when it is trained and tested more times.,2005.09634v1.pdf,"['2005.09634v1.pdf', '1206.5265v1.pdf', '2007.15958v1.pdf', '1404.7045v3.pdf']",2005.09634v1-Figure44-1.png,Figure 44 – L2 10-Fold Cross-Validation Mean Test Accuracies Partitioned between Fold and Run,"The 10-fold cross-validation results appear in the GLM ANOVA of Table 27, Figure 44, Figure 45 and Figure 46, as well as Table 29. More variation came from the Run factor, and only the orange-highlighted Run factor was statistically significant with a P value < 0.10 at 90% confidence. Figure 44 provides a graphical representation of the ANOVA results in Table 27, showing the variation between k-Fold and the variation between tests Run within the k-Fold.",Automated Copper Alloy Grain Size Evaluation Using a Deep-learning CNN,"Moog Inc. has automated the evaluation of copper (Cu) alloy grain size using
a deep-learning convolutional neural network (CNN). The proof-of-concept
automated image acquisition and batch-wise image processing offers the
potential for significantly reduced labor, improved accuracy of grain
evaluation, and decreased overall turnaround times for approving Cu alloy bar
stock for use in flight critical aircraft hardware. A classification accuracy
of 91.1% on individual sub-images of the Cu alloy coupons was achieved. Process
development included minimizing the variation in acquired image color,
brightness, and resolution to create a dataset with 12300 sub-images, and then
optimizing the CNN hyperparameters on this dataset using statistical design of
experiments (DoE).
  Over the development of the automated Cu alloy grain size evaluation, a
degree of ""explainability"" in the artificial intelligence (XAI) output was
realized, based on the decomposition of the large raw images into many smaller
dataset sub-images, through the ability to explain the CNN ensemble image
output via inspection of the classification results from the individual smaller
sub-images."
scgqa_144,1910.05107v2,"According to Figure 6, how does the EMS influence the operational behavior of the batteries in the research?","The graph shows that the EMS is able to manage the states of charge of the batteries effectively. This is done by preventing abrupt charging and discharging, and frequent switching between these two modes. This helps to preserve the longevity of the batteries, and ensures that the batteries are always charged to a sufficient level.",1910.05107v2.pdf,"['1910.05107v2.pdf', '1805.01358v2.pdf', '1911.11395v2.pdf', '1306.4036v2.pdf']",1910.05107v2-Figure6-1.png,"Figure 6: States of charge of DGUs B3, B4, and B5.","by the EMS. Working to the detriment of battery’s longevity, abrupt charging and discharging, and frequent switching between these two modes are prevented by the EMS. As for the SOCs— reported in Figure 6, they evolve while respecting the operational constraints. Moreover, the EMS tries to store surplus energy during periods of peak PV generation (see Figure 8). This energy is released later in the day when the PV generation declines.",Hierarchical Control in Islanded DC Microgrids with Flexible Structures,"Hierarchical architectures stacking primary, secondary, and tertiary layers
are widely employed for the operation and control of islanded DC microgrids
(DCmGs), composed of Distribution Generation Units (DGUs), loads, and power
lines. However, a comprehensive analysis of all the layers put together is
often missing. In this work, we remedy this limitation by setting out a
top-to-bottom hierarchical control architecture. Decentralized voltage
controllers attached to DGUs form our primary layer. Governed by an MPC--based
Energy Management System (EMS), our tertiary layer generates optimal power
references and decision variables for DGUs. In particular, decision variables
can turn DGUs ON/OFF and select their operation modes. An intermediary
secondary layer translates EMS power references into appropriate voltage
signals required by the primary layer. More specifically, to provide a voltage
solution, the secondary layer solves an optimization problem embedding
power-flow equations shown to be always solvable. Since load voltages are not
directly enforced, their uniqueness is necessary for DGUs to produce reference
powers handed down by the EMS. To this aim, we deduce a novel uniqueness
condition based only on local load parameters. Our control framework, besides
being applicable for generic DCmG topologies, can accommodate topological
changes caused by EMS commands. Its functioning is validated via simulations on
a modified 16-bus DC system."
scgqa_145,2010.00502v1,"How did the number of social media posts on various platforms, according to Figure 5, trend over the first eight months of 2020?","The graph shows that the number of posts on Facebook and Instagram increased steadily from January to August 2020. The number of posts on Twitter and YouTube also increased, but at a slower rate. The number of posts on Wikipedia remained relatively constant throughout the year.",2010.00502v1.pdf,"['2010.00502v1.pdf', '1910.11127v1.pdf', '1805.07914v3.pdf']",2010.00502v1-Figure5-1.png,"Figure 5: A timeline distribution of data collected from a number of different Social Media Platform from January 2020 to August 2020, we have presented the platform having data count more than 25.",We have cleaned the hyperlinks collected using the AMUSED framework. We filtered the social media posts by removing the duplicates using a unique identifier of social media post. We have presented a timeline plot of data collected from different social media platforms in figure 5. We plotted the data from those social media platform which has,AMUSED: An Annotation Framework of Multi-modal Social Media Data,"In this paper, we present a semi-automated framework called AMUSED for
gathering multi-modal annotated data from the multiple social media platforms.
The framework is designed to mitigate the issues of collecting and annotating
social media data by cohesively combining machine and human in the data
collection process. From a given list of the articles from professional news
media or blog, AMUSED detects links to the social media posts from news
articles and then downloads contents of the same post from the respective
social media platform to gather details about that specific post. The framework
is capable of fetching the annotated data from multiple platforms like Twitter,
YouTube, Reddit. The framework aims to reduce the workload and problems behind
the data annotation from the social media platforms. AMUSED can be applied in
multiple application domains, as a use case, we have implemented the framework
for collecting COVID-19 misinformation data from different social media
platforms."
scgqa_146,1603.04812v2,"According to Fig. 3, how many iterations are required for convergence of algorithms in this paper?","The average number of iterations needed for the convergence of the algorithms in Tables I and II is less than 20. This is a good result, as it indicates that the algorithms are relatively efficient.",1603.04812v2.pdf,"['1603.04812v2.pdf', '1006.4386v1.pdf', '2004.05579v1.pdf', '2005.14165v4.pdf']",1603.04812v2-Figure3-1.png,Fig. 3. Average number of iterations for convergence of algorithms of Table I and II when M = 3 antennas and K = 3 users without user selection.,"In Fig. 3 we show the average number of iterations needed for the convergence of the algorithms in Tables I and II. In both algorithms the average number of iterations is less than 20. It is very interesting to observe that as SNR increases the number of iterations needed for the convergence of the algorithm in Table II decreases. This is because P thresholde is set fixed at 10 −8 for all SNRs while the error probability decreases from about 10−1 to 10−5. If it is assumed that the error probability is known in advance, it would be better to change P thresholde to a small fraction of the error probability to have a faster convergence with the same reliability.","Modulation-Specific Multiuser Transmit Precoding and User Selection for
  BPSK Signalling","Motivated by challenges to existing multiuser transmission methods in a low
signal to noise ratio (SNR) regime, and emergence of massive numbers of low
data rate ehealth and internet of things (IoT) devices, in this paper we show
that it is beneficial to incorporate knowledge of modulation type into
multiuser transmit precoder design. Particularly, we propose a transmit
precoding (beamforming) specific to BPSK modulation, which has maximum power
efficiency and capacity in poor channel conditions. To be more specific, in a
multiuser scenario, an objective function is formulated based on the weighted
sum of error probabilities of BPSK modulated users. Convex optimization is used
to transform and solve this ill-behaved non-convex minimum probability of error
(MPE) precoding problem. Numerical results confirm significant performance
improvement. We then develop a low-complexity user selection algorithm for MPE
precoding. Based on line packing principles in Grassmannian manifolds, the
number of supported users is able to exceed the number of transmit antennas,
and hence the proposed approach is able to support more simultaneous users
compared with existing multiuser transmit precoding methods."
scgqa_147,1603.04153v1,What relationship is illustrated in Figure 2 between pairwise evaluations and Rank Centrality's estimation error?,"The graph shows that as the number of repeated comparisons increases, the `∞ estimation error of Rank Centrality decreases and the empirical success rate increases. This is because as we get to obtain more pairwise evaluation samples, we are able to estimate the ranking function more accurately and thus make better predictions.",1603.04153v1.pdf,"['1603.04153v1.pdf', '2008.02777v1.pdf', '2005.09814v3.pdf', '1101.0235v1.pdf']",1603.04153v1-Figure2-1.png,Figure 2: Dense regime (pdense = 0.25): empirical `∞ estimation error v.s. L (left); empirical success rate v.s. L (right).,"Figure 2 illustrates the numerical experiments conducted in the dense regime. We see that as L increases, meaning as we get to obtain pairwise evaluation samples beyond the minimal sample complexity, (1) the `∞ estimation error of Rank Centrality decreases and","Top-$K$ Ranking from Pairwise Comparisons: When Spectral Ranking is
  Optimal","We explore the top-$K$ rank aggregation problem. Suppose a collection of
items is compared in pairs repeatedly, and we aim to recover a consistent
ordering that focuses on the top-$K$ ranked items based on partially revealed
preference information. We investigate the Bradley-Terry-Luce model in which
one ranks items according to their perceived utilities modeled as noisy
observations of their underlying true utilities. Our main contributions are
two-fold. First, in a general comparison model where item pairs to compare are
given a priori, we attain an upper and lower bound on the sample size for
reliable recovery of the top-$K$ ranked items. Second, more importantly,
extending the result to a random comparison model where item pairs to compare
are chosen independently with some probability, we show that in slightly
restricted regimes, the gap between the derived bounds reduces to a constant
factor, hence reveals that a spectral method can achieve the minimax optimality
on the (order-wise) sample size required for top-$K$ ranking. That is to say,
we demonstrate a spectral method alone to be sufficient to achieve the
optimality and advantageous in terms of computational complexity, as it does
not require an additional stage of maximum likelihood estimation that a
state-of-the-art scheme employs to achieve the optimality. We corroborate our
main results by numerical experiments."
scgqa_148,2006.04002v2,"According to Fig. 10, how does GPDM enhance accuracy over DM for Robin and Dirichlet boundary conditions?","The graph shows that GPDM is more robust than DM for the case of Robin and Dirichlet boundary conditions. This is because GPDM takes into account the boundary conditions when constructing the diffusion maps, which helps to improve the accuracy of the solutions. For the Robin BC, the GPDM inverse error decays on O (N−1), whereas the DM inverse error never decays and is nearly constant. For the Dirichlet BC, the GPDM inverse error decays faster compared to the DM inverse error.",2006.04002v2.pdf,"['2006.04002v2.pdf', '1707.04849v1.pdf', '1607.06988v1.pdf', '2009.06124v1.pdf', '1902.05312v2.pdf', '1808.10082v4.pdf', '1904.01542v3.pdf', '1603.01793v2.pdf', '1405.5364v2.pdf']",2006.04002v2-Figure10-1.png,Figure 10: (Color online) Comparisons of the Inverse Errors (IEs) of the solutions of (46) as functions of N .,"L2u = f , (46) on the three boundary conditions. In this numerical experiment, the configuration is exactly the same as in Section 3.6. Particularly, Fig. 9 demonstrates the error of the solutions, ‖ûM −~uM‖∞, which we refer as the Inverse Error (IE), as a function of ² for N = 400,k = 50. Compared to the standard diffusion maps, notice that GPDM is more robust for the case of Robin and Dirichlet boundary conditions, as expected. The advantage of GPDM over DM on Robin and Dirichlet boundary conditions is more apparent in Fig. 10. Particularly, for the Robin BC, one can see that the GPDM IE decays on O (N−1) whereas the DM IE never decays and is nearly constant. For the Dirichlet BC, GPDM IE decays faster compared to the DM IE. For the Neumann BC, we see comparable IEs as functions of N , as expected.","Ghost Point Diffusion Maps for solving elliptic PDE's on Manifolds with
  Classical Boundary Conditions","In this paper, we extend the class of kernel methods, the so-called diffusion
maps (DM), and its local kernel variants, to approximate second-order
differential operators defined on smooth manifolds with boundaries that
naturally arise in elliptic PDE models. To achieve this goal, we introduce the
Ghost Point Diffusion Maps (GPDM) estimator on an extended manifold, identified
by the set of point clouds on the unknown original manifold together with a set
of ghost points, specified along the estimated tangential direction at the
sampled points at the boundary. The resulting GPDM estimator restricts the
standard DM matrix to a set of extrapolation equations that estimates the
function values at the ghost points. This adjustment is analogous to the
classical ghost point method in finite-difference scheme for solving PDEs on
flat domain. As opposed to the classical DM which diverges near the boundary,
the proposed GPDM estimator converges pointwise even near the boundary.
Applying the consistent GPDM estimator to solve the well-posed elliptic PDEs
with classical boundary conditions (Dirichlet, Neumann, and Robin), we
establish the convergence of the approximate solution under appropriate
smoothness assumptions. We numerically validate the proposed mesh-free PDE
solver on various problems defined on simple sub-manifolds embedded in
Euclidean spaces as well as on an unknown manifold. Numerically, we also found
that the GPDM is more accurate compared to DM in solving elliptic eigenvalue
problems on bounded smooth manifolds."
scgqa_149,1106.3826v2,How do the upper and lower bounds depicted in Figure 1 support the findings about power-law graphs?,"The graph shows that the upper bound is always greater than the lower bound, which is consistent with the theoretical results. The upper bound is also a constant factor of the lower bound, which implies that the Greedy NPPTS algorithm achieves a constant factor approximation ratio on power-law graphs.",1106.3826v2.pdf,"['1106.3826v2.pdf', '1808.09050v2.pdf', '2008.11326v4.pdf', '2005.09634v1.pdf']",1106.3826v2-Figure1-1.png,Fig. 1. Values of upper bound and lower bound in power-law graphs,"The estimated values of lower bound for 2 ≤ γ ≤ 2.8 is shown in Figure 1. Upper bound Suppose that one has run Greedy NPPTS algorithm under strict majority threshold on a graph with power-law degree distribution. The following theorem shows that unlike general graphs, the Greedy NPPTS algorithm guarantees a constant factor upper bound on power-law graphs.",On the Non-Progressive Spread of Influence through Social Networks,"The spread of influence in social networks is studied in two main categories:
the progressive model and the non-progressive model (see e.g. the seminal work
of Kempe, Kleinberg, and Tardos in KDD 2003). While the progressive models are
suitable for modeling the spread of influence in monopolistic settings,
non-progressive are more appropriate for modeling non-monopolistic settings,
e.g., modeling diffusion of two competing technologies over a social network.
Despite the extensive work on the progressive model, non-progressive models
have not been studied well. In this paper, we study the spread of influence in
the non-progressive model under the strict majority threshold: given a graph
$G$ with a set of initially infected nodes, each node gets infected at time
$\tau$ iff a majority of its neighbors are infected at time $\tau-1$. Our goal
in the \textit{MinPTS} problem is to find a minimum-cardinality initial set of
infected nodes that would eventually converge to a steady state where all nodes
of $G$ are infected.
  We prove that while the MinPTS is NP-hard for a restricted family of graphs,
it admits an improved constant-factor approximation algorithm for power-law
graphs. We do so by proving lower and upper bounds in terms of the minimum and
maximum degree of nodes in the graph. The upper bound is achieved in turn by
applying a natural greedy algorithm. Our experimental evaluation of the greedy
algorithm also shows its superior performance compared to other algorithms for
a set of real-world graphs as well as the random power-law graphs. Finally, we
study the convergence properties of these algorithms and show that the
non-progressive model converges in at most $O(|E(G)|)$ steps."
scgqa_150,2004.05579v1,"Referring to Figure 4 in the paper, what is the problem with using 200 Fourier terms to approximate f?","The sum of the first 200 terms of the Fourier series is nonacceptable as an approximation to f because it exhibits the Gibbs phenomenon near the ends of [0, 1] and near s∗. The Gibbs phenomenon is a phenomenon that occurs when a Fourier series is evaluated at a point where the function being approximated is not continuous. In this case, the Fourier series of f is not continuous at the ends of [0, 1] and near s∗, and as a result, the sum of the first 200 terms of the series exhibits a sharp peak at these points. This peak is not present in the actual function f, and as a result, the sum of the first 200 terms of the series is not an accurate approximation to f.",2004.05579v1.pdf,"['2004.05579v1.pdf', '2003.09700v4.pdf', '1511.04338v2.pdf', '1701.08947v1.pdf']",2004.05579v1-Figure4-1.png,Figure 4. A partial Fourier sum (left) and its first differences (right).,"As expected, the Fourier series of f is slowly convergent, and it exhibits the Gibbs phenomenon near the ends of [0, 1] and near s∗. In Figure 4, on the left, we present the sum of the first 200 terms of the Fourier series, computed at 20000 points in [0, 1]. This sum is nonacceptable as an approximation to f , and yet we can use it to obtain a good initial approximation to s0 ∼ s","Reconstruction of piecewise-smooth multivariate functions from Fourier
  data","In some applications, one is interested in reconstructing a function $f$ from
its Fourier series coefficients. The problem is that the Fourier series is
slowly convergent if the function is non-periodic, or is non-smooth. In this
paper, we suggest a method for deriving high order approximation to $f$ using a
Pad\'e-like method. Namely, by fitting some Fourier coefficients of the
approximant to the given Fourier coefficients of $f$. Given the Fourier series
coefficients of a function on a rectangular domain in $\mathbb{R}^d$, assuming
the function is piecewise smooth, we approximate the function by piecewise high
order spline functions. First, the singularity structure of the function is
identified. For example in the 2-D case, we find high accuracy approximation to
the curves separating between smooth segments of $f$. Secondly, simultaneously
we find the approximations of all the different segments of $f$. We start by
developing and demonstrating a high accuracy algorithm for the 1-D case, and we
use this algorithm to step up to the multidimensional case."
scgqa_151,1812.09355v1,"According to the results from the Cross-model Correlation Analysis in the paper, how does neuron ablation impact NLM performance?",The graph shows that the increase in perplexity (degradation in language model quality) is significantly higher when erasing the top neurons (solid lines) as compared to when ablating the bottom neurons (dotted lines). This suggests that the top neurons are more important for the language model's performance.,1812.09355v1.pdf,"['1812.09355v1.pdf', '1808.09050v2.pdf', '1708.01249v1.pdf', '2007.15404v1.pdf', '1804.00243v2.pdf', '1706.03019v1.pdf', '1804.10488v2.pdf', '1202.4232v2.pdf', '1906.03859v1.pdf']",1812.09355v1-Figure5-1.png,Figure 5: Effect of neuron ablation on perplexity when erasing from the top and bottom of the Cross-correlation ordering from the NLM,"Neuron ablation in NLM: Figure 5 presents the results of ablating neurons of NLM in the order defined by the Cross-model Correlation Analysis method. The trend found in the NMT results is also observed here, i.e. the increase in perplexity (degradation in language model quality) is significantly higher when erasing the top neurons (solid lines) as compared to when ablating the bottom neurons (dotted lines).","What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in
  Deep NLP Models","Despite the remarkable evolution of deep neural networks in natural language
processing (NLP), their interpretability remains a challenge. Previous work
largely focused on what these models learn at the representation level. We
break this analysis down further and study individual dimensions (neurons) in
the vector representation learned by end-to-end neural models in NLP tasks. We
propose two methods: Linguistic Correlation Analysis, based on a supervised
method to extract the most relevant neurons with respect to an extrinsic task,
and Cross-model Correlation Analysis, an unsupervised method to extract salient
neurons w.r.t. the model itself. We evaluate the effectiveness of our
techniques by ablating the identified neurons and reevaluating the network's
performance for two tasks: neural machine translation (NMT) and neural language
modeling (NLM). We further present a comprehensive analysis of neurons with the
aim to address the following questions: i) how localized or distributed are
different linguistic properties in the models? ii) are certain neurons
exclusive to some properties and not others? iii) is the information more or
less distributed in NMT vs. NLM? and iv) how important are the neurons
identified through the linguistic correlation method to the overall task? Our
code is publicly available as part of the NeuroX toolkit (Dalvi et al. 2019)."
scgqa_152,1904.01542v3,What does Figure 12 reveal about median relative error trends as measurements increase in the paper's experiments?,"The graph shows that the median relative error decreases rapidly as the number of measurements m increases. This is true for both the expander and Gaussian cases. In the expander case, the median relative error is nearly 0 for mN ≈ 0.45, while in the Gaussian case, it is not until mN ≈ 0.6 that the median relative error is close to 0.",1904.01542v3.pdf,"['1904.01542v3.pdf', '2001.11086v3.pdf', '1204.5592v1.pdf', '1006.3688v1.pdf', '1812.09355v1.pdf', '1409.2897v1.pdf']",1904.01542v3-Figure12-1.png,Figure 12: Median relative error for instances with dimension N = 800 and G = 5. The diagrams are presented in log-scale.,To conclude this section we selected the instances for dimension N = 800 and G = 5 and present the development of the median relative error over the number of measurements m; see Figure 12. In the expander case the median relative error decreases rapidly and is nearly 0 already for mN ≈ 0.45. Just for the MEIHT for blocks with overlap l− 1 the relative error is close to 0 not until mN ≈ 0.55. For the Gaussian case the results look similar with the only difference that a median relative error close to 0 is reached not until mN ≈ 0.6.,"Discrete Optimization Methods for Group Model Selection in Compressed
  Sensing","In this article we study the problem of signal recovery for group models.
More precisely for a given set of groups, each containing a small subset of
indices, and for given linear sketches of the true signal vector which is known
to be group-sparse in the sense that its support is contained in the union of a
small number of these groups, we study algorithms which successfully recover
the true signal just by the knowledge of its linear sketches. We derive model
projection complexity results and algorithms for more general group models than
the state-of-the-art. We consider two versions of the classical Iterative Hard
Thresholding algorithm (IHT). The classical version iteratively calculates the
exact projection of a vector onto the group model, while the approximate
version (AM-IHT) uses a head- and a tail-approximation iteratively. We apply
both variants to group models and analyse the two cases where the sensing
matrix is a Gaussian matrix and a model expander matrix.
  To solve the exact projection problem on the group model, which is known to
be equivalent to the maximum weight coverage problem, we use discrete
optimization methods based on dynamic programming and Benders' Decomposition.
The head- and tail-approximations are derived by a classical greedy-method and
LP-rounding, respectively."
scgqa_153,1604.04026v1,"According to Figure 4, what running time results support SRCD's application in large-scale datasets?",The graph shows that the running time of the proposed algorithm SRCD for 100 iterations is acceptable for large applications. This indicates that the proposed algorithm is feasible for large scale applications.,1604.04026v1.pdf,"['1604.04026v1.pdf', '1304.7375v1.pdf', '1705.00891v1.pdf', '1402.1892v2.pdf', '2001.11086v3.pdf', '1902.03993v2.pdf', '1403.2732v1.pdf', '1911.04231v2.pdf', '1212.3950v3.pdf']",1604.04026v1-Figure4-1.png,Fig. 4: Running time of 100 iterations with r = 50 and using different number of threads,"This section investigates running the proposed algorithm on large datasets with different settings. Figure 3 shows the running time of Algorithm SRCD for 100 iterations with different number of latent component using 1 thread. Clearly, the running time linearly increases, which fits the theoretical analyses about fast convergence and linear complexity for large sparse datasets in Section III. Furthermore, concerning the parallel algorithm, the running time of Algorithm SRCD for 100 iterations significantly decreases when the number of used threads increases in Figure 4. In addition, the running time is acceptable for large applications. Hence, these results indicate that the proposed algorithm SRCD is feasible for large scale applications.","Fast Parallel Randomized Algorithm for Nonnegative Matrix Factorization
  with KL Divergence for Large Sparse Datasets","Nonnegative Matrix Factorization (NMF) with Kullback-Leibler Divergence
(NMF-KL) is one of the most significant NMF problems and equivalent to
Probabilistic Latent Semantic Indexing (PLSI), which has been successfully
applied in many applications. For sparse count data, a Poisson distribution and
KL divergence provide sparse models and sparse representation, which describe
the random variation better than a normal distribution and Frobenius norm.
Specially, sparse models provide more concise understanding of the appearance
of attributes over latent components, while sparse representation provides
concise interpretability of the contribution of latent components over
instances. However, minimizing NMF with KL divergence is much more difficult
than minimizing NMF with Frobenius norm; and sparse models, sparse
representation and fast algorithms for large sparse datasets are still
challenges for NMF with KL divergence. In this paper, we propose a fast
parallel randomized coordinate descent algorithm having fast convergence for
large sparse datasets to archive sparse models and sparse representation. The
proposed algorithm's experimental results overperform the current studies' ones
in this problem."
scgqa_154,1311.6183v1,"In the context of the paper, what is shown in Figure 5 regarding throughput when increasing threads for independent commands?","The graph shows that the throughput of all the techniques, except for BDB, compare equally with one thread. As threads are added, the throughput of all the techniques, except for P-SMR, decreases. This is because P-SMR has better scalability than the other techniques.",1311.6183v1.pdf,"['1311.6183v1.pdf', '1804.04290v1.pdf', '1110.6199v1.pdf', '2007.06852v1.pdf', '1610.04213v4.pdf', '1209.3394v5.pdf', '1707.04849v1.pdf', '1905.00569v2.pdf']",1311.6183v1-Figure5-1.png,Fig. 5. The effect of the number of threads on the performance independent commands (left) and dependent commands (right); maximum throughput in Kilo commands executed per second (Kcps) (top graphs); normalized per-thread throughput (bottom graphs).,"Results: With independent commands only, the throughput of all the techniques, except for BDB, compare equally with one thread (Figure 5). As threads are added, the throughput of all the techniques, except for P-SMR, decreases. For sPSMR and no-rep this happens due to scheduling overhead at the scheduler. P-SMR has better scalability than the other techniques (see bottom left graph). With dependent-only commands, in all the approaches, except BDB, throughput decreases with the number of worker threads. We attribute this to the overhead of synchronization. The throughput of BDB increases up to 4 threads and then it decreases due to locking overhead.",Rethinking State-Machine Replication for Parallelism,"State-machine replication, a fundamental approach to designing fault-tolerant
services, requires commands to be executed in the same order by all replicas.
Moreover, command execution must be deterministic: each replica must produce
the same output upon executing the same sequence of commands. These
requirements usually result in single-threaded replicas, which hinders service
performance. This paper introduces Parallel State-Machine Replication (P-SMR),
a new approach to parallelism in state-machine replication. P-SMR scales better
than previous proposals since no component plays a centralizing role in the
execution of independent commands---those that can be executed concurrently, as
defined by the service. The paper introduces P-SMR, describes a ""commodified
architecture"" to implement it, and compares its performance to other proposals
using a key-value store and a networked file system."
scgqa_155,1907.04002v1,"Based on the findings in the paper, does the graph fully represent the relationship between donations and Bitcoin's exchange rate?","The graph does not provide a complete picture of the relationship between the total amount of donations and the exchange rate. For example, it does not take into account other factors that may influence the total amount of donations, such as the economic climate or the political situation. Additionally, the graph only shows the total amount of donations in dollars, which does not take into account the total amount of donations in Bitcoin. Therefore, the graph provides a limited view of the relationship between the total amount of donations and the exchange rate.",1907.04002v1.pdf,"['1907.04002v1.pdf', '1909.01868v3.pdf', '1704.03458v1.pdf', '1901.10423v1.pdf']",1907.04002v1-Figure2-1.png,Figure 2: Total amount of donations vs. exchange rate.,"4.2.2 Historical perspective. While it is reasonable to expect that the total amount of donations might increase as Bitcoin’s popularity increases, we found that the exchange rate, which is often described as speculative, seem to have an impact as well. As shown in Figure 2, the total amount of monthly donations in dollars has increased in a relatively small increments until 2017, during which it has increased by orders of magnitude before plummeting down in 2018 onward. Although this change in value resembles the change in bitcoin price in dollars, the resemblance is unclear if we look at the total amount",Characterizing Bitcoin donations to open source software on GitHub,"Web-based hosting services for version control, such as GitHub, have made it
easier for people to develop, share, and donate money to software repositories.
In this paper, we study the use of Bitcoin to make donations to open source
repositories on GitHub. In particular, we analyze the amount and volume of
donations over time, in addition to its relationship to the age and popularity
of a repository.
  We scanned over three million repositories looking for donation addresses. We
then extracted and analyzed their transactions from Bitcoin's public
blockchain. Overall, we found a limited adoption of Bitcoin as a payment method
for receiving donations, with nearly 44 thousand deposits adding up to only 8.3
million dollars in the last 10 years. We also found weak positive correlation
between the amount of donations in dollars and the popularity of a repository,
with highest correlation (r=0.013) associated with number of forks."
scgqa_156,1904.06587v1,"In the context of the Scene Flow experiments, how do GA layers influence the average EPE compared to 3D convolutions?","The graph shows that GA layers can significantly improve the accuracy of the model, with a reduction in EPE of 0.5-1.0 pixels. This is true even for models with a small number of 3D convolutions, such as the GA-Net2 with two 3D convolutions and two GA layers, which produces lower EPE than the GA-Net∗-11 with eleven 3D convolutions. This suggests that GA layers are more effective than 3D convolutions in improving the accuracy of the model.",1904.06587v1.pdf,"['1904.06587v1.pdf', '1811.00416v5.pdf', '1304.7375v1.pdf', '1902.07084v2.pdf', '1509.08992v2.pdf', '1606.04646v1.pdf', '1505.05173v6.pdf', '1911.05146v2.pdf', '2009.06124v1.pdf']",1904.06587v1-Figure3-1.png,Figure 3: Illustration of the effects of guided aggregations. GANets are compared with the same architectures without GA Layers. Evaluations are on Scene Flow dataset using average EPE.,"We also study the effects of the GA layers by comparing with the same architectures without GA steps. These baseline models “GA-Nets∗” have the same network architectures and all other settings except that there is no GA layer implemented. As shown in Fig. 3, for all these models, GA layers have significantly improved the models’ accuracy (by 0.5-1.0 pixels in average EPE). For example, the GA-Net2 with two 3D convolutions and two GA layers produces lower EPE (1.51) compared with GA-Net∗-11 (1.54) which utilizes eleven 3D convolutions. This implies that two GA layers are more effective than nine 3D convolutional layers.",GA-Net: Guided Aggregation Net for End-to-end Stereo Matching,"In the stereo matching task, matching cost aggregation is crucial in both
traditional methods and deep neural network models in order to accurately
estimate disparities. We propose two novel neural net layers, aimed at
capturing local and the whole-image cost dependencies respectively. The first
is a semi-global aggregation layer which is a differentiable approximation of
the semi-global matching, the second is the local guided aggregation layer
which follows a traditional cost filtering strategy to refine thin structures.
These two layers can be used to replace the widely used 3D convolutional layer
which is computationally costly and memory-consuming as it has cubic
computational/memory complexity. In the experiments, we show that nets with a
two-layer guided aggregation block easily outperform the state-of-the-art
GC-Net which has nineteen 3D convolutional layers. We also train a deep guided
aggregation network (GA-Net) which gets better accuracies than state-of-the-art
methods on both Scene Flow dataset and KITTI benchmarks."
scgqa_157,1701.05681v3,"In the context of the Google Code Jam dataset, how do different levels of label corruption impact classifier accuracy as per Figure 17?","The graph shows that the accuracy of the classifier decreases as the percentage of corrupted labels increases. However, the decline in accuracy is not linear, and the magnitude of the decline is smaller for smaller amounts of corruption. This suggests that individual incorrect labels have only minimal effect on the overall quality of the classifier, and that it would take serious systemic ground truth problems to cause extreme classification problems.",1701.05681v3.pdf,"['1701.05681v3.pdf', '1806.05387v1.pdf', '1804.06161v2.pdf', '2002.01322v1.pdf', '2003.00870v1.pdf', '1603.01185v2.pdf', '1804.00243v2.pdf', '1204.5592v1.pdf']",1701.05681v3-Figure17-1.png,Fig. 17. These are the results for ground truth corruption in the Google Code Jam dataset.,"While random forests are known to be robust against mislabeled data due to using bootstrap sampling, which causes each sample to only affect some of the trees in the overall classifier, we nevertheless performed a brief empirical assessment of the potential impact of mislabeled training data. Figure 17 shows the accuracy for various levels of ground truth corruption for varying percentages of corrupted labels in the Google Code Jam dataset. We observe that the magnitude of the decline in accuracy is close to the magnitude of the incorrect ground truth labels for relatively small amounts of corruption. Therefore, we conclude that individual incorrect labels have only minimal effect on the overall quality of the classifier, and that it would take serious systemic ground truth problems to cause extreme classification problems.","Git Blame Who?: Stylistic Authorship Attribution of Small, Incomplete
  Source Code Fragments","Program authorship attribution has implications for the privacy of
programmers who wish to contribute code anonymously. While previous work has
shown that complete files that are individually authored can be attributed, we
show here for the first time that accounts belonging to open source
contributors containing short, incomplete, and typically uncompilable fragments
can also be effectively attributed.
  We propose a technique for authorship attribution of contributor accounts
containing small source code samples, such as those that can be obtained from
version control systems or other direct comparison of sequential versions. We
show that while application of previous methods to individual small source code
samples yields an accuracy of about 73% for 106 programmers as a baseline, by
ensembling and averaging the classification probabilities of a sufficiently
large set of samples belonging to the same author we achieve 99% accuracy for
assigning the set of samples to the correct author. Through these results, we
demonstrate that attribution is an important threat to privacy for programmers
even in real-world collaborative environments such as GitHub. Additionally, we
propose the use of calibration curves to identify samples by unknown and
previously unencountered authors in the open world setting. We show that we can
also use these calibration curves in the case that we do not have linking
information and thus are forced to classify individual samples directly. This
is because the calibration curves allow us to identify which samples are more
likely to have been correctly attributed. Using such a curve can help an
analyst choose a cut-off point which will prevent most misclassifications, at
the cost of causing the rejection of some of the more dubious correct
attributions."
scgqa_158,1704.00325v1,"In the context of Figure 7, how does the number of tokens correlate with playout-speedup in the study?","The graph shows that the tree parallelization method can significantly improve the playout-speedup of the search algorithm. For example, with 128 tokens, the playout-speedup is 24x for CPU and 48x for Phi. This is because the tree parallelization method allows the search algorithm to explore more of the game tree in parallel, which leads to a faster search.",1704.00325v1.pdf,"['1704.00325v1.pdf', '1808.08442v1.pdf', '1903.10464v3.pdf', '1402.1892v2.pdf', '2008.11326v4.pdf', '1306.4036v2.pdf', '1803.04037v1.pdf']",1704.00325v1-Figure7-1.png,Fig. 7. Playout-speedup as function of the number of tokens. Each data point is an average of 10 runs. The constant Cp is 0.1. The search budget is 1024 playouts.,1) Playout-speedup for CPU: The graph in Figure 7a shows,Structured Parallel Programming for Monte Carlo Tree Search,"In this paper, we present a new algorithm for parallel Monte Carlo tree
search (MCTS). It is based on the pipeline pattern and allows flexible
management of the control flow of the operations in parallel MCTS. The pipeline
pattern provides for the first structured parallel programming approach to
MCTS. Moreover, we propose a new lock-free tree data structure for parallel
MCTS which removes synchronization overhead. The Pipeline Pattern for Parallel
MCTS algorithm (called 3PMCTS), scales very well to higher numbers of cores
when compared to the existing methods."
scgqa_159,1902.05312v2,How does Figure 1 illustrate the fitting capabilities of an overparametrized network on random noise in this study?,"The graph shows that an overparametrized neural network with the number of parameters larger than the sample size will be able to perfectly fit random noise. This is because the network has more parameters than data points, so it has the flexibility to learn the patterns in the noise. However, this is not desirable, as it means that the network is not learning the underlying structure of the data, but rather just memorizing the noise.",1902.05312v2.pdf,"['1902.05312v2.pdf', '1707.04849v1.pdf', '1901.10423v1.pdf', '1906.03859v1.pdf', '1301.5201v1.pdf', '2007.15958v1.pdf', '1909.03961v2.pdf', '1606.04646v1.pdf']",1902.05312v2-Figure1-1.png,"Figure 1: The initial fit (L) and the output function (on the train data) learned (R) by a neural network of size Ndepth = 1 and Nwidth = 500 with λ = 0.05 on random noise data. An overparametrized network trained with SGD can fit to random noise. This effect is undesired and ideally, a network will not converge on noise.","We simulate 100 datapoints from an N (0, 1) distribution. An overparametrized neural network with the number of parameters larger than the sample size will be able to perfectly fit this random noise, see Figure 1. However, in order to obtain a low loss, the network will have to significantly increase its function complexity, as can be measured by the norms of the weight and input Jacobians and Hessians. This is shown in Figures 2-3: the traces of the input and weight Hessians significantly increase to obtain a small loss value. After some number of iterations, if the MSE has converged to some value, the Hessian remains approximately constant, with small fluctuations due to the stochasticity of the optimization algorithm. When the network starts to learn the highfrequency components, here the noise, the norms of the input and weight Hessians increase significantly. In order to thus avoid convergence on noise one has to keep these norms small.","Generalisation in fully-connected neural networks for time series
  forecasting","In this paper we study the generalization capabilities of fully-connected
neural networks trained in the context of time series forecasting. Time series
do not satisfy the typical assumption in statistical learning theory of the
data being i.i.d. samples from some data-generating distribution. We use the
input and weight Hessians, that is the smoothness of the learned function with
respect to the input and the width of the minimum in weight space, to quantify
a network's ability to generalize to unseen data. While such generalization
metrics have been studied extensively in the i.i.d. setting of for example
image recognition, here we empirically validate their use in the task of time
series forecasting. Furthermore we discuss how one can control the
generalization capability of the network by means of the training process using
the learning rate, batch size and the number of training iterations as
controls. Using these hyperparameters one can efficiently control the
complexity of the output function without imposing explicit constraints."
scgqa_160,1811.00912v4,"Based on the findings illustrated in Fig. 4, which approach yields the best average cell-edge user throughput?","The graph shows that the 2-layer superposition achieves the highest weighted sum-rate and average cell-edge user throughput, followed by the OM and the unicast-only transmission. This is because the 2-layer superposition can better exploit the spatial diversity and temporal correlation of the channels. The OM, on the other hand, can only exploit the spatial diversity, while the unicast-only transmission can only exploit the temporal correlation.",1811.00912v4.pdf,"['1811.00912v4.pdf', '1603.02175v1.pdf', '1511.04338v2.pdf', '2011.08042v1.pdf', '1501.07107v1.pdf', '1212.3950v3.pdf', '1902.05312v2.pdf', '2010.11594v1.pdf', '1505.05173v6.pdf']",1811.00912v4-Figure4-1.png,"Fig. 4. Weighted sum-rates and average cell-edge user throughputs of the 2-layer superposition, the orthogonal multiplexing (OM) and the unicast-only transmission. Both optimal and equal power allocations are considered.",Fig. 4a shows the weighted sum-rate 0 1 K kk KR R   when,"Two-Layered Superposition of Broadcast/Multicast and Unicast Signals in
  Multiuser OFDMA Systems","We study optimal delivery strategies of one common and $K$ independent
messages from a source to multiple users in wireless environments. In
particular, two-layered superposition of broadcast/multicast and unicast
signals is considered in a downlink multiuser OFDMA system. In the literature
and industry, the two-layer superposition is often considered as a pragmatic
approach to make a compromise between the simple but suboptimal orthogonal
multiplexing (OM) and the optimal but complex fully-layered non-orthogonal
multiplexing. In this work, we show that only two-layers are necessary to
achieve the maximum sum-rate when the common message has higher priority than
the $K$ individual unicast messages, and OM cannot be sum-rate optimal in
general. We develop an algorithm that finds the optimal power allocation over
the two-layers and across the OFDMA radio resources in static channels and a
class of fading channels. Two main use-cases are considered: i) Multicast and
unicast multiplexing when $K$ users with uplink capabilities request both
common and independent messages, and ii) broadcast and unicast multiplexing
when the common message targets receive-only devices and $K$ users with uplink
capabilities additionally request independent messages. Finally, we develop a
transceiver design for broadcast/multicast and unicast superposition
transmission based on LTE-A-Pro physical layer and show with numerical
evaluations in mobile environments with multipath propagation that the capacity
improvements can be translated into significant practical performance gains
compared to the orthogonal schemes in the 3GPP specifications. We also analyze
the impact of real channel estimation and show that significant gains in terms
of spectral efficiency or coverage area are still available even with
estimation errors and imperfect interference cancellation for the two-layered
superposition system."
scgqa_161,1501.06137v1,What strategies could increase the number of user bridges in the context of your Twitter dataset analysis?,"One way to improve the coverage of bridges for the six bridge types would be to lower the quality threshold for Web search results. This would allow more Web search results to be used as bridges, and would likely result in a higher number of bridges being generated. Another way to improve the coverage of bridges would be to use a different method of generating bridges, such as using a social network analysis tool.",1501.06137v1.pdf,"['1501.06137v1.pdf', '1807.06736v1.pdf', '1905.00569v2.pdf', '1810.04915v1.pdf', '1510.01155v1.pdf', '1405.5329v4.pdf', '1304.7375v1.pdf', '1610.08332v1.pdf']",1501.06137v1-Figure3-1.png,"Figure 3: Number of bridges generated for each country (x axis), with maximum of 69 user bridges","Figure 3 shows the coverage of the bridges for the six bridge types. The x-axis corresponds to countries, omitted for readability, sorted by the number of users with bridges to the country, and the y-axis shows the number of users which were able to be bridged to that country. The best coverage is provided by Wikipedia, Wikitravel, and the list of famous people. Web search resulted in very few bridges, due to our high quality threshold.","Building Bridges into the Unknown: Personalizing Connections to
  Little-known Countries","How are you related to Malawi? Do recent events on the Comoros effect you in
any subtle way? Who in your extended social network is in Croatia? We seldom
ask ourselves these questions, yet a ""long tail"" of content beyond our everyday
knowledge is waiting to be explored. In this work we propose a recommendation
task of creating interest in little-known content by building personalized
""bridges"" to users. We consider an example task of interesting users in
little-known countries, and propose a system which aggregates a user's Twitter
profile, network, and tweets to create an interest model, which is then matched
to a library of knowledge about the countries. We perform a user study of 69
participants and conduct 11 in-depth interviews in order to evaluate the
efficacy of the proposed approach and gather qualitative insight into the
effect of multi-faceted use of Twitter on the perception of the bridges. We
find the increase in interest concerning little-known content to greatly depend
on the pre-existing disposition to it. Additionally, we discover a set of vital
properties good bridges must possess, including recency, novelty, emotiveness,
and a proper selection of language. Using the proposed approach we aim to
harvest the ""invisible connections"" to make explicit the idea of a ""small
world"" where even a faraway country is more closely connected to you than you
might have imagined."
scgqa_162,2004.05448v1,"Within the findings of this paper, how does the accuracy improve when changing p-FEM from 1st to 2nd order in Figure 15?",The graph shows that increasing the order of p-FEM from 1st to 2nd order results in the greatest increase in accuracy.,2004.05448v1.pdf,"['2004.05448v1.pdf', '1407.5358v1.pdf', '1906.11938v3.pdf', '2002.06090v1.pdf', '1905.12729v2.pdf', '2002.12489v3.pdf', '1809.01093v3.pdf']",2004.05448v1-Figure15-1.png,Figure 15: Normalized compliance results for different FCM accuracy.,"The accuracy of the final result mainly depends on the accuracy of the structural analysis (i.e. order of p-FEM and the number of quadtree levels). Figure 15a shows the normalized compliance for increasing p-FEM order of the final designs obtained at Stage 1 and Stage 3 of the 2D case studies. The geometries based on an LSF are evaluated using four quadtree levels. All integration cells, also for the TO result, contain 8×8 integration points to make sure there is no dominant integration error. For the Stage 3 results, κ = 25 is used for the Heaviside function. The relatively highest increase in accuracy of the structural analysis is obtained by increasing from 1st order p-FEM to 2nd order.","Automated and Accurate Geometry Extraction and Shape Optimization of 3D
  Topology Optimization Results","Designs generated by density-based topology optimization (TO) exhibit jagged
and/or smeared boundaries, which forms an obstacle to their integration with
existing CAD tools. Addressing this problem by smoothing or manual design
adjustments is time-consuming and affects the optimality of TO designs. This
paper proposes a fully automated procedure to obtain unambiguous, accurate and
optimized geometries from arbitrary 3D TO results. It consists of a geometry
extraction stage using a level-set-based design description involving radial
basis functions, followed by a shape optimization stage involving local
analysis refinements near the structural boundary using the Finite Cell Method.
Well-defined bounds on basis function weights ensure that sufficient
sensitivity information is available throughout the shape optimization process.
Our approach results in highly smooth and accurate optimized geometries, and
its effectiveness is illustrated by 2D and 3D examples."
scgqa_163,1906.07255v3,What conclusions can be drawn from Figure 2 regarding error rates with and without side information in the study?,"The graph suggests that the algorithm performs better with side information than without side information. This is because the side information helps the algorithm to learn the structure of the data better, which in turn leads to a lower error rate.",1906.07255v3.pdf,"['1906.07255v3.pdf', '1904.01542v3.pdf', '1607.08112v1.pdf', '1603.01793v2.pdf', '1909.05034v1.pdf', '1911.02623v1.pdf', '1805.05887v1.pdf']",1906.07255v3-Figure2-1.png,"Figure 2: Error rates for predicting a noisy (9, 9)-biclustered matrix with side information.","The per trial mistake rate is shown in Fig. 2 for matrix dimension n = 20, . . . , 400, where each data point is averaged over 10 runs. We observe that for random side information β = 0.5, the term D̂ could lead to a bound which is vacuous (for small n), however, the algorithm’s error rate was in the range of [0.30, 0.45], being well below chance. With ideal side information, β = 0.0, the performance improved drastically, as suggested by the bounds, to an error rate in [0.10, 0.35]. Observe that since there is 10% label noise for all values of β, the curves are converging to an online mistake rate of 10%. The data points for the plot can be found below.",Online Matrix Completion with Side Information,"We give an online algorithm and prove novel mistake and regret bounds for
online binary matrix completion with side information. The mistake bounds we
prove are of the form $\tilde{O}(D/\gamma^2)$. The term $1/\gamma^2$ is
analogous to the usual margin term in SVM (perceptron) bounds. More
specifically, if we assume that there is some factorization of the underlying
$m \times n$ matrix into $P Q^\intercal$ where the rows of $P$ are interpreted
as ""classifiers"" in $\mathcal{R}^d$ and the rows of $Q$ as ""instances"" in
$\mathcal{R}^d$, then $\gamma$ is the maximum (normalized) margin over all
factorizations $P Q^\intercal$ consistent with the observed matrix. The
quasi-dimension term $D$ measures the quality of side information. In the
presence of vacuous side information, $D= m+n$. However, if the side
information is predictive of the underlying factorization of the matrix, then
in an ideal case, $D \in O(k + \ell)$ where $k$ is the number of distinct row
factors and $\ell$ is the number of distinct column factors. We additionally
provide a generalization of our algorithm to the inductive setting. In this
setting, we provide an example where the side information is not directly
specified in advance. For this example, the quasi-dimension $D$ is now bounded
by $O(k^2 + \ell^2)$."
scgqa_164,2001.07829v1,"According to Figure 8, what is the effect of variable communication delays on training episodes in the study?",The graph shows that more episodes are required to train the model when the communication delay has uncertainty. This is because the model must be able to learn how to deal with the variable delays in order to achieve the desired performance.,2001.07829v1.pdf,"['2001.07829v1.pdf', '2001.11086v3.pdf', '1207.5027v1.pdf', '1707.02439v2.pdf', '2003.09700v4.pdf', '1612.07141v3.pdf']",2001.07829v1-Figure8-1.png,Fig. 8. Mean overall speed v̄ of the learning agent with constant and variable time delays by PLC.,"The accuracy of control methodology is not the only consideration while selecting the method, but researchers are also concerned with the speed and computational feasibility of the learning model. Therefore, we analyze the time required for learning when there is a constant time delay versus a variable uncertain time delay of the Gaussian mixture model. Fig. 8 indicates, that more episodes are required to train the model when the communication delay has uncertainty. But it is a reasonable trade-off to spend that time to enable the model to be prepared for the uncertainties in the system so that we can achieve our goal of mitigating low-frequency oscillations without losing the observability of the system by leveraging wide area measurement systems.","Wide Area Measurement System-based Low Frequency Oscillation Damping
  Control through Reinforcement Learning","Ensuring the stability of power systems is gaining more attraction today than
ever before, due to the rapid growth of uncertainties in load and renewable
energy penetration. Lately, wide area measurement system-based centralized
controlling techniques started providing a more flexible and robust control to
keep the system stable. But, such a modernization of control philosophy faces
pressing challenges due to the irregularities in delays of long-distance
communication channels and response of equipment to control actions. Therefore,
we propose an innovative approach that can revolutionize the control strategy
for damping down low frequency oscillations in transmission systems. Proposed
method is enriched with a potential of overcoming the challenges of
communication delays and other non-linearities in wide area damping control by
leveraging the capability of the reinforcement learning technique. Such a
technique has a unique characteristic to learn on diverse scenarios and
operating conditions by exploring the environment and devising an optimal
control action policy by implementing policy gradient method. Our detailed
analysis and systematically designed numerical validation prove the
feasibility, scalability and interpretability of the carefully modelled
low-frequency oscillation damping controller so that stability is ensured even
with the uncertainties of load and generation are on the rise."
scgqa_165,1707.04476v5,"According to Figure 7, how does collaborative nested sampling succeed with different galaxies in identifying lines?","The graph shows that the method is able to identify and characterize the target line with varying degrees of success. For some galaxies, the line is identified and characterized with small uncertainties (yellow, pink, black). For others, the method remains unsure (cyan, magenta). This is likely due to the factors mentioned above, such as the signal-to-noise ratio of the spectrum and the presence of other lines in the spectrum.",1707.04476v5.pdf,"['1707.04476v5.pdf', '1201.3056v1.pdf', '1209.5833v2.pdf', '1106.3242v2.pdf', '1604.06979v1.pdf', '1907.05050v3.pdf']",1707.04476v5-Figure7-1.png,"Figure 7 Parameter posterior constraints. Each error bar shows a simulated data set; the four examples from Figure 5 are shown in the same colours. The pink and yellow data sets have been well-detected and characterized, while the magenta line has larger uncertainties. The cyan constraints cover two solutions (see Figure 5). Error bars are centred at the median of the posteriors with the line lengths reflecting the 1-sigma equivalent quantiles.","We can now plot the posterior distributions of the found line locations. Figure 7 demonstrates the wide variety of uncertainties. The spectra of Figure 5 are shown in the same colours. For many, the line could be identified and characterised with small uncertainties (yellow, pink, black), for others, the method remains unsure (cyan, magenta). Figure 8 shows that the input redshift distribution is correctly recovered.",Collaborative Nested Sampling: Big Data vs. complex physical models,"The data torrent unleashed by current and upcoming astronomical surveys
demands scalable analysis methods. Many machine learning approaches scale well,
but separating the instrument measurement from the physical effects of
interest, dealing with variable errors, and deriving parameter uncertainties is
often an after-thought. Classic forward-folding analyses with Markov Chain
Monte Carlo or Nested Sampling enable parameter estimation and model
comparison, even for complex and slow-to-evaluate physical models. However,
these approaches require independent runs for each data set, implying an
unfeasible number of model evaluations in the Big Data regime. Here I present a
new algorithm, collaborative nested sampling, for deriving parameter
probability distributions for each observation. Importantly, the number of
physical model evaluations scales sub-linearly with the number of data sets,
and no assumptions about homogeneous errors, Gaussianity, the form of the model
or heterogeneity/completeness of the observations need to be made.
Collaborative nested sampling has immediate application in speeding up analyses
of large surveys, integral-field-unit observations, and Monte Carlo
simulations."
scgqa_166,1509.02054v1,Which parameters are highlighted in Figure 12 as being most directly observable during the underwater navigation simulation?,The graph shows that the DVL scale factor and misalignment angles (yaw and pitch) have the strongest observability in this simulation scenario. This is because these parameters are relatively more directly observable from the data from the IMU and DVL.,1509.02054v1.pdf,"['1509.02054v1.pdf', '1710.09234v1.pdf', '2004.04276v1.pdf', '1108.4475v4.pdf', '1606.01062v1.pdf']",1509.02054v1-Figure12-1.png,"Figure 12. Normalised standard variances for attitude, inertial sensor bias, DVL scale factor and misalignment angles.","Next we implement an Extended Kalman Filter (EKF) to estimate the states of the combined IMU/DVL system, of which the system dynamics are given by Equations (3)-(5) and the measurement is given by Equation (7). The EKF is a nonlinear state estimator widely used in numerous applications. In addition to those states in Theorem 1, the EKF also estimates the position. The position is unobservable, but it is correlated with other states through the system dynamics and the determination of other states will help mitigate the position error drift. The first 600 s static segment is used for IMU initial alignment. Additional angle errors of 0.1° (1σ, for yaw) and 0.01° (1σ, for roll and pitch) are added to the final alignment result so as to mimic the influence of non-benign alignment conditions underwater. Figures 6 and 7 respectively present the DVL scale factor estimate and misalignment angle estimate, as well as their standard variances. The scale factor in Figure 6 converges swiftly from the initial value 0.8 to the truth once the vehicle starts to move at 600 s, as are the two misalignment angles, yaw and pitch, in Figure 7. The roll angle approaches its true value when the descending motion starts at 660 s. Apparently, EKF is more accurate than the IO-DVLC observer in estimating the DVL parameters. The inertial sensor bias estimates and their standard variances are presented in Figures 8 and 9. We see that turning on the square trajectory after 750 s drives the accelerometer bias estimate to convergence (Figure 9). The gyroscope bias in Figure 8 is relatively slower in convergence, especially that in vertical direction (y-axis), due to weaker observability. Figures 10 and 11 give the attitude error and positioning error, as well as their standard variances. The normalised standard variances for attitude, gyroscope/accelerometer biases, DVL scale factor and misalignment angles are plotted in Figure 12. It is the DVL scale factor and misalignment angles (yaw and pitch) that have the strongest observability in this simulation scenario. The interaction among states are quite obvious from Figure 12; for example, yaw angle and the DVL roll angle (at 660 s), and roll/pitch angles and the x-axis accelerometer bias (at 750 s). All of the EKF’s behaviours accord with Theorem 1.",Underwater Doppler Navigation with Self-calibration,"Precise autonomous navigation remains a substantial challenge to all
underwater platforms. Inertial Measurement Units (IMU) and Doppler Velocity
Logs (DVL) have complementary characteristics and are promising sensors that
could enable fully autonomous underwater navigation in unexplored areas without
relying on additional external Global Positioning System (GPS) or acoustic
beacons. This paper addresses the combined IMU/DVL navigation system from the
viewpoint of observability. We show by analysis that under moderate conditions
the combined system is observable. Specifically, the DVL parameters, including
the scale factor and misalignment angles, can be calibrated in-situ without
using external GPS or acoustic beacon sensors. Simulation results using a
practical estimator validate the analytic conclusions."
scgqa_167,1710.10571v5,"According to Figure 15, what does the illustration suggest about WRM's efficacy against significant adversarial perturbations?","The graph shows that WRM performs worse than other methods on attacks with large adversarial budgets. This is because WRM is not designed to defend against large perturbations. However, WRM still outperforms other methods on attacks with small adversarial budgets.",1710.10571v5.pdf,"['1710.10571v5.pdf', '1911.02623v1.pdf', '1108.4475v4.pdf']",1710.10571v5-Figure15-1.png,"Figure 15. Attacks on the MNIST dataset with larger (training and test) adversarial budgets. We compare standard WRM with ∞-norm PGM, FGM, IFGM models. We illustrate test misclassification error vs. the adversarial perturbation level ǫadv. Top row: PGM attacks, middle row: FGM attacks, bottom row: IFGM attacks. Left column: Euclidean-norm attacks, right column: ∞-norm attacks. The vertical bar in (a), (c), and (e) indicates the estimated radius √ ρ̂n(θWRM). The vertical bar in (b), (d), and (f) indicates the perturbation level that was used for training the PGM, FGM, and IFGM models via (27).","In Figure 15 we show the results trained with a large training adversarial budget. In this regime (small γ, large ǫ), performance between WRM and other methods diverge. WRM, which provably defends against small perturbations, outperforms other heuristics against imperceptible attacks for both Euclidean and ∞ norms. Further, it outperforms other heuristics on natural images, showing that it consistently achieves a smaller price of robustness. On attacks with large adversarial budgets (large ǫadv), however, the performance of WRM is worse than that of the other methods (especially in the case of ∞-norm attacks). These findings verify that WRM is a practical alternative over existing heuristics for the moderate levels of robustness where our guarantees hold.","Certifying Some Distributional Robustness with Principled Adversarial
  Training","Neural networks are vulnerable to adversarial examples and researchers have
proposed many heuristic attack and defense mechanisms. We address this problem
through the principled lens of distributionally robust optimization, which
guarantees performance under adversarial input perturbations. By considering a
Lagrangian penalty formulation of perturbing the underlying data distribution
in a Wasserstein ball, we provide a training procedure that augments model
parameter updates with worst-case perturbations of training data. For smooth
losses, our procedure provably achieves moderate levels of robustness with
little computational or statistical cost relative to empirical risk
minimization. Furthermore, our statistical guarantees allow us to efficiently
certify robustness for the population loss. For imperceptible perturbations,
our method matches or outperforms heuristic approaches."
scgqa_168,1909.01868v3,How did the time series generated by StaMPS and CLSTM-ISS compare in estimating displacements for Kathmandu?,"The graph shows that StaMPS and CLSTM-ISS performed similarly in estimating displacements at individual time steps. This is evident from the fact that the two methods produced very similar time series plots of the Kathmandu city. The only difference between the two methods was that the CLSTM-ISS method produced a slightly smoother time series plot, which may be due to its ability to learn long-term dependencies in the data.",1909.01868v3.pdf,"['1909.01868v3.pdf', '2009.08716v1.pdf', '2006.11769v1.pdf', '1504.07495v1.pdf', '1908.09034v2.pdf', '1307.1204v1.pdf', '1803.09990v2.pdf', '1408.5389v1.pdf', '1803.04037v1.pdf']",1909.01868v3-Figure11-1.png,"Figure 11. Time series displacement plot of the Kathmandu city generated using StaMPS, CNN-ISS and CLSTM-ISS. The location marked as a red dot in Figure 6 (a) is used for comparison. Temporal coverage of the generated time series is from 24 March 2015 to 7 November 2015. Green line marks the occurrence of the Nepal earthquake on 25th April 2015.","Analysis of the time series displacement was further carried out to test the ability of the proposed architectures in estimating displacements at individual time steps. Figure 11 shows the time series plot of the Kathmandy city generated using StaMPS, CNN-ISS and CLSTM-ISS. The location marked as a red dot in Figure 6 (a) was selected for generating the time series displacement, having a temporal coverage of nearly eight months (24 March 2015 to 7 November 2015). This interval also included the occurrence of the Nepal earthquake on 25th April 2015). It was observed that the individual displacement estimates of the StaMPS and the CLSTM-ISS methods were similar for all the time steps.","Deep learning networks for selection of persistent scatterer pixels in
  multi-temporal SAR interferometric processing","In multi-temporal SAR interferometry (MT-InSAR), persistent scatterer (PS)
pixels are used to estimate geophysical parameters, essentially deformation.
Conventionally, PS pixels are selected on the basis of the estimated noise
present in the spatially uncorrelated phase component along with look-angle
error in a temporal interferometric stack. In this study, two deep learning
architectures, namely convolutional neural network for interferometric semantic
segmentation (CNN-ISS) and convolutional long short term memory network for
interferometric semantic segmentation (CLSTM-ISS), based on learning spatial
and spatio-temporal behaviour respectively, were proposed for selection of PS
pixels. These networks were trained to relate the interferometric phase history
to its classification into phase stable (PS) and phase unstable (non-PS)
measurement pixels using ~10,000 real world interferometric images of different
study sites containing man-made objects, forests, vegetation, uncropped land,
water bodies, and areas affected by lengthening, foreshortening, layover and
shadowing. The networks were trained using training labels obtained from the
Stanford method for Persistent Scatterer Interferometry (StaMPS) algorithm.
However, pixel selection results, when compared to a combination of R-index and
a classified image of the test dataset, reveal that CLSTM-ISS estimates
improved the classification of PS and non-PS pixels compared to those of StaMPS
and CNN-ISS. The predicted results show that CLSTM-ISS reached an accuracy of
93.50%, higher than that of CNN-ISS (89.21%). CLSTM-ISS also improved the
density of reliable PS pixels compared to StaMPS and CNN-ISS and outperformed
StaMPS and other conventional MT-InSAR methods in terms of computational
efficiency."
scgqa_169,1502.00588v1,What does Fig. 8 reveal about the relationship between step-size and convergence speed in the cognitive radio algorithm?,"The graph shows that the convergence speed of the algorithm increases as the step-size is increased. This is because a larger step-size allows the algorithm to make larger changes to the SUs' transmit powers in each iteration, which helps it to converge more quickly.",1502.00588v1.pdf,"['1502.00588v1.pdf', '1502.03556v1.pdf', '1804.00243v2.pdf', '1709.03329v1.pdf', '1512.00843v3.pdf', '2002.06090v1.pdf', '2007.15958v1.pdf', '2004.03870v1.pdf', '1910.09823v3.pdf']",1502.00588v1-Figure8-1.png,Fig. 8. Scalability of the proposed learning scheme as a function of the step-sizeγ for different values of the numberK of SUs and pricing schemes (λ0 = 0.1: solid lines;λ0 = 0.5 dashed lines).,"To investigate the scalability of the proposed learning scheme, we also examine the algorithm’s convergence speed for different numbers of SUs. In Fig. 8 we show the number of iterations needed to reach an EQL of 95%: importantly, by increasing the value of the algorithm’s step-size, it is possible to reduce the system’s transient phase to a few iterations, even for large numbers of users. Moreover, we also note that the algorithm’s convergence speed in the LP model depends on the pricing parameter λ0 (it decreases with λ0), whereas this is no longer the case under the VP model. The reason for this is again that the VP model acts as a “barrier” which is only activated when the PUs’ interference tolerance is violated.","Cost-Efficient Throughput Maximization in Multi-Carrier Cognitive Radio
  Systems","Cognitive radio (CR) systems allow opportunistic, secondary users (SUs) to
access portions of the spectrum that are unused by the network's licensed
primary users (PUs), provided that the induced interference does not compromise
the primary users' performance guarantees. To account for interference
constraints of this type, we consider a flexible spectrum access pricing scheme
that charges secondary users based on the interference that they cause to the
system's primary users (individually, globally, or both), and we examine how
secondary users can maximize their achievable transmission rate in this
setting. We show that the resulting non-cooperative game admits a unique Nash
equilibrium under very mild assumptions on the pricing mechanism employed by
the network operator, and under both static and ergodic (fast-fading) channel
conditions. In addition, we derive a dynamic power allocation policy that
converges to equilibrium within a few iterations (even for large numbers of
users), and which relies only on local signal-to-interference-and-noise
measurements; importantly, the proposed algorithm retains its convergence
properties even in the ergodic channel regime, despite the inherent
stochasticity thereof. Our theoretical analysis is complemented by extensive
numerical simulations which illustrate the performance and scalability
properties of the proposed pricing scheme under realistic network conditions."
scgqa_170,1608.06005v1,What conclusions can be drawn from Fig. 3 about the maximum sum rate of various schemes in frequency-selective channels?,"The graph shows that TR, TRBD, and EBD have a bound on the maximum sum rate when Pmax/η → ∞ since they do not eliminate ISI completely (this corroborates the fact that their multiplexing gain is r = 0). It is also observed that JPBD has the best performance at high SNR and the simulated multiplexing gain shows good agreement with the theoretical results.",1608.06005v1.pdf,"['1608.06005v1.pdf', '1908.05243v1.pdf']",1608.06005v1-Figure3-1.png,"Fig. 3. Achievable sum rate forK = 2 (left), andK = 6 users forM = 8 antennas. The theoretical reference is a line with a slope equal to the multiplexing gain.","Fig. 3 (left and center) shows the maximum achievable sum rate as a function of Pmax/η (using the power allocation scheme described in Section III-D). The figure shows that TR, TRBD, and EBD have a bound on the maximum sum rate when Pmax/η → ∞ since they do not eliminate ISI completely (this corroborates the fact that their multiplexing gain is r = 0). It is also observed that JPBD has the best performance at high SNR and the simulated multiplexing gain shows good agreement with the theoretical results. Note that, when the number of users increases, higher SNR is required to achieve the same rate since less power is allocated per user.","Space-Time Block Diagonalization for Frequency-Selective MIMO Broadcast
  Channels","The most relevant linear precoding method for frequency-flat MIMO broadcast
channels is block diagonalization (BD) which, under certain conditions, attains
the same nonlinear dirty paper coding channel capacity. However, BD is not
easily translated to frequency-selective channels, since space-time information
is required for transceiver design. In this paper, we demonstrate that BD is
feasible in frequency-selective MIMO broadcast channels to eliminate inter-user
interference (IUI) if the transmit block length is sufficiently large, and if
the number of transmit antennas is greater than the number of users. We also
propose three different approaches to mitigate/eliminate inter-symbol
interference (ISI) in block transmissions: i) time-reversal-based BD (TRBD)
which maximizes spatial focusing around the receivers using transmitter
processing only, ii) equalized BD (EBD) which minimizes the ISI using
transmitter processing only, and iii) joint processing BD (JPBD), which uses
linear processing at the transmitter and the receiver to suppress ISI. We
analyze the theoretical diversity and multiplexing gains of these techniques,
and we demonstrate that JPBD approximates full multiplexing gain for a
sufficiently large transmit block length. Extensive numerical simulations show
that the achievable rate and probability of error performance of all the
proposed methods improve those of conventional time-reversal beamforming.
Moreover, JPBD provides the highest achievable rate region for
frequency-selective MIMO broadcast channels."
scgqa_171,1302.2824v2,What does Figure 6 reveal about the interaction of βα(β) with 1/β for varying R values?,"The graph shows the relationship between βα(β) and 1/β for different values of R. The graph shows that βα(β) increases as 1/β decreases, which is consistent with the theoretical results. This suggests that the probability of a successful attack decreases as the number of rounds increases.",1302.2824v2.pdf,"['1302.2824v2.pdf', '1804.10488v2.pdf', '1702.06270v2.pdf', '1708.01249v1.pdf', '2008.01961v3.pdf', '1909.05034v1.pdf', '1710.09234v1.pdf', '1910.05107v2.pdf']",1302.2824v2-Figure6-1.png,"Figure 6: βα(β) vs. 1/β for R ∈ {2, 3, 5}.","the value of F (ρ, 2) is plotted for different values of ρ ∈ (0.87, 0.999). F (ρ, 2) is plotted versus log(1/(1 − ρ)) in order to “dilate” time around the value ρ = 1 that we are interested in (the quantity βα(β) is plotted versus 1/β in Figure 6 for the same reason), and also because it is natural to regress F (ρ, 2), as function of ρ, against log(1/(1 − ρ)) (see forthcoming discussion).",Lingering Issues in Distributed Scheduling,"Recent advances have resulted in queue-based algorithms for medium access
control which operate in a distributed fashion, and yet achieve the optimal
throughput performance of centralized scheduling algorithms. However,
fundamental performance bounds reveal that the ""cautious"" activation rules
involved in establishing throughput optimality tend to produce extremely large
delays, typically growing exponentially in 1/(1-r), with r the load of the
system, in contrast to the usual linear growth.
  Motivated by that issue, we explore to what extent more ""aggressive"" schemes
can improve the delay performance. Our main finding is that aggressive
activation rules induce a lingering effect, where individual nodes retain
possession of a shared resource for excessive lengths of time even while a
majority of other nodes idle. Using central limit theorem type arguments, we
prove that the idleness induced by the lingering effect may cause the delays to
grow with 1/(1-r) at a quadratic rate. To the best of our knowledge, these are
the first mathematical results illuminating the lingering effect and
quantifying the performance impact.
  In addition extensive simulation experiments are conducted to illustrate and
validate the various analytical results."
scgqa_172,1612.03449v3,What relationship between contention window size and packet delay is illustrated in Figure 7 of the IEEE 802.11p study?,"The graph shows that the contention window has a significant impact on the performance of the protocol. With a smaller CW, there is a higher chance of collisions, which leads to longer delays between packets. This can have a negative impact on the efficiency and latency of the protocol. However, with a larger CW, there is a lower chance of collisions, but this also leads to longer delays between packets. Therefore, the optimal CW value will depend on the specific application and the desired trade-off between efficiency and latency.",1612.03449v3.pdf,"['1612.03449v3.pdf', '1711.06964v1.pdf', '1108.4475v4.pdf', '1705.00891v1.pdf', '1808.00136v2.pdf', '1512.02567v1.pdf', '1912.00088v1.pdf', '2010.13691v1.pdf', '2010.08182v3.pdf']",1612.03449v3-Figure7-1.png,Figure 7: Time metrics in IEEE 802.11p protocol model,"Figure 7 shows time metrics of interest in the IEEE 802.11p protocol model, namely 𝑇𝐵𝑃̅̅ ̅̅̅,","IEEE 802.11p-based Packet Broadcast in Radio Channels with Hidden
  Stations and Congestion Control","The Decentralized Congestion Control (DCC) algorithms specified in ETSI ITS
standards [1] address the IEEE 802.11p MAC and provide reliability of periodic
broadcast messages at high density of vehicles. However, the deterministic
relation between controllable parameters, e.g. transmit power, frame duration,
frame transmit rate and channel clear assessment threshold, and the effects of
DCC algorithms, e.g. channel busy duration, frame interference-free reception
probability and frame channel access delay, is still unknown since a correct
mathematical analysis of the hidden station problem in CSMA networks is
lacking. In this work, the hidden station problem in a linear IEEE 802.11p
broadcast network is analyzed based on analytical results developed in [18]
employing a modified MAC protocol model based on [3]. Simulation results
validate the new analytical model for linear IEEE 802.11p networks w.r.t
reliability and latency performances of Cooperative Awareness Message
broadcast. Evidence is given that the model not only is valid for single-lane
highways but also provides good approximate results for multi-lane highway
scenarios. Our MAC layer analytical model of IEEE 802.11p broadcast reveals the
quantitative relation between DCC parameters and congestion control effects in
closed-form solution for linear vehicular networks."
scgqa_173,1603.08981v2,What thresholding approaches do Baseline1 and Baseline2 use in the context of this research?,The Our Method algorithm is the proposed algorithm in the paper. The Baseline2 algorithm is a baseline algorithm that uses the mean of the statistics as the threshold. The Baseline1 algorithm is a baseline algorithm that uses the median of the statistics as the threshold.,1603.08981v2.pdf,"['1603.08981v2.pdf', '1603.04153v1.pdf', '1305.1657v1.pdf', '1209.3394v5.pdf', '1811.01194v1.pdf']",1603.08981v2-Figure7-1.png,Fig. 7. AUC for Twitter dataset on 116 important real world events.,of these accounts and obtain a star topology graph centered around each handle. We collect tweets of all users in all these networks for a window of time before and after the real life event. For each network we compute the statistics. The AUC curves in Fig. 7 are obtained by varying the threshold. A threshold value is said to correctly identify the true changepoint if the statistic value to the right of the change-point is greater than the threshold. This demonstrates the good performance of our algorithm against two baseline algorithms.,Detecting weak changes in dynamic events over networks,"Large volume of networked streaming event data are becoming increasingly
available in a wide variety of applications, such as social network analysis,
Internet traffic monitoring and healthcare analytics. Streaming event data are
discrete observation occurred in continuous time, and the precise time interval
between two events carries a great deal of information about the dynamics of
the underlying systems. How to promptly detect changes in these dynamic systems
using these streaming event data? In this paper, we propose a novel
change-point detection framework for multi-dimensional event data over
networks. We cast the problem into sequential hypothesis test, and derive the
likelihood ratios for point processes, which are computed efficiently via an
EM-like algorithm that is parameter-free and can be computed in a distributed
fashion. We derive a highly accurate theoretical characterization of the
false-alarm-rate, and show that it can achieve weak signal detection by
aggregating local statistics over time and networks. Finally, we demonstrate
the good performance of our algorithm on numerical examples and real-world
datasets from twitter and Memetracker."
scgqa_174,2001.09043v3,What implications does the convergence in the figure have for system performance under disturbances?,"The convergence of the output tracking variable x1(t) and down-scaled x2(t)-state over perturbation shown in the graph demonstrates the system's ability to track the desired output despite the presence of external perturbations. This is an important property for a control system, as it ensures that the system will continue to operate as intended even in the presence of unexpected disturbances.",2001.09043v3.pdf,"['2001.09043v3.pdf', '1501.07107v1.pdf', '1702.06270v2.pdf', '2002.06090v1.pdf']",2001.09043v3-Figure5-1.png,Fig. 5. Phase portrait of state trajectory (a) and convergence of output tracking variable x1(t) and down-scaled x2(t)-state over perturbation (b).,"Finally we consider the motion system affected by an external stepwise perturbation appearing in form of a random binary signal. The perturbation amplitude is kept the same as in Section V-A so that the perturbed plant (28) contains ξ(t) ∈ {−0.5, 0.5} which is random realization of a binary variable. The controlled state trajectory is shown in Fig. 5 (a),",Optimal terminal sliding-mode control for second-order motion systems,"Terminal sliding mode (TSM) control algorithm and its non-singular refinement
have been elaborated for two decades and belong, since then, to a broader class
of the finite-time controllers, which are known to be robust against the
matched perturbations. While TSM manifold allows for different forms of the
sliding variable, which are satisfying the $q/p$ power ratio of the measurable
output state, we demonstrate that $q/p=0.5$ is the optimal one for the
second-order Newton's motion dynamics with a bounded control action. The paper
analyzes the time-optimal sliding surface and, based thereupon, claims the
optimal TSM control for the second-order motion systems. It is stressed that
the optimal TSM control is fully inline with the Fuller's problem of optimal
switching which minimizes the settling time, i.e. with time-optimal control of
an unperturbed double-integrator. Is is also shown that for the given plant
characteristics, i.e. the overall inertia and control bound, there is no need
for additional control parameters. The single surface design parameter might
(but not necessarily need to) be used for driving system on the boundary layer
of the twisting mode, or for forcing it to the robust terminal sliding mode.
Additional insight is given into the finite-time convergence of TSM and
robustness against the bounded perturbations. Numerical examples with different
upper-bounded perturbations are demonstrated."
scgqa_175,1704.03458v1,"In relation to the findings in Figure 1, how does the predictive performance of ToPs/R on wait list survival differ from post-transplantation?",The graph shows that ToPs/R performs better for survival in the wait list than for survival post-transplantation. This is likely because the wait list population is more heterogeneous and therefore more difficult to predict.,1704.03458v1.pdf,"['1704.03458v1.pdf', '1101.0235v1.pdf', '1006.4386v1.pdf', '1902.02518v1.pdf', '1708.07888v3.pdf', '1912.02074v1.pdf', '1912.03417v1.pdf', '1512.02567v1.pdf']",1704.03458v1-Figure1-1.png,"Figure 1. (a) ROC curve for wait-list survival prediction, (b) ROC curve for post-transplantation survival prediction","Absolute increase (or percentage increase) in AUC is a measure of improved performance, but understates the performance improvement. A predictor performs well if the region under the Receiver Operating Characteristic (ROC) curve is large; or equivalently if the area above the ROC curve is small. AUC measures the area of the region under the ROC curve, but a more revealing measure of performance improvement is in terms of predictive loss – the area of the region above the ROC curve; this is just the difference 1-AUC. We therefore express performance gain of our method over a competing method as the reduction in the predictive loss expressed as a percentage. As Table 3 demonstrates, for survival in the wait list ToPs/R out-performs the best clinical risk scores by 35.2-58.6%, the basic regression methods (on which ToPs/R is built) by 18.6-36.3%, and the best machine learning benchmarks by 5.5-12.8% at all time horizons; for survival post-transplantation, the improvements are smaller but still very substantial. All the improvements are statistically significant (p-value < 0.01). Figures 1(a) and 1(b) provide visual evidence of the extent of the improvement of our method over competing methods. Figure 1(a) shows the actual ROC curves for ToPs/R, for the best machine learning algorithm (Random Forest), and for the best clinical risk score (Meta-Analysis Global Group in Chronic Heart Failure (MAGGIC)) for 3-month survival on the wait list; Figure 1(b) shows the actual ROC curves for 3-month survival post-transplantation (18).","Personalized Survival Predictions for Cardiac Transplantation via Trees
  of Predictors","Given the limited pool of donor organs, accurate predictions of survival on
the wait list and post transplantation are crucial for cardiac transplantation
decisions and policy. However, current clinical risk scores do not yield
accurate predictions. We develop a new methodology (ToPs, Trees of Predictors)
built on the principle that specific predictors should be used for specific
clusters within the target population. ToPs discovers these specific clusters
of patients and the specific predictor that perform best for each cluster. In
comparison with current clinical risk scoring systems, our method provides
significant improvements in the prediction of survival time on the wait list
and post transplantation. For example, in terms of 3 month survival for
patients who were on the US patient wait list in the period 1985 to 2015, our
method achieves AUC of 0.847, the best commonly used clinical risk score
(MAGGIC) achieves 0.630. In terms of 3 month survival/mortality predictions (in
comparison to MAGGIC), holding specificity at 80.0 percents, our algorithm
correctly predicts survival for 1,228 (26.0 percents more patients out of 4,723
who actually survived, holding sensitivity at 80.0 percents, our algorithm
correctly predicts mortality for 839 (33.0 percents) more patients out of 2,542
who did not survive. Our method achieves similar improvements for other time
horizons and for predictions post transplantation. Therefore, we offer a more
accurate, personalized approach to survival analysis that can benefit patients,
clinicians and policymakers in making clinical decisions and setting clinical
policy. Because risk prediction is widely used in diagnostic and prognostic
clinical decision making across diseases and clinical specialties, the
implications of our methods are far reaching."
scgqa_176,1902.03993v2,"According to Figure 11, how does increasing network size affect 8-OK's performance relative to 8-KF-RTRL-AVG?","The graph shows that 8-OK decays more than 8-KF-RTRL-AVG with increasing network size. This is because the gradients in the larger network contain longer term information than the gradients in the smaller network. As a result, the 8-OK approximation is less accurate for the larger network, and the advantage of using the optimal approximation is reduced.",1902.03993v2.pdf,"['1902.03993v2.pdf', '1803.11512v1.pdf', '1910.09823v3.pdf', '2003.14319v2.pdf', '2005.13300v1.pdf', '1910.11127v1.pdf', '1301.5201v1.pdf', '1710.10733v4.pdf']",1902.03993v2-Figure11-1.png,"Figure 11. Variance analysis on the Copy task for a RHN trained for 1 million steps, with sizes either 256 or 512. Observe that 8-OK decays more than 8-KF-RTRL-AVG with the increase in network size. As in Figure 7, we remove datapoints where the true gradient is smaller than 0.0001.","size is increased in Figure 10. However, in Figure 11, OK drops more than KF-RTRL, with the advantage of using the optimal approximation almost completely vanishing. We believe that this is due to the gradients in the larger network containing longer term information than the gradients in the smaller network (that is, taking longer to vanish, due to the spectral norm of Ht being closer to 1). In particular, this effect is not present in Figure 10, as both networks were trained until they learned sequences of length 40. As a result, the gradients probably contain comparable amount of long term information. Naturally, the better test of the quality of the approximations used would be to train a network of larger size in either task. However, due to the computational costs, we have been unable to fully explore the effect of changing the network size experimentally.",Optimal Kronecker-Sum Approximation of Real Time Recurrent Learning,"One of the central goals of Recurrent Neural Networks (RNNs) is to learn
long-term dependencies in sequential data. Nevertheless, the most popular
training method, Truncated Backpropagation through Time (TBPTT), categorically
forbids learning dependencies beyond the truncation horizon. In contrast, the
online training algorithm Real Time Recurrent Learning (RTRL) provides
untruncated gradients, with the disadvantage of impractically large
computational costs. Recently published approaches reduce these costs by
providing noisy approximations of RTRL. We present a new approximation
algorithm of RTRL, Optimal Kronecker-Sum Approximation (OK). We prove that OK
is optimal for a class of approximations of RTRL, which includes all approaches
published so far. Additionally, we show that OK has empirically negligible
noise: Unlike previous algorithms it matches TBPTT in a real world task
(character-level Penn TreeBank) and can exploit online parameter updates to
outperform TBPTT in a synthetic string memorization task. Code availiable on
github."
scgqa_177,1206.6850v1,What does Figure 3 reveal about the relationship between embedding dimensions and algorithm performance for the datasets outlined?,"The graph shows that the performance of different embedding algorithms on the three datasets varies depending on the number of dimensions of the embedding space. For example, the Bandom algorithm performs best on the MovieLens dataset when the embedding space is two dimensions, but it performs worst on the Bugse dataset when the embedding space is four dimensions. This suggests that the optimal number of dimensions for an embedding algorithm may vary depending on the dataset.",1206.6850v1.pdf,"['1206.6850v1.pdf', '1101.0235v1.pdf', '1606.01062v1.pdf', '2010.13691v1.pdf']",1206.6850v1-Figure3-1.png,Figure 3: Performance of embedding algorithms on the three datasets as a function of embedding dimensions.,"Figure 3 shows another experiment with the same evaluation criteria. In this experiment, we fix the testing data as usual, and use all the remaining data for training. The plot shows Kendall’s tau as a function of the number of dimensions of the embedding space.",Visualization of Collaborative Data,"Collaborative data consist of ratings relating two distinct sets of objects:
users and items. Much of the work with such data focuses on filtering:
predicting unknown ratings for pairs of users and items. In this paper we focus
on the problem of visualizing the information. Given all of the ratings, our
task is to embed all of the users and items as points in the same Euclidean
space. We would like to place users near items that they have rated (or would
rate) high, and far away from those they would give a low rating. We pose this
problem as a real-valued non-linear Bayesian network and employ Markov chain
Monte Carlo and expectation maximization to find an embedding. We present a
metric by which to judge the quality of a visualization and compare our results
to local linear embedding and Eigentaste on three real-world datasets."
scgqa_178,1108.4475v4,What role do the performance metrics in Figure 5 play in developing solutions for multi-user MIMO channel management?,"The findings in the graph could be used to improve the performance of multi-user MIMO systems by using the distributed SCA algorithm to solve the sum rate maximization problem. This would allow for more efficient and scalable solutions to this problem, which could lead to improved performance in real-world applications.",1108.4475v4.pdf,"['1108.4475v4.pdf', '1903.10464v3.pdf', '1603.04812v2.pdf', '2009.06124v1.pdf', '1912.02074v1.pdf']",1108.4475v4-Figure5-1.png,"Fig. 5: Performance of Algorithm 2, for1/σ2 = 10 dB andη = 0.4; (a) convergence curves versus round number forNt = 8, K = 4, 6, and forNt = 12, K = 6, averaged over 500 sets of randomly generated {Qki}, (b) comparison with Algorithm 1 forNt = 8, K = 4 over 30 sets of randomly generated{Qki}.","Example 3: In this example, we examine the performance of the proposed distributed SCA algorithm (Algorithm 2). Figure 5(a) shows the convergence behaviors (the evolution of sum rate at each round) of the distributed SCA algorithm for Nt = 8, K = 4, 6, and for Nt = 12, K = 6, where 1/σ2 = 10 dB, η = 0.4. Each curve in Figure 5(a) is obtained by averaging over 500 sets of randomly generated {Qki}. It can be observed from Figure 5(a) that the sum rate performance of the distributed SCA algorithm is almost the same as its centralized counterpart for Nt = 8, K = 4; whereas there is a gap between the sum rates achieved by the centralized and distributed SCA algorithms for Nt = 8, K = 6. One explanation for this gap is that, when the system is nearly fully loaded (i.e., when K is close to Nt), the distributed SCA algorithm, which updates only the variables associated with one transmitter at a time, is more likely to get stuck at a stationary point that is not as good as that achieved by the centralized SCA algorithm which optimizes all the variables in each iteration. As also shown in Fig. 5(a), when we increase Nt to 12, the decentralized algorithm again converges to the centralized solution. Figure 5(b) shows that, for Nt = 8, K = 4 the distributed SCA algorithm yields performance similar to that achieved by its centralized counterpart for almost all of the 30 tested problem instances within 10 round-robin iterations.","Coordinated Beamforming for Multiuser MISO Interference Channel under
  Rate Outage Constraints","This paper studies the coordinated beamforming design problem for the
multiple-input single-output (MISO) interference channel, assuming only channel
distribution information (CDI) at the transmitters. Under a given requirement
on the rate outage probability for receivers, we aim to maximize the system
utility (e.g., the weighted sum rate, weighted geometric mean rate, and the
weighed harmonic mean rate) subject to the rate outage constraints and
individual power constraints. The outage constraints, however, lead to a
complicated, nonconvex structure for the considered beamforming design problem
and make the optimization problem difficult to handle. {Although} this
nonconvex optimization problem can be solved in an exhaustive search manner,
this brute-force approach is only feasible when the number of
transmitter-receiver pairs is small. For a system with a large number of
transmitter-receiver pairs, computationally efficient alternatives are
necessary. The focus of this paper is hence on the design of such efficient
approximation methods. In particular, by employing semidefinite relaxation
(SDR) and first-order approximation techniques, we propose an efficient
successive convex approximation (SCA) algorithm that provides high-quality
approximate beamforming solutions via solving a sequence of convex
approximation problems. The solution thus obtained is further shown to be a
stationary point for the SDR of the original outage constrained beamforming
design problem. {Furthermore}, we propose a distributed SCA algorithm where
each transmitter optimizes its own beamformer using local CDI and information
obtained from limited message exchange with the other transmitters. Our
simulation results demonstrate that the proposed SCA algorithm and its
distributed counterpart indeed converge, and near-optimal performance can be
achieved for all the considered system utilities."
scgqa_179,2010.11594v1,How does the performance of models using hard pseudo ground truth relate to ground truth actionness in this study?,"The graph shows that the model trained with hard pseudo ground truth achieves a performance that is close to the upper bound, which is the performance of the model trained with ground truth actionness sequence. This suggests that the hard pseudo ground truth is a good approximation of the ground truth actionness sequence.",2010.11594v1.pdf,"['2010.11594v1.pdf', '1809.07412v2.pdf', '1502.00588v1.pdf', '1804.06674v1.pdf']",2010.11594v1-Figure3-1.png,Fig. 3: Comparison between models trained with different pseudo ground truth on the THUMOS14 testing set. The upper bounds denote models trained with ground truth actionness sequence,"list the results in Table 4. The results reveal that both Lbg and Latt help improve the performance. And the proposed Latt achieves higher attention variance and better localization performance than Lbg, demonstrating that the our attention normalization term Latt can better avoid the ambiguity of attention. Surprisingly, with both Lbg and Latt, the localization performance is still lower than that with only Latt, and we think this is because the noise of background classification reduces the accuracy of action proposal scores. Ablation study on Pseudo Ground Truth. Fig. 3 plots performance comparison between different pseudo ground truth methods at different refinement iterations. Both soft and hard pseudo ground truth help improve the localization performance. The hard pseudo ground truth removes uncertainty to the model, and thus achieves higher performance improvement. However, with the same frame-level supervision, the flow stream outperforms the RGB stream by a large margin. We think this is because of the nature of two modalities: the RGB modality is less sensitive to actions than the optical flow modality. To demonstrate this, we generate a true frame-level ground truth actionness sequence (action categories are not used), train our model in the same way as the pseudo ground truth. The results are plotted in Fig. 3 as an upper bound. The results verify our hypothesis and demonstrate that the optical flow modality is more suitable for the action localization task than the RGB modality.","Two-Stream Consensus Network for Weakly-Supervised Temporal Action
  Localization","Weakly-supervised Temporal Action Localization (W-TAL) aims to classify and
localize all action instances in an untrimmed video under only video-level
supervision. However, without frame-level annotations, it is challenging for
W-TAL methods to identify false positive action proposals and generate action
proposals with precise temporal boundaries. In this paper, we present a
Two-Stream Consensus Network (TSCN) to simultaneously address these challenges.
The proposed TSCN features an iterative refinement training method, where a
frame-level pseudo ground truth is iteratively updated, and used to provide
frame-level supervision for improved model training and false positive action
proposal elimination. Furthermore, we propose a new attention normalization
loss to encourage the predicted attention to act like a binary selection, and
promote the precise localization of action instance boundaries. Experiments
conducted on the THUMOS14 and ActivityNet datasets show that the proposed TSCN
outperforms current state-of-the-art methods, and even achieves comparable
results with some recent fully-supervised methods."
scgqa_180,1907.11314v1,What does condition (3.6) ensure about the interface positions illustrated in Fig. 3.1 of the study?,The condition (3.6) is a necessary condition for the interface positions to be well-defined. This is because it ensures that the interface positions do not cross each other. The illustration of the interface positions at different time instances shows that this condition is satisfied.,1907.11314v1.pdf,"['1907.11314v1.pdf', '2005.13300v1.pdf', '1705.00891v1.pdf', '1707.02342v1.pdf', '1905.00569v2.pdf', '1908.09034v2.pdf', '1911.02623v1.pdf', '2003.06259v1.pdf', '1007.0328v1.pdf']",1907.11314v1-Figure3.1-1.png,Fig. 3.1: Illustration of interface positions at different time instances satisfying condition (3.6).,"see Fig. 3.1. In this case, cn−1 is well-defined on Γn and we can approximate the time derivative by simple finite difference",Numerical modelling of phase separation on dynamic surfaces,"The paper presents a model of lateral phase separation in a two component
material surface. The resulting fourth order nonlinear PDE can be seen as a
Cahn-Hilliard equation posed on a time-dependent surface. Only elementary
tangential calculus and the embedding of the surface in $\mathbb{R}^3$ are used
to formulate the model, thereby facilitating the development of a fully
Eulerian discretization method to solve the problem numerically. A hybrid
method, finite difference in time and trace finite element in space, is
introduced and stability of its semi-discrete version is proved. The method
avoids any triangulation of the surface and uses a surface-independent
background mesh to discretize the equation. Thus, the method is capable of
solving the Cahn-Hilliard equation numerically on implicitly defined surfaces
and surfaces undergoing strong deformations and topological transitions. We
assess the approach on a set of test problems and apply it to model spinodal
decomposition and pattern formation on colliding surfaces. Finally, we consider
the phase separation on a sphere splitting into two droplets."
scgqa_181,1808.07801v3,"Based on Figure 5 data, what predictive relationship does the Chernoff ratio have with respect to the clustering approach?","The graph shows that the Chernoff ratio is a good predictor of which method, ASE or LSE, is preferred for spectral clustering. When the Chernoff ratio is less than 1, LSE is preferred, and when the Chernoff ratio is greater than 1, ASE is preferred. This is consistent with the results of the synthetic experiments in Figure 4.",1808.07801v3.pdf,"['1808.07801v3.pdf', '1809.01093v3.pdf', '1805.06370v2.pdf', '1910.00110v2.pdf', '1403.5617v1.pdf', '1810.04915v1.pdf']",1808.07801v3-Figure5-1.png,"Fig. 5. For each of our 114 connectomes, we plot the a priori 2-block SBM projections for {Left,Right} in red and {Gray,White} in blue. The coordinates are given by x = min(a, c)/max(a, c) and y = b/max(a, c), where B = [a, b; b, c] is the observed block connectivity probability matrix. The thin black curve y = √ x represents the rank 1 submodel separating positive definite (lower right) from indefinite (upper left). The background color shading is Chernoff ratio ρ, and the thick black curves are ρ = 1 separating the region where ASE is preferred (between the curves) from LSE preferred. The point (1, 1) represents Erdős-Rényi (a = b = c). The large stars are from the a priori composite connectome projections (Figure 4). We see that the red {Left,Right} projections are in the affinity region where ρ < 1 and LSE is preferred while the blue {Gray,White} projections are in the core-periphery region where ρ > 1 and ASE is preferred. This analytical finding based on projections onto the SBM carries over to empirical spectral clustering results on the individual connectomes (Figure 7).","Figures 5, 6 and 7 present empirical results for the connectome data set,m = 114 graphs each on n ≈ 40, 000 vertices. We note that these connectomes are most assuredly not 4-block ‘Two Truths’ SBMs of the kind presented in Figures 3 and 4, but they do have ‘Two Truths’ ({Left,Right} & {Gray,White}) and, as we shall see, they do exhibit a real-data version of the synthetic results presented above, in the spirit of semiparametric SBM fitting.",On a 'Two Truths' Phenomenon in Spectral Graph Clustering,"Clustering is concerned with coherently grouping observations without any
explicit concept of true groupings. Spectral graph clustering - clustering the
vertices of a graph based on their spectral embedding - is commonly approached
via K-means (or, more generally, Gaussian mixture model) clustering composed
with either Laplacian or Adjacency spectral embedding (LSE or ASE). Recent
theoretical results provide new understanding of the problem and solutions, and
lead us to a 'Two Truths' LSE vs. ASE spectral graph clustering phenomenon
convincingly illustrated here via a diffusion MRI connectome data set: the
different embedding methods yield different clustering results, with LSE
capturing left hemisphere/right hemisphere affinity structure and ASE capturing
gray matter/white matter core-periphery structure."
scgqa_182,1910.04573v3,How do the results in Figure 9 explain the wall temperature behavior over time in the new model versus the PDE-model?,"The graph shows that the wall temperature decreases over time for both the new D(P)DE5-model and the PDE-model. This is to be expected, as the temperature of the medium decreases over time as it cools down. The new D(P)DE5-model shows a slightly better fit to the data than the PDE-model, but both models are able to accurately predict the trend of the wall temperature over time.",1910.04573v3.pdf,"['1910.04573v3.pdf', '1805.01358v2.pdf', '1905.00569v2.pdf', '2010.13691v1.pdf', '1006.4386v1.pdf', '1906.03859v1.pdf', '1607.06988v1.pdf', '1803.09990v2.pdf']",1910.04573v3-Figure9-1.png,Figure 9: Wall temperature comparison of the new D(P)DE5-model and the PDE-model with measurement data at z = l.,"models is applicable. In contrast, if the wall temperature or an intermediate medium temperature is needed the D(P)DE5-model or its simplest solution, the D(P)DE1, are a good alternatives to the PDE-model.7 Furthermore, at z = l/3 all the newly proposed models show a perfect match with the measured data (cf. Figure 10), whereas the error of the adapted DDE-model increases. A spatially dependent definition (resp. identification) of the correction factor may lead to better results. Moreover, the wall temperatures calculated by the PDE- and the D(P)DE5-model show a similar behavior as the measurement. The occurring offset of 2 ◦C (compare Figure 9 between 100 s to 180 s) is likely to be caused by the nonlinear behavior of the thermocouple, which is not compensated. At different measuring points different offsets (positive and negative) arise.","On delay-partial-differential and delay-differential thermal models for
  variable pipe flow","A new formulation of physical thermal models for variable plug flow through a
pipe is proposed. The derived model is based on a commonly used one-dimensional
distributed parameter model, which explicitly takes into account the heat
capacity of the jacket of the pipe. The main result of the present contribution
is the constitution of the equivalence of this model with a serial connection
of a pure delay or transport system and another partial differential equation
(PDE), subsequently called delay-partial-differential equation (DPDE)-model.
The means for obtaining the proposed model comprise operational calculus in the
Laplace domain as well as classical theory of characteristics. The
finite-dimensional approximation of the DPDE-model leads to a
delay-differential equation (DDE)-system, which can be seen as a generalization
of commonly used DDE-models consisting of a first-order low-pass filter subject
to an input delay. The proposed model is compared to several alternative models
in simulations and experimental studies."
scgqa_183,1708.05355v1,"According to MirrorFlow's findings, what factors are essential when determining segmentation functions in optical flow estimation?","There are a number of other factors that should be considered when choosing a function for image segmentation. These factors include the type of image, the desired level of accuracy, and the amount of noise in the image. Additionally, the computational complexity of the function should be considered.",1708.05355v1.pdf,"['1708.05355v1.pdf', '2002.01322v1.pdf', '1908.04647v1.pdf', '1710.10733v4.pdf', '1502.00588v1.pdf', '1805.01772v1.pdf', '1907.11771v1.pdf', '2001.09043v3.pdf', '1005.0416v1.pdf']",1708.05355v1-Figure6-1.png,Figure 6. (a) Sigmoid function. (b) Geman-McClure function.,"As shown in Figs. 6a and 6b, these continuous functions approximate the conventional discrete setting, but they assess subtle brightness variations more naturally when their input is near zero. In other words, they are still robust, but less brittle than the original Hamming-based definition.","MirrorFlow: Exploiting Symmetries in Joint Optical Flow and Occlusion
  Estimation","Optical flow estimation is one of the most studied problems in computer
vision, yet recent benchmark datasets continue to reveal problem areas of
today's approaches. Occlusions have remained one of the key challenges. In this
paper, we propose a symmetric optical flow method to address the well-known
chicken-and-egg relation between optical flow and occlusions. In contrast to
many state-of-the-art methods that consider occlusions as outliers, possibly
filtered out during post-processing, we highlight the importance of joint
occlusion reasoning in the optimization and show how to utilize occlusion as an
important cue for estimating optical flow. The key feature of our model is to
fully exploit the symmetry properties that characterize optical flow and
occlusions in the two consecutive images. Specifically through utilizing
forward-backward consistency and occlusion-disocclusion symmetry in the energy,
our model jointly estimates optical flow in both forward and backward
direction, as well as consistent occlusion maps in both views. We demonstrate
significant performance benefits on standard benchmarks, especially from the
occlusion-disocclusion symmetry. On the challenging KITTI dataset we report the
most accurate two-frame results to date."
scgqa_184,1911.05146v2,"According to Figure 10, what trade-off does HyPar-Flow reveal regarding batch size and performance on Frontera?","One example is that the large blue circle with diagonal lines shows results for 128 nodes using 128 modelreplicas where the model is split into 48 partitions on the single 48-core node. This leads to a batch-size of just 32,768, which is 2× smaller than the expected 65,536 if pure data-parallelism is used. However, this smaller batch-size leads to higher throughput (Img/sec).",1911.05146v2.pdf,"['1911.05146v2.pdf', '2001.11086v3.pdf', '1402.1892v2.pdf']",1911.05146v2-Figure10-1.png,"Fig. 10. Hybrid-Parallelism at Scale: ResNet-1001-v2 on Stampede and Frontera with different batch sizes, number of replicas, and number of partitions","The most comprehensive coverage of HyPar-Flow’s flexibility, performance, and scalability are presented in Figure 10(a). The figure shows performance for various combinations of hybrid-parallel training of ResNet-1001-v2 on 128 Stampede2 nodes. The figure has three dimensions: 1) the number of nodes on the X-axis, 2) Performance (Img/sec) on Y-axis, and 3) Batch Size using the diameter of the circles. The key takeaway is that hybrid-parallelism offers the user to make trade-offs between high-throughput (Img/sec) and batch size. From an accuracy (convergence) standpoint, the goal is to keep the batch-size small so model updates are more frequent. However, larger batch-size delays synchronization and thus provides higher throughput (Img/sec). HyPar-Flow offers the flexibility to control these two goals via different configurations. For instance, the large blue circle with diagonal lines shows results for 128 nodes using 128 modelreplicas where the model is split into 48 partitions on the single 48-core node. This leads to a batch-size of just 32,768, which is 2× smaller than the expected 65,536 if pure data-parallelism is used. It is worth noting that the performance of pure data-parallelism even with 2× larger batch-size will still be lesser than the hybrid-parallel case, i.e., 793 img/sec (=6.2×128 – considering ideal scaling for data-parallel case presented earlier in Figure 8(b)) vs. 940 img/sec (observed value– Figure 10(a)). This is a significant benefit of hybrid-parallel training, which is impossible with pure model and/or data parallelism. In addition to this, we also present the largest scale we know of for any model/hybrid-parallel study on the latest Frontera system. Figure 10(b)) shows near-ideal scaling on 512 Frontera nodes. Effectively, every single core out of the 28,762 cores on these 512 nodes is being utilized by HyPar-Flow. The ResNet-1001 model is split into 56 partitions as Frontera nodes have a dual-socket Cascade-Lake Xeon processor","HyPar-Flow: Exploiting MPI and Keras for Scalable Hybrid-Parallel DNN
  Training using TensorFlow","To reduce training time of large-scale DNNs, scientists have started to
explore parallelization strategies like data-parallelism, model-parallelism,
and hybrid-parallelism. While data-parallelism has been extensively studied and
developed, several problems exist in realizing model-parallelism and
hybrid-parallelism efficiently. Four major problems we focus on are: 1)
defining a notion of a distributed model across processes, 2) implementing
forward/back-propagation across process boundaries that requires explicit
communication, 3) obtaining parallel speedup on an inherently sequential task,
and 4) achieving scalability without losing out on a model's accuracy. To
address these problems, we create HyPar-Flow --- a model-size/-type agnostic,
scalable, practical, and user-transparent system for hybrid-parallel training
by exploiting MPI, Keras, and TensorFlow. HyPar-Flow provides a single API that
can be used to perform data, model, and hybrid parallel training of any Keras
model at scale. We create an internal distributed representation of the
user-provided Keras model, utilize TF's Eager execution features for
distributed forward/back-propagation across processes, exploit pipelining to
improve performance and leverage efficient MPI primitives for scalable
communication. Between model partitions, we use send and recv to exchange
layer-data/partial-errors while allreduce is used to accumulate/average
gradients across model replicas. Beyond the design and implementation of
HyPar-Flow, we also provide comprehensive correctness and performance results
on three state-of-the-art HPC systems including TACC Frontera (#5 on
Top500.org). For ResNet-1001, an ultra-deep model, HyPar-Flow provides: 1) Up
to 1.6x speedup over Horovod-based data-parallel training, 2) 110x speedup over
single-node on 128 Stampede2 nodes, and 3) 481x speedup over single-node on 512
Frontera nodes."
scgqa_185,1603.02175v1,"In the context of the research findings, what does Figure 3 indicate about interaction frequency and interest similarity?","The graph shows that interest similarity is more correlated with interaction frequency than interaction intensity. This means that users who interact more frequently are more likely to share similar interests, even if they do not interact with each other very intensely. This is likely because people who interact more frequently have more opportunities to learn about each other's interests and hobbies.",1603.02175v1.pdf,"['1603.02175v1.pdf', '2003.06259v1.pdf', '1908.05243v1.pdf', '1907.10906v1.pdf', '1809.02337v2.pdf', '1805.05887v1.pdf', '1207.3107v3.pdf', '1302.3123v1.pdf']",1603.02175v1-Figure3-1.png,Figure 3: Interest similarity vs. monthly qq message count and number of monthly communicating days.,"interaction frequency, as shown in Fig. 3. Although the fitted curves4 in the RTP case is not the same as that for PTP, they both show that users with higher intensity of interaction share more interests, particularly when the interaction intensity is very large. Also, interest similarity increases more sharply when number of communicating days is small. Interestingly, interaction frequency is more correlated with interest similarity than interaction intensity, implying that some casual or transactional interaction could also have a large intensity.","Who are Like-minded: Mining User Interest Similarity in Online Social
  Networks","In this paper, we mine and learn to predict how similar a pair of users'
interests towards videos are, based on demographic (age, gender and location)
and social (friendship, interaction and group membership) information of these
users. We use the video access patterns of active users as ground truth (a form
of benchmark). We adopt tag-based user profiling to establish this ground
truth, and justify why it is used instead of video-based methods, or many
latent topic models such as LDA and Collaborative Filtering approaches. We then
show the effectiveness of the different demographic and social features, and
their combinations and derivatives, in predicting user interest similarity,
based on different machine-learning methods for combining multiple features. We
propose a hybrid tree-encoded linear model for combining the features, and show
that it out-performs other linear and treebased models. Our methods can be used
to predict user interest similarity when the ground-truth is not available,
e.g. for new users, or inactive users whose interests may have changed from old
access data, and is useful for video recommendation. Our study is based on a
rich dataset from Tencent, a popular service provider of social networks, video
services, and various other services in China."
scgqa_186,1106.3242v2,What insights does Figure 11 provide about the effects of increasing ISP I's market share on surplus?,"The graph shows that as ISP I's market share increases, its per capita surplus and per capita consumer surplus also increase. This is because as ISP I's market share increases, it has more customers and thus more revenue. This increased revenue allows ISP I to provide better service to its customers, which in turn increases their satisfaction and leads to higher per capita surplus.",1106.3242v2.pdf,"['1106.3242v2.pdf', '1607.06988v1.pdf', '2003.06259v1.pdf', '1304.7375v1.pdf']",1106.3242v2-Figure11-1.png,Fig. 11. ISP I ’s market share mI and per capita surplus ΨI and per capita consumer surplus Φ under κ = 1.,Figure 11 and 12 show the results that are parallel to figure 7 and 8. We still find the same observations as in our previous experiments.,The Public Option: a Non-regulatory Alternative to Network Neutrality,"Network neutrality and the role of regulation on the Internet have been
heavily debated in recent times. Amongst the various definitions of network
neutrality, we focus on the one which prohibits paid prioritization of content
and we present an analytical treatment of the topic. We develop a model of the
Internet ecosystem in terms of three primary players: consumers, ISPs and
content providers. Our analysis looks at this issue from the point of view of
the consumer, and we describe the desired state of the system as one which
maximizes consumer surplus. By analyzing different scenarios of monopoly and
competition, we obtain different conclusions on the desirability of regulation.
We also introduce the notion of a Public Option ISP, an ISP that carries
traffic in a network neutral manner. Our major findings are (i) in a
monopolistic scenario, network neutral regulations benefit consumers; however,
the introduction of a Public Option ISP is even better for consumers, as it
aligns the interests of the monopolistic ISP with the consumer surplus and (ii)
in an oligopolistic situation, the presence of a Public Option ISP is again
preferable to network neutral regulations, although the presence of competing
price-discriminating ISPs provides the most desirable situation for the
consumers."
scgqa_187,2008.06134v1,What relationship does Figure 10 depict concerning image resolution and the effectiveness of slice-based ray casting?,"The graph shows that as the number of slices and image resolution increases, the performance of the slice-based ray casting algorithm increases. This is because the algorithm is more accurate and detailed when it has more data to work with. However, the performance also increases at a decreasing rate, meaning that the benefits of increasing the number of slices and image resolution are eventually outweighed by the increased computational cost.",2008.06134v1.pdf,"['2008.06134v1.pdf', '1905.08337v1.pdf', '1803.09990v2.pdf', '1405.6298v2.pdf', '2006.16705v1.pdf']",2008.06134v1-Figure10-1.png,"Figure 10: The flexible performance contrast in three different resoution of 128x128, 156x256 and 512x512 and three the number of slices of 64, 128, 256 in Figure 8.","Moreover, our slice-based ray casting allows for flexible performance, varying the number of slices and per-slice image resolution Figure 10. As the number of slices and image resolution increases, the effects of volume illumination are better, but the performance overhead is increased, as shown in Figure 9. In the test, if the number of slices exceeds 256 and the image resolution exceeds 512x512, the performance will decrease sharply. So, the parameters of slice-based ray casting should be chosen to meet flexible performance according to practical applications and hardware performance.",Interactive volume illumination of slice-based ray casting,"Volume rendering always plays an important role in the field of medical
imaging and industrial design. In recent years, the realistic and interactive
volume rendering of the global illumination can improve the perception of shape
and depth of volumetric datasets. In this paper, a novel and flexible
performance method of slice-based ray casting is proposed to implement the
volume illumination effects, such as volume shadow and other scattering
effects. This benefits from the slice-based illumination attenuation buffers of
the whole geometry slices at the viewpoint of the light source and the
high-efficiency shadow or scattering coefficient calculation per sample in ray
casting. These tests show the method can obtain much better volume illumination
effects and more scalable performance in contrast to the local volume
illumination in ray casting volume rendering or other similar slice-based
global volume illumination."
scgqa_188,2007.15176v2,"In your paper's Figure 6, which two hyper-parameters were analyzed to assess the model's effectiveness?","The two parameters that are being analyzed in the graph are λc, which is the weight on the classification loss, and λCadv, which is the weight on the category-wise alignment loss.",2007.15176v2.pdf,"['2007.15176v2.pdf', '1610.08534v1.pdf', '1910.03072v1.pdf', '1502.00588v1.pdf', '2005.13300v1.pdf', '2008.11326v4.pdf']",2007.15176v2-Figure6-1.png,Fig. 6: Plots presenting the hyper-parameter analysis of the parameters λc on the classification loss using pseudo-weak labels and λCadv on the category-wise alignment loss.,"Fig. 6 presents two plots for the parameter analysis when using pseudo-weak labels. In Fig. 6(a), we fix λCadv = 0.001 and show that our model achieves the mIoU larger than 47.5% under a range of λc = [0.005, 0.1]. When fixing λc = 0.01, Fig. 6(b) shows that the model performs well under a range of λCadv = [0.0005, 0.005]. However, when we increase λ C adv to be larger than 0.01, the adversarial training process may become unstable and decreases the performance to 46.2%. In addition, decreasing λCadv would give less focus on alignment and gradually degrades the performance, which shows the importance of our alignment process.",Domain Adaptive Semantic Segmentation Using Weak Labels,"Learning semantic segmentation models requires a huge amount of pixel-wise
labeling. However, labeled data may only be available abundantly in a domain
different from the desired target domain, which only has minimal or no
annotations. In this work, we propose a novel framework for domain adaptation
in semantic segmentation with image-level weak labels in the target domain. The
weak labels may be obtained based on a model prediction for unsupervised domain
adaptation (UDA), or from a human annotator in a new weakly-supervised domain
adaptation (WDA) paradigm for semantic segmentation. Using weak labels is both
practical and useful, since (i) collecting image-level target annotations is
comparably cheap in WDA and incurs no cost in UDA, and (ii) it opens the
opportunity for category-wise domain alignment. Our framework uses weak labels
to enable the interplay between feature alignment and pseudo-labeling,
improving both in the process of domain adaptation. Specifically, we develop a
weak-label classification module to enforce the network to attend to certain
categories, and then use such training signals to guide the proposed
category-wise alignment method. In experiments, we show considerable
improvements with respect to the existing state-of-the-arts in UDA and present
a new benchmark in the WDA setting. Project page is at
http://www.nec-labs.com/~mas/WeakSegDA."
scgqa_189,1209.3394v5,What insights does Figure 2 provide on the agreement of exact and approximate distributions in your research?,"The graph shows that the exact and approximated distributions are practically indistinguishable. This is because the approximation is in general very good for all values of the CDF of practical interest. In particular, there is an excellent agreement between the exact and approximate distributions for the right tail. The left tail is less accurate but still of small relative error for values of the CDF of practical statistical uses.",1209.3394v5.pdf,"['1209.3394v5.pdf', '1801.08825v1.pdf', '1311.1567v3.pdf', '1612.01450v1.pdf', '2003.06259v1.pdf', '1704.04828v1.pdf']",1209.3394v5-Figure2-1.png,"Figure 2: Comparison between the exact (solid line) and approximated (dashed) CDF, CCDF, Tracy-Widom T W2, log scale. The two CCDFs are practically indistinguishable.","The comparison with pre-calculated p.d.f. values from [31] is shown in Fig. 1. Since in linear scale the exact and approximated distributions are practically indistinguishable, in Fig. 2 we report the CDF and CCDF in logarithmic scale for Tracy-Widom 2 (similar for the others). It can be seen that the approximation is in general very good for all values of the CDF of practical interest. In particular there is an excellent agreement between the exact and approximate distributions for the right tail. The left tail is less accurate but still of small relative error for values of the CDF of practical statistical uses. Note that, differently from the true distribution which goes to zero only asymptotically, the left tail is exactly zero for x < −α.","Distribution of the largest eigenvalue for real Wishart and Gaussian
  random matrices and a simple approximation for the Tracy-Widom distribution","We derive efficient recursive formulas giving the exact distribution of the
largest eigenvalue for finite dimensional real Wishart matrices and for the
Gaussian Orthogonal Ensemble (GOE). In comparing the exact distribution with
the limiting distribution of large random matrices, we also found that the
Tracy-Widom law can be approximated by a properly scaled and shifted Gamma
distribution, with great accuracy for the values of common interest in
statistical applications."
scgqa_190,1906.07610v2,"From Figure 4, what conclusions can we draw about the impact of different negation expressions on model performance?","The graph suggests that exposing the model to a variety of negation examples could be beneficial, even if the number of examples is relatively small. This is because the model may be able to learn from the different ways in which negation is expressed in natural language.",1906.07610v2.pdf,"['1906.07610v2.pdf', '1705.00891v1.pdf', '2006.03632v1.pdf', '1910.08413v1.pdf', '1603.04153v1.pdf', '1910.03072v1.pdf']",1906.07610v2-Figure4-1.png,Fig. 4: Mean accuracy on the SST-binary task when training MTL negation model with differing amounts of negation data from the SFU dataset (left) and sentiment data (right).,"the SFU dataset (from 10 to 800 in intervals of 100) and accuracy is calculated for each number of examples. Figure 4 (left) shows that the MTL model improves over the baseline with as few as ten negation examples and plateaus somewhere near 600. An analysis on the SST-fine setup showed a similar pattern. There is nearly always an effect of diminishing returns when it comes to adding training examples, but if we were to instead plot this learning curve with a log-scale on the x-axis, i.e. doubling the amount data for each increment, it would seem to indicate that having more data could indeed still prove useful, as long as there were sufficient amounts. In any case, regardless of the amount of data, exposing the model to a larger variety of negation examples could also prove beneficial – we follow up on this point in the next subsection.",Improving Sentiment Analysis with Multi-task Learning of Negation,"Sentiment analysis is directly affected by compositional phenomena in
language that act on the prior polarity of the words and phrases found in the
text. Negation is the most prevalent of these phenomena and in order to
correctly predict sentiment, a classifier must be able to identify negation and
disentangle the effect that its scope has on the final polarity of a text. This
paper proposes a multi-task approach to explicitly incorporate information
about negation in sentiment analysis, which we show outperforms learning
negation implicitly in a data-driven manner. We describe our approach, a
cascading neural architecture with selective sharing of LSTM layers, and show
that explicitly training the model with negation as an auxiliary task helps
improve the main task of sentiment analysis. The effect is demonstrated across
several different standard English-language data sets for both tasks and we
analyze several aspects of our system related to its performance, varying types
and amounts of input data and different multi-task setups."
scgqa_191,1810.04824v1,What insights does Fig. 1 provide on the relationship between observed logs and user attrition in this study?,"The graph shows that there is a strong relationship between user statuses and observed activity logs. For example, users who are active in the first snapshot are more likely to be retained in the future, while users who are inactive in the first snapshot are more likely to be attrition.",1810.04824v1.pdf,"['1810.04824v1.pdf', '1806.05387v1.pdf', '1405.5364v2.pdf']",1810.04824v1-Figure1-1.png,"Fig. 1. Schematic overview of different user statuses with varied types of observed activity logs. There are C = T/τ snapshots. × and X denote attrition and retention, respectively. y indicates that the ground-truth label of user activities during snapshot t − 1 is the attrition status within future snapshot t, 2 ≤ t ≤ C.","In this section, we focus on formulating the attrition prediction problem. To facilitate the problem formulation, we give a schematic illustration of user statuses as shown in Fig. 1. Suppose there are a set of N samples or users D = {(Xi, yi)}Ni=1, for which we collect user data Xi from the historical time period [Γ-T+1, Γ] of length T, and we aim to predict their",A Blended Deep Learning Approach for Predicting User Intended Actions,"User intended actions are widely seen in many areas. Forecasting these
actions and taking proactive measures to optimize business outcome is a crucial
step towards sustaining the steady business growth. In this work, we focus on
pre- dicting attrition, which is one of typical user intended actions.
Conventional attrition predictive modeling strategies suffer a few inherent
drawbacks. To overcome these limitations, we propose a novel end-to-end
learning scheme to keep track of the evolution of attrition patterns for the
predictive modeling. It integrates user activity logs, dynamic and static user
profiles based on multi-path learning. It exploits historical user records by
establishing a decaying multi-snapshot technique. And finally it employs the
precedent user intentions via guiding them to the subsequent learning
procedure. As a result, it addresses all disadvantages of conventional methods.
We evaluate our methodology on two public data repositories and one private
user usage dataset provided by Adobe Creative Cloud. The extensive experiments
demonstrate that it can offer the appealing performance in comparison with
several existing approaches as rated by different popular metrics. Furthermore,
we introduce an advanced interpretation and visualization strategy to
effectively characterize the periodicity of user activity logs. It can help to
pinpoint important factors that are critical to user attrition and retention
and thus suggests actionable improvement targets for business practice. Our
work will provide useful insights into the prediction and elucidation of other
user intended actions as well."
scgqa_192,1509.08992v2,"According to Figure 2, how does the difference in log-likelihood evolve with further iterations of the algorithm?","The graph shows that the difference between the current test log-likelihood and the optimal log-likelihood decreases as the number of iterations increases. This is to be expected, as the algorithm is designed to converge to the optimal solution.",1509.08992v2.pdf,"['1509.08992v2.pdf', '1911.05146v2.pdf', '1910.10700v1.pdf', '1907.11771v1.pdf', '1210.1356v2.pdf', '1703.07626v1.pdf', '1707.02342v1.pdf']",1509.08992v2-Figure2-1.png,"Figure 2: Ising Model Example. Left: The difference of the current test log-likelihood from the optimal log-likelihood on 5 random runs. Center: The distance of the current estimated parameters from the optimal parameters on 5 random runs. Right: The current estimated parameters on one run, as compared to the optimal parameters (far right).","24× (2× .2)2, C = log(16) and α = exp(−(1 − 4 tanh .2)/16). Applying Corollary 9 with β1 = .01, β2 = .9 and β3 = .1 gives K = 46, M = 1533 and v = 561. Fig. 2 shows the results. In practice, the algorithm finds a solution tighter than the specified ǫθ, indicating a degree of conservatism in the theoretical bound.","Maximum Likelihood Learning With Arbitrary Treewidth via Fast-Mixing
  Parameter Sets","Inference is typically intractable in high-treewidth undirected graphical
models, making maximum likelihood learning a challenge. One way to overcome
this is to restrict parameters to a tractable set, most typically the set of
tree-structured parameters. This paper explores an alternative notion of a
tractable set, namely a set of ""fast-mixing parameters"" where Markov chain
Monte Carlo (MCMC) inference can be guaranteed to quickly converge to the
stationary distribution. While it is common in practice to approximate the
likelihood gradient using samples obtained from MCMC, such procedures lack
theoretical guarantees. This paper proves that for any exponential family with
bounded sufficient statistics, (not just graphical models) when parameters are
constrained to a fast-mixing set, gradient descent with gradients approximated
by sampling will approximate the maximum likelihood solution inside the set
with high-probability. When unregularized, to find a solution epsilon-accurate
in log-likelihood requires a total amount of effort cubic in 1/epsilon,
disregarding logarithmic factors. When ridge-regularized, strong convexity
allows a solution epsilon-accurate in parameter distance with effort quadratic
in 1/epsilon. Both of these provide of a fully-polynomial time randomized
approximation scheme."
scgqa_193,1611.04706v2,"According to Figure 17, how does the number of discretization nodes relate to convergence rates in the quadcopter experiment?","The graph shows that the convergence rate of the one-way multigrid strategy improves as the number of discretization nodes increases. This is because the discretization nodes provide a more accurate representation of the state space, which allows the algorithm to converge more quickly.",1611.04706v2.pdf,"['1611.04706v2.pdf', '1209.3394v5.pdf', '1912.03417v1.pdf', '2006.16705v1.pdf', '1701.05681v3.pdf']",1611.04706v2-Figure17-1.png,Fig. 17: One-way multigrid for solving the quadcopter control problem. Maximum rank FT rank is restricted to 10.,"Figure 17 shows the convergence diagnostics. We used a one-way multigrid strategy with n = 20, n = 60, and n = 120 discretization nodes. The difference between iterates, shown in the upper right panel, is between 1 and 0.1. In fact, this means that the relative difference is approximately 10−4, which matches well with the algorithm tolerances = (δcross, round).","High-Dimensional Stochastic Optimal Control using Continuous Tensor
  Decompositions","Motion planning and control problems are embedded and essential in almost all
robotics applications. These problems are often formulated as stochastic
optimal control problems and solved using dynamic programming algorithms.
Unfortunately, most existing algorithms that guarantee convergence to optimal
solutions suffer from the curse of dimensionality: the run time of the
algorithm grows exponentially with the dimension of the state space of the
system. We propose novel dynamic programming algorithms that alleviate the
curse of dimensionality in problems that exhibit certain low-rank structure.
The proposed algorithms are based on continuous tensor decompositions recently
developed by the authors. Essentially, the algorithms represent
high-dimensional functions (e.g., the value function) in a compressed format,
and directly perform dynamic programming computations (e.g., value iteration,
policy iteration) in this format. Under certain technical assumptions, the new
algorithms guarantee convergence towards optimal solutions with arbitrary
precision. Furthermore, the run times of the new algorithms scale polynomially
with the state dimension and polynomially with the ranks of the value function.
This approach realizes substantial computational savings in ""compressible""
problem instances, where value functions admit low-rank approximations. We
demonstrate the new algorithms in a wide range of problems, including a
simulated six-dimensional agile quadcopter maneuvering example and a
seven-dimensional aircraft perching example. In some of these examples, we
estimate computational savings of up to ten orders of magnitude over standard
value iteration algorithms. We further demonstrate the algorithms running in
real time on board a quadcopter during a flight experiment under motion
capture."
scgqa_194,1804.00243v2,How should future object trackers be designed according to the precision plots in this research paper?,"The precision plots in the OTB dataset provide valuable insights into the design of future object trackers. They show that it is important to consider a variety of attributes when designing a tracker, and that it is important to have a tracker that can handle severe variations. The results also suggest that the manifold structure of the learned features is an important factor in the performance of object trackers.",1804.00243v2.pdf,"['1804.00243v2.pdf', '1801.06867v1.pdf', '1412.4318v1.pdf', '2007.15176v2.pdf', '1608.00887v1.pdf', '1707.04849v1.pdf']",1804.00243v2-Figure7-1.png,Fig. 7: Precision plots for all attributes of the OTB dataset.,"Here, we also show the scale variation and lighting attributes in Fig. 6, where STM performs much better than other trackers again. The reason for this phenomenon is that the manifold structure of the learned features is data-dependent. This property leads the learned features can handle nonlinear of variations in object tracking. Again, STM shows its super capability of handling severe variations. The experimental results for full set of plots generated by the benchmark toolbox are reported in Fig. 7 and Fig. 8 in Appendix C.",The Structure Transfer Machine Theory and Applications,"Representation learning is a fundamental but challenging problem, especially
when the distribution of data is unknown. We propose a new representation
learning method, termed Structure Transfer Machine (STM), which enables feature
learning process to converge at the representation expectation in a
probabilistic way. We theoretically show that such an expected value of the
representation (mean) is achievable if the manifold structure can be
transferred from the data space to the feature space. The resulting structure
regularization term, named manifold loss, is incorporated into the loss
function of the typical deep learning pipeline. The STM architecture is
constructed to enforce the learned deep representation to satisfy the intrinsic
manifold structure from the data, which results in robust features that suit
various application scenarios, such as digit recognition, image classification
and object tracking. Compared to state-of-the-art CNN architectures, we achieve
the better results on several commonly used benchmarks\footnote{The source code
is available. https://github.com/stmstmstm/stm }."
scgqa_195,1804.06674v1,"In the context of your decentralized e-voting system, what does the ballot processing time indicate about ring signatures?","The graph suggests that ring signatures are feasible for use in e-voting systems. This is because the generation and verification times are acceptable for voters to keep their anonymity. Additionally, the computation of a ring signature is linear to the ring size, which means that the time required to generate and verify a ballot does not increase significantly as the number of voters increases.",1804.06674v1.pdf,"['1804.06674v1.pdf', '1405.5329v4.pdf', '1311.6183v1.pdf', '2003.14319v2.pdf', '1604.06979v1.pdf', '1502.03556v1.pdf', '2004.03870v1.pdf', '2009.08716v1.pdf', '1807.06736v1.pdf']",1804.06674v1-Figure1-1.png,Fig. 1. The generation and verification time of a ballot.,"Our measurements are performed on a MacBook Pro running OS X 10.12.5 equipped with 2 core, 2.7 GHz Intel Core i5 and 8 GB DDR3 RAM. For communication with the blockchain conveniently, we written our code in Javascript. As figure 1. shows, the time spent by voter is mainly to sign the ballot with ring signature, which is linear to ring size. And the verification time is almost the same as the time of genration, since the bottleneck of both is computation of ring signature, but it is still acceptable for voters to keep their anonymity.",An efficient and effective Decentralized Anonymous Voting System,"A trusted electronic election system requires that all the involved
information must go public, that is, it focuses not only on transparency but
also privacy issues. In other words, each ballot should be counted anonymously,
correctly, and efficiently. In this work, a lightweight E-voting system is
proposed for voters to minimize their trust in the authority or government. We
ensure the transparency of election by putting all message on the Ethereum
blockchain, in the meantime, the privacy of individual voter is protected via
an efficient and effective ring signature mechanism. Besides, the attractive
self-tallying feature is also built in our system, which guarantees that
everyone who can access the blockchain network is able to tally the result on
his own, no third party is required after voting phase. More importantly, we
ensure the correctness of voting results and keep the Ethereum gas cost of
individual participant as low as possible, at the same time. Clearly, the
pre-described characteristics make our system more suitable for large-scale
election."
scgqa_196,1607.00675v1,What key aspect differentiates the energy landscapes of noisy 1-bit CS from binary variable 1-bit CS in this research?,"The key difference between the phase diagrams of noisy 1-bit CS and 1-bit CS of binary variables is the presence of the spinodal phase transition in the former. This phase transition occurs when the noise level is high enough to cause the energy landscapes to become degenerate. In this case, the inference problem becomes impossible, as there is no unique solution to the optimization problem.",1607.00675v1.pdf,"['1607.00675v1.pdf', '1606.06377v1.pdf', '1505.05173v6.pdf', '2008.11326v4.pdf', '1910.05107v2.pdf', '2002.01322v1.pdf', '1106.3826v2.pdf', '1703.01827v3.pdf', '1803.06598v1.pdf']",1607.00675v1-Figure3.8-1.png,"Figure 3.8: Phase diagrams obtained (by state evolution) for 1-bit CS of quantized signals. The signals follow the distribution pX(x) = (1 − ρ)δ(x − x−) + ρδ(x − x+). As in CS, the energy landscapes define 3 phases, in which inference is easy, hard or impossible.","• The phase diagrams of noisy 1-bit CS and 1-bit CS of binary variables (Fig. 3.7b, Fig. 3.8).",Statistical physics of linear and bilinear inference problems,"The recent development of compressed sensing has led to spectacular advances
in the understanding of sparse linear estimation problems as well as in
algorithms to solve them. It has also triggered a new wave of developments in
the related fields of generalized linear and bilinear inference problems, that
have very diverse applications in signal processing and are furthermore a
building block of deep neural networks. These problems have in common that they
combine a linear mixing step and a nonlinear, probabilistic sensing step,
producing indirect measurements of a signal of interest. Such a setting arises
in problems as different as medical or astronomical imaging, clustering, matrix
completion or blind source separation. The aim of this thesis is to propose
efficient algorithms for this class of problems and to perform their
theoretical analysis. To this end, it uses belief propagation, thanks to which
high-dimensional distributions can be sampled efficiently, thus making a
Bayesian approach to inference tractable. The resulting algorithms undergo
phase transitions just as physical systems do. These phase transitions can be
analyzed using the replica method, initially developed in statistical physics
of disordered systems. The analysis reveals phases in which inference is easy,
hard or impossible. These phases correspond to different energy landscapes of
the problem. The main contributions of this thesis can be divided into three
categories. First, the application of known algorithms to concrete problems:
community detection, superposition codes and an innovative imaging system.
Second, a new, efficient message-passing algorithm for a class of problems
called blind sensor calibration. Third, a theoretical analysis of matrix
compressed sensing and of instabilities in Bayesian bilinear inference
algorithms."
scgqa_197,1707.02342v1,How does the performance of the neural network in Figure 10 differ when moving clockwise and counter-clockwise?,"The graph shows that the neural network model is able to accurately predict the vehicle's trajectory in the counter-clockwise direction, but it incorrectly predicts oversteer in the clockwise direction. This is likely due to the fact that the model was trained on data from a vehicle that was traveling in the counter-clockwise direction, and it does not have enough information to accurately predict the vehicle's trajectory in the clockwise direction.",1707.02342v1.pdf,"['1707.02342v1.pdf', '1205.4213v2.pdf', '2006.09358v2.pdf', '1808.06818v1.pdf']",1707.02342v1-Figure10-1.png,"Fig. 10. Neural network modeling error at 11 m/s target. Going counterclockwise the model prediction is accurate, but clockwise the model predicts severe over-steer when it should have predicted under-steer. The predicted trajectory is generated by taking the applied input sequence from the data recording and running it through the neural net model starting from the same initial condition.","In order to navigate the vehicle around the track, the controller has to be robust to modeling error (Table I). Figure 10 shows how the predicted model differs from reality around a typical turn at the 11 m/s target with the neural network model. Going counter-clockwise the model is able to accurately predict out to the 2 second time horizon. However, in the clockwise direction the model incorrectly predicts oversteer when in fact the vehicle under-steers.","Information Theoretic Model Predictive Control: Theory and Applications
  to Autonomous Driving","We present an information theoretic approach to stochastic optimal control
problems that can be used to derive general sampling based optimization
schemes. This new mathematical method is used to develop a sampling based model
predictive control algorithm. We apply this information theoretic model
predictive control (IT-MPC) scheme to the task of aggressive autonomous driving
around a dirt test track, and compare its performance to a model predictive
control version of the cross-entropy method."
scgqa_198,1407.7736v1,What limitations does the paper mention about the lift chart's effectiveness for the churn-prediction model in Wikipedia?,"The limitations of the lift chart are that it is only a graphical representation of the performance of the model. It does not provide any information about the accuracy of the model. Additionally, the lift chart is only valid for the specific dataset that was used to create it.",1407.7736v1.pdf,"['1407.7736v1.pdf', '1808.06304v2.pdf', '1808.07801v3.pdf']",1407.7736v1-Figure5-1.png,Fig. 5: Lift chart obtained by the proposed churn-prediction model. Different curves represent the lift curves for different sliding windows.,"Cumulative gains for churn prediction. The lift factors are widely used by researchers to evaluate the performance of churn-prediction models (e.g. [12]). The lift factors achieved by our model are shown in Figure 5. In lift chart, the diagonal line represents a baseline which randomly selects a subset of editors as potential churners, i.e., it selects s% of the editors that will contain s% of the true churners, resulting in a lift factor of 1. In Figure 5, on average, our model was capable of identifying 10% of editors that contained 21.2% of true churners (i.e. a lift factor of 2.12), 20% of editors that contained 39.3% of true churners (i.e. a lift factor of 1.97), and 30% of editors that contained 54.7% of true churners (i.e. a lift factor of 1.82). Evidently, our model achieved higher lift factors than the baseline. Thus if the objective of the lift analysis is to identify a",A Latent Space Analysis of Editor Lifecycles in Wikipedia,"Collaborations such as Wikipedia are a key part of the value of the modern
Internet. At the same time there is concern that these collaborations are
threatened by high levels of member turnover. In this paper we borrow ideas
from topic analysis to editor activity on Wikipedia over time into a latent
space that offers an insight into the evolving patterns of editor behavior.
This latent space representation reveals a number of different categories of
editor (e.g. content experts, social networkers) and we show that it does
provide a signal that predicts an editor's departure from the community. We
also show that long term editors gradually diversify their participation by
shifting edit preference from one or two namespaces to multiple namespaces and
experience relatively soft evolution in their editor profiles, while short term
editors generally distribute their contribution randomly among the namespaces
and experience considerably fluctuated evolution in their editor profiles."
scgqa_199,1910.09592v1,What conclusion can be drawn from Figure 2 regarding the efficiency of the variable exponent circuit for larger instances?,"The graph's findings suggest that the variable exponent circuit is not practical for use in applications where the number of variables is large. This is because the solving times for the circuit scale exponentially with the number of variables, and this exponential growth can quickly make the circuit impractical to use.",1910.09592v1.pdf,"['1910.09592v1.pdf', '1106.3242v2.pdf', '2010.11594v1.pdf', '1512.02567v1.pdf']",1910.09592v1-Figure2-1.png,Figure 2: Scaling of solving times for the variable exponent circuit,"Figure 2 shows the benchmarking results. For each N ≤ 218 we measured the median time of solving the same instance many times, for larger N we report the solver runtime directly. Each measured runtime is multiplied by y(N).",On speeding up factoring with quantum SAT solvers,"There have been several efforts to apply quantum SAT solving methods to
factor large integers. While these methods may provide insight into quantum SAT
solving, to date they have not led to a convincing path to integer
factorization that is competitive with the best known classical method, the
Number Field Sieve. Many of the techniques tried involved directly encoding
multiplication to SAT or an equivalent NP-hard problem and looking for
satisfying assignments of the variables representing the prime factors. The
main challenge in these cases is that, to compete with the Number Field Sieve,
the quantum SAT solver would need to be superpolynomially faster than classical
SAT solvers. In this paper the use of SAT solvers is restricted to a smaller
task related to factoring: finding smooth numbers, which is an essential step
of the Number Field Sieve. We present a SAT circuit that can be given to
quantum SAT solvers such as annealers in order to perform this step of
factoring. If quantum SAT solvers achieve any speedup over classical
brute-force search, then our factoring algorithm is faster than the classical
NFS."
scgqa_200,1910.00110v2,What relationship is depicted in Figure 1 regarding error and noise level for Gaussian noise in the experiment?,"The graph shows that the mean of error grows linearly with the standard deviation of noise as long as condition (11) of Theorem 1 is satisfied. This is in agreement with Theorem 1, which states that the mean of error should grow linearly with the standard deviation of noise as long as condition (11) is satisfied.",1910.00110v2.pdf,"['1910.00110v2.pdf', '1902.05312v2.pdf', '1701.00365v2.pdf']",1910.00110v2-Figure1-1.png,"Figure 1: CD player: Plots (a) and (c) show the growth of the mean of error (30) over 10 replicates of independent noise samples. Plots (b) and (d) show the number of test points for which condition (11) of Theorem 1 is violated. The mean of the error (30) grows linearly with σ as long as condition (11) is satisfied, which is in agreement with Theorem 1. The error bars in (a) and (c) show the minimum and maximum of the error (30) over the 10 replicates of the noise samples.","which is an average of the error (21) over all 200 test points. Figure 1a shows the mean of e(σ) over 10 replicates of independent noise samples. The standard deviation σ is in the range [10−15, 105] and the dimension is n = 20. The error bars in Figure 1a show the minimum and maximum of e(σ) over the 10 replicates. A linear growth of the mean of error (30) with the standard deviation σ is observed for σ < 10−5. Figure 1b shows the number of test points that violate condition (11) of Theorem 1. The results indicate that for σ ≥ 10−7 condition (11) is violated for all 200 test points, which seems to align with Figure 1a that shows a linear growth for σ < 10−5. Thus, the results in Figure 1a are in agreement with Theorem 1. Similar observations can be made for n = 28 in Figure 1c and Figure 1d.","Learning low-dimensional dynamical-system models from noisy
  frequency-response data with Loewner rational interpolation","Loewner rational interpolation provides a versatile tool to learn
low-dimensional dynamical-system models from frequency-response measurements.
This work investigates the robustness of the Loewner approach to noise. The key
finding is that if the measurements are polluted with Gaussian noise, then the
error due to noise grows at most linearly with the standard deviation with high
probability under certain conditions. The analysis gives insights into making
the Loewner approach robust against noise via linear transformations and
judicious selections of measurements. Numerical results demonstrate the linear
growth of the error on benchmark examples."
scgqa_201,1307.1204v1,"According to the dynamics shown in Fig. 22, how are queue length and marking probability related in heavy traffic?","The graph shows that the queue length and marking probability are inversely related. As the queue length increases, the marking probability decreases. This is because when the queue length is high, there is more traffic waiting to be transmitted, which means that the probability of a packet being marked is lower.",1307.1204v1.pdf,"['1307.1204v1.pdf', '1908.05243v1.pdf', '1712.03538v1.pdf']",1307.1204v1-Figure22-1.png,Fig. 22: Queue length and marking probability versus time for PI AQM with link capacity C is 15 Mb/s.,"We now consider two cases where the link capacities are 15Mb/s and 95 Mb/s. By (2) and (28), the p0 of Simplified MGT and of Scenario B model (ρ = 1) for the two cases are given in Table X. From the NS2 simulations, the marking probabilities for the cases of C = 15 Mb/s and C = 95 Mb/s are 0.3426 and 0.0973, respectively. Then the corresponding ρ values are 2.0297 and 1.6286, respectively. The results are shown in Fig. 22 and Fig. 23. In the two figures, the Scenario B curves with the ρ values of 2.0297 and 1.6286 match the NS2 simulation resuls very well, and the curves generated by Scenario B (ρ = 1) are closer to the NS2 simulation results than the curves of the Simplified MGT model.",A New TCP/AQM System Analysis,"The MGT fluid model has been used extensively to guide designs of AQM schemes
aiming to alleviate adverse effects of Internet congestion. In this paper, we
provide a new analysis of a TCP/AQM system that aims to improve the accuracy of
the MGT fluid model especially in heavy traffic conditions. The analysis is
based on the consideration of two extreme congestion scenarios that leads to
the derivation of upper and lower bounds for the queue length and marking
probability dynamics and showing that they approach each other in steady state.
Both discrete and continuous time models are provided. Simulation results
demonstrate that the new model achieves a significantly higher level of
accuracy than a simplified version of the MGT fluid model."
scgqa_202,1212.3950v3,What does Figure 9 indicate about the effectiveness of the localization procedure in different environmental models outlined in the paper?,"The graph shows that the localization procedure performs better in the free space model than in the shadowing model. This is because the free space model is a more accurate representation of the real world environment. In the shadowing model, the presence of obstacles introduces errors into the localization procedure.",1212.3950v3.pdf,"['1212.3950v3.pdf', '1409.3924v1.pdf', '1403.2732v1.pdf', '1910.11127v1.pdf', '1501.07107v1.pdf', '2001.09043v3.pdf']",1212.3950v3-Figure9-1.png,Fig. 9. Associated error after protocol selection,3) Error: This measure illustrates the average distance in meters between each node’s estimated location and its real position. The prefix Loc. Proc. in Figure 9 highlights the fact that these are results gathered from the execution of the localization procedure.,Localization Procedure for Randomly Deployed Wireless Sensor Networks,"Wireless Sensor Networks (WSNs) are composed of nodes that gather metrics
like temperature, pollution or pressure from events generated by external
entities. Localization in WSNs is paramount, given that the collected metrics
must be related to the place of occurrence. This document presents an
alternative way towards localization in randomly deployed WSNs based on the
composability of localization protocols. Results show a totally distributed
localization procedure that achieves a higher number of located nodes than the
conventional, individual execution of localization protocols while maintaining
the same low levels of battery consumption."
scgqa_203,1701.08947v1,"In the context of sparse phase retrieval, what conclusions can be drawn from Figure 1 about the Prony method's performance?",The figure shows that the approximate Prony method is able to recover sparse signals very accurately. This is because the method is able to exploit the sparse structure of the signal to reduce the number of coefficients that need to be estimated. This results in a more efficient and accurate recovery algorithm.,1701.08947v1.pdf,"['1701.08947v1.pdf', '1809.01093v3.pdf', '1810.04915v1.pdf', '2009.06124v1.pdf', '1906.07610v2.pdf', '1905.11471v1.pdf', '1703.07020v4.pdf']",1701.08947v1-Figure1-1.png,Figure 1: Results of Algorithm 5.2 for the spike function in Example 5.3,"−10. The results of the phase retrieval algorithm and the absolute errors of the knots and coefficients of the recovered spike function are shown in Figure 1. Although the approximate Prony method has to recover 211 knot differences, the knots and coefficients of f are reconstructed very accurately. ©",Sparse phase retrieval of one-dimensional signals by Prony's method,"In this paper, we show that sparse signals f representable as a linear
combination of a finite number N of spikes at arbitrary real locations or as a
finite linear combination of B-splines of order m with arbitrary real knots can
be almost surely recovered from O(N^2) Fourier intensity measurements up to
trivial ambiguities. The constructive proof consists of two steps, where in the
first step the Prony method is applied to recover all parameters of the
autocorrelation function and in the second step the parameters of f are
derived. Moreover, we present an algorithm to evaluate f from its Fourier
intensities and illustrate it at different numerical examples."
scgqa_204,1910.05107v2,"Referring to Figure 6 in the paper, how does the EMS handle surplus energy during peak solar generation?","The graph shows that the EMS is able to store surplus energy during periods of peak PV generation. This energy is then released later in the day when the PV generation declines. This helps to ensure that the batteries are always charged to a sufficient level, and that the system is able to meet the demand for electricity at all times.",1910.05107v2.pdf,"['1910.05107v2.pdf', '1904.01542v3.pdf', '1506.06213v1.pdf', '1902.06156v1.pdf', '1708.01249v1.pdf']",1910.05107v2-Figure6-1.png,"Figure 6: States of charge of DGUs B3, B4, and B5.","by the EMS. Working to the detriment of battery’s longevity, abrupt charging and discharging, and frequent switching between these two modes are prevented by the EMS. As for the SOCs— reported in Figure 6, they evolve while respecting the operational constraints. Moreover, the EMS tries to store surplus energy during periods of peak PV generation (see Figure 8). This energy is released later in the day when the PV generation declines.",Hierarchical Control in Islanded DC Microgrids with Flexible Structures,"Hierarchical architectures stacking primary, secondary, and tertiary layers
are widely employed for the operation and control of islanded DC microgrids
(DCmGs), composed of Distribution Generation Units (DGUs), loads, and power
lines. However, a comprehensive analysis of all the layers put together is
often missing. In this work, we remedy this limitation by setting out a
top-to-bottom hierarchical control architecture. Decentralized voltage
controllers attached to DGUs form our primary layer. Governed by an MPC--based
Energy Management System (EMS), our tertiary layer generates optimal power
references and decision variables for DGUs. In particular, decision variables
can turn DGUs ON/OFF and select their operation modes. An intermediary
secondary layer translates EMS power references into appropriate voltage
signals required by the primary layer. More specifically, to provide a voltage
solution, the secondary layer solves an optimization problem embedding
power-flow equations shown to be always solvable. Since load voltages are not
directly enforced, their uniqueness is necessary for DGUs to produce reference
powers handed down by the EMS. To this aim, we deduce a novel uniqueness
condition based only on local load parameters. Our control framework, besides
being applicable for generic DCmG topologies, can accommodate topological
changes caused by EMS commands. Its functioning is validated via simulations on
a modified 16-bus DC system."
scgqa_205,1803.09990v2,"How did the number of domains utilizing Cedexis change before and after the May 2017 incident, according to the paper?","The graph shows that the number of domains utilizing Cedexis has been increasing over time, with a slight dip in May 2017. This dip is likely due to a DDoS attack on Cedexis' infrastructure that caused some customers to remove CNAME pointers to Cedexis in favor of pointing to operational CDNs instead.",1803.09990v2.pdf,"['1803.09990v2.pdf', '2002.12489v3.pdf', '1703.10422v2.pdf', '1511.07907v2.pdf', '2006.11769v1.pdf', '1304.2109v1.pdf']",1803.09990v2-Figure2-1.png,"Fig. 2: Domains utilizing Cedexis in the Alexa 1M over time. The drop in May ’17 was caused by a DDoS Attack on Cedexis [2]. Unfortunately, the measurement probe located at our chair experienced two outages. However, the overlap of both scans motivates the further use of the OpenIntel data set.","We show the popularity of Cedexis over time among Alexa-listed domains in Figure 2. We set up our own regular DNS resolutions in December 2016 and further show regular Alexa resolutions performed by OpenINTEL [22] in the Netherlands for the same resource records. First, we observe that both data sets overlap, suggesting that both are suitable for monitoring. Minor fluctuations in the number of domains per day can mainly be attributed to fluctuations in the Alexa listed domains. Second, we observe an outage of Cedexis in May 2017 which was caused by a DDoS attack on their infrastructure [2]. The outage motivated some customers to remove CNAME pointers to Cedexis in favor of pointing to operational CDNs instead, causing a drop of > 120 domains in Figure 2. Customer Classification. We next classify the discovered customers to highlight the variety of Cedexis customer’ profiles. To base this on an open classification scheme, we first tried to match customer domains against the Alexa Web Information Service API. However, Alexa classifications exist only for 17% of the queried domains and some classifications do not reflect the web pages’ content. To obtain a broader picture, we instructed a single human classifier to visit each web site and categorize it according to an evolving set of categories. We show the resulting categorized web site content in Table 1(a). The table shows that Cedexis is used by a broad range of customers. We further classify the used service in Table 1(b). The table shows that most customers use Cedexis for general web content delivery. This includes few but large bulk download services, e.g., www.download.windowsupdate.com. This is in contrast to, e.g., Conviva which is dedicated to video delivery. Takeaway. Cedexis is utilized by a number of (large) web services. Decisions taken by Cedexis have the potential to impact larger bulks of Internet traffic.",Characterizing a Meta-CDN,"CDNs have reshaped the Internet architecture at large. They operate
(globally) distributed networks of servers to reduce latencies as well as to
increase availability for content and to handle large traffic bursts.
Traditionally, content providers were mostly limited to a single CDN operator.
However, in recent years, more and more content providers employ multiple CDNs
to serve the same content and provide the same services. Thus, switching
between CDNs, which can be beneficial to reduce costs or to select CDNs by
optimal performance in different geographic regions or to overcome CDN-specific
outages, becomes an important task. Services that tackle this task emerged,
also known as CDN broker, Multi-CDN selectors, or Meta-CDNs. Despite their
existence, little is known about Meta-CDN operation in the wild. In this paper,
we thus shed light on this topic by dissecting a major Meta-CDN. Our analysis
provides insights into its infrastructure, its operation in practice, and its
usage by Internet sites. We leverage PlanetLab and Ripe Atlas as distributed
infrastructures to study how a Meta-CDN impacts the web latency."
scgqa_206,1905.08337v1,What relationship between tweet ingestion rates and DBMS performance is illustrated in the findings from this paper?,"The graph shows that the DBMS server's performance deteriorates as the number of tweets ingested increases. This is evident from the fact that the time it takes to process each tweet increases, and the number of tweets that can be processed per second decreases. This deterioration in performance is likely due to the fact that the DBMS server is not able to keep up with the increasing load.",1905.08337v1.pdf,"['1905.08337v1.pdf', '1608.00887v1.pdf', '2005.09634v1.pdf', '2007.11391v1.pdf', '1910.09823v3.pdf', '1911.11395v2.pdf']",1905.08337v1-Figure3-1.png,Fig. 3: Performance Measurement During Direct Tweet Stream ingestion,"availability. The deterioration of system efficiency is further evidenced from the speed of context switching (Figure 3). With no intervention, this results in a significant slowdown or a total failure of the DBMS server. This system failure points",Ingesting High-Velocity Streaming Graphs from Social Media Sources,"Many data science applications like social network analysis use graphs as
their primary form of data. However, acquiring graph-structured data from
social media presents some interesting challenges. The first challenge is the
high data velocity and bursty nature of the social media data. The second
challenge is that the complex nature of the data makes the ingestion process
expensive. If we want to store the streaming graph data in a graph database, we
face a third challenge -- the database is very often unable to sustain the
ingestion of high-velocity, high-burst data. We have developed an adaptive
buffering mechanism and a graph compression technique that effectively
mitigates the problem. A novel aspect of our method is that the adaptive
buffering algorithm uses the data rate, the data content as well as the CPU
resources of the database machine to determine an optimal data ingestion
mechanism. We further show that an ingestion-time graph-compression strategy
improves the efficiency of the data ingestion into the database. We have
verified the efficacy of our ingestion optimization strategy through extensive
experiments."
scgqa_207,1512.00843v3,"In the context of DeepCNF's performance, what does Figure 4A reveal about layer count and Q8 accuracy?",The graph in Figure 4A suggests that the Q8 accuracy of the DeepCNF model increases as the number of layers increases. This is because the model becomes more complex and is able to learn more complex patterns in the data.,1512.00843v3.pdf,"['1512.00843v3.pdf', '1907.11314v1.pdf', '1509.02054v1.pdf']",1512.00843v3-Figure4-1.png,"Figure 4. The Q8 accuracy on CB513 by the models of different number of layers of 1, 3, 5, and 7 (the same window size is used). (A) Each layer of the 4 models has 100 neurons for a position. The total parameter number of the 4 models is different. (B) Each layer of the models has different neurons for a position. The total parameter number of the 4 models is similar.","segment results in a lower SOV score than a wrong prediction at the terminal regions. A detailed definition of SOV is described in 32 , and also in our Supplemental File. Determining the regularization factor by cross validation. Our DeepCNF has only a hyper-parameter, i.e., the regularization factor, which is used to avoid overfitting. Once it is fixed, we can estimate all the model parameters by solving the optimization problem in Eq. (10). To choose the right regularization factor and examine the stability of our DeepCNF model, we conduct a five-fold cross-validation test. In particular, we randomly divide the training set (containing 5600 CullPDB proteins) into 5 subsets and then use 4 subsets as training and one as validation. Figure 3 shows the Q8 accuracy of our DeepCNF model with respect to the regularization factor. The optimal regularization factor is around 50, which yields 73.2% Q8 accuracy on average. At this point, the Q8 accuracy difference of all the models is less than 1%, consistent with the previous report 33 . Determining the DeepCNF architecture. The architecture of our DeepCNF model is mainly determined by the following 3 factors (see Figure 2): (i) the number of hidden layers; (ii) the number of different neurons at each layer; and (iii) the window size at each layer. We fix the window size to 11 because the average length of an alpha helix is around eleven residues 58 and that of a beta strand is around six 59 . To show the relationship between the performance and the number of hidden layers, we trained four different DeepCNF models with 1, 3, 5, and 7 layers, respectively. All the models use the same window size (i.e., 11) and the same number (i.e., 100) of different neurons at each layer. In total these four models have ~50K, ~270K, ~500K, and ~700K parameters, respectively. We trained the models with different regularization factors. As shown in Figure 4A, when only one hidden layer is used, DeepCNF becomes CNF 50 and its performance is quite similar to RaptorX-SS8 (single model) as shown in Table 1. When more layers are applied, the Q8 accuracy gradually improves. To balance the model complexity and performance, by default we set window size to 11 and use 5 hidden layers, each with 100 different neurons.","Protein secondary structure prediction using deep convolutional neural
  fields","Protein secondary structure (SS) prediction is important for studying protein
structure and function. When only the sequence (profile) information is used as
input feature, currently the best predictors can obtain ~80% Q3 accuracy, which
has not been improved in the past decade. Here we present DeepCNF (Deep
Convolutional Neural Fields) for protein SS prediction. DeepCNF is a Deep
Learning extension of Conditional Neural Fields (CNF), which is an integration
of Conditional Random Fields (CRF) and shallow neural networks. DeepCNF can
model not only complex sequence-structure relationship by a deep hierarchical
architecture, but also interdependency between adjacent SS labels, so it is
much more powerful than CNF. Experimental results show that DeepCNF can obtain
~84% Q3 accuracy, ~85% SOV score, and ~72% Q8 accuracy, respectively, on the
CASP and CAMEO test proteins, greatly outperforming currently popular
predictors. As a general framework, DeepCNF can be used to predict other
protein structure properties such as contact number, disorder regions, and
solvent accessibility."
scgqa_208,1907.04002v1,What does Figure 2 illustrate about monthly Bitcoin donations' fluctuations related to the exchange rate during the study?,"The graph shows that the total amount of donations in dollars has increased in a relatively small increments until 2017, during which it has increased by orders of magnitude before plummeting down in 2018 onward. This change in value resembles the change in bitcoin price in dollars, but the resemblance is unclear if we look at the total amount.",1907.04002v1.pdf,"['1907.04002v1.pdf', '1907.10906v1.pdf', '1805.00184v1.pdf', '1205.4213v2.pdf']",1907.04002v1-Figure2-1.png,Figure 2: Total amount of donations vs. exchange rate.,"4.2.2 Historical perspective. While it is reasonable to expect that the total amount of donations might increase as Bitcoin’s popularity increases, we found that the exchange rate, which is often described as speculative, seem to have an impact as well. As shown in Figure 2, the total amount of monthly donations in dollars has increased in a relatively small increments until 2017, during which it has increased by orders of magnitude before plummeting down in 2018 onward. Although this change in value resembles the change in bitcoin price in dollars, the resemblance is unclear if we look at the total amount",Characterizing Bitcoin donations to open source software on GitHub,"Web-based hosting services for version control, such as GitHub, have made it
easier for people to develop, share, and donate money to software repositories.
In this paper, we study the use of Bitcoin to make donations to open source
repositories on GitHub. In particular, we analyze the amount and volume of
donations over time, in addition to its relationship to the age and popularity
of a repository.
  We scanned over three million repositories looking for donation addresses. We
then extracted and analyzed their transactions from Bitcoin's public
blockchain. Overall, we found a limited adoption of Bitcoin as a payment method
for receiving donations, with nearly 44 thousand deposits adding up to only 8.3
million dollars in the last 10 years. We also found weak positive correlation
between the amount of donations in dollars and the popularity of a repository,
with highest correlation (r=0.013) associated with number of forks."
scgqa_209,1707.04476v5,"According to Figure 7, how effectively does the collaborative nested sampling method recover redshift distributions?","The graph shows that the method is able to correctly recover the input redshift distribution. This is evident from the fact that the distribution of the recovered redshifts (blue line) matches the distribution of the input redshifts (red line). This shows that the method is able to accurately estimate the redshift of the galaxy, even in the presence of uncertainty in the line location.",1707.04476v5.pdf,"['1707.04476v5.pdf', '1505.02851v1.pdf', '1803.01118v2.pdf', '2009.07756v1.pdf', '1903.10464v3.pdf', '1908.04655v1.pdf']",1707.04476v5-Figure7-1.png,"Figure 7 Parameter posterior constraints. Each error bar shows a simulated data set; the four examples from Figure 5 are shown in the same colours. The pink and yellow data sets have been well-detected and characterized, while the magenta line has larger uncertainties. The cyan constraints cover two solutions (see Figure 5). Error bars are centred at the median of the posteriors with the line lengths reflecting the 1-sigma equivalent quantiles.","We can now plot the posterior distributions of the found line locations. Figure 7 demonstrates the wide variety of uncertainties. The spectra of Figure 5 are shown in the same colours. For many, the line could be identified and characterised with small uncertainties (yellow, pink, black), for others, the method remains unsure (cyan, magenta). Figure 8 shows that the input redshift distribution is correctly recovered.",Collaborative Nested Sampling: Big Data vs. complex physical models,"The data torrent unleashed by current and upcoming astronomical surveys
demands scalable analysis methods. Many machine learning approaches scale well,
but separating the instrument measurement from the physical effects of
interest, dealing with variable errors, and deriving parameter uncertainties is
often an after-thought. Classic forward-folding analyses with Markov Chain
Monte Carlo or Nested Sampling enable parameter estimation and model
comparison, even for complex and slow-to-evaluate physical models. However,
these approaches require independent runs for each data set, implying an
unfeasible number of model evaluations in the Big Data regime. Here I present a
new algorithm, collaborative nested sampling, for deriving parameter
probability distributions for each observation. Importantly, the number of
physical model evaluations scales sub-linearly with the number of data sets,
and no assumptions about homogeneous errors, Gaussianity, the form of the model
or heterogeneity/completeness of the observations need to be made.
Collaborative nested sampling has immediate application in speeding up analyses
of large surveys, integral-field-unit observations, and Monte Carlo
simulations."
scgqa_210,1703.03892v5,"According to Figure 9, how does increased privacy knowledge affect the number of privacy measures identified?","The graph shows that there is a positive correlation between privacy knowledge and the number of privacy measures identified. This means that as privacy knowledge increases, the number of privacy measures identified also increases. This is likely because people with more privacy knowledge are more likely to be aware of the different privacy measures that can be taken to protect data.",1703.03892v5.pdf,"['1703.03892v5.pdf', '1804.00243v2.pdf', '1502.03556v1.pdf', '1612.07141v3.pdf', '1905.12868v5.pdf']",1703.03892v5-Figure9-1.png,Figure 9: Number of privacy measures identified in each round,"The heat-maps clearly show that both novice and expert software engineers were able to identify a greater number of privacy protecting measures by using the PbD guidelines than they would do otherwise. In Figure 9, we illustrate how the mean of the ‘number of privacy measures’ identified changes at different privacy knowledge levels, for novice and experts software engineers. The average number of privacy measures identified, in Round 1, by novices is 0.2 and experts",Designing Privacy-aware Internet of Things Applications,"Internet of Things (IoT) applications typically collect and analyse personal
data that can be used to derive sensitive information about individuals.
However, thus far, privacy concerns have not been explicitly considered in
software engineering processes when designing IoT applications. The advent of
behaviour driven security mechanisms, failing to address privacy concerns in
the design of IoT applications can have security implications. In this paper,
we explore how a Privacy-by-Design (PbD) framework, formulated as a set of
guidelines, can help software engineers integrate data privacy considerations
into the design of IoT applications. We studied the utility of this PbD
framework by studying how software engineers use it to design IoT applications.
We also explore the challenges in using the set of guidelines to influence the
IoT applications design process. In addition to highlighting the benefits of
having a PbD framework to make privacy features explicit during the design of
IoT applications, our studies also surfaced a number of challenges associated
with the approach. A key finding of our research is that the PbD framework
significantly increases both novice and expert software engineers' ability to
design privacy into IoT applications."
scgqa_211,1807.09483v2,"In the results shown in Fig. 14, how does increasing vertex number correlate with iteration numbers across configurations?","The graph shows that the number of iterations increases with the number of vertices. This is because a larger graph has more edges and vertices to be explored, which requires more iterations to complete. The graph also shows that the number of iterations varies depending on the configuration. The Sloppy configuration performs more iterations than the Medium and Precise configurations, as it is less concerned with finding the optimal solution. The Medium configuration performs fewer iterations than the Sloppy configuration, but more than the Precise configuration. The Precise configuration performs the fewest iterations, as it is most concerned with finding the optimal solution.",1807.09483v2.pdf,"['1807.09483v2.pdf', '1402.0808v1.pdf', '2004.05448v1.pdf', '1810.03742v1.pdf', '1509.02054v1.pdf', '1705.00891v1.pdf']",1807.09483v2-Figure14-1.png,Fig. 14: Number n of vertices vs. number of iterations.,"Fig. 14 compares the number of vertices of a graph with the number of iterations done in the given time limit. The color indicate the configuration; green is Sloppy, orange is Medium, purple is Precise. It shows that independent of the configuration, on each n-vertex graph at least n iterations have been performed. The actual number of iterations depend on the configuration. Especially, the Sloppy configuration is able to do at least 10n iterations on most graphs. On the other hand, Precise moves considerably less vertices.",A Greedy Heuristic for Crossing-Angle Maximization,"The crossing angle of a straight-line drawing $\Gamma$ of a graph $G=(V, E)$
is the smallest angle between two crossing edges in $\Gamma$. Deciding whether
a graph $G$ has a straight-line drawing with a crossing angle of $90^\circ$ is
$\mathcal NP$-hard. We propose a simple heuristic to compute a drawing with a
large crossing angle. The heuristic greedily selects the best position for a
single vertex in a random set of points. The algorithm is accompanied by a
speed-up technique to compute the crossing angle of a straight-line drawing. We
show the effectiveness of the heuristic in an extensive empirical evaluation.
Our heuristic was clearly the winning algorithm (CoffeeVM) in the Graph Drawing
Challenge 2017."
scgqa_212,1907.05050v3,"In terms of stability and accuracy, how does the number of measurements N influence the results depicted in Figure 4?","The graph suggests that the control law performs better as the number of measurements increases. This is because the approximation of the ideal control law becomes closer to the true value, which results in a more accurate control action. This is important for ensuring that the system remains stable and that the desired state is achieved.",1907.05050v3.pdf,"['1907.05050v3.pdf', '2009.06124v1.pdf', '2011.07119v1.pdf', '2008.13170v1.pdf', '1209.3394v5.pdf', '1604.04026v1.pdf']",1907.05050v3-Figure4-1.png,"Figure 4. Time evolution of u⋆(w(t)) and of its approximation γ̂(θ(t), η(t)) for N = 1, 3, 5. In abscissa: time (in seconds).","Finally, Figure 4 shows the time evolution of the ideal steady-state control law u⋆(w) and of its approximation given by γ̂(θ(j), η(t)) in the three cases in which N = 1, 3, 5.","Approximate Nonlinear Regulation via Identification-Based Adaptive
  Internal Models","This paper concerns the problem of adaptive output regulation for
multivariable nonlinear systems in normal form. We present a regulator
employing an adaptive internal model of the exogenous signals based on the
theory of nonlinear Luenberger observers. Adaptation is performed by means of
discrete-time system identification schemes, in which every algorithm
fulfilling some optimality and stability conditions can be used. Practical and
approximate regulation results are given relating the prediction capabilities
of the identified model to the asymptotic bound on the regulated variables,
which become asymptotic whenever a ""right"" internal model exists in the
identifier's model set. The proposed approach, moreover, does not require
""high-gain"" stabilization actions."
scgqa_213,2008.07011v1,How does the Pro-IBMAC algorithm influence admitted sessions with varying link capacities as shown in the paper?,The graph shows that the number of admitted sessions increases as the link capacity increases. This is because the Pro-IBMAC algorithm is more flexible when there is more bandwidth available.,2008.07011v1.pdf,"['2008.07011v1.pdf', '2002.01322v1.pdf', '1608.06005v1.pdf', '1101.0235v1.pdf', '1909.05034v1.pdf', '2005.13300v1.pdf']",2008.07011v1-Figure7-1.png,Fig. 7. Admitted sessions of CBAC and Pro-IBMAC for different link capacities,"As mentioned earlier, parameter β controls the degree of risk between the admission decision and QoE of existing sessions. Fig. 6 shows IAAR(t) (dash-dot line) and the upper limit of the exceedable aggregate rate (solid line) that allows more sessions (compared to sessions allowed by IAAR(t)), without QoE degradation of enrolled video sessions. The proposed value of β for four scenarios (22, 30, 36 and 40Mbps) is shown in the figure. It can be seen that the lower the value of β, the wider the gap between the two rates. Decreasing β causes increase in the limit of the exceedable rate. This makes ProIBMAC more flexible and it accepts more sessions. This can be better observed in Fig. 7. It depicts the number of admitted","A Novel Traffic Rate Measurement Algorithm for QoE-Aware Video Admission
  Control","With the inevitable dominance of video traffic on the Internet, providing
perceptually good video quality is becoming a challenging task. This is partly
due to the bursty nature of video traffic, changing network conditions and
limitations of network transport protocols. This growth of video traffic has
made Quality of Experience (QoE) of the end user the focus of the research
community. In contrast, Internet service providers are concerned about
maximizing revenue by accepting as many sessions as possible, as long as
customers remain satisfied. However, there is still no entirely satisfactory
admission algorithm for flows with variable rate. The trade-off between the
number of sessions and perceived QoE can be optimized by exploiting the bursty
nature of video traffic. This paper proposes a novel algorithm to determine the
upper limit of the aggregate video rate that can exceed the available bandwidth
without degrading the QoE of accepted video sessions. A parameter $\beta$ that
defines the exceedable limit is defined. The proposed algorithm results in
accepting more sessions without compromising the QoE of on-going video
sessions. Thus it contributes to the optimization of the QoE-Session trade-off
in support of the expected growth of video traffic on the Internet."
scgqa_214,1405.7705v1,Why does the analysis of the closed kinematic chain indicate one independent motion in Figure 20's right plot?,"The right plot shows that the approach correctly estimates the number of DOFs to one already after the first few observations because the closed kinematic chain has one DOF. This means that the chain can only perform one independent motion, which is to rotate around its center.",1405.7705v1.pdf,"['1405.7705v1.pdf', '1608.06005v1.pdf', '2005.14165v4.pdf', '1603.04153v1.pdf', '1106.3826v2.pdf', '1007.0328v1.pdf', '1811.01194v1.pdf', '2009.07756v1.pdf', '1802.05945v1.pdf']",1405.7705v1-Figure20-1.png,Figure 20: Estimated number of DOFs for the open and the closed kinematic chain object (see Fig. 19). Left: open kinematic chain. Right: closed kinematic chain.,"We also analyzed the progression of model selection while the training data is incorporated. The left plot of Fig. 20 shows the DOFs of the learned kinematic model for the open kinematic chain. Note that we opened the yardstick segment by segment, therefore the number of DOFs increases step-wise from zero to three. The right plot shows the estimated number of DOFs for the closed kinematic chain: our approach correctly estimates the number of DOFs to one already after the first few observations.","A Probabilistic Framework for Learning Kinematic Models of Articulated
  Objects","Robots operating in domestic environments generally need to interact with
articulated objects, such as doors, cabinets, dishwashers or fridges. In this
work, we present a novel, probabilistic framework for modeling articulated
objects as kinematic graphs. Vertices in this graph correspond to object parts,
while edges between them model their kinematic relationship. In particular, we
present a set of parametric and non-parametric edge models and how they can
robustly be estimated from noisy pose observations. We furthermore describe how
to estimate the kinematic structure and how to use the learned kinematic models
for pose prediction and for robotic manipulation tasks. We finally present how
the learned models can be generalized to new and previously unseen objects. In
various experiments using real robots with different camera systems as well as
in simulation, we show that our approach is valid, accurate and efficient.
Further, we demonstrate that our approach has a broad set of applications, in
particular for the emerging fields of mobile manipulation and service robotics."
scgqa_215,1512.02567v1,What factors contribute to the lower MSD of adaptive sparse NLMF algorithms in local conditions versus distributed setups in this study?,"The MSD of the proposed adaptive sparse NLMF algorithms is lower for the local scenario than for the distributed scenario because the local scenario has less noise than the distributed scenario. The noise in the distributed scenario makes it more difficult for the algorithms to learn the sparse representation of the input signal, which results in a higher MSD.",1512.02567v1.pdf,"['1512.02567v1.pdf', '1811.00912v4.pdf', '1810.04915v1.pdf', '1910.11127v1.pdf', '2011.09375v1.pdf', '1807.06736v1.pdf', '2009.06124v1.pdf', '1607.08438v1.pdf']",1512.02567v1-Figure6-1.png,"Fig. 6. MSD performances of Local Sparse NLMS, Local Sparse NLMF and Distributed Sparse NLMF algorithms.","The performance of the proposed adaptive sparse NLMF algorithms evaluated by computer simulations are shown in Fig. 5, and Fig. 6, for local and distributed scenario, respectively.","Distributed Adaptive LMF Algorithm for Sparse Parameter Estimation in
  Gaussian Mixture Noise","A distributed adaptive algorithm for estimation of sparse unknown parameters
in the presence of nonGaussian noise is proposed in this paper based on
normalized least mean fourth (NLMF) criterion. At the first step, local
adaptive NLMF algorithm is modified by zero norm in order to speed up the
convergence rate and also to reduce the steady state error power in sparse
conditions. Then, the proposed algorithm is extended for distributed scenario
in which more improvement in estimation performance is achieved due to
cooperation of local adaptive filters. Simulation results show the superiority
of the proposed algorithm in comparison with conventional NLMF algorithms."
scgqa_216,1612.01450v1,"In the context of the paper, what does the graph indicate about predicting influential enablers for 2010 and 2011?","The graph shows that the predictive model is fairly scalable, quickly converging using less than 10 iterations in both cases. Furthermore, most of the selected papers A indeed appear in the reading set Q, with precision over 83% in both cases. This highlights the effectiveness of the predictive model in identifying the most influential enablers for the papers published in 2010 and 2011.",1612.01450v1.pdf,"['1612.01450v1.pdf', '1808.08442v1.pdf', '1902.03993v2.pdf']",1612.01450v1-Figure11-1.png,Figure 11: Effectiveness of predicting the most influential enablers for the papers published in (a) 2010 and (b) 2011.,"Figure 11 illustrates the measurement results for t = 2010 and 2011. It is observed that our prediction model is fairly scalable: in both cases, it quickly converges using less than 10 iterations. Furthermore, among the selected papers A, most of them indeed appear in the reading set Q (with precision over 83% in both cases), highlighting the effectiveness of our predictive model.","Inspiration or Preparation? Explaining Creativity in Scientific
  Enterprise","Human creativity is the ultimate driving force behind scientific progress.
While the building blocks of innovations are often embodied in existing
knowledge, it is creativity that blends seemingly disparate ideas. Existing
studies have made striding advances in quantifying creativity of scientific
publications by investigating their citation relationships. Yet, little is
known hitherto about the underlying mechanisms governing scientific creative
processes, largely due to that a paper's references, at best, only partially
reflect its authors' actual information consumption. This work represents an
initial step towards fine-grained understanding of creative processes in
scientific enterprise. In specific, using two web-scale longitudinal datasets
(120.1 million papers and 53.5 billion web requests spanning 4 years), we
directly contrast authors' information consumption behaviors against their
knowledge products. We find that, of 59.0\% papers across all scientific
fields, 25.7\% of their creativity can be readily explained by information
consumed by their authors. Further, by leveraging these findings, we develop a
predictive framework that accurately identifies the most critical knowledge to
fostering target scientific innovations. We believe that our framework is of
fundamental importance to the study of scientific creativity. It promotes
strategies to stimulate and potentially automate creative processes, and
provides insights towards more effective designs of information recommendation
platforms."
scgqa_217,1204.5592v1,"In the context of TCP connections from the KDD dataset, what does Fig. 10 reveal about detection versus false positive rates?","The results in this graph suggest that there is a trade-off between detection rate and false positive rate in intrusion detection systems. This trade-off must be carefully considered when designing an intrusion detection system, as the desired level of detection rate and false positive rate will vary depending on the specific application.",1204.5592v1.pdf,"['1204.5592v1.pdf', '1402.1892v2.pdf', '1805.00184v1.pdf', '2007.15404v1.pdf', '1505.02851v1.pdf', '1302.2824v2.pdf', '1512.02567v1.pdf', '1708.01249v1.pdf', '2002.10790v1.pdf']",1204.5592v1-Figure10-1.png,Fig. 10 Percentage of detection and false positive rates with varying tolerance factors r1 and r2 for TCP connections,"Three types of connections are there in KDD dataset: TCP connections, UDP connections and ICMP connections. Distribution of these connections in both training and testing datasets is shown in fig. 10.","Dynamic and Auto Responsive Solution for Distributed Denial-of-Service
  Attacks Detection in ISP Network","Denial of service (DoS) attacks and more particularly the distributed ones
(DDoS) are one of the latest threat and pose a grave danger to users,
organizations and infrastructures of the Internet. Several schemes have been
proposed on how to detect some of these attacks, but they suffer from a range
of problems, some of them being impractical and others not being effective
against these attacks. This paper reports the design principles and evaluation
results of our proposed framework that autonomously detects and accurately
characterizes a wide range of flooding DDoS attacks in ISP network. Attacks are
detected by the constant monitoring of propagation of abrupt traffic changes
inside ISP network. For this, a newly designed flow-volume based approach
(FVBA) is used to construct profile of the traffic normally seen in the
network, and identify anomalies whenever traffic goes out of profile.
Consideration of varying tolerance factors make proposed detection system
scalable to the varying network conditions and attack loads in real time.
Six-sigma method is used to identify threshold values accurately for malicious
flows characterization. FVBA has been extensively evaluated in a controlled
test-bed environment. Detection thresholds and efficiency is justified using
receiver operating characteristics (ROC) curve. For validation, KDD 99, a
publicly available benchmark dataset is used. The results show that our
proposed system gives a drastic improvement in terms of detection and false
alarm rate."
scgqa_218,1612.01450v1,What does Figure 11 reveal about the precision of the predictive framework for identifying knowledge enablers in scientific publications?,The graph shows that the predictive model has a precision of over 83% in both cases. This means that the model is able to correctly identify the most influential enablers for a large majority of the papers. This is important because it means that the model can be used to identify the most important factors that contribute to the success of a paper.,1612.01450v1.pdf,"['1612.01450v1.pdf', '1803.09990v2.pdf', '1403.2732v1.pdf', '2003.14319v2.pdf', '1209.3394v5.pdf', '1906.07610v2.pdf', '1805.01772v1.pdf', '1804.04290v1.pdf']",1612.01450v1-Figure11-1.png,Figure 11: Effectiveness of predicting the most influential enablers for the papers published in (a) 2010 and (b) 2011.,"Figure 11 illustrates the measurement results for t = 2010 and 2011. It is observed that our prediction model is fairly scalable: in both cases, it quickly converges using less than 10 iterations. Furthermore, among the selected papers A, most of them indeed appear in the reading set Q (with precision over 83% in both cases), highlighting the effectiveness of our predictive model.","Inspiration or Preparation? Explaining Creativity in Scientific
  Enterprise","Human creativity is the ultimate driving force behind scientific progress.
While the building blocks of innovations are often embodied in existing
knowledge, it is creativity that blends seemingly disparate ideas. Existing
studies have made striding advances in quantifying creativity of scientific
publications by investigating their citation relationships. Yet, little is
known hitherto about the underlying mechanisms governing scientific creative
processes, largely due to that a paper's references, at best, only partially
reflect its authors' actual information consumption. This work represents an
initial step towards fine-grained understanding of creative processes in
scientific enterprise. In specific, using two web-scale longitudinal datasets
(120.1 million papers and 53.5 billion web requests spanning 4 years), we
directly contrast authors' information consumption behaviors against their
knowledge products. We find that, of 59.0\% papers across all scientific
fields, 25.7\% of their creativity can be readily explained by information
consumed by their authors. Further, by leveraging these findings, we develop a
predictive framework that accurately identifies the most critical knowledge to
fostering target scientific innovations. We believe that our framework is of
fundamental importance to the study of scientific creativity. It promotes
strategies to stimulate and potentially automate creative processes, and
provides insights towards more effective designs of information recommendation
platforms."
scgqa_219,1206.6850v1,"In the context of this research, what findings correlate embedding dimensions with the performance across datasets?","The graph suggests that the optimal number of dimensions for an embedding algorithm may vary depending on the dataset. This is likely due to the fact that different datasets have different characteristics, and different embedding algorithms may be better suited for different types of data. Therefore, it is important to experiment with different embedding algorithms and different numbers of dimensions to find the best combination for a particular application.",1206.6850v1.pdf,"['1206.6850v1.pdf', '1502.00588v1.pdf', '1204.5592v1.pdf']",1206.6850v1-Figure3-1.png,Figure 3: Performance of embedding algorithms on the three datasets as a function of embedding dimensions.,"Figure 3 shows another experiment with the same evaluation criteria. In this experiment, we fix the testing data as usual, and use all the remaining data for training. The plot shows Kendall’s tau as a function of the number of dimensions of the embedding space.",Visualization of Collaborative Data,"Collaborative data consist of ratings relating two distinct sets of objects:
users and items. Much of the work with such data focuses on filtering:
predicting unknown ratings for pairs of users and items. In this paper we focus
on the problem of visualizing the information. Given all of the ratings, our
task is to embed all of the users and items as points in the same Euclidean
space. We would like to place users near items that they have rated (or would
rate) high, and far away from those they would give a low rating. We pose this
problem as a real-valued non-linear Bayesian network and employ Markov chain
Monte Carlo and expectation maximization to find an embedding. We present a
metric by which to judge the quality of a visualization and compare our results
to local linear embedding and Eigentaste on three real-world datasets."
scgqa_220,1607.08112v1,What does the runtime graph in the MLPnP paper reveal about MLPnP's efficiency with fewer points?,"The main takeaways from this graph are that MLPnP is the most accurate PnP algorithm, and it is also one of the fastest. For less than 20 points, MLPnP is even faster than EPnP, which is the fastest PnP solution.",1607.08112v1.pdf,"['1607.08112v1.pdf', '2008.01961v3.pdf', '1709.03329v1.pdf', '1209.3394v5.pdf']",1607.08112v1-Figure2-1.png,Figure 2. Runtime. (a) All methods. (b) Zoom to the fastest methods. CEPPnP and MLPnP+Σ incorporate measurement uncertainty. All other algorithms assume equally well measured image points.,"The experiments for the ordinary case show, that MLPnP outperforms all other state-of-the-art algorithms in terms of accuracy. Moreover, it stays among the fastest algorithms as depicted in Fig. 2. For less than 20 points, the algorithm is even faster than EPnP, that is still the fastest PnP solution.","MLPnP - A Real-Time Maximum Likelihood Solution to the
  Perspective-n-Point Problem","In this paper, a statistically optimal solution to the Perspective-n-Point
(PnP) problem is presented. Many solutions to the PnP problem are geometrically
optimal, but do not consider the uncertainties of the observations. In
addition, it would be desirable to have an internal estimation of the accuracy
of the estimated rotation and translation parameters of the camera pose. Thus,
we propose a novel maximum likelihood solution to the PnP problem, that
incorporates image observation uncertainties and remains real-time capable at
the same time. Further, the presented method is general, as is works with 3D
direction vectors instead of 2D image points and is thus able to cope with
arbitrary central camera models. This is achieved by projecting (and thus
reducing) the covariance matrices of the observations to the corresponding
vector tangent space."
scgqa_221,1906.07610v2,What does the relationship of sentiment examples indicate for the MTL negation model's performance on SST-binary?,"The graph shows that the MTL negation model also improves with the number of sentiment examples, but the effect is not as pronounced as with the negation scope examples. This suggests that the model is able to learn from sentiment data, but that it is more important to have a large number of negation scope examples.",1906.07610v2.pdf,"['1906.07610v2.pdf', '1808.07801v3.pdf', '1610.08534v1.pdf']",1906.07610v2-Figure4-1.png,Fig. 4: Mean accuracy on the SST-binary task when training MTL negation model with differing amounts of negation data from the SFU dataset (left) and sentiment data (right).,"the SFU dataset (from 10 to 800 in intervals of 100) and accuracy is calculated for each number of examples. Figure 4 (left) shows that the MTL model improves over the baseline with as few as ten negation examples and plateaus somewhere near 600. An analysis on the SST-fine setup showed a similar pattern. There is nearly always an effect of diminishing returns when it comes to adding training examples, but if we were to instead plot this learning curve with a log-scale on the x-axis, i.e. doubling the amount data for each increment, it would seem to indicate that having more data could indeed still prove useful, as long as there were sufficient amounts. In any case, regardless of the amount of data, exposing the model to a larger variety of negation examples could also prove beneficial – we follow up on this point in the next subsection.",Improving Sentiment Analysis with Multi-task Learning of Negation,"Sentiment analysis is directly affected by compositional phenomena in
language that act on the prior polarity of the words and phrases found in the
text. Negation is the most prevalent of these phenomena and in order to
correctly predict sentiment, a classifier must be able to identify negation and
disentangle the effect that its scope has on the final polarity of a text. This
paper proposes a multi-task approach to explicitly incorporate information
about negation in sentiment analysis, which we show outperforms learning
negation implicitly in a data-driven manner. We describe our approach, a
cascading neural architecture with selective sharing of LSTM layers, and show
that explicitly training the model with negation as an auxiliary task helps
improve the main task of sentiment analysis. The effect is demonstrated across
several different standard English-language data sets for both tasks and we
analyze several aspects of our system related to its performance, varying types
and amounts of input data and different multi-task setups."
scgqa_222,1304.2109v1,"In the context of this paper, what does the relationship between poor and improved images reveal about minutiae points?","The graph shows that the number of minutiae points in poor images is less than that of the improved images. This is because the poor images are more noisy, which makes it difficult to detect minutiae points. The improved images are less noisy, which makes it easier to detect minutiae points.",1304.2109v1.pdf,"['1304.2109v1.pdf', '2011.08042v1.pdf', '1803.04037v1.pdf', '1912.00088v1.pdf', '1905.00569v2.pdf', '1610.00017v2.pdf', '1705.00891v1.pdf', '1509.00374v2.pdf']",1304.2109v1-Figure2-1.png,Fig. 2: Graph represents the poor versus improved image curve,"In Fig. 2, it is shown that the number of minutiae points of poor image and improved image is not similar but linear and it takes a decision that number of minutiae points in poor image is less than that of the improved image, as the poor image is so noisy compared with that of the improved images. In Fig. 3, it shows that the intensity variation and relationship among the intensity variation of the steps, such as input image, filtered image, enhanced image, lined image and shaped image.","Automatic Fingerprint Recognition Using Minutiae Matching Technique for
  the Large Fingerprint Database","Extracting minutiae from fingerprint images is one of the most important
steps in automatic fingerprint identification system. Because minutiae matching
are certainly the most well-known and widely used method for fingerprint
matching, minutiae are local discontinuities in the fingerprint pattern. In
this paper a fingerprint matching algorithm is proposed using some specific
feature of the minutiae points, also the acquired fingerprint image is
considered by minimizing its size by generating a corresponding fingerprint
template for a large fingerprint database. The results achieved are compared
with those obtained through some other methods also shows some improvement in
the minutiae detection process in terms of memory and time required."
scgqa_223,1902.06156v1,What specific aspect of distributed learning was tested with 20% corrupt workers in the CIFAR10 experiment?,"The goal of the experiment was to evaluate the performance of different aggregation rules under attack. The experiment was conducted with m = 20% corrupt workers, and the parameters of the workers were changed by only 1σ.",1902.06156v1.pdf,"['1902.06156v1.pdf', '1603.04812v2.pdf', '2009.08716v1.pdf', '2010.12427v3.pdf', '2004.05579v1.pdf']",1902.06156v1-Figure3-1.png,Figure 3: Model accuracy on CIFAR10. m = 20% and z = 1. No Attack is plotted for reference.,"Experiment results on CIFAR10 are shown in Figure 3. Since fewer standard deviations can cause a significant impact on CIFAR10 (see Table 1), we choose m = 20% corrupt workers, and change the parameters by only 1σ. Again, the best accuracy was achieved with the simplest aggregation rule, i.e. averaging the workers’ parameters, but still the accuracy dropped by 28%. Krum performed worst again for the same reason with a drop of 66%, Bulyan dropped by 52% and TrimmedMean performed slightly better but still dropped by 45%.",A Little Is Enough: Circumventing Defenses For Distributed Learning,"Distributed learning is central for large-scale training of deep-learning
models. However, they are exposed to a security threat in which Byzantine
participants can interrupt or control the learning process. Previous attack
models and their corresponding defenses assume that the rogue participants are
(a) omniscient (know the data of all other participants), and (b) introduce
large change to the parameters. We show that small but well-crafted changes are
sufficient, leading to a novel non-omniscient attack on distributed learning
that go undetected by all existing defenses. We demonstrate our attack method
works not only for preventing convergence but also for repurposing of the model
behavior (backdooring). We show that 20% of corrupt workers are sufficient to
degrade a CIFAR10 model accuracy by 50%, as well as to introduce backdoors into
MNIST and CIFAR10 models without hurting their accuracy"
scgqa_224,1610.08332v1,"According to Figure 11 in this paper, which amplifier primarily contributes to non-linear distortion in the Doherty amplifier?","The graph shows that the main amplifier is the dominant source of non-linear distortion in the Doherty amplifier. This can be expected, as the auxiliary amplifier only kicks in for limited amounts of time in this Doherty configuration. A similar Doherty amplifier was analysed in [26] with a Volterra-based DCA under two-tone excitation. It was concluded there that the auxiliary amplifier only contributes significantly to the distortion for very high amplitudes in the two-tone. With modulated signals, like the multisines used in the BLA-based DCA, the peaks only occur from time to time, so the average contribution of the auxiliary amplifier to the total distortion is low.",1610.08332v1.pdf,"['1610.08332v1.pdf', '1307.1204v1.pdf', '1608.06005v1.pdf']",1610.08332v1-Figure11-1.png,Figure 11. The distortion contributions show that the main amplifier is the dominant source of non-linear distortion in the Doherty amplifier.,"The main distortion contributor is found to be the main transistor (Fig. 11). This can be expected, as the auxiliary amplifier only kicks in for limited amounts of time in this Doherty configuration. A similar Doherty amplifier was analysed in [26] with a Volterra-based DCA under two-tone excitation5. It was concluded there that the auxiliary amplifier only contributes significantly to the distortion for very high amplitudes in the two-tone. With modulated signals, like the multisines used in the BLA-based DCA, the peaks only occur from time to time, so the average contribution of the auxiliary amplifier to the total distortion is low.",Distortion Contribution Analysis with the Best Linear Approximation,"A Distortion Contribution Analysis (DCA) obtains the distortion at the output
of an analog electronic circuit as a sum of distortion contributions of its
sub-circuits. Similar to a noise analysis, a DCA helps a designer to pinpoint
the actual source of the distortion. Classically, the DCA uses the Volterra
theory to model the circuit and its sub-circuits. This DCA has been proven
useful for small circuits or heavily simplified examples. In more complex
circuits however, the amount of contributions increases quickly, making the
interpretation of the results difficult. In this paper, the Best Linear
Approximation (BLA) is used to perform the DCA instead. The BLA represents the
behaviour of a sub-circuit as a linear circuit with the unmodelled distortion
represented by a noise source. Combining the BLA with a classic noise analysis
yields a DCA that is simple to understand, yet capable to handle complex
excitation signals and complex strongly non-linear circuits."
scgqa_225,1705.00891v1,"According to figure 2 in the research paper, how does the maxima envelope of absolute returns compare to normal distribution?","The graph shows that the maxima envelope of the absolute returns is not normally distributed. However, it does appear to be log-normally distributed, which is consistent with the assumption in the GP regression model.",1705.00891v1.pdf,"['1705.00891v1.pdf', '1610.00017v2.pdf', '1811.01194v1.pdf', '2007.06852v1.pdf', '1803.04037v1.pdf', '1804.06161v2.pdf', '1809.07412v2.pdf', '1808.10082v4.pdf', '1306.4036v2.pdf']",1705.00891v1-Figure2-1.png,"Fig. 2. QQ plots vs the standard normal, standard normal quantiles on the horizontal axis and quantiles of the input sample on the vertical (a) QQ plot of the maxima (upper) envelope. (b) QQ plot of the minima (lower) envelope. (c) QQ plot of the log of the maxima (upper) envelope. (d) QQ plot of the log of the minima (lower) envelope","For financial time series we can regress on the maxima envelope of the absolute returns,1 shown in figure 1(b). We can also regress on the maxima envelope of the positive returns, the minima envelope of the negative returns, shown in figure 1(d), and then combine the results by using Eq. (22). The envelopes of financial time series display a log-normal distribution as can be seen in figure 2, and GP regression on envelopes maintains the assumption in Eq. (3). When regressing on the envelope, we are dealing with a lesser number of data points (practically about only one-third of the original data series), and our regression is less affected by the intermediate fluctuation of the data points and we get a relatively smoother estimate of the underlying volatility function.","A Novel Approach to Forecasting Financial Volatility with Gaussian
  Process Envelopes","In this paper we use Gaussian Process (GP) regression to propose a novel
approach for predicting volatility of financial returns by forecasting the
envelopes of the time series. We provide a direct comparison of their
performance to traditional approaches such as GARCH. We compare the forecasting
power of three approaches: GP regression on the absolute and squared returns;
regression on the envelope of the returns and the absolute returns; and
regression on the envelope of the negative and positive returns separately. We
use a maximum a posteriori estimate with a Gaussian prior to determine our
hyperparameters. We also test the effect of hyperparameter updating at each
forecasting step. We use our approaches to forecast out-of-sample volatility of
four currency pairs over a 2 year period, at half-hourly intervals. From three
kernels, we select the kernel giving the best performance for our data. We use
two published accuracy measures and four statistical loss functions to evaluate
the forecasting ability of GARCH vs GPs. In mean squared error the GP's perform
20% better than a random walk model, and 50% better than GARCH for the same
data."
scgqa_226,1307.3687v1,What relationship does Fig. 2 highlight regarding the Grnd model and item inference accuracy?,"The graph shows that the accuracy of inferring items is highest for the Grnd model, followed by the GIPA and GIPA-A models. This is because the Grnd model is the most connected of the three models, which makes it easier for the MAPE estimator to infer the items' ratings.",1307.3687v1.pdf,"['1307.3687v1.pdf', '1207.5027v1.pdf', '1502.00588v1.pdf']",1307.3687v1-Figure2-1.png,Fig. 2. Items classification accuracy comparison.,"The results are shown in Fig. 2. We first generated graphs with number of nodes |V | = 500 and varying number of edges (|E| = 1000, 2000, 3000, 4000, 5000) using different graph models. In each figure, we generated review samples of different sizes (500 ≤ |R| ≤ 5000), and show accuracy of inferring items averaged over 100 experiments respectively. We observe that when |R| increases, the accuracy also increases and approaches 1. This confirms that the MAPE estimator is asymptotically unbiased. For different graph models, we observe that the accuracy on Grnd is larger than the other two models. This indicates that constrained connections will make the inference performance poor. However, the accuracy curves","On Analyzing Estimation Errors due to Constrained Connections in Online
  Review Systems","Constrained connection is the phenomenon that a reviewer can only review a
subset of products/services due to narrow range of interests or limited
attention capacity. In this work, we study how constrained connections can
affect estimation performance in online review systems (ORS). We find that
reviewers' constrained connections will cause poor estimation performance, both
from the measurements of estimation accuracy and Bayesian Cramer Rao lower
bound."
scgqa_227,1603.02175v1,"According to the analysis in the paper, what does Figure 3 indicate about communicational patterns and interest similarity?","The graph shows that interest similarity increases more sharply when the number of monthly communicating days is small. This means that users who interact with each other more frequently are more likely to share similar interests, even if they do not interact with each other for a long time. This is likely because people who interact with each other more frequently have more opportunities to learn about each other's interests and hobbies.",1603.02175v1.pdf,"['1603.02175v1.pdf', '1606.01062v1.pdf', '1510.01155v1.pdf']",1603.02175v1-Figure3-1.png,Figure 3: Interest similarity vs. monthly qq message count and number of monthly communicating days.,"interaction frequency, as shown in Fig. 3. Although the fitted curves4 in the RTP case is not the same as that for PTP, they both show that users with higher intensity of interaction share more interests, particularly when the interaction intensity is very large. Also, interest similarity increases more sharply when number of communicating days is small. Interestingly, interaction frequency is more correlated with interest similarity than interaction intensity, implying that some casual or transactional interaction could also have a large intensity.","Who are Like-minded: Mining User Interest Similarity in Online Social
  Networks","In this paper, we mine and learn to predict how similar a pair of users'
interests towards videos are, based on demographic (age, gender and location)
and social (friendship, interaction and group membership) information of these
users. We use the video access patterns of active users as ground truth (a form
of benchmark). We adopt tag-based user profiling to establish this ground
truth, and justify why it is used instead of video-based methods, or many
latent topic models such as LDA and Collaborative Filtering approaches. We then
show the effectiveness of the different demographic and social features, and
their combinations and derivatives, in predicting user interest similarity,
based on different machine-learning methods for combining multiple features. We
propose a hybrid tree-encoded linear model for combining the features, and show
that it out-performs other linear and treebased models. Our methods can be used
to predict user interest similarity when the ground-truth is not available,
e.g. for new users, or inactive users whose interests may have changed from old
access data, and is useful for video recommendation. Our study is based on a
rich dataset from Tencent, a popular service provider of social networks, video
services, and various other services in China."
scgqa_228,1408.5389v1,"According to the results in your paper's Figure 7, how does extra time scale with the number of extra statistics?","The graph shows that the extra time stands in a nearly linear relationship to the number of extra statistics. This means that as the number of extra statistics increases, the extra time also increases. This is likely due to the fact that the extra statistics need to be processed and stored in the ct-table, which takes time.",1408.5389v1.pdf,"['1408.5389v1.pdf', '1707.02327v1.pdf', '2011.03519v1.pdf', '1905.05538v1.pdf', '1911.05146v2.pdf', '1902.06156v1.pdf', '2005.09634v1.pdf']",1408.5389v1-Figure7-1.png,Figure 7: Möbius Join Extra Time (s),"As Figure 7 illustrates, the Extra Time stands in a nearly linear relationship to the number of Extra Statistics, which confirms the analysis of Section 4.3. Figure 8 shows that most of the MJ run time is spent on the Pivot component (Algorithm 1) rather than the main loop (Algorithm 2). In terms of ct-table operations, most time is spent on subtraction/union rather than cross product.",Computing Multi-Relational Sufficient Statistics for Large Databases,"Databases contain information about which relationships do and do not hold
among entities. To make this information accessible for statistical analysis
requires computing sufficient statistics that combine information from
different database tables. Such statistics may involve any number of {\em
positive and negative} relationships. With a naive enumeration approach,
computing sufficient statistics for negative relationships is feasible only for
small databases. We solve this problem with a new dynamic programming algorithm
that performs a virtual join, where the requisite counts are computed without
materializing join tables. Contingency table algebra is a new extension of
relational algebra, that facilitates the efficient implementation of this
M\""obius virtual join operation. The M\""obius Join scales to large datasets
(over 1M tuples) with complex schemas. Empirical evaluation with seven
benchmark datasets showed that information about the presence and absence of
links can be exploited in feature selection, association rule mining, and
Bayesian network learning."
scgqa_229,1303.1635v1,"In the context of the paper's experiments shown in Figure 11, what trend is observed for packet delivery ratio with multiple sources?","The graph shows that the packet delivery ratio decreases as the number of sources increases. This is because as the number of sources increases, the network becomes more congested and there is less bandwidth available for each source. This can lead to packets being dropped or delayed, which reduces the overall packet delivery ratio.",1303.1635v1.pdf,"['1303.1635v1.pdf', '1311.1567v3.pdf', '1610.01283v4.pdf', '2006.16705v1.pdf', '1612.01450v1.pdf', '1409.3924v1.pdf', '1612.03449v3.pdf']",1303.1635v1-Figure11-1.png,"Figure 11. Packet delivery ratio vs. No. of Sources (Max speed=40Km/h, Pause time=15ms,Trffic=35Kbps)","Figure 9 shows packet delivery ratio versus max speed of nodes in random way point model, figure 10 shows packet delivery ratio versus pause time of nodes in random way point model, figure 11 shows packet delivery ratio versus number of data sources (number of connections) and figure 12 shows packet delivery ratio versus offered traffic.",Improving Energy Efficiency in MANETs by Multi-Path Routing,"Some multi-path routing algorithm in MANET, simultaneously send information
to the destination through several directions to reduce end-to-end delay. In
all these algorithms, the sent traffic through a path affects the adjacent path
and unintentionally increases the delay due to the use of adjacent paths.
Because, there are repetitive competitions among neighboring nodes, in order to
obtain the joint channel in adjacent paths. The represented algorithm in this
study tries to discover the distinct paths between source and destination nodes
with using Omni directional antennas, to send information through these
simultaneously. For this purpose, the number of active neighbors is counted in
each direction with using a strategy. These criterions are effectively used to
select routes. Proposed algorithm is based on AODV routing algorithm, and in
the end it is compared with AOMDV, AODVM, and IZM-DSR algorithms which are
multi-path routing algorithms based on AODV and DSR. Simulation results show
that using the proposed algorithm creates a significant improvement in energy
efficiency and reducing end-to-end delay."
scgqa_230,1710.07771v1,What implications do the results in Figure 5.2 have for using SLiSe RFFs in specific machine learning scenarios?,"The results in this graph suggest that SLiSe RFFs can be used to represent functions with increasing extrema, which is not possible with traditional RFFs. This means that SLiSe RFFs can be used to improve the performance of machine learning models on tasks that require the representation of functions with increasing extrema.",1710.07771v1.pdf,"['1710.07771v1.pdf', '2010.13032v1.pdf', '2003.14319v2.pdf']",1710.07771v1-Figure5.2-1.png,Figure 5.2.: Logarithmic absolute function values of non-increasing and increasing 16-pole RFFs.,"There exist SLiSe RFFs having increasing extrema. An example of such is depicted in Figure 5.2. The Gauss-Legendre RFF has non-increasing extrema, whereas the “Increasing RFF” has a local extremum at about 2 with a larger absolute value than the extrema inside [0, 2).","Constrained Optimisation of Rational Functions for Accelerating Subspace
  Iteration","Earlier this decade, the so-called FEAST algorithm was released for computing
the eigenvalues of a matrix in a given interval. Previously, rational filter
functions have been examined as a parameter of FEAST. In this thesis, we expand
on existing work with the following contributions: (i) Obtaining
well-performing rational filter functions via standard minimisation algorithms,
(ii) Obtaining constrained rational filter functions efficiently, and (iii)
Improving existing rational filter functions algorithmically. Using our new
rational filter functions, FEAST requires up to one quarter fewer iterations on
average compared to state-of-art rational filter functions."
scgqa_231,1510.01155v1,How does the ASGD algorithm's performance compare for small message sizes on different network connections in the study?,"The graph suggests that the performance of the ASGD algorithm for problems with small message sizes is hardly influenced by the network bandwidth. This is because the messages are small and can be transmitted quickly, even over a low-bandwidth network.",1510.01155v1.pdf,"['1510.01155v1.pdf', '2008.13170v1.pdf', '2003.14319v2.pdf', '1207.5027v1.pdf', '2006.16705v1.pdf', '1611.02955v1.pdf']",1510.01155v1-Figure4-1.png,"Fig. 4. Comparing ASGD performance on Gigabit-Ethernet vs. Infiniband interconnections. LEFT: Median runtime of ASGD for altering communication frequencies 1 b . RIGHT: Median Error rates of ASGD Results on Synthetic data withD = 10,K = 10, which results in small messages (50 byte) shows hardly any difference between the performance of both connection types.",Results. Figure 4 shows that the performance of the ASGD algorithm for problems with small message sizes is hardly influenced by the network bandwidth. Gigabit-Ethernet and Infiniband implementations have approximately the same,"Balancing the Communication Load of Asynchronously Parallelized Machine
  Learning Algorithms","Stochastic Gradient Descent (SGD) is the standard numerical method used to
solve the core optimization problem for the vast majority of machine learning
(ML) algorithms. In the context of large scale learning, as utilized by many
Big Data applications, efficient parallelization of SGD is in the focus of
active research. Recently, we were able to show that the asynchronous
communication paradigm can be applied to achieve a fast and scalable
parallelization of SGD. Asynchronous Stochastic Gradient Descent (ASGD)
outperforms other, mostly MapReduce based, parallel algorithms solving large
scale machine learning problems. In this paper, we investigate the impact of
asynchronous communication frequency and message size on the performance of
ASGD applied to large scale ML on HTC cluster and cloud environments. We
introduce a novel algorithm for the automatic balancing of the asynchronous
communication load, which allows to adapt ASGD to changing network bandwidths
and latencies."
scgqa_232,1512.02567v1,Can you explain the relationship between iterations and MSD in the adaptive sparse NLMF algorithms in this research?,"The MSD of the proposed adaptive sparse NLMF algorithms decreases as the number of iterations increases because the algorithms are able to learn the sparse representation of the input signal more accurately over time. As the algorithms learn the sparse representation, they are able to better predict the future values of the signal, which results in a lower MSD.",1512.02567v1.pdf,"['1512.02567v1.pdf', '1402.7063v1.pdf', '1408.5389v1.pdf', '2007.15176v2.pdf', '1204.5592v1.pdf']",1512.02567v1-Figure6-1.png,"Fig. 6. MSD performances of Local Sparse NLMS, Local Sparse NLMF and Distributed Sparse NLMF algorithms.","The performance of the proposed adaptive sparse NLMF algorithms evaluated by computer simulations are shown in Fig. 5, and Fig. 6, for local and distributed scenario, respectively.","Distributed Adaptive LMF Algorithm for Sparse Parameter Estimation in
  Gaussian Mixture Noise","A distributed adaptive algorithm for estimation of sparse unknown parameters
in the presence of nonGaussian noise is proposed in this paper based on
normalized least mean fourth (NLMF) criterion. At the first step, local
adaptive NLMF algorithm is modified by zero norm in order to speed up the
convergence rate and also to reduce the steady state error power in sparse
conditions. Then, the proposed algorithm is extended for distributed scenario
in which more improvement in estimation performance is achieved due to
cooperation of local adaptive filters. Simulation results show the superiority
of the proposed algorithm in comparison with conventional NLMF algorithms."
scgqa_233,1910.00110v2,"In the context of the noisy frequency-response data study, how does condition (11) behave with σ?","The graph shows that for σ ≥ 10−7, condition (11) is violated for all 200 test points. This is consistent with the results in Figure 1a, which show a linear growth for σ < 10−5.",1910.00110v2.pdf,"['1910.00110v2.pdf', '1804.04290v1.pdf', '1811.01194v1.pdf']",1910.00110v2-Figure1-1.png,"Figure 1: CD player: Plots (a) and (c) show the growth of the mean of error (30) over 10 replicates of independent noise samples. Plots (b) and (d) show the number of test points for which condition (11) of Theorem 1 is violated. The mean of the error (30) grows linearly with σ as long as condition (11) is satisfied, which is in agreement with Theorem 1. The error bars in (a) and (c) show the minimum and maximum of the error (30) over the 10 replicates of the noise samples.","which is an average of the error (21) over all 200 test points. Figure 1a shows the mean of e(σ) over 10 replicates of independent noise samples. The standard deviation σ is in the range [10−15, 105] and the dimension is n = 20. The error bars in Figure 1a show the minimum and maximum of e(σ) over the 10 replicates. A linear growth of the mean of error (30) with the standard deviation σ is observed for σ < 10−5. Figure 1b shows the number of test points that violate condition (11) of Theorem 1. The results indicate that for σ ≥ 10−7 condition (11) is violated for all 200 test points, which seems to align with Figure 1a that shows a linear growth for σ < 10−5. Thus, the results in Figure 1a are in agreement with Theorem 1. Similar observations can be made for n = 28 in Figure 1c and Figure 1d.","Learning low-dimensional dynamical-system models from noisy
  frequency-response data with Loewner rational interpolation","Loewner rational interpolation provides a versatile tool to learn
low-dimensional dynamical-system models from frequency-response measurements.
This work investigates the robustness of the Loewner approach to noise. The key
finding is that if the measurements are polluted with Gaussian noise, then the
error due to noise grows at most linearly with the standard deviation with high
probability under certain conditions. The analysis gives insights into making
the Loewner approach robust against noise via linear transformations and
judicious selections of measurements. Numerical results demonstrate the linear
growth of the error on benchmark examples."
scgqa_234,1305.1657v1,What does Fig. 5 indicate about the efficiency and accuracy of the localization algorithm discussed in the paper?,"The graph shows that the proposed data fusion algorithm is able to achieve a high level of accuracy. The maximum positioning error is equal to 2.23 m, while the RMS error is 0.88 m. These results demonstrate that the algorithm is both efficient and accurate enough to satisfy the large part of non-critical WSNs applications.",1305.1657v1.pdf,"['1305.1657v1.pdf', '1708.07972v1.pdf', '1202.4232v2.pdf', '1504.07495v1.pdf', '1910.11127v1.pdf', '1405.6298v2.pdf', '1911.04231v2.pdf', '1607.08112v1.pdf']",1305.1657v1-Figure5-1.png,Fig. 5. Localization results and relative errors using the proposed data fusion algorithm.,"The tracking path output is shown in Fig. 5. It stands out that the tracking algorithm follows the real path during almost all the test. The maximum positioning errors is equal to 2.23 m, while the RMS one is 0.88 m. These results demonstrate that our sub-optimal data fusion technique is both efficient and accurate enough to satisfy the large part of non-critical WSNs applications.","Low Complexity Indoor Localization in Wireless Sensor Networks by UWB
  and Inertial Data Fusion","Precise indoor localization of moving targets is a challenging activity which
cannot be easily accomplished without combining different sources of
information. In this sense, the combination of different data sources with an
appropriate filter might improve both positioning and tracking performance.
This work proposes an algorithm for hybrid positioning in Wireless Sensor
Networks based on data fusion of UWB and inertial information. A constant-gain
Steady State Kalman Filter is used to bound the complexity of the system,
simplifying its implementation on a typical low-power WSN node. The performance
of the presented data fusion algorithm has been evaluated in a realistic
scenario using both simulations and realistic datasets. The obtained results
prove the validity of this approach, which efficiently fuses different
positioning data sources, reducing the localization error."
scgqa_235,1908.04655v1,"According to Figure 5, which approach demonstrates better accuracy and efficiency in parameter estimation?","Based on the information provided in the graph, the autoPR method is more effective than the standard NS approach in terms of both accuracy and efficiency. The autoPR method achieves lower RMSE values and requires significantly fewer likelihood evaluations than the standard NS approach. This suggests that the autoPR method is a more robust and efficient approach for estimating the parameters of a non-linear regression model.",1908.04655v1.pdf,"['1908.04655v1.pdf', '1809.07412v2.pdf', '1703.01827v3.pdf', '1707.04849v1.pdf', '1402.0808v1.pdf', '1909.03961v2.pdf', '2004.04276v1.pdf']",1908.04655v1-Figure5-1.png,"Fig. 5 Algorithm performance comparison in the univariate example over 10 realisations of the data for each value of θ∗. The left panel shows Log10(RMSE) of the estimate θ̂, and the right panel shows the mean number of likelihood evaluations, both of which are obtained using the standard NS approach (blue star points) and the autoPR method (red square points).","is given in Figure 5(a), which shows the (logarithm of the) root mean squared error (RMSE) of the estimate θ̂ over 10 realisations of the data for each value of θ∗, for both the standard NS approach and the autoPR method.",Bayesian posterior repartitioning for nested sampling,"Priors in Bayesian analyses often encode informative domain knowledge that
can be useful in making the inference process more efficient. Occasionally,
however, priors may be unrepresentative of the parameter values for a given
dataset, which can result in inefficient parameter space exploration, or even
incorrect inferences, particularly for nested sampling (NS) algorithms. Simply
broadening the prior in such cases may be inappropriate or impossible in some
applications. Hence our previous solution to this problem, known as posterior
repartitioning (PR), redefines the prior and likelihood while keeping their
product fixed, so that the posterior inferences and evidence estimates remain
unchanged, but the efficiency of the NS process is significantly increased. In
its most practical form, PR raises the prior to some power beta, which is
introduced as an auxiliary variable that must be determined on a case-by-case
basis, usually by lowering beta from unity according to some pre-defined
`annealing schedule' until the resulting inferences converge to a consistent
solution. Here we present a very simple yet powerful alternative Bayesian
approach, in which beta is instead treated as a hyperparameter that is inferred
from the data alongside the original parameters of the problem, and then
marginalised over to obtain the final inference. We show through numerical
examples that this Bayesian PR (BPR) method provides a very robust,
self-adapting and computationally efficient `hands-off' solution to the problem
of unrepresentative priors in Bayesian inference using NS. Moreover, unlike the
original PR method, we show that even for representative priors BPR has a
negligible computational overhead relative to standard nesting sampling, which
suggests that it should be used as the default in all NS analyses."
scgqa_236,1703.03892v5,"According to the research, how do the PbD guidelines influence novices' identification of privacy measures compared to experts?","The graph shows that the PbD guidelines are effective in helping novices identify privacy measures. This is evident in the fact that the average number of privacy measures identified by novices in Round 2, when they are provided with the PbD guidelines, is 0.6, which is comparable to the average number of privacy measures identified by experts in Round 1. This suggests that the PbD guidelines are able to help novices identify privacy measures that they would not have otherwise identified.",1703.03892v5.pdf,"['1703.03892v5.pdf', '1404.7045v3.pdf', '2001.09043v3.pdf']",1703.03892v5-Figure9-1.png,Figure 9: Number of privacy measures identified in each round,"The heat-maps clearly show that both novice and expert software engineers were able to identify a greater number of privacy protecting measures by using the PbD guidelines than they would do otherwise. In Figure 9, we illustrate how the mean of the ‘number of privacy measures’ identified changes at different privacy knowledge levels, for novice and experts software engineers. The average number of privacy measures identified, in Round 1, by novices is 0.2 and experts",Designing Privacy-aware Internet of Things Applications,"Internet of Things (IoT) applications typically collect and analyse personal
data that can be used to derive sensitive information about individuals.
However, thus far, privacy concerns have not been explicitly considered in
software engineering processes when designing IoT applications. The advent of
behaviour driven security mechanisms, failing to address privacy concerns in
the design of IoT applications can have security implications. In this paper,
we explore how a Privacy-by-Design (PbD) framework, formulated as a set of
guidelines, can help software engineers integrate data privacy considerations
into the design of IoT applications. We studied the utility of this PbD
framework by studying how software engineers use it to design IoT applications.
We also explore the challenges in using the set of guidelines to influence the
IoT applications design process. In addition to highlighting the benefits of
having a PbD framework to make privacy features explicit during the design of
IoT applications, our studies also surfaced a number of challenges associated
with the approach. A key finding of our research is that the PbD framework
significantly increases both novice and expert software engineers' ability to
design privacy into IoT applications."
scgqa_237,1612.01450v1,What does the result depicted in Figure 11 indicate about the number of iterations for the model's convergence across two years?,"The graph shows that the predictive model converges quickly, requiring less than 10 iterations in both cases. This is important because it means that the model can be used to identify the most influential enablers for a large number of papers without requiring a significant amount of time or resources.",1612.01450v1.pdf,"['1612.01450v1.pdf', '2005.14165v4.pdf', '1803.06598v1.pdf', '1707.02439v2.pdf', '1208.2451v1.pdf', '1309.3959v1.pdf', '2008.06431v1.pdf']",1612.01450v1-Figure11-1.png,Figure 11: Effectiveness of predicting the most influential enablers for the papers published in (a) 2010 and (b) 2011.,"Figure 11 illustrates the measurement results for t = 2010 and 2011. It is observed that our prediction model is fairly scalable: in both cases, it quickly converges using less than 10 iterations. Furthermore, among the selected papers A, most of them indeed appear in the reading set Q (with precision over 83% in both cases), highlighting the effectiveness of our predictive model.","Inspiration or Preparation? Explaining Creativity in Scientific
  Enterprise","Human creativity is the ultimate driving force behind scientific progress.
While the building blocks of innovations are often embodied in existing
knowledge, it is creativity that blends seemingly disparate ideas. Existing
studies have made striding advances in quantifying creativity of scientific
publications by investigating their citation relationships. Yet, little is
known hitherto about the underlying mechanisms governing scientific creative
processes, largely due to that a paper's references, at best, only partially
reflect its authors' actual information consumption. This work represents an
initial step towards fine-grained understanding of creative processes in
scientific enterprise. In specific, using two web-scale longitudinal datasets
(120.1 million papers and 53.5 billion web requests spanning 4 years), we
directly contrast authors' information consumption behaviors against their
knowledge products. We find that, of 59.0\% papers across all scientific
fields, 25.7\% of their creativity can be readily explained by information
consumed by their authors. Further, by leveraging these findings, we develop a
predictive framework that accurately identifies the most critical knowledge to
fostering target scientific innovations. We believe that our framework is of
fundamental importance to the study of scientific creativity. It promotes
strategies to stimulate and potentially automate creative processes, and
provides insights towards more effective designs of information recommendation
platforms."
scgqa_238,1311.6183v1,"In the performance comparison of different techniques in Rethinking State-Machine Replication, what happens with dependent commands?","The graph shows that with dependent-only commands, in all the approaches, except BDB, throughput decreases with the number of worker threads. This is because of the overhead of synchronization. The throughput of BDB increases up to 4 threads and then it decreases due to locking overhead.",1311.6183v1.pdf,"['1311.6183v1.pdf', '1905.12729v2.pdf', '1006.4386v1.pdf', '1311.1567v3.pdf', '2004.05579v1.pdf', '2001.07829v1.pdf', '1805.05887v1.pdf']",1311.6183v1-Figure5-1.png,Fig. 5. The effect of the number of threads on the performance independent commands (left) and dependent commands (right); maximum throughput in Kilo commands executed per second (Kcps) (top graphs); normalized per-thread throughput (bottom graphs).,"Results: With independent commands only, the throughput of all the techniques, except for BDB, compare equally with one thread (Figure 5). As threads are added, the throughput of all the techniques, except for P-SMR, decreases. For sPSMR and no-rep this happens due to scheduling overhead at the scheduler. P-SMR has better scalability than the other techniques (see bottom left graph). With dependent-only commands, in all the approaches, except BDB, throughput decreases with the number of worker threads. We attribute this to the overhead of synchronization. The throughput of BDB increases up to 4 threads and then it decreases due to locking overhead.",Rethinking State-Machine Replication for Parallelism,"State-machine replication, a fundamental approach to designing fault-tolerant
services, requires commands to be executed in the same order by all replicas.
Moreover, command execution must be deterministic: each replica must produce
the same output upon executing the same sequence of commands. These
requirements usually result in single-threaded replicas, which hinders service
performance. This paper introduces Parallel State-Machine Replication (P-SMR),
a new approach to parallelism in state-machine replication. P-SMR scales better
than previous proposals since no component plays a centralizing role in the
execution of independent commands---those that can be executed concurrently, as
defined by the service. The paper introduces P-SMR, describes a ""commodified
architecture"" to implement it, and compares its performance to other proposals
using a key-value store and a networked file system."
scgqa_239,1712.02030v2,What operational inefficiencies could affect the execution time ratios illustrated in Figure 27 of this research paper?,"There are several possible reasons why the results of the graph do not match perfectly with the theoretical time analysis. One possibility is that there is overhead from other operations in the code or operating system. Another possibility is that the Matlab ""\\"" operator has a best case O(n) and worst case O(n3) complexity, which could affect the execution time of the Projection Method.",1712.02030v2.pdf,"['1712.02030v2.pdf', '1806.05387v1.pdf', '1804.06161v2.pdf', '1912.02074v1.pdf', '1006.4386v1.pdf']",1712.02030v2-Figure27-1.png,Figure 27: Graph of the ratios of the execution times for various discretization values between the Decoupling Method and the Projection Method: Decoupling method time divided by Projection method time.,"As demonstrated in the Time Analysis section, the Projection Method should theoretically run three times faster than the Decoupling method and 27 times faster than the Saddle-Point method. Upon observing figure 26, the ratio of execution time between the Projection and Saddle-Point method seems to be dependent upon the number of discretization points M , taking on the ratio of 27 around M = 180 but then exceeding it as M grows. Figure 27 shows the ratio asymptotically approaching 3, which matches the theoretical results. Possible confounding factors that cause these results to not match perfectly with the theoretical time analysis include overhead from other operations in the code or operating system and the Matlab ""\"" operator having a best case O(n) and worst case O(n3) complexity.",Projection Method for Solving Stokes Flow,"Various methods for numerically solving Stokes Flow, where a small Reynolds
number is assumed to be zero, are investigated. If pressure, horizontal
velocity, and vertical velocity can be decoupled into three different
equations, the numerical solution can be obtained with significantly less
computation cost than when compared to solving a fully coupled system. Two
existing methods for numerically solving Stokes Flow are explored: One where
the variables can be decoupled and one where they cannot. The existing
decoupling method the limitation that the viscosity must be spatially constant.
A new method is introduced where the variables are decoupled without the
viscosity limitation. This has potential applications in the modeling of red
blood cells as vesicles to assist in storage techniques that do not require
extreme temperatures, such as those needed for cyropreservation."
scgqa_240,1309.3959v1,"In the context of the second example, what does the dashed and dotted line in Fig. 10 represent regarding decentralized risk?",The dashed and dotted line in the Bayes risk plot for the second example represents the decentralized majority vote Bayes optimal risk. This is the lowest possible Bayes risk that can be achieved by any decentralized fusion rule that is a function of the individual opinion givers' votes.,1309.3959v1.pdf,"['1309.3959v1.pdf', '1803.06598v1.pdf', '1610.00017v2.pdf', '1405.6408v2.pdf', '1512.02567v1.pdf']",1309.3959v1-Figure10-1.png,"Fig. 10. Bayes risk at convergence of the majority vote of 101opinion givers as a function of the noise standard deviationin the second example, averaged over 200 trials. The error bars indicate the standard deviation of the number of clusters over the trials. The dashed line is the centralized Bayes optimal risk, the gray line is the decentralized Chair-Varshney rule Bayes optimal risk, and the dashed and dotted line is the decentralized majority vote Bayes optimal risk.","The same plots are presented for the second example with beta-distributed initial beliefs in Fig. 7–Fig. 11 with θ = 0.025. In this example, the convergence time displays a different character than that seen in the first example, but the behavior of the number of converged clusters is similar. Also in the Bayes risk plot for this example, Fig. 10, there is one additional performance curve for comparison, the Chair-Varshney optimal fusion rule. (The Chair-Varshney rule for the first example is precisely the majority vote rule.)","Bounded Confidence Opinion Dynamics in a Social Network of Bayesian
  Decision Makers","Bounded confidence opinion dynamics model the propagation of information in
social networks. However in the existing literature, opinions are only viewed
as abstract quantities without semantics rather than as part of a
decision-making system. In this work, opinion dynamics are examined when agents
are Bayesian decision makers that perform hypothesis testing or signal
detection, and the dynamics are applied to prior probabilities of hypotheses.
Bounded confidence is defined on prior probabilities through Bayes risk error
divergence, the appropriate measure between priors in hypothesis testing. This
definition contrasts with the measure used between opinions in standard models:
absolute error. It is shown that the rapid convergence of prior probabilities
to a small number of limiting values is similar to that seen in the standard
Krause-Hegselmann model. The most interesting finding in this work is that the
number of these limiting values and the time to convergence changes with the
signal-to-noise ratio in the detection task. The number of final values or
clusters is maximal at intermediate signal-to-noise ratios, suggesting that the
most contentious issues lead to the largest number of factions. It is at these
same intermediate signal-to-noise ratios at which the degradation in detection
performance of the aggregate vote of the decision makers is greatest in
comparison to the Bayes optimal detection performance."
scgqa_241,1810.04915v1,Why did the authors choose a logarithmic scale for the x-axis in Figure 22 of their Redis experiments?,"The logscale indicates that the x-axis is a logarithmic scale, which means that the distance between each point is not equal. This is done to make it easier to see the trends in the data when there is a large range of values. In this case, the x-axis shows the record count, which ranges from 1 million to 16 million. The logscale makes it easier to see the differences in latency between the different record counts.",1810.04915v1.pdf,"['1810.04915v1.pdf', '1808.06304v2.pdf', '1209.3394v5.pdf', '1905.00569v2.pdf', '1610.08332v1.pdf']",1810.04915v1-Figure22-1.png,Figure 22. Redis: YCSB record count vs. Maximum latency,"increase of the dataset size. Fig. 22 plots the maximum latency with the record count from 1 million to 16 million, approximately up to 50GB (with update proportion = 0.1).","A Comparative Study of Consistent Snapshot Algorithms for Main-Memory
  Database Systems","In-memory databases (IMDBs) are gaining increasing popularity in big data
applications, where clients commit updates intensively. Specifically, it is
necessary for IMDBs to have efficient snapshot performance to support certain
special applications (e.g., consistent checkpoint, HTAP). Formally, the
in-memory consistent snapshot problem refers to taking an in-memory consistent
time-in-point snapshot with the constraints that 1) clients can read the latest
data items and 2) any data item in the snapshot should not be overwritten.
Various snapshot algorithms have been proposed in academia to trade off
throughput and latency, but industrial IMDBs such as Redis adhere to the simple
fork algorithm. To understand this phenomenon, we conduct comprehensive
performance evaluations on mainstream snapshot algorithms. Surprisingly, we
observe that the simple fork algorithm indeed outperforms the state-of-the-arts
in update-intensive workload scenarios. On this basis, we identify the
drawbacks of existing research and propose two lightweight improvements.
Extensive evaluations on synthetic data and Redis show that our lightweight
improvements yield better performance than fork, the current industrial
standard, and the representative snapshot algorithms from academia. Finally, we
have opensourced the implementation of all the above snapshot algorithms so
that practitioners are able to benchmark the performance of each algorithm and
select proper methods for different application scenarios."
scgqa_242,1912.03417v1,How does the query time of cross-polytope LSH change relative to brute force in AutoBlock's experiments?,"The graph shows that cross-polytope LSH is substantially faster than brute force, and the speedup improves as the number of points increases. This is because brute force has a strictly O(n) query complexity, and so the speedup should scale O(n1−ρ ) where ρ < 1.",1912.03417v1.pdf,"['1912.03417v1.pdf', '2010.13032v1.pdf', '1005.0416v1.pdf', '1902.05922v1.pdf']",1912.03417v1-Figure4-1.png,"Figure 4: The average query time speedups achieved by cross-polytope LSH over brute force on different datasets. LSH is substantially faster than brute force, and the speedup improves as the number of points increases.","Finally, we investigate the empirical performance of cross-polytope LSH. Figure 4 shows how much speedup of average query time (measured with single CPU core) cross-polytope LSH can achieve over brute force with different the number of points in the retrieval set. Huge speedup of cross-polytope over brute force is observed: when the number of points reaches 106, the former is about 40– 80 times faster than the latter. In addition, the speedup improves sub-linearly as the number of points increase, which is expected since brute force has a strictly O(n) query complexity, and so the speedup should scale O(n1−ρ ) where ρ < 1.",AutoBlock: A Hands-off Blocking Framework for Entity Matching,"Entity matching seeks to identify data records over one or multiple data
sources that refer to the same real-world entity. Virtually every entity
matching task on large datasets requires blocking, a step that reduces the
number of record pairs to be matched. However, most of the traditional blocking
methods are learning-free and key-based, and their successes are largely built
on laborious human effort in cleaning data and designing blocking keys.
  In this paper, we propose AutoBlock, a novel hands-off blocking framework for
entity matching, based on similarity-preserving representation learning and
nearest neighbor search. Our contributions include: (a) Automation: AutoBlock
frees users from laborious data cleaning and blocking key tuning. (b)
Scalability: AutoBlock has a sub-quadratic total time complexity and can be
easily deployed for millions of records. (c) Effectiveness: AutoBlock
outperforms a wide range of competitive baselines on multiple large-scale,
real-world datasets, especially when datasets are dirty and/or unstructured."
scgqa_243,1304.2109v1,What conclusions can be drawn about the intensity variations among processed images in the fingerprint algorithm presented?,"The graph shows that the intensity variation of the input image is the highest, followed by the filtered image, enhanced image, lined image, and shaped image. This is because the input image is the most noisy, and the other images are progressively filtered and enhanced to reduce the noise.",1304.2109v1.pdf,"['1304.2109v1.pdf', '1808.00136v2.pdf', '1003.1655v1.pdf', '1208.2451v1.pdf', '1204.5592v1.pdf']",1304.2109v1-Figure2-1.png,Fig. 2: Graph represents the poor versus improved image curve,"In Fig. 2, it is shown that the number of minutiae points of poor image and improved image is not similar but linear and it takes a decision that number of minutiae points in poor image is less than that of the improved image, as the poor image is so noisy compared with that of the improved images. In Fig. 3, it shows that the intensity variation and relationship among the intensity variation of the steps, such as input image, filtered image, enhanced image, lined image and shaped image.","Automatic Fingerprint Recognition Using Minutiae Matching Technique for
  the Large Fingerprint Database","Extracting minutiae from fingerprint images is one of the most important
steps in automatic fingerprint identification system. Because minutiae matching
are certainly the most well-known and widely used method for fingerprint
matching, minutiae are local discontinuities in the fingerprint pattern. In
this paper a fingerprint matching algorithm is proposed using some specific
feature of the minutiae points, also the acquired fingerprint image is
considered by minimizing its size by generating a corresponding fingerprint
template for a large fingerprint database. The results achieved are compared
with those obtained through some other methods also shows some improvement in
the minutiae detection process in terms of memory and time required."
scgqa_244,1701.06190v1,What performance conclusion can be drawn from the PR and ROC curves in Figure 8 regarding the proposed method?,"The main conclusion that can be drawn from this graph is that the proposed method has the best performance on both PR and ROC curves. This means that the proposed method is able to achieve a high recall rate while keeping the false positive rate low. This is important for a face recognition system, as it is desirable to have a high recall rate so that all faces are recognized, while also keeping the false positive rate low so that only true faces are recognized.",1701.06190v1.pdf,"['1701.06190v1.pdf', '2010.08182v3.pdf', '1911.04231v2.pdf', '1811.01194v1.pdf', '1910.09823v3.pdf', '1912.02074v1.pdf', '1907.05050v3.pdf']",1701.06190v1-Figure8-1.png,Figure 8. Comparison of PR (left) and ROC (right) curves on Pratheepan dataset.,"Comparison on Pratheepan dataset is shown in Figure 8, where it can be seen that the proposed method has the best performance on both PR and ROC curves. Moreover, it has much better performance than others even in the case of using only gray channel image. Table 3 also shows that proposed method yields good results in terms of many performance measures. Finally, the subjective comparison for Pratheepan dataset is shown in Figure 10.","A New Convolutional Network-in-Network Structure and Its Applications in
  Skin Detection, Semantic Segmentation, and Artifact Reduction","The inception network has been shown to provide good performance on image
classification problems, but there are not much evidences that it is also
effective for the image restoration or pixel-wise labeling problems. For image
restoration problems, the pooling is generally not used because the decimated
features are not helpful for the reconstruction of an image as the output.
Moreover, most deep learning architectures for the restoration problems do not
use dense prediction that need lots of training parameters. From these
observations, for enjoying the performance of inception-like structure on the
image based problems we propose a new convolutional network-in-network
structure. The proposed network can be considered a modification of inception
structure where pool projection and pooling layer are removed for maintaining
the entire feature map size, and a larger kernel filter is added instead.
Proposed network greatly reduces the number of parameters on account of removed
dense prediction and pooling, which is an advantage, but may also reduce the
receptive field in each layer. Hence, we add a larger kernel than the original
inception structure for not increasing the depth of layers. The proposed
structure is applied to typical image-to-image learning problems, i.e., the
problems where the size of input and output are same such as skin detection,
semantic segmentation, and compression artifacts reduction. Extensive
experiments show that the proposed network brings comparable or better results
than the state-of-the-art convolutional neural networks for these problems."
scgqa_245,1505.05173v6,What insights can be drawn from Figure 9 regarding the relationship between guard size and attacker-free pairs in Tor?,"The graph shows that reducing the size of the guard set has a significant impact on the fraction of attacker-free (entry, exit) pairs. With 3 guards, the fraction of attacker-free pairs is 0.8, but with 2 guards, the fraction drops to 0.6, and with 1 guard, the fraction drops to 0.4. This suggests that the guard set plays an important role in protecting the anonymity of Tor users.",1505.05173v6.pdf,"['1505.05173v6.pdf', '2005.09814v3.pdf', '1606.04646v1.pdf', '1412.4318v1.pdf', '1810.04915v1.pdf', '1905.05538v1.pdf']",1505.05173v6-Figure9-1.png,"Fig. 9: Distribution of the fraction of attacker-free (entry, exit) pairs for vanilla Tor with 3, 2, and 1 guard(s).",Figure 9 illustrates the effect that reducing the size of the,Measuring and mitigating AS-level adversaries against Tor,"The popularity of Tor as an anonymity system has made it a popular target for
a variety of attacks. We focus on traffic correlation attacks, which are no
longer solely in the realm of academic research with recent revelations about
the NSA and GCHQ actively working to implement them in practice.
  Our first contribution is an empirical study that allows us to gain a high
fidelity snapshot of the threat of traffic correlation attacks in the wild. We
find that up to 40% of all circuits created by Tor are vulnerable to attacks by
traffic correlation from Autonomous System (AS)-level adversaries, 42% from
colluding AS-level adversaries, and 85% from state-level adversaries. In
addition, we find that in some regions (notably, China and Iran) there exist
many cases where over 95% of all possible circuits are vulnerable to
correlation attacks, emphasizing the need for AS-aware relay-selection.
  To mitigate the threat of such attacks, we build Astoria--an AS-aware Tor
client. Astoria leverages recent developments in network measurement to perform
path-prediction and intelligent relay selection. Astoria reduces the number of
vulnerable circuits to 2% against AS-level adversaries, under 5% against
colluding AS-level adversaries, and 25% against state-level adversaries. In
addition, Astoria load balances across the Tor network so as to not overload
any set of relays."
scgqa_246,1603.08983v6,What evidence from Figure 9 supports the claim that lower τ values enhance ACT's effectiveness in logic circuit learning?,"The graph supports the authors' claim that ACT is a more efficient method for learning logic circuits by showing that it can significantly reduce the computational cost of learning logic circuits. This is because the ACT method allows the network to learn composite truth tables for multiple successive logic operations, which reduces the number of ponder values that need to be computed. This is evident from the clustering of the lowest τ networks around a ponder time of 5–6, which is approximately the mean number of logic gates applied per sequence.",1603.08983v6.pdf,"['1603.08983v6.pdf', '1610.04213v4.pdf', '1609.06577v1.pdf']",1603.08983v6-Figure9-1.png,Figure 9: Logic Learning Curves and Error Rates Versus Ponder Time.,"Figure 8 shows that the network reaches a minimum sequence error rate of around 0.2 without ACT (compared to 0.5 for random guessing), and virtually zero error for all τ ≤ 0.01. From Figure 9 it can be seen that low τ ACT networks solve the task very quickly, requiring about 10,000 training iterations. For higher τ values ponder time reduces to 1, at which point the networks trained with ACT behave identically to those without. For lower τ values, the spread of ponder values, and hence computational cost, is quite large. Again we speculate that this is due to the network learning more or less ‘chunked’ solutions in which composite truth table are learned for multiple successive logic operations. This is somewhat supported by the clustering of the lowest τ networks around a ponder time of 5–6, which is approximately the mean number of logic gates applied per sequence,",Adaptive Computation Time for Recurrent Neural Networks,"This paper introduces Adaptive Computation Time (ACT), an algorithm that
allows recurrent neural networks to learn how many computational steps to take
between receiving an input and emitting an output. ACT requires minimal changes
to the network architecture, is deterministic and differentiable, and does not
add any noise to the parameter gradients. Experimental results are provided for
four synthetic problems: determining the parity of binary vectors, applying
binary logic operations, adding integers, and sorting real numbers. Overall,
performance is dramatically improved by the use of ACT, which successfully
adapts the number of computational steps to the requirements of the problem. We
also present character-level language modelling results on the Hutter prize
Wikipedia dataset. In this case ACT does not yield large gains in performance;
however it does provide intriguing insight into the structure of the data, with
more computation allocated to harder-to-predict transitions, such as spaces
between words and ends of sentences. This suggests that ACT or other adaptive
computation methods could provide a generic method for inferring segment
boundaries in sequence data."
scgqa_247,1902.02518v1,"In the context of the experiments shown in Figure 2, how does the Naive agent's performance correlate with Dtarget values?","The graph shows that the performance of the Naive agent improves as the Dtarget value increases. This is because as the Dtarget value increases, more exploration is performed, and the Naive agent is more likely to find a good parameter set.",1902.02518v1.pdf,"['1902.02518v1.pdf', '1409.2897v1.pdf', '1106.3242v2.pdf']",1902.02518v1-Figure2-1.png,Figure 2: Average Fitnessp value of each generation for the Naive agent.,"This experiment investigated the effectiveness of our Fitnessp function for the Naive agent and different Dtarget values. We also performed the same analysis for a hyperagent that selects from our three skilled agents (Datalab, SeaBirds and Eagle’s Wing), based on the same score prediction models described in (Stephenson and Renz 2017a). Three Dtarget values were tested, 25%, 50% and 75%. The average Fitnessp values over all parameter sets in each generation, for both the Naive agent and the skilled hyper-agent, are shown in Figures 2 and 3 respectively. From these results","Agent-Based Adaptive Level Generation for Dynamic Difficulty Adjustment
  in Angry Birds","This paper presents an adaptive level generation algorithm for the
physics-based puzzle game Angry Birds. The proposed algorithm is based on a
pre-existing level generator for this game, but where the difficulty of the
generated levels can be adjusted based on the player's performance. This allows
for the creation of personalised levels tailored specifically to the player's
own abilities. The effectiveness of our proposed method is evaluated using
several agents with differing strategies and AI techniques. By using these
agents as models / representations of real human player's characteristics, we
can optimise level properties efficiently over a large number of generations.
As a secondary investigation, we also demonstrate that by combining the
performance of several agents together it is possible to generate levels that
are especially challenging for certain players but not others."
scgqa_248,1708.07972v1,What relationship does Figure 3 illustrate between noise level and classification success in the experiments with VGGNet features?,"The graph shows that the recognition accuracy decreases as the noise level increases. This is because as the noise level increases, the features become more noisy and less discriminative, making it more difficult for the classifier to correctly classify the frames.",1708.07972v1.pdf,"['1708.07972v1.pdf', '1202.4232v2.pdf', '2004.05448v1.pdf', '1806.05387v1.pdf', '1810.04824v1.pdf', '1905.12729v2.pdf', '1407.7736v1.pdf', '1803.11512v1.pdf']",1708.07972v1-Figure3-1.png,"Figure 3: Dependence of the recognition accuracy (%) on the noise level Xmax, VGGNet, Euclidean distance.","and 3) estimation of the MAP class for each frame (18) (hereinafter ”MAP”). The parameter n of the proposed criterion and MAP rule is tuned in order to provide the highest validation accuracy. The main results of these experiments are shown in Fig. 3, Fig. 4 and Fig. 5 for VGGNet features matched with the Euclidean distance and the KL divergence, together with the Lightened CNN features, respectively.","Maximum A Posteriori Estimation of Distances Between Deep Features in
  Still-to-Video Face Recognition","The paper deals with the still-to-video face recognition for the small sample
size problem based on computation of distances between high-dimensional deep
bottleneck features. We present the novel statistical recognition method, in
which the still-to-video recognition task is casted into Maximum A Posteriori
estimation. In this method we maximize the joint probabilistic density of the
distances to all reference still images. It is shown that this likelihood can
be estimated with the known asymptotically normal distribution of the
Kullback-Leibler discriminations between nonnegative features. The experimental
study with the LFW (Labeled Faces in the Wild), YTF (YouTube Faces) and IJB-A
(IARPA Janus Benchmark A) datasets has been provided. We demonstrated, that the
proposed approach can be applied with the state-of-the-art deep features and
dissimilarity measures. Our algorithm achieves 3-5% higher accuracy when
compared with conventional aggregation of decisions obtained for all frames."
scgqa_249,2004.04276v1,What findings regarding solution accuracy and algorithm robustness were reported for nPINNs in this research?,"The main findings of this study are that the nPINNs algorithm can accurately match the reference solutions for both problems. This demonstrates the flexibility of the algorithm and its robustness with respect to rough solutions. Additionally, the algorithm is able to achieve good performance with a relatively small number of residual points and testing points.",2004.04276v1.pdf,"['2004.04276v1.pdf', '2005.14165v4.pdf', '1710.10733v4.pdf', '1410.7867v1.pdf', '1604.06979v1.pdf']",2004.04276v1-Figure8-1.png,"Figure 8: For problem (I): nPINNs solution and exact solution on the left; trained −Lδ,αuNN and exact f on the right.","f(x) =  0 x ∈ [0, 0.51111]1 x ∈ (0.51111, 1.0], (28) for which the corresponding reference solution is computed by using discontinuous piecewise linear finite elements [7] on a grid with discretization size 2−12. Results in Figures 8 and 9 show that nPINNs can accurately match the reference solutions. In fact, we have =1.7e-03 for problem (I) using 600 residual points and 2000 testing points; and =4.9e-04 for problem (II) using 600 residual points and 2000 testing points. These results demonstrate the flexibility of the algorithm and its robustness with respect to rough solutions. We take m = 40 and M = 30 in composite Gauss quadrature.","nPINNs: nonlocal Physics-Informed Neural Networks for a parametrized
  nonlocal universal Laplacian operator. Algorithms and Applications","Physics-informed neural networks (PINNs) are effective in solving inverse
problems based on differential and integral equations with sparse, noisy,
unstructured, and multi-fidelity data. PINNs incorporate all available
information into a loss function, thus recasting the original problem into an
optimization problem. In this paper, we extend PINNs to parameter and function
inference for integral equations such as nonlocal Poisson and nonlocal
turbulence models, and we refer to them as nonlocal PINNs (nPINNs). The
contribution of the paper is three-fold. First, we propose a unified nonlocal
operator, which converges to the classical Laplacian as one of the operator
parameters, the nonlocal interaction radius $\delta$ goes to zero, and to the
fractional Laplacian as $\delta$ goes to infinity. This universal operator
forms a super-set of classical Laplacian and fractional Laplacian operators
and, thus, has the potential to fit a broad spectrum of data sets. We provide
theoretical convergence rates with respect to $\delta$ and verify them via
numerical experiments. Second, we use nPINNs to estimate the two parameters,
$\delta$ and $\alpha$. The strong non-convexity of the loss function yielding
multiple (good) local minima reveals the occurrence of the operator mimicking
phenomenon: different pairs of estimated parameters could produce multiple
solutions of comparable accuracy. Third, we propose another nonlocal operator
with spatially variable order $\alpha(y)$, which is more suitable for modeling
turbulent Couette flow. Our results show that nPINNs can jointly infer this
function as well as $\delta$. Also, these parameters exhibit a universal
behavior with respect to the Reynolds number, a finding that contributes to our
understanding of nonlocal interactions in wall-bounded turbulence."
scgqa_250,1403.5801v2,"According to the simulations in Fig. 6 from the paper, what relationship is observed between sweep rate and I-V curve linearity?","The graph shows that as the voltage sweep rate increases, the I-V characteristics become more linear. This is because the higher the voltage sweep rate, the less time the cell has to charge and discharge, resulting in a more linear I-V curve.",1403.5801v2.pdf,"['1403.5801v2.pdf', '1911.04231v2.pdf', '1810.04915v1.pdf', '1710.10571v5.pdf', '1707.04849v1.pdf', '1803.10225v1.pdf', '1710.09234v1.pdf', '2007.15176v2.pdf', '1208.4662v2.pdf']",1403.5801v2-Figure6-1.png,"Fig. 6. I-V-characteristics of single as well as complementary cell configuration: simulations for three sweep rates of 1 V/s, 10 V/s and 100 V/s. (a,b) Pickett’s model, (c,d) Laiho’s model, (e,f) Chang’s model and (g,h) Yakopcic’s model.",In Fig. 6a the simulation results for different voltage sweep rates are depicted. Note that an additional external serial,"Applicability of Well-Established Memristive Models for Simulations of
  Resistive Switching Devices","Highly accurate and predictive models of resistive switching devices are
needed to enable future memory and logic design. Widely used is the memristive
modeling approach considering resistive switches as dynamical systems. Here we
introduce three evaluation criteria for memristor models, checking for
plausibility of the I-V characteristics, the presence of a sufficiently
non-linearity of the switching kinetics, and the feasibility of predicting the
behavior of two anti-serially connected devices correctly. We analyzed two
classes of models: the first class comprises common linear memristor models and
the second class widely used non-linear memristive models. The linear memristor
models are based on Strukovs initial memristor model extended by different
window functions, while the non-linear models include Picketts physics-based
memristor model and models derived thereof. This study reveals lacking
predictivity of the first class of models, independent of the applied window
function. Only the physics-based model is able to fulfill most of the basic
evaluation criteria."
scgqa_251,1805.01772v1,"Regarding the performance graph in Figure 11, how does increasing machines affect iteration rates in the model?","The graph shows that the performance of a distributed while-loop with a trivial body on a GPU cluster decreases as the number of machines increases. This is because the loop body has no cross-device dependencies, so each machine must wait for the previous machine to finish before it can start. This results in a decrease in the number of iterations that can be completed per second.",1805.01772v1.pdf,"['1805.01772v1.pdf', '1704.00325v1.pdf', '1410.7867v1.pdf', '1403.5801v2.pdf', '1912.03417v1.pdf', '1808.09050v2.pdf', '1802.05945v1.pdf', '1202.4232v2.pdf', '1905.07512v3.pdf']",1805.01772v1-Figure11-1.png,Figure 11: Performance of a distributed while-loop with a trivial body on a GPU cluster.,"Figure 11 shows the number of iterations achieved per second as we vary the number of machines from 1 to 64. We plot the median and 5th/95th percentile performance from 5000 trials. When the loop body has no cross-device dependencies, the system can support over 20, 000 iterations per second on a single machine, decreasing to 2014with 64machines. (i.e., 457µs per iteration). If the loop contains a barrier operation, this reduces to 809 iterations per second (1235µs per iteration).",Dynamic Control Flow in Large-Scale Machine Learning,"Many recent machine learning models rely on fine-grained dynamic control flow
for training and inference. In particular, models based on recurrent neural
networks and on reinforcement learning depend on recurrence relations,
data-dependent conditional execution, and other features that call for dynamic
control flow. These applications benefit from the ability to make rapid
control-flow decisions across a set of computing devices in a distributed
system. For performance, scalability, and expressiveness, a machine learning
system must support dynamic control flow in distributed and heterogeneous
environments.
  This paper presents a programming model for distributed machine learning that
supports dynamic control flow. We describe the design of the programming model,
and its implementation in TensorFlow, a distributed machine learning system.
Our approach extends the use of dataflow graphs to represent machine learning
models, offering several distinctive features. First, the branches of
conditionals and bodies of loops can be partitioned across many machines to run
on a set of heterogeneous devices, including CPUs, GPUs, and custom ASICs.
Second, programs written in our model support automatic differentiation and
distributed gradient computations, which are necessary for training machine
learning models that use control flow. Third, our choice of non-strict
semantics enables multiple loop iterations to execute in parallel across
machines, and to overlap compute and I/O operations.
  We have done our work in the context of TensorFlow, and it has been used
extensively in research and production. We evaluate it using several real-world
applications, and demonstrate its performance and scalability."
scgqa_252,1707.02327v1,What does the violin plot in Figure 3b indicate about the distribution of fork stars in failed projects?,"The graph shows that the number of stars of the fork with the highest number of stars is generally low, with a median of 13 stars. This suggests that most forks are not popular at all, with only a few exceptions. There are two outliers in the graph, which are an audio player with 1,080 stars and a dependency injector for Android with 6,178 stars. These outliers are likely due to the fact that they were forked by major companies, which gave them a boost in popularity.",1707.02327v1.pdf,"['1707.02327v1.pdf', '1608.00887v1.pdf', '2005.14165v4.pdf', '1904.03292v2.pdf', '1809.09034v1.pdf', '1706.01341v1.pdf', '1906.02003v1.pdf', '2011.09375v1.pdf', '1811.00912v4.pdf']",1707.02327v1-Figure3-1.png,"Figure 3: Distribution of the (a) number of forks of the failed projects and (b) number of stars of the fork with the highest number of stars, for each failed project; both violin plots without outliers","Figure 3a shows the distribution of the number of forks of the failed projects. They usually have a relevant number of forks, since it is very simple to fork projects on GitHub. The first, median, and third quartile measures are 244, 400, and 638 forks, respectively. The violin plot in Figure 3b aims to reveal the relevance of these forks. For each project, we computed the fork with the highest number of stars. The violin plot shows the distribution of the number of stars of these most successful forks. As we can see, most forks are not popular at all. They are probably only used to submit pull requests or to create a copy of a repository, for backup purposes [20]. For example, the third quartile measure is 13 stars. However, there are two systems with an outlier behavior. The first one is an audio player, whose fork has 1,080 stars. In our survey, the developer of the original project answered that he abandoned the project due to other interests. However, his code was used to fork a new project, whose README acknowledges that this version “is a substantial rewrite of the fantastic work done in version 1.0 by [Projet-Owner] and others”. Besides 1,080 stars, the forked project has 70 contributors (as February, 2017). The second outlier is a dependency injector for Android, whose fork was made by Google and has 6,178 stars. The forked project’s README mentions that it “is currently in active development, primarily internally at Google, with regular pushes to the open source community"".",Why Modern Open Source Projects Fail,"Open source is experiencing a renaissance period, due to the appearance of
modern platforms and workflows for developing and maintaining public code. As a
result, developers are creating open source software at speeds never seen
before. Consequently, these projects are also facing unprecedented mortality
rates. To better understand the reasons for the failure of modern open source
projects, this paper describes the results of a survey with the maintainers of
104 popular GitHub systems that have been deprecated. We provide a set of nine
reasons for the failure of these open source projects. We also show that some
maintenance practices -- specifically the adoption of contributing guidelines
and continuous integration -- have an important association with a project
failure or success. Finally, we discuss and reveal the principal strategies
developers have tried to overcome the failure of the studied projects."
scgqa_253,1810.03742v1,"For the Sudoku sizes studied in the paper, what relationship does Fig. 10 illustrate between backbone fraction and clue density?","The graph shows that the backbone fraction decreases with increasing clue density. This is because as the clue density increases, there are fewer variables that can be frozen, and thus the backbone size decreases.",1810.03742v1.pdf,"['1810.03742v1.pdf', '1402.7063v1.pdf', '1303.1635v1.pdf', '1006.4386v1.pdf', '1906.03859v1.pdf', '1006.3688v1.pdf']",1810.03742v1-Figure10-1.png,"Fig. 10: Variation of the backbone fraction with the clue density of random ensembles of a hundred 9 × 9, 16 × 16 and 25×25 puzzles. The inset shows the respective backbone sizes.","One way to evaluate the constrainedness of Sudoku puzzles whilst preserving algorithm independency is to look at its backbone. The backbone of an instance is given by the set of its variables which are frozen, meaning they assume the same value in all solutions. Large backbones impose hardship to search methods, and as a result the backbone size is closely linked to algorithmic hardness. Random k-SAT is known to undergo a freezing phase transition just before the more famous satisfiability transition [12]. [24] have provided evidence that published Sudoku puzzles become unfrozen when a critical number of constraints are relaxed (i.e. the connectivity of the Sudoku graph in Figure 1 is decreased). Here we show that the backbone also undergoes critical behavior when the puzzles’ clue density is calibrated. The inset of Figure 10 shows the variation in the backbone size for 9× 9, 16× 16 and 25× 25","Problem Solving at the Edge of Chaos: Entropy, Puzzles and the Sudoku
  Freezing Transition","Sudoku is a widely popular $\mathcal{NP}$-Complete combinatorial puzzle whose
prospects for studying human computation have recently received attention, but
the algorithmic hardness of Sudoku solving is yet largely unexplored. In this
paper, we study the statistical mechanical properties of random Sudoku grids,
showing that puzzles of varying sizes attain a hardness peak associated with a
critical behavior in the constrainedness of random instances. In doing so, we
provide the first description of a Sudoku \emph{freezing} transition, showing
that the fraction of backbone variables undergoes a phase transition as the
density of pre-filled cells is calibrated. We also uncover a variety of
critical phenomena in the applicability of Sudoku elimination strategies,
providing explanations as to why puzzles become boring outside the typical
range of clue densities adopted by Sudoku publishers. We further show that the
constrainedness of Sudoku puzzles can be understood in terms of the
informational (Shannon) entropy of their solutions, which only increases up to
the critical point where variables become frozen. Our findings shed light on
the nature of the $k$-coloring transition when the graph topology is fixed, and
are an invitation to the study of phase transition phenomena in problems
defined over \emph{alldifferent} constraints. They also suggest advantages to
studying the statistical mechanics of popular $\mathcal{NP}$-Hard puzzles,
which can both aid the design of hard instances and help understand the
difficulty of human problem solving."
scgqa_254,1710.06548v1,Can you explain the significance of joint angles in the push recovery task for Subject2 from the paper?,"The graph titled ""Subject2 left handed person Push recovery plot for all six joint"" is a plot of the joint angles of a subject during a push recovery task. The graph shows the angles of the left hip, left knee, left ankle, right knee, right hip, and right ankle over time. The graph is significant because it provides a visual representation of the joint angles during a push recovery task, which can be used to analyze the subject's movement and identify any potential problems.",1710.06548v1.pdf,"['1710.06548v1.pdf', '1301.5201v1.pdf', '1908.05243v1.pdf']",1710.06548v1-Figure4-15-1.png,Figure 4-15:Subject2 left handed person Push recovery plot for all six joint,Figure 4-1: Person wearing HMCD Suit (a). Frontal (b). Back view ............................... 60,Data Driven Computational Model for Bipedal Walking and Push Recovery,"In this research, we have developed the data driven computational walking
model to overcome the problem with traditional kinematics based model. Our
model is adaptable and can adjust the parameter morphological similar to human.
The human walk is a combination of different discrete sub-phases with their
continuous dynamics. Any system which exhibits the discrete switching logic and
continuous dynamics can be represented using a hybrid system. In this research,
the bipedal locomotion is analyzed which is important for understanding the
stability and to negotiate with the external perturbations. We have also
studied the other important behavior push recovery. The Push recovery is also a
very important behavior acquired by human with continuous interaction with
environment. The researchers are trying to develop robots that must have the
capability of push recovery to safely maneuver in a dynamic environment. The
push is a very commonly experienced phenomenon in cluttered environment. The
human beings can recover from external push up to a certain extent using
different strategies of hip, knee and ankle. The different human beings have
different push recovery capabilities. For example a wrestler has a better push
negotiation capability compared to normal human beings. The push negotiation
capability acquired by human, therefore, is based on learning but the learning
mechanism is still unknown to researchers. The research community across the
world is trying to develop various humanoid models to solve this mystery.
Seeing all the conventional mechanics and control based models have some
inherent limitations, a learning based computational model has been developed
to address effectively this issue. In this research we will discuss how we have
framed this problem as hybrid system."
scgqa_255,1905.11471v1,How does the effectiveness of greedy XLDA differ between pretrained and randomly initialized models in the study?,"The graph shows that greedy XLDA is more effective for randomly initialized models than pretrained models. This is because randomly initialized models have less prior knowledge about the language, and therefore benefit more from the additional information provided by the cross-lingual augmentors. As can be seen in Figure 6, the LSTM baseline sees gains from greedy XLDA that are even greater than they were for BERTML. German’s XLDA performance was improved by 3.3% over using only the LSTM baseline, while the English and Hindi XLDA performances were improved by 2.1% and 1.6%, respectively. This shows that greedy XLDA can be a valuable tool for improving the performance of randomly initialized models on cross-lingual tasks.",1905.11471v1.pdf,"['1905.11471v1.pdf', '1902.03993v2.pdf', '1912.00088v1.pdf']",1905.11471v1-Figure6-1.png,Figure 6: Greedily adding cross-lingual augmentors to an LSTM baseline.,"Greedy XLDA is more effective for randomly initialized models than pretrained models. As can be seen in Figure 6, the LSTM baseline sees gains from greedy XLDA that are even greater than they were for BERTML. German’s XLDA performance was improved by 3.3% over using","XLDA: Cross-Lingual Data Augmentation for Natural Language Inference and
  Question Answering","While natural language processing systems often focus on a single language,
multilingual transfer learning has the potential to improve performance,
especially for low-resource languages. We introduce XLDA, cross-lingual data
augmentation, a method that replaces a segment of the input text with its
translation in another language. XLDA enhances performance of all 14 tested
languages of the cross-lingual natural language inference (XNLI) benchmark.
With improvements of up to $4.8\%$, training with XLDA achieves
state-of-the-art performance for Greek, Turkish, and Urdu. XLDA is in contrast
to, and performs markedly better than, a more naive approach that aggregates
examples in various languages in a way that each example is solely in one
language. On the SQuAD question answering task, we see that XLDA provides a
$1.0\%$ performance increase on the English evaluation set. Comprehensive
experiments suggest that most languages are effective as cross-lingual
augmentors, that XLDA is robust to a wide range of translation quality, and
that XLDA is even more effective for randomly initialized models than for
pretrained models."
scgqa_256,1609.06577v1,What relationship does the semi-supervised approach reveal in the figure regarding snippets per seed and system performance?,"The graph shows that the recall and precision of the system increase as the number of snippets per seed increases. This is because adding more snippets to the training set provides the system with more information about the variety of data, which helps it to better identify relevant terms.",1609.06577v1.pdf,"['1609.06577v1.pdf', '1607.08112v1.pdf', '1903.10464v3.pdf', '1908.09653v1.pdf', '1809.08207v1.pdf', '1809.01628v1.pdf', '1208.2451v1.pdf', '1703.01827v3.pdf']",1609.06577v1-Figure6-1.png,"Fig. 6. Recall and precision varying the number of contexts (snippets) per seed, 10 seeds used","To achieve comparable performances, we have conducted experiments changing the number of seeds and θc used to keep relevant terms. The results are shown in Table 4 and Figure 5. The higher the threshold, the higher the precision, while increasing the number of seeds improves the recall, which is to be expected: adding seeds “teaches” the system more about the variety of the data. Moreover, recall augments when we increase the number of contexts per seed used to train the system (Table 5 and Figure 6).","Semi-supervised knowledge extraction for detection of drugs and their
  effects","New Psychoactive Substances (NPS) are drugs that lay in a grey area of
legislation, since they are not internationally and officially banned, possibly
leading to their not prosecutable trade. The exacerbation of the phenomenon is
that NPS can be easily sold and bought online. Here, we consider large corpora
of textual posts, published on online forums specialized on drug discussions,
plus a small set of known substances and associated effects, which we call
seeds. We propose a semi-supervised approach to knowledge extraction, applied
to the detection of drugs (comprising NPS) and effects from the corpora under
investigation. Based on the very small set of initial seeds, the work
highlights how a contrastive approach and context deduction are effective in
detecting substances and effects from the corpora. Our promising results, which
feature a F1 score close to 0.9, pave the way for shortening the detection time
of new psychoactive substances, once these are discussed and advertised on the
Internet."
scgqa_257,1703.07626v1,"According to the paper's experiments, what is the power consumption difference between TCP and UDP for LOFAR traffic?",The graph shows that the power consumption of CPU and DRAM is higher when receiving TCP traffic than when receiving UDP traffic. This is because TCP traffic has a higher overhead than UDP traffic. The average power consumption of TCP in this experiment is 45.09 W and for UDP it is 40.05 W. This difference in power consumption is likely due to the additional overhead of the TCP/IP protocol stack.,1703.07626v1.pdf,"['1703.07626v1.pdf', '1909.00392v1.pdf', '1808.06304v2.pdf', '1905.12729v2.pdf', '1704.00325v1.pdf', '2009.08716v1.pdf']",1703.07626v1-Figure3-1.png,Figure 3: Power consumption of CPU and DRAM for receiving a transfer of LOFAR-like traffic over TCP and UDP sockets.,"ate 50 data streams in our experimental setup, which at 37.5 Gb/s corresponds to roughly 16 th of the total LOFAR data flow. Preliminary designs of the SKA system data flow make it likely that data transported between the CSP and SDP will have very similar characteristics, albeit with much higher data rates at longer distances. Our generator is capable of transmitting the said data stream using TCP and UDP sockets and also with TCP and UDP SoftiWarp. In Fig. 3 we show the power consumed by receiving 50 emulated LOFAR data streams using TCP sockets in the left image, and UDP sockets in the right one. The energy consumed by receiving TCP traffic is measurably higher than when using UDP due to the additional overhead of the TCP/IP protocol stack. This is a clear indication that reducing this protocol overhead will result in a smaller energy consumption. The average power consumption of TCP in this experiment is 45.09 W and for UDP it is 40.05 W. In Fig. 4 we present the power consumption measurements obtained with the LOFAR traffic generator using SoftiWarp TCP in the left image, and SoftiWarp UDP in the right image. The power consumption during","Energy-Efficient Data Transfers in Radio Astronomy with Software UDP
  RDMA","Modern radio astronomy relies on very large amounts of data that need to be
transferred between various parts of astronomical instruments, over distances
that are often in the range of tens or hundreds of kilometres. The Square
Kilometre Array (SKA) will be the world's largest radio telescope, data rates
between its components will exceed Terabits per second. This will impose a huge
challenge on its data transport system, especially with regard to power
consumption. High-speed data transfers using modern off-the-shelf hardware may
impose a significant load on the receiving system with respect to CPU and DRAM
usage. The SKA has a strict energy budget which demands a new, custom-designed
data transport solution. In this paper we present SoftiWARP UDP, an unreliable
datagram-based Remote Direct Memory Access (RDMA) protocol, which can
significantly increase the energy-efficiency of high-speed data transfers for
radio astronomy. We have implemented a fully functional software prototype of
such a protocol, supporting RDMA Read and Write operations and zero-copy
capabilities. We present measurements of power consumption and achieved
bandwidth and investigate the behaviour of all examined protocols when
subjected to packet loss."
scgqa_258,1910.10700v1,"According to the results in this paper, how does the SG method with Q1 elements behave in near-incompressibility conditions?","The SG method with Q1 elements shows the same behavior in displacement H1 error convergence here as for the first two examples, as does the same method with SRI (Figures 27a and 27b): without SRI, convergence is optimal when ν = 0.3 and poor when ν = 0.49995, while with SRI convergence is optimal throughout.",1910.10700v1.pdf,"['1910.10700v1.pdf', '1907.04002v1.pdf', '1811.01194v1.pdf', '1209.5833v2.pdf']",1910.10700v1-Figure27-1.png,Figure 27: Cube: Displacement H1 error convergence. The hypotenuse of the triangle has a slope of 1 in each case.,"The SG method with Q1 elements shows the same behaviour in displacement H 1 error convergence here as for the first two examples, as does the same method with SRI (Figures 27a and 27b): without SRI, convergence is optimal when ν = 0.3 and poor when ν = 0.49995, while with SRI convergence is optimal throughout. The original IP methods (Figure 27c) display varied performance for near-incompressibility, depending on method and element regularity, with convergence rates improving significantly with re-","Convergence in the incompressible limit of new discontinuous Galerkin
  methods with general quadrilateral and hexahedral elements","Standard low-order finite elements, which perform well for problems involving
compressible elastic materials, are known to under-perform when nearly
incompressible materials are involved, commonly exhibiting the locking
phenomenon. Interior penalty (IP) discontinuous Galerkin methods have been
shown to circumvent locking when simplicial elements are used. The same IP
methods, however, result in locking on meshes of quadrilaterals. The authors
have shown in earlier work that under-integration of specified terms in the IP
formulation eliminates the locking problem for rectangular elements. Here it is
demonstrated through an extensive numerical investigation that the effect of
using under-integration carries over successfully to meshes of more general
quadrilateral elements, as would likely be used in practical applications, and
results in accurate displacement approximations. Uniform convergence with
respect to the compressibility parameter is shown numerically. Additionally, a
stress approximation obtained here by postprocessing shows good convergence in
the incompressible limit."
scgqa_259,1711.06964v1,"According to Figure 13 in the Cyclone study, how does latency change with more physical logs during ganged operations?","The graph in Figure 13 shows that unloaded latency increases slowly as the number of active physical logs increases. This is because ganged operations require all of the cores to send replication messages to their replicas before they can be processed. This can increase the amount of serial latency, as all of the cores must wait for each other before they can continue processing the request.",1711.06964v1.pdf,"['1711.06964v1.pdf', '1910.04573v3.pdf', '1910.00110v2.pdf', '1512.02567v1.pdf', '1911.09804v2.pdf', '1505.02851v1.pdf', '1912.00035v1.pdf', '1403.5617v1.pdf']",1711.06964v1-Figure13-1.png,Figure 13: Ganged Operations,"We now evaluate ganged operations. The primary purpose of ganged operations is to avoid the need for distributed transactions to manipulate what is a single shared memory image and therefore we were most concerned about unloaded latency given the complexity of synchronizing different replication quorums as well executing our rendezvous protocol on multiple cores. We therefore setup an experiment where a single client – reflecting the unloaded case – made ganged requests to the replicas. We varied the number of physical logs participating in the request from one to the full complement of 8. Figure 13 shows the results. Unloaded latency increases slowly as we increase the number of active physical logs - to around 32 us from the baseline of 21 us. The reason for this increase is that the DPDK userspace NIC driver pipelines request processing for communication between the CPU core and the NIC. Simultaneous replication on multiple physical logs represents a worst case for this arrangement as all eight cores try to send replication messages to their replicas, adding about a microsecond of serial latency each, to dispatch requests to the NIC. Regardless, the experiment underlines the value of our ganged operation design over a distributed transaction that would need 8 network hops for the two phase commit (and replication) using an external co-ordinator, a minimum of 80us.",Cyclone: High Availability for Persistent Key Value Stores,"Persistent key value stores are an important component of many distributed
data serving solutions with innovations targeted at taking advantage of growing
flash speeds. Unfortunately their performance is hampered by the need to
maintain and replicate a write ahead log to guarantee availability in the face
of machine and storage failures. Cyclone is a replicated log plug-in for key
value stores that systematically addresses various sources of this bottleneck.
It uses a small amount of non-volatile memory directly addressable by the CPU -
such as in the form of NVDIMMs or Intel 3DXPoint - to remove block oriented IO
devices such as SSDs from the critical path for appending to the log. This
enables it to address network overheads using an implementation of the RAFT
consensus protocol that is designed around a userspace network stack to relieve
the CPU of the burden of data copies. Finally, it provides a way to efficiently
map the commutativity in key-value store APIs to the parallelism available in
commodity NICs. Cyclone is able to replicate millions of small updates per
second using only commodity 10 gigabit ethernet adapters. As a practical
application, we use it to improve the performance (and availability) of
RocksDB, a popular persistent key value store by an order of magnitude when
compared to its own write ahead log without replication."
scgqa_260,1811.01194v1,What trends in misclassification rates do audiovisual architectures display when tested against audio-only networks in high noise levels?,"The audiovisual architectures achieve higher MCRs than audio-only and visual-only architectures, with the relative improvement being larger under extreme noisy conditions. This suggests that audiovisual architectures are more robust to noise and can better handle noisy speech.",1811.01194v1.pdf,"['1811.01194v1.pdf', '2008.02777v1.pdf', '1708.09328v1.pdf']",1811.01194v1-Figure5-1.png,Fig. 5. MCR for audio and three audiovisual networks using DEMAND noise database (25dB correspond to speech without additive noise).,"In Section 7.2 we reported results averaged over 7 different noise levels and 6 different categories. We provide here a more detailed analysis, by breaking down the error rates by noise category and noise level. In Fig. 5 we present MCR attained by 5 architectures, where each MCR is derived as the average over all noise categories (i.e. 6 × 25000 = 150000 videos). Although the relative improvement attained by audiovisual architectures is larger under extreme noisy conditions (-10dB to -5dB), notable improvements are attained even in higher SNRs (0dB to 20dB) as well as on speech without additive noise. Moreover, by comparing the MCR of our best visual-only network (V15) with our best audio-only network (A4) we conclude that the performance of the former is equivalent to that of the latter with SNR ≈ −4.5 additive noise.","Pushing the boundaries of audiovisual word recognition using Residual
  Networks and LSTMs","Visual and audiovisual speech recognition are witnessing a renaissance which
is largely due to the advent of deep learning methods. In this paper, we
present a deep learning architecture for lipreading and audiovisual word
recognition, which combines Residual Networks equipped with spatiotemporal
input layers and Bidirectional LSTMs. The lipreading architecture attains
11.92% misclassification rate on the challenging Lipreading-In-The-Wild
database, which is composed of excerpts from BBC-TV, each containing one of the
500 target words. Audiovisual experiments are performed using both intermediate
and late integration, as well as several types and levels of environmental
noise, and notable improvements over the audio-only network are reported, even
in the case of clean speech. A further analysis on the utility of target word
boundaries is provided, as well as on the capacity of the network in modeling
the linguistic context of the target word. Finally, we examine difficult word
pairs and discuss how visual information helps towards attaining higher
recognition accuracy."
scgqa_261,1902.02518v1,"In the context of the adaptive level generation for Angry Birds, what does the Fitnessp value represent for the Naive agent?","The Fitnessp value is a measure of the performance of the Naive agent. It is calculated by taking the average of the scores of all the parameter sets in a generation. The higher the Fitnessp value, the better the performance of the Naive agent.",1902.02518v1.pdf,"['1902.02518v1.pdf', '1311.1567v3.pdf', '1712.03538v1.pdf', '2006.03632v1.pdf', '2009.06124v1.pdf', '1701.08947v1.pdf', '2005.09634v1.pdf']",1902.02518v1-Figure2-1.png,Figure 2: Average Fitnessp value of each generation for the Naive agent.,"This experiment investigated the effectiveness of our Fitnessp function for the Naive agent and different Dtarget values. We also performed the same analysis for a hyperagent that selects from our three skilled agents (Datalab, SeaBirds and Eagle’s Wing), based on the same score prediction models described in (Stephenson and Renz 2017a). Three Dtarget values were tested, 25%, 50% and 75%. The average Fitnessp values over all parameter sets in each generation, for both the Naive agent and the skilled hyper-agent, are shown in Figures 2 and 3 respectively. From these results","Agent-Based Adaptive Level Generation for Dynamic Difficulty Adjustment
  in Angry Birds","This paper presents an adaptive level generation algorithm for the
physics-based puzzle game Angry Birds. The proposed algorithm is based on a
pre-existing level generator for this game, but where the difficulty of the
generated levels can be adjusted based on the player's performance. This allows
for the creation of personalised levels tailored specifically to the player's
own abilities. The effectiveness of our proposed method is evaluated using
several agents with differing strategies and AI techniques. By using these
agents as models / representations of real human player's characteristics, we
can optimise level properties efficiently over a large number of generations.
As a secondary investigation, we also demonstrate that by combining the
performance of several agents together it is possible to generate levels that
are especially challenging for certain players but not others."
scgqa_262,1604.06979v1,"According to the findings in the paper, how do the TFG and short time variance relate to abnormal cardiac motion?","The TFG is a time-frequency representation of the image sequence. It is computed by taking the Fourier transform of the image sequence, and then taking the absolute value of the Fourier transform. The short time variance plot is a plot of the variance of the TFG over time. Abnormal motions can be detected by examining the short time variance plot. If the variance of the TFG is high in a particular region, then this indicates that there is an abnormal motion in that region.",1604.06979v1.pdf,"['1604.06979v1.pdf', '1712.03538v1.pdf', '1912.00035v1.pdf', '1707.04849v1.pdf', '1806.05387v1.pdf', '1910.05107v2.pdf', '1610.08534v1.pdf']",1604.06979v1-Figure2-1.png,Fig. 2. Abnormal motion detection,"Abnormal motions can be detected by a local analysis on TFGs, such as energy content in the signal in short time intervals. For this experiment, a synthetic US image sequence was generated in which a beat-pause was introduced in 58 frames (∼ 2 seconds). The myocardial point on this sequence was automatically detected first using a series of morphological operations such as smoothing, binarization, closing and fill holes. Next the TFG was computed for this myocardial point and is as shown in Fig.2(a). The length of the beat-pause is detected by examining the signal variance associated with beat-pulse region. This is larger in normal regions compared to the beat-pause region as it does not have any energy. Hence, a short-time-variance",Cardiac Motion Analysis by Temporal Flow Graphs,"Cardiac motion analysis from B-mode ultrasound sequence is a key task in
assessing the health of the heart. The paper proposes a new methodology for
cardiac motion analysis based on the temporal behaviour of points of interest
on the myocardium. We define a new signal called the Temporal Flow Graph (TFG)
which depicts the movement of a point of interest over time. It is a graphical
representation derived from a flow field and describes the temporal evolution
of a point. We prove that TFG for an object undergoing periodic motion is also
periodic. This principle can be utilized to derive both global and local
information from a given sequence. We demonstrate this for detecting motion
irregularities at the sequence, as well as regional levels on real and
synthetic data. A coarse localisation of anatomical landmarks such as centres
of left/right cavities and valve points is also demonstrated using TFGs."
scgqa_263,1101.0235v1,"In the context of the photo manipulation findings, which technology enhances performance for web applications according to figure 6?",The findings of this study suggest that web developers who are looking to create web applications that include image manipulation features should consider using the Canvas layer rendering system. This is because the Canvas layer is the most performant web graphics technology and will provide the best user experience.,1101.0235v1.pdf,"['1101.0235v1.pdf', '1610.00017v2.pdf', '1607.08112v1.pdf', '1708.07972v1.pdf']",1101.0235v1-Figure7-1.png,Figure 7. Measured delta movement times against a scripted mouse move of a photo,"LIST OF FIGURES Figure 1. Impression of Flickr, PhotoBucket and Picasa websites showing the grid-like layout . 16 Figure 2. Mock-up of the main web page web describing the overall interface .......................... 19 Figure 3. The MVC architecture as used with the prototype ...................................................... 19 Figure 4. A UML model of the relations between the data objects used within the application .. 20 Figure 5. Representation of the internals of our JavaScript web graphics rendering engine ....... 22 Figure 6. Representation of the Canvas layer rendering system ................................................. 25 Figure 7. Measured delta movement times against a scripted mouse move of a photo ............... 38 Figure 8. Average CPU utilization during a scripted mouse move of a photo. ........................... 39 Figure 9. Delta movement times of a scripted photo mouse move with the colours inverted. ..... 40 Figure 10. Average CPU utilization during a mouse moveof a photo with the colours inverted. 41 Figure 11. Fluctuations in memory usage during the move experiment ..................................... 42 Figure 12. Memory fluctuations during mouse move with colours inverted effect applied......... 43 Figure 13. Rotation times of a single photo when multiple photos on the page. ......................... 44 LIST OF TABLES Table 1. Comparison of web graphics on image manipulation operations ................................. 28 Table 2. Average application times in milliseconds of a crop operation (50, 50, 300, 300) ........ 36 Table 3. Average application times in milliseconds of applying a 70° rotation to a photo. ......... 37 Table 4. Average application times of applying a grayscale effect to a photo. ........................... 37 Table 5. Average application times of inverting the colours within a photo. .............................. 38","Analysis of Using Browser-native Technology to Build Rich Internet
  Applications for Image Manipulation","In this work we investigate whether browser-native technologies can be used
to perform photo manipulation tasks e.g cropping, resizing or rotating an image
within the current mainstream browser. By the use of a case study we will
analyze problems that have occurred during the implementation of a prototype
web application that utilizes browser-native web technology in order to create
an online version of a real world photo scrapbook. Implementation of a
prototype will allows us to analyze the strengths and weaknesses of current web
technology when it comes to browser-based image manipulation. Furthermore we
explore the possibilities of the Ajax in combination Canvas, SVG and VML to
provide a more interactive graphical user interface to perform image
manipulation tasks on the web."
scgqa_264,1801.09097v2,What correlation does Figure 5 illustrate between illusive training samples and model over-fitting in the CIFAR-10 dataset?,"The graph shows that there is a correlation between over-fitting and the illusive samples that cannot be mapped to the right equivalence classes. This is because over-fitting occurs when the model learns the noise in the training data, which can lead to the model making incorrect predictions on new data. The illusive samples are those that are consistently misclassified by the model, and they can be seen as a form of noise in the training data. As a result, the model is more likely to over-fit when it is trained on data that contains illusive samples.",1801.09097v2.pdf,"['1801.09097v2.pdf', '1804.03842v1.pdf']",1801.09097v2-Figure5-1.png,Figure 5: Training without consistently misclassified samples help prevent over-fitting.,"We begin by retraining the network without these socalled illusive training samples that are consistently misclassified. As Figure 5 shows, despite a slight drop in test accuracy during early epochs, over-fitting is alleviated. If we further remove test samples that are consistently misclassified (i.e., we “cheat” a bit as our goal is studying the topological properties of the input space rather than achieving state-of-the-art accuracy), over-fitting is weakened even more. A correlation seems to exist between over-fitting and the illusive samples that cannot be mapped to the right equivalence classes. Moreover, the rationale behind dataset bias and over-fitting may essentially be the same; the difference is that over-fitting occurs at equivalence classes that are already observed whereas dataset bias happens at un-",Towards an Understanding of Neural Networks in Natural-Image Spaces,"Two major uncertainties, dataset bias and adversarial examples, prevail in
state-of-the-art AI algorithms with deep neural networks. In this paper, we
present an intuitive explanation for these issues as well as an interpretation
of the performance of deep networks in a natural-image space. The explanation
consists of two parts: the philosophy of neural networks and a hypothetical
model of natural-image spaces. Following the explanation, we 1) demonstrate
that the values of training samples differ, 2) provide incremental boost to the
accuracy of a CIFAR-10 classifier by introducing an additional ""random-noise""
category during training, 3) alleviate over-fitting thereby enhancing the
robustness against adversarial examples by detecting and excluding illusive
training samples that are consistently misclassified. Our overall contribution
is therefore twofold. First, while most existing algorithms treat data equally
and have a strong appetite for more data, we demonstrate in contrast that an
individual datum can sometimes have disproportionate and counterproductive
influence and that it is not always better to train neural networks with more
data. Next, we consider more thoughtful strategies by taking into account the
geometric and topological properties of natural-image spaces to which deep
networks are applied."
scgqa_265,1412.4318v1,"In the study represented by Fig. 8.3, how does the proposed scheme manage bandwidth allocation among video sessions?","The graph shows that the allocated bandwidth for each video session is gradually decreased with the decrease of popularity in the proposed scheme. This is because the proposed scheme allocates bandwidth to video sessions based on their popularity, with more bandwidth being allocated to more popular sessions. The maximum allowable bandwidth βmax can be allocated for more than one video session depending on the network bandwidth and the traffic conditions. However, the allocated bandwidth for any of the active broadcasting/multicasting video sessions does not go below a certain threshold, which is set to be 0.4 in this study. This is done to ensure that these sessions are always able to provide a certain level of quality of service (QoS).",1412.4318v1.pdf,"['1412.4318v1.pdf', '1611.04706v2.pdf', '1107.4161v1.pdf', '1808.06818v1.pdf']",1412.4318v1-Figure8.3-1.png,"Fig. 8.3: Allocated bandwidths for various video sessions when 30 video sessions are active (a) scenario 1 traffic environment, (b) scenario 2 traffic environment.","Fig. 8.3 shows the allocated bandwidth for each of the video sessions when 30 video sessions are active. It shows that the allocated bandwidth to a video session is gradually decreased with the decrease of popularity in the proposed scheme. The maximum allowable bandwidth βmax can be allocated for more than one video sessions depending on the network bandwidth and the traffic conditions. However, the allocated bandwidth for any of the active broadcasting/multicasting video sessions does not go below a","Adaptive Resource Management for Multimedia Applications in
  Femtocellular and Macrocellular Networks","The increasing demands of various high data rate wireless applications have
been seen in the recent years and it will continue in the future. To fulfill
these demands, the limited existing wireless resources should be utilized
properly or new wireless technology should be developed. Therefore, we propose
some novel idea to manage the wireless resources and deployment of
femtocellular network technology. The study was mainly divided into two parts:
(a) femtocellular network deployment and resource allocation and (b) resource
management for macrocellular networks. The femtocellular network deployment
scenarios, integrated femtocell/macrocell network architectures, cost-effective
frequency planning, and mobility management schemes are presented in first
part. In the second part, we provide a CAC based on adaptive bandwidth
allocation for the wireless network in. The proposed CAC relies on adaptive
multi-level bandwidth-allocation scheme for non-real-time calls. We propose
video service provisioning over wireless networks. We provide a QoS adaptive
radio resource allocation as well as popularity based bandwidth allocation
schemes for scalable videos over wireless cellular networks. All the proposed
schemes are verified through several numerical and simulation results. The
research results presented in this dissertation clearly imply the advantages of
our proposed schemes."
scgqa_266,1805.07914v3,"How does the agent's performance on the easy task compare to the hard task in CoinRun, according to Figure 5?","The graph shows that the agent is able to learn to solve the easy task almost immediately, but it takes longer to learn to solve the hard task. However, the agent is able to learn to solve both tasks, and it performs significantly better than BCO.",1805.07914v3.pdf,"['1805.07914v3.pdf', '1606.01062v1.pdf', '1604.04026v1.pdf', '1712.02030v2.pdf', '1904.03292v2.pdf', '1903.10464v3.pdf', '1907.10906v1.pdf', '1910.11127v1.pdf']",1805.07914v3-Figure5-1.png,Figure 5: CoinRun imitation learning results. The trials were averaged over 50 runs for ILPO and BCO the policy was evaluated every 200 steps and averaged out of 10 policy runs. The reward used for training the expert and evaluation was +10 after reaching the coin.,"Figure 5 shows the results for imitation learning. In both the easy and hard tasks, ILPO was not able to perform as well as the expert, but performed significantly better than BCO. As this environment is high-dimensional, it takes more steps to learn the alignment policy than the previous experiments. However, ILPO often learned to solve the task almost immediately, but some random seeds led to bad initialization that resulted in the agent not learning at all. However, good initialization sometimes allows the agent to learn good initial policies zero-shot (see videos in the supplementary for an example). As such, we found that it was possible for the agent to sometimes perform as well as the expert. The results consist of all of the seeds averaged, including those that yielded poor results.",Imitating Latent Policies from Observation,"In this paper, we describe a novel approach to imitation learning that infers
latent policies directly from state observations. We introduce a method that
characterizes the causal effects of latent actions on observations while
simultaneously predicting their likelihood. We then outline an action alignment
procedure that leverages a small amount of environment interactions to
determine a mapping between the latent and real-world actions. We show that
this corrected labeling can be used for imitating the observed behavior, even
though no expert actions are given. We evaluate our approach within classic
control environments and a platform game and demonstrate that it performs
better than standard approaches. Code for this work is available at
https://github.com/ashedwards/ILPO."
scgqa_267,1808.06304v2,"According to the findings presented in the paper, how does the STAMP model's performance vary with training dataset size?","The graph shows that the accuracy of the STAMP model increases with the number of training examples. This is a positive result, as it suggests that the model is able to learn from more data and improve its performance. The graph also shows that the accuracy of the model increases at a logarithmic rate, which means that the model learns more quickly at first and then slows down as it approaches its maximum accuracy. This is also a positive result, as it suggests that the model is able to learn efficiently and does not require a large number of training examples to achieve good performance.",1808.06304v2.pdf,"['1808.06304v2.pdf', '1305.1657v1.pdf', '2001.07829v1.pdf']",1808.06304v2-Figure2-1.png,"Figure 2: Semantic parsing accuracies of the STAMP model on WikiSQL. The x-axis is the training data size in log-scale, and the y-axis includes two evaluation metrics Acclf and Accex.","In this experiment, we randomly sample 20 subsets of examples from the WikiSQL training data, incrementally increased by 3K examples (about 1/20 of the full WikiSQL training data). We use the same training protocol and report the accuracy of the STAMP model on the dev set. Results are given in Figure 2. It is not surprising that more training examples bring higher accuracy. Interestingly, we observe that both accuracies of the neural network based semantic parser grow logarith-",Question Generation from SQL Queries Improves Neural Semantic Parsing,"We study how to learn a semantic parser of state-of-the-art accuracy with
less supervised training data. We conduct our study on WikiSQL, the largest
hand-annotated semantic parsing dataset to date. First, we demonstrate that
question generation is an effective method that empowers us to learn a
state-of-the-art neural network based semantic parser with thirty percent of
the supervised training data. Second, we show that applying question generation
to the full supervised training data further improves the state-of-the-art
model. In addition, we observe that there is a logarithmic relationship between
the accuracy of a semantic parser and the amount of training data."
scgqa_268,2002.12489v3,What trend does the performance of the cm-SSFT model exhibit with varying auxiliary set sizes on the MM01 dataset?,"The graph shows that the performance of the model increases as the size of the auxiliary set increases. However, the performance saturates quickly after a certain point. This suggests that the model is able to learn from a limited number of auxiliary images, and that additional images do not provide much benefit.",2002.12489v3.pdf,"['2002.12489v3.pdf', '1902.02518v1.pdf', '1511.04338v2.pdf', '1910.10700v1.pdf', '1807.06736v1.pdf', '1807.09483v2.pdf', '1708.01249v1.pdf']",2002.12489v3-Figure4-1.png,Figure 4. Influence of number of queries. Dashed lines correspond to the SOTA method. Solid lines correspond to ours.,"Besides, we also test the influence of the auxiliary set. The experiments are run on MM01 dataset for its large query set. We randomly sample n images from the query sets and watch the performance changing. For a specific n, we run 10 times to get the average performance. n is ranging from 1 (single query) to all query size. The results are shown in Figure 4. We can see that with the size of the auxiliary set growing, the performance saturates quickly.","Cross-modality Person re-identification with Shared-Specific Feature
  Transfer","Cross-modality person re-identification (cm-ReID) is a challenging but key
technology for intelligent video analysis. Existing works mainly focus on
learning common representation by embedding different modalities into a same
feature space. However, only learning the common characteristics means great
information loss, lowering the upper bound of feature distinctiveness. In this
paper, we tackle the above limitation by proposing a novel cross-modality
shared-specific feature transfer algorithm (termed cm-SSFT) to explore the
potential of both the modality-shared information and the modality-specific
characteristics to boost the re-identification performance. We model the
affinities of different modality samples according to the shared features and
then transfer both shared and specific features among and across modalities. We
also propose a complementary feature learning strategy including modality
adaption, project adversarial learning and reconstruction enhancement to learn
discriminative and complementary shared and specific features of each modality,
respectively. The entire cm-SSFT algorithm can be trained in an end-to-end
manner. We conducted comprehensive experiments to validate the superiority of
the overall algorithm and the effectiveness of each component. The proposed
algorithm significantly outperforms state-of-the-arts by 22.5% and 19.3% mAP on
the two mainstream benchmark datasets SYSU-MM01 and RegDB, respectively."
scgqa_269,1207.3107v3,What does the experimental data in Fig. 14 illustrate regarding runtime differences among algorithms for recovery of Bernoulli-Rademacher signals?,"The graph shows that the runtime of the algorithms increases with the signal length N. This is because the algorithms need to process more data as the signal length increases. However, the runtime of the algorithms also depends on the signal sparsity. For example, the runtime of EM-GM-AMP is much lower than that of T-MSBL and OMP for the same signal length N. This is because EM-GM-AMP is a more efficient algorithm for sparse signals.",1207.3107v3.pdf,"['1207.3107v3.pdf', '1912.02074v1.pdf', '1805.07914v3.pdf', '1804.04290v1.pdf', '1710.06548v1.pdf', '1908.04655v1.pdf', '2008.11326v4.pdf', '2005.14165v4.pdf']",1207.3107v3-Figure14-1.png,Fig. 14. Runtime versus signal lengthN for noisy recovery of BernoulliRademacher signals.,"Next we investigated how complexity scales with signal length N by evaluating the runtime of each algorithm on a typical personal computer. For this, we fixed K/N = 0.1, M/N = 0.5, SNR = 25 dB and varied the signal length N . Figure 14 shows the runtimes for noisy recovery of a Bernoulli-Rademacher signal, while Fig. 15 shows the corresponding NMSEs. In these plots, each datapoint represents an average over R = 50 realizations. The algorithms that we tested are the same ones that we described earlier. However, to fairly evaluate runtime, we configured some a bit differently than before. In particular, for genie-tuned SPGL1, in order to yield a better runtime-vs-NMSE tradeoff, we reduced the tolerance grid (recall footnote 14) to σ2 ∈ {0.6, 0.8, . . . , 1.4}×Mψ and turned off debiasing. For OMP and SP, we used the fixed support size Klasso = MρSE( M N ) rather than searching for the size that minimizes NMSE over a grid of 10 hypotheses, as before. Otherwise, all algorithms were run under the suggested defaults, with T-MSBL run under noise=small and EMGM-AMP run in “sparse” mode.",Expectation-Maximization Gaussian-Mixture Approximate Message Passing,"When recovering a sparse signal from noisy compressive linear measurements,
the distribution of the signal's non-zero coefficients can have a profound
effect on recovery mean-squared error (MSE). If this distribution was apriori
known, then one could use computationally efficient approximate message passing
(AMP) techniques for nearly minimum MSE (MMSE) recovery. In practice, though,
the distribution is unknown, motivating the use of robust algorithms like
LASSO---which is nearly minimax optimal---at the cost of significantly larger
MSE for non-least-favorable distributions. As an alternative, we propose an
empirical-Bayesian technique that simultaneously learns the signal distribution
while MMSE-recovering the signal---according to the learned
distribution---using AMP. In particular, we model the non-zero distribution as
a Gaussian mixture, and learn its parameters through expectation maximization,
using AMP to implement the expectation step. Numerical experiments on a wide
range of signal classes confirm the state-of-the-art performance of our
approach, in both reconstruction error and runtime, in the high-dimensional
regime, for most (but not all) sensing operators."
scgqa_270,1604.04026v1,What consistent trend is illustrated in the graph regarding SRCD's running time for 100 iterations with a single thread?,The graph shows that the running time of the proposed algorithm SRCD for 100 iterations with different number of latent components using 1 thread linearly increases. This is consistent with the theoretical analyses about fast convergence and linear complexity for large sparse datasets in Section III.,1604.04026v1.pdf,"['1604.04026v1.pdf', '1912.08775v1.pdf', '1501.06137v1.pdf', '1707.02342v1.pdf', '2003.00870v1.pdf', '1506.06213v1.pdf', '2008.01961v3.pdf', '1902.05312v2.pdf']",1604.04026v1-Figure4-1.png,Fig. 4: Running time of 100 iterations with r = 50 and using different number of threads,"This section investigates running the proposed algorithm on large datasets with different settings. Figure 3 shows the running time of Algorithm SRCD for 100 iterations with different number of latent component using 1 thread. Clearly, the running time linearly increases, which fits the theoretical analyses about fast convergence and linear complexity for large sparse datasets in Section III. Furthermore, concerning the parallel algorithm, the running time of Algorithm SRCD for 100 iterations significantly decreases when the number of used threads increases in Figure 4. In addition, the running time is acceptable for large applications. Hence, these results indicate that the proposed algorithm SRCD is feasible for large scale applications.","Fast Parallel Randomized Algorithm for Nonnegative Matrix Factorization
  with KL Divergence for Large Sparse Datasets","Nonnegative Matrix Factorization (NMF) with Kullback-Leibler Divergence
(NMF-KL) is one of the most significant NMF problems and equivalent to
Probabilistic Latent Semantic Indexing (PLSI), which has been successfully
applied in many applications. For sparse count data, a Poisson distribution and
KL divergence provide sparse models and sparse representation, which describe
the random variation better than a normal distribution and Frobenius norm.
Specially, sparse models provide more concise understanding of the appearance
of attributes over latent components, while sparse representation provides
concise interpretability of the contribution of latent components over
instances. However, minimizing NMF with KL divergence is much more difficult
than minimizing NMF with Frobenius norm; and sparse models, sparse
representation and fast algorithms for large sparse datasets are still
challenges for NMF with KL divergence. In this paper, we propose a fast
parallel randomized coordinate descent algorithm having fast convergence for
large sparse datasets to archive sparse models and sparse representation. The
proposed algorithm's experimental results overperform the current studies' ones
in this problem."
scgqa_271,2010.13032v1,How is the performance degradation of normal agents illustrated in the graph for human action recognition with Byzantine interference in the paper?,"The graph shows that the presence of Byzantine agents has a negative impact on the performance of normal agents in the human action recognition task. This is evident from the fact that the average testing loss and accuracy of normal agents decreases as the number of Byzantine agents increases. For example, when there are 10 Byzantine agents, the average testing loss of normal agents increases by 0.05, and the average accuracy decreases by 2%. This suggests that Byzantine agents can significantly disrupt the performance of normal agents in the human action recognition task.",2010.13032v1.pdf,"['2010.13032v1.pdf', '1402.1892v2.pdf', '2003.13216v1.pdf', '1903.10464v3.pdf', '1805.05887v1.pdf', '2006.03632v1.pdf', '1701.00365v2.pdf', '1809.02337v2.pdf', '1708.01249v1.pdf']",2010.13032v1-Figure2-1.png,Figure 2: Human Action Recognition: average testing loss and accuracy for normal agents.,"We plot the mean and range of the average loss of every normal agent using streaming data for the target localization problem in Figure 1b–d. Similarly, we plot the mean and range of the average testing loss and classification accuracy of every normal agent for human action recognition in Figure 2, and for digit classification in Figure 3 (for group 1) and Figure 4 (for group 2). At each iteration, Byzantine agents send random values (for each dimension) from the interval [15, 16] for target localization, and [0, 0.1] for the other two case studies.",Byzantine Resilient Distributed Multi-Task Learning,"Distributed multi-task learning provides significant advantages in
multi-agent networks with heterogeneous data sources where agents aim to learn
distinct but correlated models simultaneously.However, distributed algorithms
for learning relatedness among tasks are not resilient in the presence of
Byzantine agents. In this paper, we present an approach for Byzantine resilient
distributed multi-task learning. We propose an efficient online weight
assignment rule by measuring the accumulated loss using an agent's data and its
neighbors' models. A small accumulated loss indicates a large similarity
between the two tasks. In order to ensure the Byzantine resilience of the
aggregation at a normal agent, we introduce a step for filtering out larger
losses. We analyze the approach for convex models and show that normal agents
converge resiliently towards the global minimum.Further, aggregation with the
proposed weight assignment rule always results in an improved expected regret
than the non-cooperative case. Finally, we demonstrate the approach using three
case studies, including regression and classification problems, and show that
our method exhibits good empirical performance for non-convex models, such as
convolutional neural networks."
scgqa_272,1809.01628v1,"In the results shown in Figure 6, what does the trend indicate about kDN's impact on DCS technique accuracy?",The shape of the graph tells us that the kDN measure is a good predictor of the accuracy rate of both DCS techniques. This is because the accuracy rate of both techniques increases as the kDN value increases. This suggests that the kDN measure can be used to select samples that are more likely to be correctly classified by the DCS techniques.,1809.01628v1.pdf,"['1809.01628v1.pdf', '1603.04153v1.pdf', '1906.09756v1.pdf', '1907.04002v1.pdf', '1708.05355v1.pdf']",1809.01628v1-Figure6-1.png,"Figure 6: Mean accuracy rate of OLA and LCA for each group of kDN value, for all datasets from Table 1. The neighborhood sizes of the DCS techniques and the kDN measure are ks = kh = 7.","In order to further characterize the relationship between the kDN measure and the DCS techniques, all samples from each dataset were grouped by their true hardness value and the mean accuracy rate of both DCS techniques on each group was calculated. The results are summarized in Figure 6.","Online local pool generation for dynamic classifier selection: an
  extended version","Dynamic Classifier Selection (DCS) techniques have difficulty in selecting
the most competent classifier in a pool, even when its presence is assured.
Since the DCS techniques rely only on local data to estimate a classifier's
competence, the manner in which the pool is generated could affect the choice
of the best classifier for a given sample. That is, the global perspective in
which pools are generated may not help the DCS techniques in selecting a
competent classifier for samples that are likely to be mislabelled. Thus, we
propose in this work an online pool generation method that produces a locally
accurate pool for test samples in difficult regions of the feature space. The
difficulty of a given area is determined by the classification difficulty of
the samples in it. That way, by using classifiers that were generated in a
local scope, it could be easier for the DCS techniques to select the best one
for the difficult samples. For the query samples in easy regions, a simple
nearest neighbors rule is used. In the extended version of this work, a deep
analysis on the correlation between instance hardness and the performance of
DCS techniques is presented. An instance hardness measure that conveys the
degree of local class overlap is then used to decide when the local pool is
used in the proposed scheme. The proposed method yielded significantly greater
recognition rates in comparison to a Bagging-generated pool and two other
global pool generation schemes for all DCS techniques evaluated. The proposed
scheme's performance was also significantly superior to three state-of-the-art
classification models and statistically equivalent to five of them. Moreover,
an extended analysis on the computational complexity of the proposed method and
of several DS techniques is presented in this version. We also provide the
implementation of the proposed technique using the DESLib library on GitHub."
scgqa_273,1907.05050v3,"In Figure 4, how does the approximation γ̂(θ(t), η(t)) differ from the steady-state control law?","The graph shows that the ideal steady-state control law u⋆(w) is a constant value, while its approximation γ̂(θ(t), η(t)) is a function of time. This is because the ideal control law is based on the knowledge of the true state of the system, while the approximation is based on noisy measurements of the state. As a result, the approximation is subject to error, which increases over time as the measurements become less accurate.",1907.05050v3.pdf,"['1907.05050v3.pdf', '1801.06867v1.pdf', '1609.06577v1.pdf', '1902.05922v1.pdf', '1603.01185v2.pdf', '1307.3687v1.pdf', '1911.07924v1.pdf', '1902.06156v1.pdf']",1907.05050v3-Figure4-1.png,"Figure 4. Time evolution of u⋆(w(t)) and of its approximation γ̂(θ(t), η(t)) for N = 1, 3, 5. In abscissa: time (in seconds).","Finally, Figure 4 shows the time evolution of the ideal steady-state control law u⋆(w) and of its approximation given by γ̂(θ(j), η(t)) in the three cases in which N = 1, 3, 5.","Approximate Nonlinear Regulation via Identification-Based Adaptive
  Internal Models","This paper concerns the problem of adaptive output regulation for
multivariable nonlinear systems in normal form. We present a regulator
employing an adaptive internal model of the exogenous signals based on the
theory of nonlinear Luenberger observers. Adaptation is performed by means of
discrete-time system identification schemes, in which every algorithm
fulfilling some optimality and stability conditions can be used. Practical and
approximate regulation results are given relating the prediction capabilities
of the identified model to the asymptotic bound on the regulated variables,
which become asymptotic whenever a ""right"" internal model exists in the
identifier's model set. The proposed approach, moreover, does not require
""high-gain"" stabilization actions."
scgqa_274,1912.00088v1,"Considering the data presented in Figure 5, how might different geometries of actin bundles influence voltage circuits?",One possible future direction of research would be to study the evolution of bundles with different geometries. Another direction would be to study the evolution of bundles with different materials.,1912.00088v1.pdf,"['1912.00088v1.pdf', '1703.10422v2.pdf', '1910.09823v3.pdf', '1903.10464v3.pdf', '1905.05538v1.pdf', '1909.00392v1.pdf', '1912.00035v1.pdf', '1805.01358v2.pdf', '1705.00891v1.pdf']",1912.00088v1-Figure5-1.png,"FIG. 5. Evolution of some elements of a closed low density (L.D.) bundle 32 elements long, with input = 1 and -1.",be found in Fig. 4 (high density open bundle) and Fig. 5 (low density closed bundle).,Actin Networks Voltage Circuits,"Starting with an experimentally observed networks of actin bundles, we model
their network structure in terms of edges and nodes. We then compute and
discuss the main electrical parameters, considering the bundles as electrical
wires. A set of equations describing the network is solved with several initial
conditions. Input voltages, that can be considered as information bits, are
applied in a set of points and output voltages are computed in another set of
positions. We consider both an idealized situation, where point-like electrodes
can be inserted in any points of the bundles and a more realistic one, where
electrodes lay on a surface and have typical dimensions available in the
industry. We find that in both cases such a system can implement the main
logical gates and a finite state machine."
scgqa_275,1707.02439v2,What are the differences in performance between the 1-stack and 2-stack hourglasses regarding adversarial training as shown in Fig. 11?,"The graph shows that adversarial training can improve the performance of a stacked hourglass network, especially for 1-stack hourglass. For 2-stack hourglass, the gain of adversarial training is not as obvious, but our method still achieves a higher accuracy than the original hourglass. In addition, 4-stack hourglass plus a discriminator is a better choice than 8-stack hourglass. Finally, the strategy of learning rate decay is helpful for both methods, but ours is a bit more stable and achieves better performance in the end.",1707.02439v2.pdf,"['1707.02439v2.pdf', '1003.1655v1.pdf', '1810.03742v1.pdf', '1804.06674v1.pdf', '1906.11938v3.pdf', '1905.11471v1.pdf', '1303.1635v1.pdf']",1707.02439v2-Figure11-1.png,"Figure 11. PCK on the LSP dataset. The blue line is the approach of [28] while the green line is ours. (a) 1-stack hourglass. (b) 2-stack hourglass. (c) 8-stack standard hourglass versus 4-stack hourglass plus a discriminator. In this setting we decrease the learning rate by 10−1 at epoch 60. (d) We zoom in the part of curve after epoch 60. We find that the strategy of learning rate decay is helpful for both methods, but ours is a bit more stable and achieves better performance in the end.","To investigate the benefit of adversarial training, we compare our method with the original stacked hourglass network. In Fig. 11(a), the improvement of adding adversarial training is significant. Our method converges faster and ends at a higher accuracy. But when it comes to 2-stack hourglass, in Fig. 11(b), the gain of adversarial training does not seem so obvious like 1-stack hourglass. The lines are staggered across training iterations, although at the end our method is a little higher than the original hourglass. The 8- stack hourglass is the best setting released by the authors of [28], but in our experiment, in Fig. 11(c), 4-stack hourglass plus a discriminator is a better choice. In this setting we decrease the learning rate by 10−1 at epoch 60. In Fig. 11(d), we zoom in the part of curve after epoch 60. We find that the strategy of learning rate decay is helpful for both methods, but ours is a bit more stable and achieves better performance in the end.",Self Adversarial Training for Human Pose Estimation,"This paper presents a deep learning based approach to the problem of human
pose estimation. We employ generative adversarial networks as our learning
paradigm in which we set up two stacked hourglass networks with the same
architecture, one as the generator and the other as the discriminator. The
generator is used as a human pose estimator after the training is done. The
discriminator distinguishes ground-truth heatmaps from generated ones, and
back-propagates the adversarial loss to the generator. This process enables the
generator to learn plausible human body configurations and is shown to be
useful for improving the prediction accuracy."
scgqa_276,1603.01793v2,"According to Fig. 5 in the paper, what is the impact of changing truncation distances on solution quality around scatterers?","The graph shows that the quality of the solution is not significantly affected by the domain truncation. This is because the boundary Γext is placed at several distances: R+h, R+5h and R+10h with a circular scatterer of radius R = 10h. It can be seen how the results are almost insensitive (or without clear meaningful trend) to the truncation distance. This is important because it allows the use of the thinnest finite element mesh around the scatterer, only conditioned by scatterer shape and meshing procedures.",1603.01793v2.pdf,"['1603.01793v2.pdf', '1905.05284v1.pdf', '1603.08981v2.pdf', '2003.13216v1.pdf']",1603.01793v2-Figure5-1.png,Fig. 5: Influence of the domain truncation in the quality of the solution for a scatterer with raius R = 10h: (a) error for different FEM meshes truncated at Rext; (b) comparison between a case where only BAE is used and the case when a small layer of finite elements is placed around the scatterer.,"Fig. 5 shows the effect of the finite element mesh truncation. First, the boundary Γext is placed at several distances: R+h, R+5h and R+10h with a circular scatterer of radius R = 10h. It can be seen how the results are almost insensitive (or without clear meaningful trend) to the truncation distance. This is important because it allows the use of the thinnest finite element mesh around the scatterer, only conditioned by scatterer shape and meshing procedures. The use of a small mesh contributes to the reduction of computational costs. On the one hand, there are less unknowns. On the other hand, the range of required values of the discrete Green’s function is smaller.",A New Numerical Method for Solving the Acoustic Radiation Problem,"A numerical method of solving the problem of acoustic wave radiation in the
presence of a rigid scatterer is described. It combines the finite element
method and the boundary algebraic equations. In the proposed method, the
exterior domain around the scatterer is discretized, so that there appear an
infinite domain with regular discretization and a relatively small layer with
irregular mesh. For the infinite regular mesh, the boundary algebraic equation
method is used with spurious resonance suppression according to Burton and
Miller. In the thin layer with irregular mesh, the finite element method is
used. The proposed method is characterized by simple implementation, fair
accuracy, and absence of spurious resonances."
scgqa_277,1910.09823v3,"According to the data presented in Fig. 3, how does ActInf with λ = 1 differ from small λ in performance?","The graph shows that for small but nonzero λ, the results of the ActInf controller approaches the results of the LQG controller as expected. This is because, as λ approaches zero, the ActInf controller becomes more and more similar to the LQG controller. The graph also shows that ActInf control with λ = 1 accumulates higher cost in terms of `(xk, uk) in (7) than ActInf control with small λ. However, ActInf control with λ = 1 achieves lower free energy than ActInf control with small λ. This is because, with λ = 1, the ActInf controller is more aggressive in its control actions, which leads to faster state adjustments and lower free energy.",1910.09823v3.pdf,"['1910.09823v3.pdf', '1904.03292v2.pdf', '1809.07412v2.pdf', '2008.13170v1.pdf', '1405.6408v2.pdf']",1910.09823v3-Figure3-1.png,"Figure 3: Results comparing LQG with ActInf control, where the time-axis is log-scaled. The more aggressive LQG control (bottom left), leads to faster state adjustments (top left). ActInf control for small but nonzero λ reduces to LQG control. Notably, although ActInf control with λ = 1 accumulates higher cost in terms of `(xk, uk) in (7) (bottom right), it achieves lower free energy than ActInf control with small λ (top right).","The results are presented in Fig. 3, which leads to several interesting observations. Firstly, for small but nonzero λ, the results (controls, state trajectory, the accumulated cost for `(xk, uk) and also Gt) of the ActInf controller approaches the results of the LQG controller as expected; see Sec. 3.1.2 and also the discussions at the end of Sec. 5. We note that, for the current system, λ = 0.01 (not plotted) already renders the results of the ActInf and LQG controller nearly visually indistinguishable.",Application of the Free Energy Principle to Estimation and Control,"Based on a generative model (GM) and beliefs over hidden states, the free
energy principle (FEP) enables an agent to sense and act by minimizing a free
energy bound on Bayesian surprise. Inclusion of prior beliefs in the GM about
desired states leads to active inference (ActInf). In this work, we aim to
reveal connections between ActInf and stochastic optimal control. We reveal
that, in contrast to standard cost and constraint-based solutions, ActInf gives
rise to a minimization problem that includes both an information-theoretic
surprise term and a model-predictive control cost term. We further show under
which conditions both methodologies yield the same solution for estimation and
control. For a case with linear Gaussian dynamics and a quadratic cost, we
illustrate the performance of ActInf under varying system parameters and
compare to classical solutions for estimation and control."
scgqa_278,2005.13300v1,"According to Figure 5, how does the frame index relate to the maximum perturbation in the recurrent neural network study?","The graph shows that the maximum perturbation decreases as the frame index increases. This is because the approximation error on frame 1 propagates through the later frames to the classifying layer, while the error on frame 7 only affects the last layer. As a result, the later frames are less sensitive to perturbations.",2005.13300v1.pdf,"['2005.13300v1.pdf', '1809.02337v2.pdf', '1307.1204v1.pdf']",2005.13300v1-Figure5-1.png,Figure 5. Results for the comparison between R2 and POPQORN. Plotted points represent the maximum L∞ norm perturbation for each frame index 1 through 7.,"We show the results of this experiment in Fig. 5. First, we observe that perturbations in early frames allow us to certify much smaller ǫ than that in later frames. This is because the approximation error on frame 1 propagates through the later frames to the classifying layer while the error on frame 7 only affects the last layer. Across all frames, both our methods, LP and OPT, outperform POPQORN. We can also see that OPT which can dynamically adjust the bounds can prove more than LP, which uses fixed single linear relaxation per neuron.",Scalable Polyhedral Verification of Recurrent Neural Networks,"We present a scalable and precise verifier for recurrent neural networks,
called Prover based on two novel ideas: (i) a method to compute a set of
polyhedral abstractions for the non-convex and nonlinear recurrent update
functions by combining sampling, optimization, and Fermat's theorem, and (ii) a
gradient descent based algorithm for abstraction refinement guided by the
certification problem that combines multiple abstractions for each neuron.
Using Prover, we present the first study of certifying a non-trivial use case
of recurrent neural networks, namely speech classification. To achieve this, we
additionally develop custom abstractions for the non-linear speech
preprocessing pipeline. Our evaluation shows that Prover successfully verifies
several challenging recurrent models in computer vision, speech, and motion
sensor data classification beyond the reach of prior work."
scgqa_279,1210.1356v2,"In the context of the paper, how does Figure 2 demonstrate the NC's predictive capability for adaptive networks?","The results from the figure indicate that the NC is able to accurately capture the dynamics of adaptive networks. The figure shows that the NC is able to accurately predict the prevalence and mean S-lifetimes of adaptive networks, which provides further validation of the model.",1210.1356v2.pdf,"['1210.1356v2.pdf', '2010.13032v1.pdf', '1808.08442v1.pdf', '1405.5329v4.pdf', '2005.09814v3.pdf', '2008.13170v1.pdf', '1509.00374v2.pdf']",1210.1356v2-Figure2-1.png,"Figure 2. (Color online) Characteristic distributions of a network in DE for SR (a)-b)), MR (c)-d)) and BR (e)-f)). Left column: Degree distributions for S-nodes (circles), I-nodes (diamonds), as well as of initial degrees of I-nodes (squares). Right column: Plots of survival functions of S-nodes. Solid lines are predictions by the NC.Insets: Comparison of prevalence [I]MC taken from MC simulations and[I]NC computed by in the NC (left column), and of mean S-lifetimes obtained from MC simulations and the NC (right column).w = 0.05, p = 0.008, r = 0.005; 〈k〉 = 5, kmax = 80. a)-b): SR (stable active branch of bistable phase) withw̃ = 0.095, p̃S = 0.017, p̃I = 0.027. c)-d): MR (simple endemic phase) with w̃ = 0.12, p̃S = 0.022, p̃I = 0.031. e)-f): BR (stable active branch bistable phase) with w̃ = 0.17, p̃S = 0.026, p̃I = 0.035. MC simulations according to [48] withN = 5 · 104 nodes, initial Erdős-Rényi graph and initial prevalence[I]0 = 0.6. Statistics were recorded at t = 2 · 104 for 104 network realizations. Error bars are smaller than markers.","Having been provided with the set of model parameters and the mean degree specifying an adaptive network, the NC identifies the DEs and the corresponding sets of optimal correspondence parameters. For all three considered rewiring mechanisms, one can then extract the various probability distributions and averages described in Secs. 3 and 4, and compare them to the output of MC simulations (Fig. 2). Due to recovery being neighborhood-independent and happening at a constant rate, PI(x, y) = ΦS(x, y) in all three scenarios, i.e the steadystate degree distribution of I-nodes is identical to the distribution of their final degrees right before recovery [45].",Detecting and Describing Dynamic Equilibria in Adaptive Networks,"We review modeling attempts for the paradigmatic contact process (or SIS
model) on adaptive networks. Elaborating on one particular proposed mechanism
of topology change (rewiring) and its mean field analysis, we obtain a
coarse-grained view of coevolving network topology in the stationary active
phase of the system. Introducing an alternative framework applicable to a wide
class of adaptive networks, active stationary states are detected, and an
extended description of the resulting steady-state statistics is given for
three different rewiring schemes. We find that slight modifications of the
standard rewiring rule can result in either minuscule or drastic change of
steady-state network topologies."
scgqa_280,1007.0328v1,"In the context of this research, what does Figure 7.16 reveal about outliers affecting experiment reproducibility?","The NSD cumulative frequency plot and the perfect reproducibility line provide evidence that the experiments were reproducible. The majority of the values are clustered around 0.0, which indicates that the experiments were reproducible. The plot also shows that there are some outliers, which may be due to factors such as noise or errors in the experimental setup. However, the fact that the majority of the values are clustered around 0.0 suggests that the experiments were reproducible overall.",1007.0328v1.pdf,"['1007.0328v1.pdf', '1603.08983v6.pdf', '2007.15404v1.pdf', '1911.05146v2.pdf']",1007.0328v1-Figure7.16-1.png,Figure 7.16: NSD cumulative frequency plot.,"Figure 7.16 shows the cumulative frequency distribution of all similarity metric values, computed for all three policies in all sixteen groups of experiments. Perfect reproducibility would have been represented by a NSD of 0.0, which is illustrated as a line in figure 7.17.",Autonomic Management in a Distributed Storage System,"This thesis investigates the application of autonomic management to a
distributed storage system. Effects on performance and resource consumption
were measured in experiments, which were carried out in a local area test-bed.
The experiments were conducted with components of one specific distributed
storage system, but seek to be applicable to a wide range of such systems, in
particular those exposed to varying conditions. The perceived characteristics
of distributed storage systems depend on their configuration parameters and on
various dynamic conditions. For a given set of conditions, one specific
configuration may be better than another with respect to measures such as
resource consumption and performance. Here, configuration parameter values were
set dynamically and the results compared with a static configuration. It was
hypothesised that under non-changing conditions this would allow the system to
converge on a configuration that was more suitable than any that could be set a
priori. Furthermore, the system could react to a change in conditions by
adopting a more appropriate configuration. Autonomic management was applied to
the peer-to-peer (P2P) and data retrieval components of ASA, a distributed
storage system. The effects were measured experimentally for various workload
and churn patterns. The management policies and mechanisms were implemented
using a generic autonomic management framework developed during this work. The
experimental evaluations of autonomic management show promising results, and
suggest several future research topics. The findings of this thesis could be
exploited in building other distributed storage systems that focus on
harnessing storage on user workstations, since these are particularly likely to
be exposed to varying, unpredictable conditions."
scgqa_281,1003.1655v1,"According to Figure 2, what is the threshold for auxiliary RV sizes in the outer region's capacity bounds?","The figure suggests that the upper bound on the auxiliary RV alphabet sizes for the outer region must be at least 7 for the binary problem. This is because the two subsets of the outer region are obtained by setting the alphabet sizes to |T1| = |T2| = 6 and |T1| = |T2| = 7, respectively. If the upper bound were less than 7, then the two subsets would not be subsets of the outer region.",1003.1655v1.pdf,"['1003.1655v1.pdf', '1701.06190v1.pdf', '1707.02342v1.pdf']",1003.1655v1-Figure2-1.png,"Figure 2: The inner boundR∗in(0.45, 0.4) for the Example and two subsets ofR ∗ out(0.45, 0.4) obtained by setting|T1| = |T2| = 6 and|T1| = |T2| = 7. The obtained regions lie between the corresponding solid or dashed lines and the horizontal and vertical axes.","Example Let the covertexts be independent binary sources with U1 = U2 = {0, 1} and QU1(U1 = 0) = 0.05 and QU2(U2 = 0) = 0.1. Let the MAAC be a binary additive channel with X1 = X2 = Y = Z = {0, 1} and Y = X1 ⊕ X2 ⊕ Z , where Z is independent of (X1,X2) with Pr(Z = 1) = 0.02 and ⊕ denotes modulo 2 addition. Let D1 = 0.45 and D2 = 0.4. Fig. 2 illustrates the numerically computed inner and outer regions of Theorems 1 and 2 (which coincide with the regions of Theorem 3 since U1 and U2 are independent). To compute R∗in(0.45, 0.4), we only need to consider auxiliary RVs with alphabets |T1| = |T2| = 5. For comparison, we also plot two subsets of the region R∗out(0.45, 0.4) by setting |T1| = |T2| = 6 and |T1| = |T2| = 7, respectively (recall that Theorem 3 does not give an upper bound on the alphabet sizes for T2 and T2 for the outer bound). It is seen that there exist noticeable gaps between R∗in(0.45, 0.4) and the numerically obtained subsets of R ∗ out(0.45, 0.4). When computing the above regions, we quantized the unit interval using a step-size of resolution 0.1 to calculate the joint distributions. We can conclude that the obtained inner and outer bounds do not coincide, and furthermore, that in case there exists a finite upper bound on the auxiliary RV alphabet sizes for the outer region, this upper bound must be at least 7 for the binary problem.","Inner and Outer Bounds for the Public Information Embedding Capacity
  Region Under Multiple Access Attacks","We consider a public multi-user information embedding (watermarking) system
in which two messages (watermarks) are independently embedded into two
correlated covertexts and are transmitted through a multiple-access attack
channel. The tradeoff between the achievable embedding rates and the average
distortions for the two embedders is studied. For given distortion levels,
inner and outer bounds for the embedding capacity region are obtained in
single-letter form. Tighter bounds are also given for independent covertexts."
scgqa_282,1405.6298v2,How does the paper's Figure 10 illustrate the relationship between differential positivity and the pendulum's oscillations?,"The graph shows that the pendulum can exhibit bistability when k ≤ kc. This is because the region of bistable behaviors is delineated by a homoclinic orbit, which is ruled out by differential positivity.",1405.6298v2.pdf,"['1405.6298v2.pdf', '1511.04338v2.pdf', '1609.06577v1.pdf', '1802.03830v1.pdf', '1701.08947v1.pdf', '1911.02623v1.pdf', '1204.5592v1.pdf', '2004.05579v1.pdf', '1808.10082v4.pdf']",1405.6298v2-Figure10-1.png,"Figure 10. The qualitative behavior of the pendulum for large/small values of torque and damping, as reported in [46, p. 272].","It is of interest to interpret differential positivity against the textbook analysis [46]. Following [46, Chapter 8], Figure 10 summarizes the qualitative behavior of the pendulum for different values of the damping coefficient k ≥ 0 and of the constant torque input u ≥ 0 (the behavior of the pendulum for u ≤ 0 is symmetric). The nonlinear pendulum cannot be differentially positive for arbitrary values of the torque when k ≤ kc. This is because the region of bistable behaviors (coexistence of small and large oscillations) is delineated by a homoclinic orbit, which is ruled out by differental positivity (Corollary 3). For instance, looking at Figure 10, for any k < kc there exists a value u = uc(k) for which the pendulum encounters a homoclinic bifurcation (see [46, Section 8.5] and",Differentially positive systems,"The paper introduces and studies differentially positive systems, that is,
systems whose linearization along an arbitrary trajectory is positive. A
generalization of Perron Frobenius theory is developed in this differential
framework to show that the property induces a (conal) order that strongly
constrains the asymptotic behavior of solutions. The results illustrate that
behaviors constrained by local order properties extend beyond the well-studied
class of linear positive systems and monotone systems, which both require a
constant cone field and a linear state space."
scgqa_283,2010.11594v1,What does Fig. 3 indicate about the sensitivity of model performance to refinement iterations with hard versus soft pseudo ground truth?,"The graph shows that the performance of the model trained with hard pseudo ground truth improves with the increase in refinement iterations. The performance of the model trained with soft pseudo ground truth also improves with the increase in refinement iterations, but to a lesser extent. The performance of the model trained with RGB upper bound and flow upper bound remains the same at different refinement iterations. This suggests that the model trained with hard pseudo ground truth is more sensitive to the refinement iterations.",2010.11594v1.pdf,"['2010.11594v1.pdf', '1407.5358v1.pdf', '1409.2897v1.pdf']",2010.11594v1-Figure3-1.png,Fig. 3: Comparison between models trained with different pseudo ground truth on the THUMOS14 testing set. The upper bounds denote models trained with ground truth actionness sequence,"list the results in Table 4. The results reveal that both Lbg and Latt help improve the performance. And the proposed Latt achieves higher attention variance and better localization performance than Lbg, demonstrating that the our attention normalization term Latt can better avoid the ambiguity of attention. Surprisingly, with both Lbg and Latt, the localization performance is still lower than that with only Latt, and we think this is because the noise of background classification reduces the accuracy of action proposal scores. Ablation study on Pseudo Ground Truth. Fig. 3 plots performance comparison between different pseudo ground truth methods at different refinement iterations. Both soft and hard pseudo ground truth help improve the localization performance. The hard pseudo ground truth removes uncertainty to the model, and thus achieves higher performance improvement. However, with the same frame-level supervision, the flow stream outperforms the RGB stream by a large margin. We think this is because of the nature of two modalities: the RGB modality is less sensitive to actions than the optical flow modality. To demonstrate this, we generate a true frame-level ground truth actionness sequence (action categories are not used), train our model in the same way as the pseudo ground truth. The results are plotted in Fig. 3 as an upper bound. The results verify our hypothesis and demonstrate that the optical flow modality is more suitable for the action localization task than the RGB modality.","Two-Stream Consensus Network for Weakly-Supervised Temporal Action
  Localization","Weakly-supervised Temporal Action Localization (W-TAL) aims to classify and
localize all action instances in an untrimmed video under only video-level
supervision. However, without frame-level annotations, it is challenging for
W-TAL methods to identify false positive action proposals and generate action
proposals with precise temporal boundaries. In this paper, we present a
Two-Stream Consensus Network (TSCN) to simultaneously address these challenges.
The proposed TSCN features an iterative refinement training method, where a
frame-level pseudo ground truth is iteratively updated, and used to provide
frame-level supervision for improved model training and false positive action
proposal elimination. Furthermore, we propose a new attention normalization
loss to encourage the predicted attention to act like a binary selection, and
promote the precise localization of action instance boundaries. Experiments
conducted on the THUMOS14 and ActivityNet datasets show that the proposed TSCN
outperforms current state-of-the-art methods, and even achieves comparable
results with some recent fully-supervised methods."
scgqa_284,1902.07084v2,"According to your findings, what relationship does the graph illustrate between zealots and polarization in varying network densities?","The graph shows how the average polarization of a network changes as the number of zealots or contrarians is increased. For SICs, the average polarization is almost unaffected when the network is not too dense, i.e. when c is small. However, when c is large, for small values of pz, there is a modest increase in the polarization, and it saturates for larger pz values. For RICs, when the network is sparse, φ steadily decreases as the number of zealots is increased, but for denser networks it first increases, and then goes to zero for large values of pz.",1902.07084v2.pdf,"['1902.07084v2.pdf', '1607.08438v1.pdf', '1904.01542v3.pdf', '2009.06124v1.pdf']",1902.07084v2-Figure2-1.png,"FIG. 2. Variation of the average value of the polarization 〈φ〉 with pz for the Erdős-Rényi graph with SICs (top panel) and RICs (bottom panel) for different values of the average degree c. The network size in each case is N = 5000, and averages are obtained using 1000 random realizations. Error bars show the range containing middle 90% of the data.","values of c, are shown in Fig. 2. As can be seen from the figure, average polarization 〈φ〉 is almost unaffected for SICs when the network is not too dense, i.e. when c is small. When c is large, for small values of pz, there is a modest increase in the polarization, and it saturates for larger pz values. For RICs, when the network is sparse, φ steadily decreases as the number of zealots is increased, but for denser networks it first increases, and then goes to zero for large values of pz. A deeper insight into these",Do zealots increase or decrease the polarization in social networks?,"Zealots are the vertices in a social network who do not change their opinions
under social pressure, and are crucial to the study of opinion dynamics on
complex networks. In this paper, we study the effect of zealots on the
polarization dynamics of a deterministic majority-rule model using the
configuration model as a substrate. To this end, we propose a novel quantifier,
called `correlated polarization', for measuring the amount of polarization in
the network when vertices can exists in two opposite states. The quantifier
takes into account not only the fraction of vertices with each opinion, but
also how they are connected to each other. We then show that the presence of
zealots does not have a fixed effect on the polarization, and can change it in
positive, negative or neutral way depending upon their topological
characteristics like degree, their total fraction in the network, density and
degree heterogeneity of the network, and the type of initial conditions of the
dynamics. Our results particularly highlight the importance of the role played
by the initial conditions in drifting the polarization towards lower or higher
values as the total number of zealots is increased."
scgqa_285,1809.02337v2,"According to Fig. 4, what is AULC's role in evaluating machine learning algorithm performance for ITAL?","The area under the learning curve (AULC) is a measure of the performance of a machine learning algorithm. It is calculated by taking the area under the curve of the algorithm's accuracy as a function of the number of training examples. A higher AULC indicates that the algorithm is more accurate, while a lower AULC indicates that the algorithm is less accurate.",1809.02337v2.pdf,"['1809.02337v2.pdf', '1509.08992v2.pdf', '2007.15176v2.pdf', '1804.04818v1.pdf', '2004.05448v1.pdf', '1207.5027v1.pdf', '1607.05970v2.pdf', '1504.07495v1.pdf', '1511.04338v2.pdf']",1809.02337v2-Figure4-1.png,Fig. 4. Area under Learning Curve (AULC) of various BMAL methods on MIRFLICKR with varying feature dimensionality.,"To assess to which extent the results presented in the paper are affected by certain transformations applied to the features, we experimented with different dimensionalities of the feature space on the MIRFLICKR dataset. To this end, we have applied PCA to the features extracted from the first fully-connected layer of VGG16, which comprise 4096 dimensions, and projected them onto spaces with 64, 128, 256, 512, and 1024 features. Experiments with all BMAL methods have been conducted on those features for 10 rounds of user feedback and the area under the learning curve (AULC) for the various dimensionalities is reported in Fig. 4.",Information-Theoretic Active Learning for Content-Based Image Retrieval,"We propose Information-Theoretic Active Learning (ITAL), a novel batch-mode
active learning method for binary classification, and apply it for acquiring
meaningful user feedback in the context of content-based image retrieval.
Instead of combining different heuristics such as uncertainty, diversity, or
density, our method is based on maximizing the mutual information between the
predicted relevance of the images and the expected user feedback regarding the
selected batch. We propose suitable approximations to this computationally
demanding problem and also integrate an explicit model of user behavior that
accounts for possible incorrect labels and unnameable instances. Furthermore,
our approach does not only take the structure of the data but also the expected
model output change caused by the user feedback into account. In contrast to
other methods, ITAL turns out to be highly flexible and provides
state-of-the-art performance across various datasets, such as MIRFLICKR and
ImageNet."
scgqa_286,1107.4161v1,"According to Figure 5 in the QAP study, what effect does increasing problem dimension have on solution fitness proportions?","The graph shows that the proportion of search space whose solutions climb to a fitness value within 5% from the global best value decreases as the problem dimension increases. This is because as the problem dimension increases, the search space becomes more complex and it becomes more difficult to find a solution that is close to the global best value.",1107.4161v1.pdf,"['1107.4161v1.pdf', '1502.03556v1.pdf', '1905.12868v5.pdf', '1410.7867v1.pdf', '1907.05050v3.pdf']",1107.4161v1-Figure5-1.png,"Figure 5: Proportion of search space whose solutions climb to a fitness value within 5% from the global best value. Triangular points correspond to real-like problems, rounded points to uniform ones; for each problem dimension, averages from 30 independent and randomly generated instances are shown.","From what has been studied, real-like instances are easier to solve exactly using heuristic search. However, Merz and Freisleben [12] have shown that the quality of local optima decreases when there are few off-diagonal zeros in the flow matrix: the cost contributions to the fitness value in eq.1 are in that case more interdependent, as there was a higher epistasis. Thus it should be easier to find a suboptimal solution for a uniform instance than for a reallike one. To confirm this result in another way, figure 5 shows the proportion of solutions from whose a best improvement hill-climbing conducts to a local optima within a 5% value from the global optimum cost. As problem size grows, sub-optimal solutions are distinctively easier to reach in the uniform case, i.e. uniform instances are easier to be solved approximatively. This also agrees with the fact that for large enough instances, the cost ratio between the best and the worst solution has been proved to converge to one in the random uniform case [13].",Local Optima Networks of the Quadratic Assignment Problem,"Using a recently proposed model for combinatorial landscapes, Local Optima
Networks (LON), we conduct a thorough analysis of two types of instances of the
Quadratic Assignment Problem (QAP). This network model is a reduction of the
landscape in which the nodes correspond to the local optima, and the edges
account for the notion of adjacency between their basins of attraction. The
model was inspired by the notion of 'inherent network' of potential energy
surfaces proposed in physical-chemistry. The local optima networks extracted
from the so called uniform and real-like QAP instances, show features clearly
distinguishing these two types of instances. Apart from a clear confirmation
that the search difficulty increases with the problem dimension, the analysis
provides new confirming evidence explaining why the real-like instances are
easier to solve exactly using heuristic search, while the uniform instances are
easier to solve approximately. Although the local optima network model is still
under development, we argue that it provides a novel view of combinatorial
landscapes, opening up the possibilities for new analytical tools and
understanding of problem difficulty in combinatorial optimization."
scgqa_287,1809.07412v2,What does the learning progress in Figure 3 indicate regarding standard RNNs and LSTMs with different hidden units?,"The graph shows that standard RNNs with one hidden layer of 27, 36, and 54 neurons perform consistently worse than long short-term memory (LSTM) RNNs with forget gates and peephole connections. LSTMs with 16 hidden memory cells outperform LSTMs with 8 hidden memory cells, but the advantage of adding another 8 hidden cells is less pronounced.",1809.07412v2.pdf,"['1809.07412v2.pdf', '1302.3123v1.pdf', '1207.3107v3.pdf', '1610.01283v4.pdf']",1809.07412v2-Figure3-1.png,Figure 3: Learning progress comparing standard RNNs and LSTMs with different numbers of hidden units. Learning rate is reduced by a factor of .1 after 1k and 2k epochs.,"Figure 3 contrasts the sensory prediction error development during learning for several RNN architectures, showing averaged mean errors and standard deviations across 20 independently weightinitialized (normally distributed values with standard deviation 0.1) networks. Standard RNNs with one hidden layer of 27 (1026), 36 (1692), and 54 neurons (3510 weights) perform consistently worse than long short-term memory (LSTM) RNNs with forget gates and peephole connections (Gers et al., 2002). While 16 hidden memory cells (1680 weights) clearly outperform 8 hidden memory cells (584 weights), the advantage of yet another 8 hidden cells, that is, 24 cells in total (3288 weights) is less pronounced.","Learning, Planning, and Control in a Monolithic Neural Event Inference
  Architecture","We introduce REPRISE, a REtrospective and PRospective Inference SchEme, which
learns temporal event-predictive models of dynamical systems. REPRISE infers
the unobservable contextual event state and accompanying temporal predictive
models that best explain the recently encountered sensorimotor experiences
retrospectively. Meanwhile, it optimizes upcoming motor activities
prospectively in a goal-directed manner. Here, REPRISE is implemented by a
recurrent neural network (RNN), which learns temporal forward models of the
sensorimotor contingencies generated by different simulated dynamic vehicles.
The RNN is augmented with contextual neurons, which enable the encoding of
distinct, but related, sensorimotor dynamics as compact event codes. We show
that REPRISE concurrently learns to separate and approximate the encountered
sensorimotor dynamics: it analyzes sensorimotor error signals adapting both
internal contextual neural activities and connection weight values. Moreover,
we show that REPRISE can exploit the learned model to induce goal-directed,
model-predictive control, that is, approximate active inference: Given a goal
state, the system imagines a motor command sequence optimizing it with the
prospective objective to minimize the distance to the goal. The RNN activities
thus continuously imagine the upcoming future and reflect on the recent past,
optimizing the predictive model, the hidden neural state activities, and the
upcoming motor activities. As a result, event-predictive neural encodings
develop, which allow the invocation of highly effective and adaptive
goal-directed sensorimotor control."
scgqa_288,1912.02074v1,What performance metrics does Figure 2 indicate for AlgaeDICE and actor-critic in online and offline experiments?,"The graph shows that AlgaeDICE performs better than actor-critic in both the online and offline settings. In the online setting, AlgaeDICE achieves an average per-step reward of 0.35, while actor-critic achieves an average per-step reward of 0.25. In the offline setting, AlgaeDICE achieves an average per-step reward of 0.20, while actor-critic achieves an average per-step reward of 0.15. This shows that AlgaeDICE is more robust to the type of dataset, and is able to perform well in both online and offline settings.",1912.02074v1.pdf,"['1912.02074v1.pdf', '1405.6408v2.pdf', '2001.09043v3.pdf', '2005.14165v4.pdf', '1904.06587v1.pdf', '2002.10790v1.pdf']",1912.02074v1-Figure2-1.png,Figure 2: Average per-step reward of policies on Four Rooms learned by AlgaeDICE compared to actorcritic (AC) over training iterations.,"We further provide quantitative results in Figure 2. We plot the average per-step reward of AlgaeDICE compared to actor-critic in both online and offline settings. As a point of comparison, the behavior policy used to collect data for the offline setting achieves average reward of 0.03. Although all the variants are able to significantly improve upon this baseline, we see that AlgaeDICE performance is only negligibly affected by the type of dataset, while performance of actor-critic degrades in the offline regime. See Appendix B for experimental details.",AlgaeDICE: Policy Gradient from Arbitrary Experience,"In many real-world applications of reinforcement learning (RL), interactions
with the environment are limited due to cost or feasibility. This presents a
challenge to traditional RL algorithms since the max-return objective involves
an expectation over on-policy samples. We introduce a new formulation of
max-return optimization that allows the problem to be re-expressed by an
expectation over an arbitrary behavior-agnostic and off-policy data
distribution. We first derive this result by considering a regularized version
of the dual max-return objective before extending our findings to unregularized
objectives through the use of a Lagrangian formulation of the linear
programming characterization of Q-values. We show that, if auxiliary dual
variables of the objective are optimized, then the gradient of the off-policy
objective is exactly the on-policy policy gradient, without any use of
importance weighting. In addition to revealing the appealing theoretical
properties of this approach, we also show that it delivers good practical
performance."
scgqa_289,1803.01118v2,What does Figure 4 in 'Some Considerations on Learning to Explore' reveal about E-RL2's variance on Krazy World?,"The graph shows that E-RL2 achieves the best final results, but has the highest initial variance. This means that it takes longer for E-RL2 to converge to its optimal performance, but once it does, it outperforms all other algorithms. Crucially, EMAML converges faster than MAML, although both algorithms do manage to converge. RL2 has relatively poor performance and high variance. A random agent achieves a score of around 0.05.",1803.01118v2.pdf,"['1803.01118v2.pdf', '1106.3242v2.pdf', '1405.7705v1.pdf', '1309.3959v1.pdf', '1307.1204v1.pdf', '1907.11314v1.pdf', '1805.01892v1.pdf', '2010.13032v1.pdf']",1803.01118v2-Figure4-1.png,"Figure 4: Meta learning curves on Krazy World. We see that E-RL2 achieves the best final results, but has the highest initial variance. Crucially, EMAML converges faster than MAML, although both algorithms do manage to converge. RL2 has relatively poor performance and high variance. A random agent achieves a score of around 0.05.","When plotting learning curves in Figure 4 and Figure 5, the Y axis is the reward obtained after training at test time on a set of 64 held-out test environments. The X axis is the total number of environmental time-steps the algorithm has used for training. Every time the environment advances forward by one step, this count increments by one. This is done to keep the timescale consistent across meta-learning curves.","Some Considerations on Learning to Explore via Meta-Reinforcement
  Learning","We consider the problem of exploration in meta reinforcement learning. Two
new meta reinforcement learning algorithms are suggested: E-MAML and
E-$\text{RL}^2$. Results are presented on a novel environment we call `Krazy
World' and a set of maze environments. We show E-MAML and E-$\text{RL}^2$
deliver better performance on tasks where exploration is important."
scgqa_290,1405.6298v2,"In the context of differentially positive systems, what does the graph reveal about k's dependence on torque u?",The graph shows that the critical value of k decreases as the torque input u increases. This is because the homoclinic orbit becomes closer to the equilibrium point with large oscillations as u increases.,1405.6298v2.pdf,"['1405.6298v2.pdf', '1007.0328v1.pdf', '1302.2824v2.pdf', '1509.01310v1.pdf', '2003.13216v1.pdf', '2010.13032v1.pdf', '1106.3826v2.pdf', '1805.00184v1.pdf']",1405.6298v2-Figure10-1.png,"Figure 10. The qualitative behavior of the pendulum for large/small values of torque and damping, as reported in [46, p. 272].","It is of interest to interpret differential positivity against the textbook analysis [46]. Following [46, Chapter 8], Figure 10 summarizes the qualitative behavior of the pendulum for different values of the damping coefficient k ≥ 0 and of the constant torque input u ≥ 0 (the behavior of the pendulum for u ≤ 0 is symmetric). The nonlinear pendulum cannot be differentially positive for arbitrary values of the torque when k ≤ kc. This is because the region of bistable behaviors (coexistence of small and large oscillations) is delineated by a homoclinic orbit, which is ruled out by differental positivity (Corollary 3). For instance, looking at Figure 10, for any k < kc there exists a value u = uc(k) for which the pendulum encounters a homoclinic bifurcation (see [46, Section 8.5] and",Differentially positive systems,"The paper introduces and studies differentially positive systems, that is,
systems whose linearization along an arbitrary trajectory is positive. A
generalization of Perron Frobenius theory is developed in this differential
framework to show that the property induces a (conal) order that strongly
constrains the asymptotic behavior of solutions. The results illustrate that
behaviors constrained by local order properties extend beyond the well-studied
class of linear positive systems and monotone systems, which both require a
constant cone field and a linear state space."
scgqa_291,1810.03742v1,"Based on Figure 10 in the paper, what is the relationship between the backbone fraction and the clue density of Sudoku puzzles?","The graph suggests that there is a negative correlation between the backbone fraction and the clue density of Sudoku puzzles. This means that as the clue density increases, the backbone fraction decreases.",1810.03742v1.pdf,"['1810.03742v1.pdf', '1610.01283v4.pdf', '1306.4036v2.pdf', '1907.11314v1.pdf']",1810.03742v1-Figure10-1.png,"Fig. 10: Variation of the backbone fraction with the clue density of random ensembles of a hundred 9 × 9, 16 × 16 and 25×25 puzzles. The inset shows the respective backbone sizes.","One way to evaluate the constrainedness of Sudoku puzzles whilst preserving algorithm independency is to look at its backbone. The backbone of an instance is given by the set of its variables which are frozen, meaning they assume the same value in all solutions. Large backbones impose hardship to search methods, and as a result the backbone size is closely linked to algorithmic hardness. Random k-SAT is known to undergo a freezing phase transition just before the more famous satisfiability transition [12]. [24] have provided evidence that published Sudoku puzzles become unfrozen when a critical number of constraints are relaxed (i.e. the connectivity of the Sudoku graph in Figure 1 is decreased). Here we show that the backbone also undergoes critical behavior when the puzzles’ clue density is calibrated. The inset of Figure 10 shows the variation in the backbone size for 9× 9, 16× 16 and 25× 25","Problem Solving at the Edge of Chaos: Entropy, Puzzles and the Sudoku
  Freezing Transition","Sudoku is a widely popular $\mathcal{NP}$-Complete combinatorial puzzle whose
prospects for studying human computation have recently received attention, but
the algorithmic hardness of Sudoku solving is yet largely unexplored. In this
paper, we study the statistical mechanical properties of random Sudoku grids,
showing that puzzles of varying sizes attain a hardness peak associated with a
critical behavior in the constrainedness of random instances. In doing so, we
provide the first description of a Sudoku \emph{freezing} transition, showing
that the fraction of backbone variables undergoes a phase transition as the
density of pre-filled cells is calibrated. We also uncover a variety of
critical phenomena in the applicability of Sudoku elimination strategies,
providing explanations as to why puzzles become boring outside the typical
range of clue densities adopted by Sudoku publishers. We further show that the
constrainedness of Sudoku puzzles can be understood in terms of the
informational (Shannon) entropy of their solutions, which only increases up to
the critical point where variables become frozen. Our findings shed light on
the nature of the $k$-coloring transition when the graph topology is fixed, and
are an invitation to the study of phase transition phenomena in problems
defined over \emph{alldifferent} constraints. They also suggest advantages to
studying the statistical mechanics of popular $\mathcal{NP}$-Hard puzzles,
which can both aid the design of hard instances and help understand the
difficulty of human problem solving."
scgqa_292,1908.04647v1,What does the analysis in Figure 5 reveal about the effectiveness of different geometric refinements in maintaining stability?,"The graph shows that the canonical geometric edge, corner, and corner-edge refinements all lead to similar results in terms of the inf-sup constant. This suggests that all three refinements are effective in ensuring stability of the finite element method.",1908.04647v1.pdf,"['1908.04647v1.pdf', '1812.09355v1.pdf', '1405.6298v2.pdf', '1907.11314v1.pdf', '1907.06845v5.pdf', '1607.08112v1.pdf']",1908.04647v1-Figure5-1.png,Figure 5. Stabilized values from Figure 4 (on the right with logarithmic scaling; the dashed line shows a slope of −3).,"We focus on the canonical geometric edge, corner, and corner-edge refinements from Section 3.1. As before, we first fix the approximation degree k, and refine the meshes step by step in order to monitor the inf-sup constant; see Figure 4 for the resulting plots with different approximation degrees. Again, we display the stabilized values of γa for increasing k in Figure 5. The results are qualitatively similar to the inf-sup constant γB discussed earlier. There is a k-dependence of the inf-sup constant γa which is again much weaker than k","Stability and Convergence of Spectral Mixed Discontinuous Galerkin
  Methods for 3D Linear Elasticity on Anisotropic Geometric Meshes","We consider spectral mixed discontinuous Galerkin finite element
discretizations of the Lam\'e system of linear elasticity in polyhedral domains
in $\mathbb{R}^3$. In order to resolve possible corner, edge, and corner-edge
singularities, anisotropic geometric edge meshes consisting of hexahedral
elements are applied. We perform a computational study on the discrete inf-sup
stability of these methods, and especially focus on the robustness with respect
to the Poisson ratio close to the incompressible limit (i.e. the Stokes
system). Furthermore, under certain realistic assumptions (for analytic data)
on the regularity of the exact solution, we illustrate numerically that the
proposed mixed DG schemes converge exponentially in a natural DG norm."
scgqa_293,1806.05387v1,What specific trend in Figure 24 indicates the particle filter's adaptation and noise reduction with respect to γ?,"The graph demonstrates the ability of the particle filter to learn to reduce excess noise by showing how the filter learns to reduce excess noise for increasing values of γ. This is evident from the results in Figure 25 and Figure 26, which show that the particle filter learns to reduce excess noise as the value of γ increases.",1806.05387v1.pdf,"['1806.05387v1.pdf', '1908.09034v2.pdf', '1903.10464v3.pdf']",1806.05387v1-Figure24-1.png,"Figure 24: Comparing the change in estimated posterior expected value (black) vs simulation input value (red) of σ with regime change after 10,000 steps, using the Liu and West filter with learning for different values of γ (i)0.0001 (ii)0.001 (iii)0.01 (iv)0.1","The ability of the proposed approach to accelerate adaptation is demonstrated by adding noise parameter perturbation to the filter configuration used to produce the results shown in Figure 19, chart (ii). The results, shown in Figure 23 and Figure 24 for increasing values of γ, demonstrate very effective acceleration of adaptation coupled with a significant reduction in noise compared to the implementation in the previous section. The adaptation of the filter to a regime change demonstrates the ability to rapidly increase the speed of adaptation when required. The same mechanism forces the additional noise to decrease when the adaptation phase is finished. The decrease in the noise factor can be further demonstrated with stochastic volatility model simulated data and the filter configured with a very high starting point for φ as in Figure 15, chart (iv). The results in Figure 25 and Figure 26 demonstrate how the particle filter learns to reduce excess noise for increasing values of γ. It is also evident from the results, particularly Figure 24, that the reduction in noise post adaptation phase tends to be slower than the initial increase. To speed up this reversal a dampening parameter is introduced in the next section.","Parameter Learning and Change Detection Using a Particle Filter With
  Accelerated Adaptation","This paper presents the construction of a particle filter, which incorporates
elements inspired by genetic algorithms, in order to achieve accelerated
adaptation of the estimated posterior distribution to changes in model
parameters. Specifically, the filter is designed for the situation where the
subsequent data in online sequential filtering does not match the model
posterior filtered based on data up to a current point in time. The examples
considered encompass parameter regime shifts and stochastic volatility. The
filter adapts to regime shifts extremely rapidly and delivers a clear heuristic
for distinguishing between regime shifts and stochastic volatility, even though
the model dynamics assumed by the filter exhibit neither of those features."
scgqa_294,1911.02623v1,What does Figure 3 indicate about the effectiveness of E-GC and EL-GC for trips with multiple node connections in road networks?,"The graph suggests that the two modified models, E-GC and EL-GC, perform better for trips that have the subsequent nodes connected by a series of nodes in the road network. This is because the inclusion of embeddings along with the nodes helps in a better interpretation of spatial dependencies among the nodes, thus aiding in better prediction of the trips with large difference between map and coordinate distance.",1911.02623v1.pdf,"['1911.02623v1.pdf', '1910.08413v1.pdf', '2006.03632v1.pdf', '1907.04002v1.pdf', '1505.02851v1.pdf', '2010.07597v2.pdf', '2011.07119v1.pdf', '1803.04037v1.pdf', '2010.11594v1.pdf']",1911.02623v1-Figure3-1.png,"Figure 3: Distribution of MAPE(y-axis) across L-GC, E-GC and EL-GC over the varying difference between Coordinate and Map distance buckets (x-axis) on Beijing Dataset.","Following the same set of experiments, wemodified on the grounds of using distance metric. We concatenated both the Map Distance and Coordinate Distance to the output of the convolution layer (locconvi ). Table 1 (under Map & Coordinate Distance) shows the comparison of the baseline method L-GCwith that of the twomodifications E-GC and EL-GC.We observe that both themodifications E-GC and EL-GC outperform L-GC. We also plot the MAPE across varying buckets holding the difference between Map Distance and Coordinate Distance. The assumption that holds in coordinate distance, that all nodes are directly connected is violated by map distance which includes the map information to find the shortest distance between two nodes (which can be connected by a series of nodes in map). It can be clearly seen from Figure 3 that although at a lower difference value in distance (on x-axis), L-GC starts at a lower MAPE, it keeps increasing as the difference gets higher. On the other hand, E-GC and EL-GC remain on the lower side as the difference in distance increases. The inclusion of embeddings along with the nodes help in a better interpretation of spatial dependencies among the nodes thus aiding in better prediction of the trips with large difference between map and coordinate distance. This suggests that our proposed modifications perform better for trips which have the subsequent nodes connected by a series of nodes in the road network (and not directly connected).",Map Enhanced Route Travel Time Prediction using Deep Neural Networks,"Travel time estimation is a fundamental problem in transportation science
with extensive literature. The study of these techniques has intensified due to
availability of many publicly available large trip datasets. Recently developed
deep learning based models have improved the generality and performance and
have focused on estimating times for individual sub-trajectories and
aggregating them to predict the travel time of the entire trajectory. However,
these techniques ignore the road network information. In this work, we propose
and study techniques for incorporating road networks along with historical
trips' data into travel time prediction. We incorporate both node embeddings as
well as road distance into the existing model. Experiments on large real-world
benchmark datasets suggest improved performance, especially when the train data
is small. As expected, the proposed method performs better than the baseline
when there is a larger difference between road distance and Vincenty distance
between start and end points."
scgqa_295,2008.13170v1,"In the context of the advection equation in this paper, what effect do the SIAC filters have on error reduction?","The graph shows that the filtering techniques are effective in reducing the error and recovering the smoothness in the approximation. This is evident from the fact that the error plots for the filtered solutions are much smoother than the error plot for the DG solution. Additionally, the error for the filtered solutions is significantly smaller than the error for the DG solution. This suggests that the filtering techniques are able to significantly improve the accuracy of the DG method.",2008.13170v1.pdf,"['2008.13170v1.pdf', '1803.04037v1.pdf', '1708.07888v3.pdf', '2005.13300v1.pdf', '1905.05284v1.pdf', '2007.11446v1.pdf', '1707.02342v1.pdf']",2008.13170v1-Figure4.7-1.png,"Figure 4.7: The point-wise error plots for advection equation (3.20) for the DG method with the filtering techniques with polynomial P2. Here, the final time T = 1. We observe that the compared to the DG solution, the solutions after filtering with both the original and compact SIAC filtering have recovered the smoothness in the approximation and reduced the error.","The L2 norm errors and respective accuracy orders are given in Table 4.4. Compared to the DG solutions with the regular accuracy order of k+ 1, the filtered solutions (with both the original and compact SIAC filtering) have the superconvergence order of 2k+ 1. Furthermore, we observe that compared to the original SIAC filter, the compact filter has a much smaller support size while shows noticeable better accuracy for higher-order cases. For example, in the k = 3 case, the compact SIAC filter has only half the original SIAC filter’s support size, while the filtered errors are more than 10 times smaller. In Figure 4.7, we show the point-wise error plots before and after applying the original and compact SIAC filtering. We note that both filters recover smoothness in the DG solution as well as reduce the error.","How to Design A Generic Accuracy-Enhancing Filter for Discontinuous
  Galerkin Methods","Higher-order accuracy (order of $k+1$ in the $L^2$ norm) is one of the well
known beneficial properties of the discontinuous Galerkin (DG) method.
Furthermore, many studies have demonstrated the superconvergence property
(order of $2k+1$ in the negative norm) of the semi-discrete DG method. One can
take advantage of this superconvergence property by post-processing techniques
to enhance the accuracy of the DG solution. A popular class of post-processing
techniques to raise the convergence rate from order $k+1$ to order $2k+1$ in
the $L^2$ norm is the Smoothness-Increasing Accuracy-Conserving (SIAC)
filtering. In addition to enhancing the accuracy, the SIAC filtering also
increases the inter-element smoothness of the DG solution. The SIAC filtering
was introduced for the DG method of the linear hyperbolic equation by Cockburn
et al. in 2003. Since then, there are many generalizations of the SIAC
filtering have been proposed. However, the development of SIAC filtering has
never gone beyond the framework of using spline functions (mostly B-splines) to
construct the filter function. In this paper, we first investigate the general
basis function (beyond the spline functions) that can be used to construct the
SIAC filter. The studies of the general basis function relax the SIAC filter
structure and provide more specific properties, such as extra smoothness, etc.
Secondly, we study the basis functions' distribution and propose a new SIAC
filter called compact SIAC filter that significantly reduces the original SIAC
filter's support size while preserving (or even improving) its ability to
enhance the accuracy of the DG solution. We show that the proofs of the new
SIAC filters' ability to extract the superconvergence and provide numerical
results to confirm the theoretical results and demonstrate the new finding's
good numerical performance."
scgqa_296,1201.3056v1,How does the optimal relay pricing illustrated in Figure 5 relate to the variance of fi and revenue?,"The graph shows that as the variance of fi increases, the maximum relay revenue also increases. This is because as the demand increases, the relay is able to sell more power and charge a higher price, so the revenue increases.",1201.3056v1.pdf,"['1201.3056v1.pdf', '1909.01868v3.pdf', '2002.11440v1.pdf', '1311.6183v1.pdf']",1201.3056v1-Figure5-1.png,Fig. 5. Three-user relay network with Rayleigh fading channels and different variances offi.,"Next, we examine the trend of the optimal relay price with an increasing demand. From Lemma 2, P Ii (λ) is a non-decreasing function of |fi|2. So, we can use an increasing |fi|2 to simulate the increasing user demand. In this numerical experiment, we again consider a threeuser network and model all channels as independent circularly symmetric complex Gaussian random variables with zero-mean, that is, they are independent Rayleigh flat-fading channels. The variances of all gi’s and hi’s are 1, while the variance of all fi’s ranges from 1 to 20. A larger variance means a higher average value of |fi|2, which on average means a higher power demand from the users. The transmit power of the users is set to be 10 dB and relay power is set to be 20 dB. Figure 5 shows the optimal relay power price, the actual relay power sold, and the maximum relay revenue with different variances of fi. We can see that as the variance of fi increases, the optimal relay price increases, more relay power is sold, and the maximum relay revenue increases. This fits one of the laws of supply and demand, which says, if the supply is unchanged and demand increases, it leads to higher equilibrium price and quantity.","Power Allocation and Pricing in Multi-User Relay Networks Using
  Stackelberg and Bargaining Games","This paper considers a multi-user single-relay wireless network, where the
relay gets paid for helping the users forward signals, and the users pay to
receive the relay service. We study the relay power allocation and pricing
problems, and model the interaction between the users and the relay as a
two-level Stackelberg game. In this game, the relay, modeled as the service
provider and the leader of the game, sets the relay price to maximize its
revenue; while the users are modeled as customers and the followers who buy
power from the relay for higher transmission rates. We use a bargaining game to
model the negotiation among users to achieve a fair allocation of the relay
power. Based on the proposed fair relay power allocation rule, the optimal
relay power price that maximizes the relay revenue is derived analytically.
Simulation shows that the proposed power allocation scheme achieves higher
network sum-rate and relay revenue than the even power allocation. Furthermore,
compared with the sum-rate-optimal solution, simulation shows that the proposed
scheme achieves better fairness with comparable network sum-rate for a wide
range of network scenarios. The proposed pricing and power allocation solutions
are also shown to be consistent with the laws of supply and demand."
scgqa_297,1610.08534v1,What information does the title of Fig. 2 convey regarding the Monte Carlo simulations performed?,"The title of the graph, ""Average FIT for several methods, obtained from 100 Monte Carlo runs with random systems"", provides a concise overview of the data it contains. The graph shows the average FIT, or fitness, for several methods, including PEM, MORSM1, MORSM20, and BJSM20. These methods were tested on 100 Monte Carlo runs, each with a random system.",1610.08534v1.pdf,"['1610.08534v1.pdf', '1506.06213v1.pdf', '2005.11699v2.pdf']",1610.08534v1-Figure2-1.png,"Fig. 2. Average FIT for several methods, obtained from 100 Monte Carlo runs with random systems.","The results are presented in Fig. 2, with the average FIT as function of sample size. We assume that PEM, when initialized at the true parameters, converges to the global optimum. Comparing PEM initialized at the true parameters and with the standard MATLAB procedure, we conclude that the latter must sometimes fail to reach the global optimum.","Optimal model order reduction with the Steiglitz-McBride method for
  open-loop data","In system identification, it is often difficult to find a physical intuition
to choose a noise model structure. The importance of this choice is that, for
the prediction error method (PEM) to provide asymptotically efficient
estimates, the model orders must be chosen according to the true system.
However, if only the plant estimates are of interest and the experiment is
performed in open loop, the noise model may be over-parameterized without
affecting the asymptotic properties of the plant. The limitation is that, as
PEM suffers in general from non-convexity, estimating an unnecessarily large
number of parameters will increase the chances of getting trapped in local
minima. To avoid this, a high order ARX model can first be estimated by least
squares, providing non-parametric estimates of the plant and noise model. Then,
model order reduction can be used to obtain a parametric model of the plant
only. We review existing methods to perform this, pointing out limitations and
connections between them. Then, we propose a method that connects favorable
properties from the previously reviewed approaches. We show that the proposed
method provides asymptotically efficient estimates of the plant with open loop
data. Finally, we perform a simulation study, which suggests that the proposed
method is competitive with PEM and other similar methods."
scgqa_298,2010.11594v1,"In the context of THUMOS14, how do the RGB and flow modalities compare in model performance as shown in Fig. 3?","The graph shows that the model trained with hard pseudo ground truth achieves the best performance, followed by the model trained with soft pseudo ground truth. The model trained with RGB upper bound achieves the best performance, followed by the model trained with flow upper bound. This suggests that the optical flow modality is more suitable for the action localization task than the RGB modality.",2010.11594v1.pdf,"['2010.11594v1.pdf', '1703.07020v4.pdf', '1501.07107v1.pdf', '1303.1635v1.pdf', '2011.07119v1.pdf', '1902.06156v1.pdf', '2003.09700v4.pdf', '2009.08716v1.pdf']",2010.11594v1-Figure3-1.png,Fig. 3: Comparison between models trained with different pseudo ground truth on the THUMOS14 testing set. The upper bounds denote models trained with ground truth actionness sequence,"list the results in Table 4. The results reveal that both Lbg and Latt help improve the performance. And the proposed Latt achieves higher attention variance and better localization performance than Lbg, demonstrating that the our attention normalization term Latt can better avoid the ambiguity of attention. Surprisingly, with both Lbg and Latt, the localization performance is still lower than that with only Latt, and we think this is because the noise of background classification reduces the accuracy of action proposal scores. Ablation study on Pseudo Ground Truth. Fig. 3 plots performance comparison between different pseudo ground truth methods at different refinement iterations. Both soft and hard pseudo ground truth help improve the localization performance. The hard pseudo ground truth removes uncertainty to the model, and thus achieves higher performance improvement. However, with the same frame-level supervision, the flow stream outperforms the RGB stream by a large margin. We think this is because of the nature of two modalities: the RGB modality is less sensitive to actions than the optical flow modality. To demonstrate this, we generate a true frame-level ground truth actionness sequence (action categories are not used), train our model in the same way as the pseudo ground truth. The results are plotted in Fig. 3 as an upper bound. The results verify our hypothesis and demonstrate that the optical flow modality is more suitable for the action localization task than the RGB modality.","Two-Stream Consensus Network for Weakly-Supervised Temporal Action
  Localization","Weakly-supervised Temporal Action Localization (W-TAL) aims to classify and
localize all action instances in an untrimmed video under only video-level
supervision. However, without frame-level annotations, it is challenging for
W-TAL methods to identify false positive action proposals and generate action
proposals with precise temporal boundaries. In this paper, we present a
Two-Stream Consensus Network (TSCN) to simultaneously address these challenges.
The proposed TSCN features an iterative refinement training method, where a
frame-level pseudo ground truth is iteratively updated, and used to provide
frame-level supervision for improved model training and false positive action
proposal elimination. Furthermore, we propose a new attention normalization
loss to encourage the predicted attention to act like a binary selection, and
promote the precise localization of action instance boundaries. Experiments
conducted on the THUMOS14 and ActivityNet datasets show that the proposed TSCN
outperforms current state-of-the-art methods, and even achieves comparable
results with some recent fully-supervised methods."
scgqa_299,1205.4213v2,What does the research paper's Figure 3 reveal regarding the performance of SVM and Preference Perceptron with noisy feedback?,"The graph shows that the regret for both algorithms converges to a non-zero value as time increases. This is because the feedback is now based on noisy relevance labels, which means that the algorithms are not able to perfectly learn the user's preferences. However, the Preference Perceptron algorithm performs significantly better than the SVM algorithm, with a lower regret value. This is likely due to the fact that the perceptron algorithm is more robust to noise than the SVM algorithm.",1205.4213v2.pdf,"['1205.4213v2.pdf', '1703.01827v3.pdf', '1307.3687v1.pdf', '1706.03112v1.pdf', '1710.09234v1.pdf', '1204.5592v1.pdf']",1205.4213v2-Figure3-1.png,Figure 3. Regret vs time based on noisy feedback.,"Results of this experiment are presented in Figure 3. Since the feedback is now based on noisy relevance labels, the utility regret converges to a non-zero value as predicted by our theoretical results. Over most of the range, the Preference Perceptron performs significantly4 better than the SVM. Moreover, the perceptron experiment took around 30 minutes to run, whereas the SVM experiment took about 20 hours on the same machine. We conjecture that the regret values for both the algorithms can be improved with better features or kernels, but these extensions are orthogonal to the main focus of this paper.",Online Structured Prediction via Coactive Learning,"We propose Coactive Learning as a model of interaction between a learning
system and a human user, where both have the common goal of providing results
of maximum utility to the user. At each step, the system (e.g. search engine)
receives a context (e.g. query) and predicts an object (e.g. ranking). The user
responds by correcting the system if necessary, providing a slightly improved
-- but not necessarily optimal -- object as feedback. We argue that such
feedback can often be inferred from observable user behavior, for example, from
clicks in web-search. Evaluating predictions by their cardinal utility to the
user, we propose efficient learning algorithms that have ${\cal
O}(\frac{1}{\sqrt{T}})$ average regret, even though the learning algorithm
never observes cardinal utility values as in conventional online learning. We
demonstrate the applicability of our model and learning algorithms on a movie
recommendation task, as well as ranking for web-search."
scgqa_300,1908.04655v1,"According to Figure 5 in the paper, what efficiency difference exists in likelihood evaluations between autoPR and standard NS?","The right panel of the graph shows the mean number of likelihood evaluations required by the standard NS approach and the autoPR method to find the estimate θ̂. The autoPR method requires significantly fewer likelihood evaluations than the standard NS approach, indicating that it is more efficient. This is likely due to the fact that the autoPR method uses a more efficient optimization algorithm that is able to converge to the global minimum of the likelihood function more quickly.",1908.04655v1.pdf,"['1908.04655v1.pdf', '1402.7063v1.pdf', '1611.03254v1.pdf', '1504.07495v1.pdf', '1501.07107v1.pdf', '1306.4036v2.pdf', '1603.04812v2.pdf']",1908.04655v1-Figure5-1.png,"Fig. 5 Algorithm performance comparison in the univariate example over 10 realisations of the data for each value of θ∗. The left panel shows Log10(RMSE) of the estimate θ̂, and the right panel shows the mean number of likelihood evaluations, both of which are obtained using the standard NS approach (blue star points) and the autoPR method (red square points).","is given in Figure 5(a), which shows the (logarithm of the) root mean squared error (RMSE) of the estimate θ̂ over 10 realisations of the data for each value of θ∗, for both the standard NS approach and the autoPR method.",Bayesian posterior repartitioning for nested sampling,"Priors in Bayesian analyses often encode informative domain knowledge that
can be useful in making the inference process more efficient. Occasionally,
however, priors may be unrepresentative of the parameter values for a given
dataset, which can result in inefficient parameter space exploration, or even
incorrect inferences, particularly for nested sampling (NS) algorithms. Simply
broadening the prior in such cases may be inappropriate or impossible in some
applications. Hence our previous solution to this problem, known as posterior
repartitioning (PR), redefines the prior and likelihood while keeping their
product fixed, so that the posterior inferences and evidence estimates remain
unchanged, but the efficiency of the NS process is significantly increased. In
its most practical form, PR raises the prior to some power beta, which is
introduced as an auxiliary variable that must be determined on a case-by-case
basis, usually by lowering beta from unity according to some pre-defined
`annealing schedule' until the resulting inferences converge to a consistent
solution. Here we present a very simple yet powerful alternative Bayesian
approach, in which beta is instead treated as a hyperparameter that is inferred
from the data alongside the original parameters of the problem, and then
marginalised over to obtain the final inference. We show through numerical
examples that this Bayesian PR (BPR) method provides a very robust,
self-adapting and computationally efficient `hands-off' solution to the problem
of unrepresentative priors in Bayesian inference using NS. Moreover, unlike the
original PR method, we show that even for representative priors BPR has a
negligible computational overhead relative to standard nesting sampling, which
suggests that it should be used as the default in all NS analyses."
scgqa_301,1512.00843v3,"According to Figure 4B, how does Q8 accuracy relate to the number of layers in the DeepCNF architecture?",The graph in Figure 4B suggests that the Q8 accuracy of the DeepCNF model is not as strongly affected by the number of layers as it is in Figure 4A. This is because the model has a different architecture in which each layer has a different number of neurons. This allows the model to learn different patterns in the data at different layers.,1512.00843v3.pdf,"['1512.00843v3.pdf', '1902.06156v1.pdf', '1803.01118v2.pdf', '1805.01892v1.pdf', '1808.06304v2.pdf', '1502.00588v1.pdf']",1512.00843v3-Figure4-1.png,"Figure 4. The Q8 accuracy on CB513 by the models of different number of layers of 1, 3, 5, and 7 (the same window size is used). (A) Each layer of the 4 models has 100 neurons for a position. The total parameter number of the 4 models is different. (B) Each layer of the models has different neurons for a position. The total parameter number of the 4 models is similar.","segment results in a lower SOV score than a wrong prediction at the terminal regions. A detailed definition of SOV is described in 32 , and also in our Supplemental File. Determining the regularization factor by cross validation. Our DeepCNF has only a hyper-parameter, i.e., the regularization factor, which is used to avoid overfitting. Once it is fixed, we can estimate all the model parameters by solving the optimization problem in Eq. (10). To choose the right regularization factor and examine the stability of our DeepCNF model, we conduct a five-fold cross-validation test. In particular, we randomly divide the training set (containing 5600 CullPDB proteins) into 5 subsets and then use 4 subsets as training and one as validation. Figure 3 shows the Q8 accuracy of our DeepCNF model with respect to the regularization factor. The optimal regularization factor is around 50, which yields 73.2% Q8 accuracy on average. At this point, the Q8 accuracy difference of all the models is less than 1%, consistent with the previous report 33 . Determining the DeepCNF architecture. The architecture of our DeepCNF model is mainly determined by the following 3 factors (see Figure 2): (i) the number of hidden layers; (ii) the number of different neurons at each layer; and (iii) the window size at each layer. We fix the window size to 11 because the average length of an alpha helix is around eleven residues 58 and that of a beta strand is around six 59 . To show the relationship between the performance and the number of hidden layers, we trained four different DeepCNF models with 1, 3, 5, and 7 layers, respectively. All the models use the same window size (i.e., 11) and the same number (i.e., 100) of different neurons at each layer. In total these four models have ~50K, ~270K, ~500K, and ~700K parameters, respectively. We trained the models with different regularization factors. As shown in Figure 4A, when only one hidden layer is used, DeepCNF becomes CNF 50 and its performance is quite similar to RaptorX-SS8 (single model) as shown in Table 1. When more layers are applied, the Q8 accuracy gradually improves. To balance the model complexity and performance, by default we set window size to 11 and use 5 hidden layers, each with 100 different neurons.","Protein secondary structure prediction using deep convolutional neural
  fields","Protein secondary structure (SS) prediction is important for studying protein
structure and function. When only the sequence (profile) information is used as
input feature, currently the best predictors can obtain ~80% Q3 accuracy, which
has not been improved in the past decade. Here we present DeepCNF (Deep
Convolutional Neural Fields) for protein SS prediction. DeepCNF is a Deep
Learning extension of Conditional Neural Fields (CNF), which is an integration
of Conditional Random Fields (CRF) and shallow neural networks. DeepCNF can
model not only complex sequence-structure relationship by a deep hierarchical
architecture, but also interdependency between adjacent SS labels, so it is
much more powerful than CNF. Experimental results show that DeepCNF can obtain
~84% Q3 accuracy, ~85% SOV score, and ~72% Q8 accuracy, respectively, on the
CASP and CAMEO test proteins, greatly outperforming currently popular
predictors. As a general framework, DeepCNF can be used to predict other
protein structure properties such as contact number, disorder regions, and
solvent accessibility."
scgqa_302,1708.05355v1,What are the continuous and piecewise properties of the sigmoid and Geman-McClure functions as described in this research?,"The sigmoid function and the Geman-McClure function are both continuous functions that approximate the conventional discrete setting. However, there are some key differences between the two functions. The sigmoid function is a monotonically increasing function, while the Geman-McClure function is not. This means that the sigmoid function always increases in value as its input increases, while the Geman-McClure function can decrease in value as its input increases. Additionally, the sigmoid function is a smooth function, while the Geman-McClure function is a piecewise linear function. This means that the sigmoid function does not have any sharp corners, while the Geman-McClure function does.",1708.05355v1.pdf,"['1708.05355v1.pdf', '1911.11395v2.pdf', '1810.03742v1.pdf', '2010.12427v3.pdf']",1708.05355v1-Figure6-1.png,Figure 6. (a) Sigmoid function. (b) Geman-McClure function.,"As shown in Figs. 6a and 6b, these continuous functions approximate the conventional discrete setting, but they assess subtle brightness variations more naturally when their input is near zero. In other words, they are still robust, but less brittle than the original Hamming-based definition.","MirrorFlow: Exploiting Symmetries in Joint Optical Flow and Occlusion
  Estimation","Optical flow estimation is one of the most studied problems in computer
vision, yet recent benchmark datasets continue to reveal problem areas of
today's approaches. Occlusions have remained one of the key challenges. In this
paper, we propose a symmetric optical flow method to address the well-known
chicken-and-egg relation between optical flow and occlusions. In contrast to
many state-of-the-art methods that consider occlusions as outliers, possibly
filtered out during post-processing, we highlight the importance of joint
occlusion reasoning in the optimization and show how to utilize occlusion as an
important cue for estimating optical flow. The key feature of our model is to
fully exploit the symmetry properties that characterize optical flow and
occlusions in the two consecutive images. Specifically through utilizing
forward-backward consistency and occlusion-disocclusion symmetry in the energy,
our model jointly estimates optical flow in both forward and backward
direction, as well as consistent occlusion maps in both views. We demonstrate
significant performance benefits on standard benchmarks, especially from the
occlusion-disocclusion symmetry. On the challenging KITTI dataset we report the
most accurate two-frame results to date."
scgqa_303,2006.11769v1,How many experiments did the authors conclude were necessary for the effective training of encoders in this research?,"The graph suggests that just one experiment is needed for the training of the encoders. This is because the encoders are able to learn the underlying dynamics of the environment after just a few epochs of training. This is a significant improvement over previous methods, which required many more experiments to train the encoders.",2006.11769v1.pdf,"['2006.11769v1.pdf', '1806.05387v1.pdf', '1505.02851v1.pdf', '1911.04231v2.pdf']",2006.11769v1-Figure3-1.png,Figure 3: Reconstruction error on the validation set over epochs of training for the encoder E1 (left) and the encoder E2 (right).,"Figure 3 shows the reconstruction error obtained by the encoders of the sensors on the validation set among epochs. It can be seen that around the fourth epoch for Ex, and sixth epoch for Ey, the reconstruction error converges to zero. At this point we consider that the encoders have learned to successfully represent any observation of the environment, and therefore, just one experiment was carried out for its training.",Emergent cooperation through mutual information maximization,"With artificial intelligence systems becoming ubiquitous in our society, its
designers will soon have to start to consider its social dimension, as many of
these systems will have to interact among them to work efficiently. With this
in mind, we propose a decentralized deep reinforcement learning algorithm for
the design of cooperative multi-agent systems. The algorithm is based on the
hypothesis that highly correlated actions are a feature of cooperative systems,
and hence, we propose the insertion of an auxiliary objective of maximization
of the mutual information between the actions of agents in the learning
problem. Our system is applied to a social dilemma, a problem whose optimal
solution requires that agents cooperate to maximize a macroscopic performance
function despite the divergent individual objectives of each agent. By
comparing the performance of the proposed system to a system without the
auxiliary objective, we conclude that the maximization of mutual information
among agents promotes the emergence of cooperation in social dilemmas."
scgqa_304,1912.08775v1,"Referring to findings in the MRI paper, does dropout enhance cumulative aggregate saliency in the integration networks?",The graph shows that the cumulative aggregate saliency is higher for the input-level integration networks trained with dropout. This suggests that dropout is helping to prevent the neural network from overfitting to the training data.,1912.08775v1.pdf,"['1912.08775v1.pdf', '1804.06674v1.pdf', '1612.07141v3.pdf', '1912.03417v1.pdf', '1804.06161v2.pdf', '1709.03329v1.pdf', '1304.7375v1.pdf']",1912.08775v1-Figure5-1.png,Fig. 5: Cumulative Aggregate Saliencies for Network Trained without Dropout.,"Below, we investigate the cumulative aggregate saliency (Equation 5b) for our input-level integration networks trained with and without dropout. In Figures 5 and 6, we look at a single input-level integration network (corresponding to the input-level integration networks shown in Tables 1-6). On the left, we see the cumulative aggregate saliencies separated by input pulse sequences. On the right, we see the cumulative aggregate saliencies separated by z-slice position.","MRI Pulse Sequence Integration for Deep-Learning Based Brain Metastasis
  Segmentation","Magnetic resonance (MR) imaging is an essential diagnostic tool in clinical
medicine. Recently, a variety of deep learning methods have been applied to
segmentation tasks in medical images, with promising results for computer-aided
diagnosis. For MR images, effectively integrating different pulse sequences is
important to optimize performance. However, the best way to integrate different
pulse sequences remains unclear. In this study, we evaluate multiple
architectural features and characterize their effects in the task of metastasis
segmentation. Specifically, we consider (1) different pulse sequence
integration schemas, (2) different modes of weight sharing for parallel network
branches, and (3) a new approach for enabling robustness to missing pulse
sequences. We find that levels of integration and modes of weight sharing that
favor low variance work best in our regime of small data (n = 100). By adding
an input-level dropout layer, we could preserve the overall performance of
these networks while allowing for inference on inputs with missing pulse
sequence. We illustrate not only the generalizability of the network but also
the utility of this robustness when applying the trained model to data from a
different center, which does not use the same pulse sequences. Finally, we
apply network visualization methods to better understand which input features
are most important for network performance. Together, these results provide a
framework for building networks with enhanced robustness to missing data while
maintaining comparable performance in medical imaging applications."
scgqa_305,1805.00184v1,What does the data in Figure 1 reveal about linear factorization's ability to approximate the identity matrix?,"The graph shows that linear factorization is unable to approximate matrices with low-rank, even if they have a constant generalized round-rank. This is because the singular values of a matrix are not necessarily indicative of its round-rank. For example, the identity matrix has singular values of 1, but its round-rank is 2. This means that even though the identity matrix can be approximated by a linear factorization of rank 1, it cannot be approximated by a linear factorization of rank 2.",1805.00184v1.pdf,"['1805.00184v1.pdf', '2007.11391v1.pdf', '1703.01827v3.pdf']",1805.00184v1-Figure1-1.png,"Figure 1: Comparison of the optimal linear factorization approximation as the rank k is varied for a number of matrices (of size n× n), demonstrating that linear factorization is unable to approximate these matrices with low-rank. All of these matrices have a constant generalized round-rank (≤ 2).","Let us consider Y, the identity binary matrix of size n, for which the singular values of Y are all 1s. By using Theorem 3.2, any linear factorization Y′ of rank k will have ||Y −Y′||2F ≥ (n− k). As a result, the identity matrix cannot be approximated by any rank-k linear factorization for < n− k. On the other hand, such a matrix can be reconstructed exactly with a rank 2 factorization if using the round-link function, since round-rank(Y) = 2. In Figure 1, we illustrate a number of other such matrices, i.e. they can be exactly represented by a factorization with GRR of 2, but cannot be approximated by any compact linear factorization.",Compact Factorization of Matrices Using Generalized Round-Rank,"Matrix factorization is a well-studied task in machine learning for compactly
representing large, noisy data. In our approach, instead of using the
traditional concept of matrix rank, we define a new notion of link-rank based
on a non-linear link function used within factorization. In particular, by
applying the round function on a factorization to obtain ordinal-valued
matrices, we introduce generalized round-rank (GRR). We show that not only are
there many full-rank matrices that are low GRR, but further, that these
matrices cannot be approximated well by low-rank linear factorization. We
provide uniqueness conditions of this formulation and provide gradient
descent-based algorithms. Finally, we present experiments on real-world
datasets to demonstrate that the GRR-based factorization is significantly more
accurate than linear factorization, while converging faster and using lower
rank representations."
scgqa_306,2003.09700v4,"Based on Figure 6 in the XTDrone paper, which algorithm shows superior performance during yaw maneuvers?","The graph shows that ORB-SLAM2 performs better than VINS-Fusion without IMU in terms of trajectory accuracy. This is evident from the fact that the ORB-SLAM2 trajectory is closer to the ground truth than the VINS-Fusion without IMU trajectory. This is likely due to the fact that ORB-SLAM2 uses a more sophisticated algorithm to estimate the camera pose, which is better able to handle the large-scale yaw maneuverings that are present in the dataset.",2003.09700v4.pdf,"['2003.09700v4.pdf', '1608.08469v1.pdf', '1107.4161v1.pdf', '1309.3959v1.pdf', '1805.06370v2.pdf', '1910.10700v1.pdf']",2003.09700v4-Figure6-1.png,Figure 6. Trajectories compared with the ground truth,"Fig. 6 shows the trajectories generated by ORB-SLAM2 and VINS-Fusion without IMU, compared with the true flight trajectory. There are four large-scale yaw maneuverings, which is a great challenge to SLAM algorithms. Fig. 7 shows that the relative pose error (RPE) with regard to rotation angle. It can be seen that great errors happen when yaw maneuvering, and ORB-SLAM2 behaves better than VINS-Fusion without IMU.",XTDrone: A Customizable Multi-Rotor UAVs Simulation Platform,"A customizable multi-rotor UAVs simulation platform based on ROS, Gazebo and
PX4 is presented. The platform, which is called XTDrone, integrates dynamic
models, sensor models, control algorithm, state estimation algorithm, and 3D
scenes. The platform supports multi UAVs and other robots. The platform is
modular and each module can be modified, which means that users can test its
own algorithm, such as SLAM, object detection, motion planning, attitude
control, multi-UAV cooperation, and cooperation with other robots on the
platform. The platform runs in lockstep, so the simulation speed can be
adjusted according to the computer performance. In this paper, two cases,
evaluating different visual SLAM algorithm and realizing UAV formation, are
shown to demonstrate how the platform works."
scgqa_307,1902.06156v1,What accuracy results do Figure 3 indicate for different aggregation methods on the CIFAR10 dataset under attack?,"The graph suggests that the simplest aggregation rule, i.e. averaging the workers' parameters, is the most effective under attack. However, even this rule is not immune to attack, as the accuracy dropped by 28%. Krum performed worst again for the same reason with a drop of 66%, Bulyan dropped by 52% and TrimmedMean performed slightly better but still dropped by 45%.",1902.06156v1.pdf,"['1902.06156v1.pdf', '1808.09050v2.pdf', '1805.00184v1.pdf', '1702.06270v2.pdf', '1612.03449v3.pdf', '2004.03870v1.pdf', '1107.4161v1.pdf']",1902.06156v1-Figure3-1.png,Figure 3: Model accuracy on CIFAR10. m = 20% and z = 1. No Attack is plotted for reference.,"Experiment results on CIFAR10 are shown in Figure 3. Since fewer standard deviations can cause a significant impact on CIFAR10 (see Table 1), we choose m = 20% corrupt workers, and change the parameters by only 1σ. Again, the best accuracy was achieved with the simplest aggregation rule, i.e. averaging the workers’ parameters, but still the accuracy dropped by 28%. Krum performed worst again for the same reason with a drop of 66%, Bulyan dropped by 52% and TrimmedMean performed slightly better but still dropped by 45%.",A Little Is Enough: Circumventing Defenses For Distributed Learning,"Distributed learning is central for large-scale training of deep-learning
models. However, they are exposed to a security threat in which Byzantine
participants can interrupt or control the learning process. Previous attack
models and their corresponding defenses assume that the rogue participants are
(a) omniscient (know the data of all other participants), and (b) introduce
large change to the parameters. We show that small but well-crafted changes are
sufficient, leading to a novel non-omniscient attack on distributed learning
that go undetected by all existing defenses. We demonstrate our attack method
works not only for preventing convergence but also for repurposing of the model
behavior (backdooring). We show that 20% of corrupt workers are sufficient to
degrade a CIFAR10 model accuracy by 50%, as well as to introduce backdoors into
MNIST and CIFAR10 models without hurting their accuracy"
scgqa_308,1701.06190v1,"What conclusion can be drawn about the proposed method's reliance on color, based on Figure 8's gray channel results?","The fact that the proposed method has better performance than others even in the case of using only gray channel image is significant because it shows that the proposed method is not reliant on color information. This is important for a face recognition system, as it means that the system can still perform well even if the image is in black and white.",1701.06190v1.pdf,"['1701.06190v1.pdf', '1809.01628v1.pdf', '1501.07107v1.pdf', '1402.0808v1.pdf', '1706.03112v1.pdf']",1701.06190v1-Figure8-1.png,Figure 8. Comparison of PR (left) and ROC (right) curves on Pratheepan dataset.,"Comparison on Pratheepan dataset is shown in Figure 8, where it can be seen that the proposed method has the best performance on both PR and ROC curves. Moreover, it has much better performance than others even in the case of using only gray channel image. Table 3 also shows that proposed method yields good results in terms of many performance measures. Finally, the subjective comparison for Pratheepan dataset is shown in Figure 10.","A New Convolutional Network-in-Network Structure and Its Applications in
  Skin Detection, Semantic Segmentation, and Artifact Reduction","The inception network has been shown to provide good performance on image
classification problems, but there are not much evidences that it is also
effective for the image restoration or pixel-wise labeling problems. For image
restoration problems, the pooling is generally not used because the decimated
features are not helpful for the reconstruction of an image as the output.
Moreover, most deep learning architectures for the restoration problems do not
use dense prediction that need lots of training parameters. From these
observations, for enjoying the performance of inception-like structure on the
image based problems we propose a new convolutional network-in-network
structure. The proposed network can be considered a modification of inception
structure where pool projection and pooling layer are removed for maintaining
the entire feature map size, and a larger kernel filter is added instead.
Proposed network greatly reduces the number of parameters on account of removed
dense prediction and pooling, which is an advantage, but may also reduce the
receptive field in each layer. Hence, we add a larger kernel than the original
inception structure for not increasing the depth of layers. The proposed
structure is applied to typical image-to-image learning problems, i.e., the
problems where the size of input and output are same such as skin detection,
semantic segmentation, and compression artifacts reduction. Extensive
experiments show that the proposed network brings comparable or better results
than the state-of-the-art convolutional neural networks for these problems."
scgqa_309,1608.00887v1,What conclusions can be drawn from Fig. 5 about the effectiveness of single-frame versus multi-frame detection in the study?,"The graph shows that the multi-frame CNN outperforms the single-frame CNN, and that the LSTM-CNN performs much better than both by a significant margin. This suggests that detecting transparent liquid must be done over a series of frames, rather than a single frame.",1608.00887v1.pdf,"['1608.00887v1.pdf', '1502.03556v1.pdf', '1206.5265v1.pdf', '1006.4386v1.pdf', '2006.04002v2.pdf', '1711.06964v1.pdf', '1003.1655v1.pdf']",1608.00887v1-Figure5-1.png,"Fig. 5: Quantitative fixed- and multi-viewpoint liquid detection results. The graphs indicate the precision and recall for each of the three networks on fixed-viewpoint detection and the LSTM-CNN on multi-viewpoint detection. The colored lines indicate the variation in the number of slack pixels we allowed for prediction, i.e., how many pixels a positive classification could be away from a positive ground truth labeling and still be counted as correct.","Fig. 4 shows qualitative results for the three networks on the liquid detection task2. The frames in this figure were randomly selected from the training set, and it is clear from the figure that all three networks detect the liquid at least to some degree. Figures 5a to 5c show a quantitative comparison between the three networks. As expected, the multi-frame CNN outperforms the single-frame. Surprisingly, the LSTM-CNN performs much better than both by a significant margin. These results strongly suggest that detecting transparent liquid must be done over a series of frames, rather than a single frame. 2 Video of the full sequences at https://youtu.be/m5z0aFZgEX8",Towards Learning to Perceive and Reason About Liquids,"Recent advances in AI and robotics have claimed many incredible results with
deep learning, yet no work to date has applied deep learning to the problem of
liquid perception and reasoning. In this paper, we apply fully-convolutional
deep neural networks to the tasks of detecting and tracking liquids. We
evaluate three models: a single-frame network, multi-frame network, and a LSTM
recurrent network. Our results show that the best liquid detection results are
achieved when aggregating data over multiple frames and that the LSTM network
outperforms the other two in both tasks. This suggests that LSTM-based neural
networks have the potential to be a key component for enabling robots to handle
liquids using robust, closed-loop controllers."
scgqa_310,1509.01310v1,"In the context of the experiments shown in Figure 5, how do sentence lengths correlate with MDD values?","The graph shows that the MDD of a sentence increases as the sentence length increases. This is because a longer sentence has more words, and each word can be a source of ambiguity. As a result, it is more difficult to generate a minimal MDD for a longer sentence.",1509.01310v1.pdf,"['1509.01310v1.pdf', '1403.2732v1.pdf', '1803.03080v1.pdf', '1704.00325v1.pdf', '1407.5358v1.pdf', '2005.09634v1.pdf', '1805.01358v2.pdf', '1905.00569v2.pdf']",1509.01310v1-Figure5-1.png,Fig. 5: Relations between sentence lengths and MDDs.,"We obtained the minimal MDDs of random sentences that are chunked, and compared it with continous random language(RL2) and natural language (NL), as illustraded in Fig. 5. In this figure, RL3_RC and RL3_FC indicates the minimal MDDs of random sentences when maximal chunk size is controlled and when all chunks are of the same length, as we formerly mentioned in Fig 4 (a) and Fig 4 (b).",The influence of Chunking on Dependency Crossing and Distance,"This paper hypothesizes that chunking plays important role in reducing
dependency distance and dependency crossings. Computer simulations, when
compared with natural languages,show that chunking reduces mean dependency
distance (MDD) of a linear sequence of nodes (constrained by continuity or
projectivity) to that of natural languages. More interestingly, chunking alone
brings about less dependency crossings as well, though having failed to reduce
them, to such rarity as found in human languages. These results suggest that
chunking may play a vital role in the minimization of dependency distance, and
a somewhat contributing role in the rarity of dependency crossing. In addition,
the results point to a possibility that the rarity of dependency crossings is
not a mere side-effect of minimization of dependency distance, but a linguistic
phenomenon with its own motivations."
scgqa_311,1802.05945v1,"According to the funding analysis in the paper, what is the outlook for rare disease research in the UK?","The trends in the graph suggest that rare diseases research in the United Kingdom is on the rise, with increasing output and impact scores. This is likely due to the increasing focus on rare diseases research in the UK, as well as the increasing availability of funding for this research.",1802.05945v1.pdf,"['1802.05945v1.pdf', '1908.05243v1.pdf', '1807.06736v1.pdf', '1701.05681v3.pdf', '1608.00887v1.pdf', '1809.02337v2.pdf']",1802.05945v1-Figure7-1.png,"Figure 7a: Output numbers, United Kingdom Figure 7b: Impact scores, United Kingdom","In Figures 4-7 we show parts a and b. In part a the output numbers per funding source are shown, while the b parts display the impact scores related to the output numbers shown in part a. In Figures 4a and 4b the situation for France is depicted. In terms of output development, the none funded is decreasing, while the other types of funding sources show slow increases in numbers. The ‘gap’ between no funding, and the funded share of the French output is relatively large. When we shift our attention to the impact scores, the no funded part has the lowest impact, and is stable on that low level. The other four types of funding sources show higher impact level (fluctuating between 20% above worldwide average impact level to 60% above worldwide average impact level, for the Europe-national funding source). Figures 5a and 5b display the output and impact for Great Britain. Contrary to France, we now observe some funding source types with higher numbers of publications (national and other funding). Europe and Europe-national show the lowest number of publications from Great Britain related to rare diseases research. Focusing on impact scores related to the output from Great Britain on rare diseases, we notice that the non-funded part has a low impact, similar to the situation found for France on this type, albeit it at a somewhat higher level. For Europe, Other funding, and nation, we observe impact level that fluctuate around 50% above average impact level for worldwide, while finally the output in Europe-national context displays a very high impact level, that is increasing to twice worldwide average impact level in 2013-2014/15. In figures 6a and 6b, output and impact for the Netherlands are shown, in which we find some similarity to the situation described for Great Britain: decrease in output for no funding, increasing output for other funding and national (with some stabilization for the latter in the final stages of the period). Like for Great Britain, Europe-national and Europe have small output numbers. Impact wise, the no funding part has an average impact level, while we observe some strong fluctuation for the impact of in particular Europe-national and Europe (which is partially explained by the small numbers involved, which tends to occur more frequently in bibliometric analyses). Other funding and national shows more stable impact levels around 50% above worldwide average impact level. Finally, in Figure 7a and 7b, the situation for Spain is displayed. In Figure 7a, the output is shown. Here we observe a somewhat different pattern as found for the other three countries, as here no funding is covering the largest part of the output, decreasing in numbers, while national as the second largest type of funding source is increasing in numbers. Other funding","How does undone science get funded? A bibliometric analysis linking rare
  diseases publications to national and European funding sources","One of the notable features of undone science debates is how formation of new
interest groups becomes pivotal in mobilizing and championing emerging research
on undone topics. Clearly money is one of the most important mediums through
which different types of actors can support and steer scientists to work on
undone topics. Yet which actors are more visible in their support for
scientific research is something which has seldom been measured. This study
delves into research funding in the context of rare diseases research, a topic
which has evolved from the margins of medical research into a priority area
articulated by many contemporary funding agencies. Rare diseases refer to
conditions affecting relatively few people in a population. Given low
incidences, interest groups have articulated a lack of attention within medical
research compared to more common conditions. The rise to prominence of rare
diseases in research funding policies is often explained in the science studies
literature in terms of effective lobbying by social movements Likewise,
innovative fundraising initiatives, infrastructure building, and close
partnerships with research groups are other means through which interested
actors have sought to build capacity for research into rare medical conditions.
To date however systematic empirical evidence to compare the relative
importance of different actors in funding rare disease research has not been
produced. Building on interest in undone science in STS and science policy
studies, our study hopes to map-out different kinds of funding actors and their
influence on leading scientific research on rare diseases, by use of
bibliometric tools. The approach we are developing relies on the use of Funding
Acknowledgement data provided in Web of Science database."
scgqa_312,1807.06736v1,How do positive and negative bias values impact synthetic speech duration in the FA-TA system according to the paper?,"The graph shows how the average duration of sentences generated by the FA-TA system changes when the bias value is varied. A positive bias value increases the transition probability, which leads to a faster generation of sentences. A negative bias value decreases the transition probability, which leads to a slower generation of sentences. The graph shows that the average duration of sentences can be increased or decreased by more than 10% by controlling the bias value.",1807.06736v1.pdf,"['1807.06736v1.pdf', '2007.15958v1.pdf', '1707.04476v5.pdf']",1807.06736v1-Figure3-1.png,Fig. 3. Average ratios of sentence duration modification achieved by controlling the bias value in the FA-TA system. Error bars represent the standard deviations.,"In the proposed forward attention with transition agent, as we adding positive or negative bias to the sigmoid output units of the DNN transition agent during generation, the transition probability gets increased or decreased. This leads to a faster or slower of attention results. An experiment was conducted using the plain FA-TA system to evaluate the effectiveness of speed control using this property. We used the same test set of the 20 utterances in Section 4.3. We increased or decreased the bias value from 0 with a step of 0.2, and synthesized all sentences in the test set. We stopped once one of the generated samples had the problem of missing phones, repeating phones, or making any perceivable mistakes. Then we calculated the average ratios between the lengths of synthetic sentences using modified bias and the lengths of synthetic sentences without bias modification. Fig. 3 show the results in a range of bias modification where all samples were generated correctly. From this figure, we can see that more than 10% speed control can be achieved using the proposed forward attention with transition agent. Informal listening test showed that such modification did not degrade the naturalness of synthetic speech.","Forward Attention in Sequence-to-sequence Acoustic Modelling for Speech
  Synthesis","This paper proposes a forward attention method for the sequenceto- sequence
acoustic modeling of speech synthesis. This method is motivated by the nature
of the monotonic alignment from phone sequences to acoustic sequences. Only the
alignment paths that satisfy the monotonic condition are taken into
consideration at each decoder timestep. The modified attention probabilities at
each timestep are computed recursively using a forward algorithm. A transition
agent for forward attention is further proposed, which helps the attention
mechanism to make decisions whether to move forward or stay at each decoder
timestep. Experimental results show that the proposed forward attention method
achieves faster convergence speed and higher stability than the baseline
attention method. Besides, the method of forward attention with transition
agent can also help improve the naturalness of synthetic speech and control the
speed of synthetic speech effectively."
scgqa_313,1905.12729v2,What does Figure 1 indicate about the speed of objective value reduction for our algorithms versus other methods?,"The graph shows that the objective values of our algorithms decrease faster than the other algorithms, as the CPU time increases. This indicates that our algorithms are more efficient in terms of both time and accuracy.",1905.12729v2.pdf,"['1905.12729v2.pdf', '2001.11086v3.pdf', '1407.7736v1.pdf', '1206.5265v1.pdf', '2010.08182v3.pdf', '2008.07011v1.pdf']",1905.12729v2-Figure1-1.png,Figure 1: Objective value gaps versus CPU time on benchmark datasets.,"−5. In the experiment, we use some public real datasets1, which are summarized in Table 2. For each dataset, we use half of the samples as training data and the rest as testing data. Figure 1 shows that the objective values of our algorithms faster decrease than the other algorithms, as the CPU time increases. In particular, our algorithms show better performances than the zeroth-order proximal algorithms. It is relatively difficult that these zeroth-order proximal methods deal with the nonsmooth penalties in the problem (6). Thus, we have to use some iterative methods to solve the proximal operator in these proximal methods.","Zeroth-Order Stochastic Alternating Direction Method of Multipliers for
  Nonconvex Nonsmooth Optimization","Alternating direction method of multipliers (ADMM) is a popular optimization
tool for the composite and constrained problems in machine learning. However,
in many machine learning problems such as black-box attacks and bandit
feedback, ADMM could fail because the explicit gradients of these problems are
difficult or infeasible to obtain. Zeroth-order (gradient-free) methods can
effectively solve these problems due to that the objective function values are
only required in the optimization. Recently, though there exist a few
zeroth-order ADMM methods, they build on the convexity of objective function.
Clearly, these existing zeroth-order methods are limited in many applications.
In the paper, thus, we propose a class of fast zeroth-order stochastic ADMM
methods (i.e., ZO-SVRG-ADMM and ZO-SAGA-ADMM) for solving nonconvex problems
with multiple nonsmooth penalties, based on the coordinate smoothing gradient
estimator. Moreover, we prove that both the ZO-SVRG-ADMM and ZO-SAGA-ADMM have
convergence rate of $O(1/T)$, where $T$ denotes the number of iterations. In
particular, our methods not only reach the best convergence rate $O(1/T)$ for
the nonconvex optimization, but also are able to effectively solve many complex
machine learning problems with multiple regularized penalties and constraints.
Finally, we conduct the experiments of black-box binary classification and
structured adversarial attack on black-box deep neural network to validate the
efficiency of our algorithms."
scgqa_314,1712.02030v2,"As illustrated in Figure 27 of the paper, how does the ratio of execution times for the two methods behave with varying discretization values?","The graph shows that the Projection Method is faster than the Decoupling Method for all values of the discretization parameter M. The ratio of execution times between the two methods decreases as M increases, approaching a value of 3 as M approaches infinity. This is consistent with the theoretical results, which show that the Projection Method should theoretically run three times faster than the Decoupling method.",1712.02030v2.pdf,"['1712.02030v2.pdf', '1505.02851v1.pdf', '1901.10423v1.pdf']",1712.02030v2-Figure27-1.png,Figure 27: Graph of the ratios of the execution times for various discretization values between the Decoupling Method and the Projection Method: Decoupling method time divided by Projection method time.,"As demonstrated in the Time Analysis section, the Projection Method should theoretically run three times faster than the Decoupling method and 27 times faster than the Saddle-Point method. Upon observing figure 26, the ratio of execution time between the Projection and Saddle-Point method seems to be dependent upon the number of discretization points M , taking on the ratio of 27 around M = 180 but then exceeding it as M grows. Figure 27 shows the ratio asymptotically approaching 3, which matches the theoretical results. Possible confounding factors that cause these results to not match perfectly with the theoretical time analysis include overhead from other operations in the code or operating system and the Matlab ""\"" operator having a best case O(n) and worst case O(n3) complexity.",Projection Method for Solving Stokes Flow,"Various methods for numerically solving Stokes Flow, where a small Reynolds
number is assumed to be zero, are investigated. If pressure, horizontal
velocity, and vertical velocity can be decoupled into three different
equations, the numerical solution can be obtained with significantly less
computation cost than when compared to solving a fully coupled system. Two
existing methods for numerically solving Stokes Flow are explored: One where
the variables can be decoupled and one where they cannot. The existing
decoupling method the limitation that the viscosity must be spatially constant.
A new method is introduced where the variables are decoupled without the
viscosity limitation. This has potential applications in the modeling of red
blood cells as vesicles to assist in storage techniques that do not require
extreme temperatures, such as those needed for cyropreservation."
scgqa_315,2007.11391v1,"In the context of the experiments presented, how does the choice of α influence the outcomes displayed in Fig. 2?",The graph illustrates the importance of choosing an optimal regularisation parameter α by showing the results of reconstructions with too small and too large values of α. The reconstruction with too small a value of α is too smooth and does not capture the true characteristics of the signal. The reconstruction with too large a value of α is too noisy and does not accurately represent the signal. The optimal value of α lies between these two extremes and results in a reconstruction that is both smooth and accurate.,2007.11391v1.pdf,"['2007.11391v1.pdf', '1603.01793v2.pdf', '1804.10488v2.pdf', '1906.03859v1.pdf', '1307.3687v1.pdf', '1805.07914v3.pdf', '1602.07579v1.pdf']",2007.11391v1-Figure2-1.png,"Fig. 2: Results with the Cauchy difference prior for 1% noise and τ = 0.25 with different values of α indicated on top of the figures. Additionally, we present the estimated parameter τ̂ for each case. Top row: reconstruction of the unknown signal f . Bottom row: estimates of the logarithmic lengthscale function.","An important aspect in the reconstruction is the choice of regularisation parameter α for the priors in (5), which will influence the reconstructed characteristics. To find such an optimal regularisation parameter α, we performed a grid search for each dataset and prior, to find the parameter such that the mean squared error between the estimate of f and ground truth was minimised. This method of choosing α is clearly unsuitable for measured signals, where the ground truth is not available and hence secondary indicators would be needed to choose a suitable regularisation parameter. Nevertheless, we concentrate here on demonstrating the potential of the proposed framework and in order to illustrate the importance of this choice we present results with too small and too large value of the regularisation parameter α in Figure 2 and 3.",Blind hierarchical deconvolution,"Deconvolution is a fundamental inverse problem in signal processing and the
prototypical model for recovering a signal from its noisy measurement.
Nevertheless, the majority of model-based inversion techniques require
knowledge on the convolution kernel to recover an accurate reconstruction and
additionally prior assumptions on the regularity of the signal are needed. To
overcome these limitations, we parametrise the convolution kernel and prior
length-scales, which are then jointly estimated in the inversion procedure. The
proposed framework of blind hierarchical deconvolution enables accurate
reconstructions of functions with varying regularity and unknown kernel size
and can be solved efficiently with an empirical Bayes two-step procedure, where
hyperparameters are first estimated by optimisation and other unknowns then by
an analytical formula."
scgqa_316,1402.7063v1,How does Figure 13 illustrate the scalability differences between KDANN+ and KDANN on power law distributed datasets?,"The graph shows that KDANN+ scales almost linearly as the data size increases, while KDANN fails to generate any results even for very small datasets. This is because the merging step continues to be an inhibitor factor in kdANN's performance. In addition, kdANN+ scales better than kdANN in the case of synthetic dataset and the running time increases almost linearly as in the case of power law distribution.",1402.7063v1.pdf,"['1402.7063v1.pdf', '1208.2451v1.pdf', '1908.09653v1.pdf']",1402.7063v1-Figure13-1.png,Figure 13: Scalability,"Figure 13 presents the scalability results for real and synthetic datasets. In the case of power law distribution, the results display that kdANN+ scales almost linearly as the data size increases. In contrast, kdANN fails to generate any results even for very small datasets since the merging step continues to be an inhibitor factor in kdANN’s performance. In addition, we can see that kdANN+ scales better than kdANN in the case of synthetic dataset and the running time increases almost linearly as in the case of power","Rapid AkNN Query Processing for Fast Classification of Multidimensional
  Data in the Cloud","A $k$-nearest neighbor ($k$NN) query determines the $k$ nearest points, using
distance metrics, from a specific location. An all $k$-nearest neighbor
(A$k$NN) query constitutes a variation of a $k$NN query and retrieves the $k$
nearest points for each point inside a database. Their main usage resonates in
spatial databases and they consist the backbone of many location-based
applications and not only (i.e. $k$NN joins in databases, classification in
data mining). So, it is very crucial to develop methods that answer them
efficiently. In this work, we propose a novel method for classifying
multidimensional data using an A$k$NN algorithm in the MapReduce framework. Our
approach exploits space decomposition techniques for processing the
classification procedure in a parallel and distributed manner. To our
knowledge, we are the first to study the classification of multidimensional
objects under this perspective. Through an extensive experimental evaluation we
prove that our solution is efficient and scalable in processing the given
queries. We investigate many different perspectives that can affect the total
computational cost, such as different dataset distributions, number of
dimensions, growth of $k$ value and granularity of space decomposition and
prove that our system is efficient, robust and scalable."
scgqa_317,1901.10423v1,What trend does Figure 5 illustrate regarding interwheel distance and the values of self-sustainability conditions?,"The graph in Figure 5 shows that the value of the conditions decreases as the interwheel distance increases. This is because as the interwheel distance increases, the robot has more room to maneuver and is less likely to tip over. This means that the conditions for self-sustainability are less likely to be violated when the interwheel distance is larger.",1901.10423v1.pdf,"['1901.10423v1.pdf', '1809.08207v1.pdf', '1603.04153v1.pdf']",1901.10423v1-Figure5-1.png,"Fig. 5. A graphical proof that (S0) is the strictest condition. The graph relates the length of the interwheel distance to the value of the conditions, for a constant value of the robot body radius..","self-sustains if the three conditions are satisfied at the same time. This occurs when the strictest among them holds. As shown graphically in Fig. 5, the strictest condition is (S0).",A Minimalistic Approach to Segregation in Robot Swarms,"We present a decentralized algorithm to achieve segregation into an arbitrary
number of groups with swarms of autonomous robots. The distinguishing feature
of our approach is in the minimalistic assumptions on which it is based.
Specifically, we assume that (i) Each robot is equipped with a ternary sensor
capable of detecting the presence of a single nearby robot, and, if that robot
is present, whether or not it belongs to the same group as the sensing robot;
(ii) The robots move according to a differential drive model; and (iii) The
structure of the control system is purely reactive, and it maps directly the
sensor readings to the wheel speeds with a simple 'if' statement. We present a
thorough analysis of the parameter space that enables this behavior to emerge,
along with conditions for guaranteed convergence and a study of non-ideal
aspects in the robot design."
scgqa_318,1505.02851v1,How do the proposed multiplexed network coding schemes enhance performance in the DCSK system as shown in Fig. 6?,"The multiplexed network coding schemes 2 and 3 outperform the PNC-DCSK and the ANC-DCSK systems because they are able to exploit the spatial diversity of the multipath Rayleigh fading channel more effectively. This is because the multiplexed network coding schemes use multiple antennas at the transmitter and receiver, which allows them to transmit and receive multiple data streams simultaneously. This results in a higher degree of diversity, which in turn leads to a lower BER.",1505.02851v1.pdf,"['1505.02851v1.pdf', '1807.09483v2.pdf', '1809.09034v1.pdf']",1505.02851v1-Figure6-1.png,Fig. 6. BER performance comparison between the PNC-DCSK scheme 1 and the multiplexed network coding schemes 2 and 3.,"Fig. 6 compares the simulated BER performance of the end-to-end BER for ANC-DCSK, PNC-DCSK system and the proposed schemes 2 and 3, when communicating over multipath Rayleigh fading channel. In these results, the spreading factor used is β = 25 in addition to the channel parameters listed in the table IV. The figure shows that the multiplexed network coding schemes 2 and 3 outperform the PNC-DCSK and the ANC-DCSK systems. This can be attributed to the","Analysis of Network Coding Schemes for Differential Chaos Shift Keying
  Communication System","In this paper we design network coding schemes for Differential Chaos Shift
Keying (DCSK) modulation. In this work, non-coherent chaos-based communication
system is used due to its simplicity and robustness to multipath propagation
effects, while dispensing with any channel state information knowledge at the
receiver. We propose a relay network using network coding and DCSK, where we
first present a Physical layer Network Coding (PNC) scheme with two users,
$\mathcal{A}$ and $\mathcal{B}$, sharing the same spreading code and bandwidth,
while synchronously transmitting their signals to the relay node $\mathcal{R}$.
We show that the main drawback of this design in multipath channels is the high
level of interference in the resultant signal, which severely degrades the
system performance. Hence, in order to address this problem, we propose two
coding schemes, which separate the users' signals in the frequency or the time
domains. We show also in this paper that the performance of the Analog Network
Coding (ANC) with DCSK modulation suffers from the same interference problem as
the PNC scheme. We present the analytical bit error rate performance for
multipath Rayleigh fading channel for the different scenarios and we analyse
these schemes in terms of complexity, throughput and link spectral efficiency."
scgqa_319,1808.09050v2,"In the context of this research, what can be inferred about anomaly detection when z follows different distributions as shown in Figure 4?",The graph shows that the proposed approach can detect anomaly signals when z is sampled from any distribution. This is because the Nφ − t curves for all three distributions cross the threshold at the same time. This indicates that the proposed approach is robust to the choice of z-sampling distribution.,1808.09050v2.pdf,"['1808.09050v2.pdf', '1405.6408v2.pdf', '1610.00017v2.pdf', '1304.2109v1.pdf', '1904.03292v2.pdf', '1808.06304v2.pdf', '1307.3687v1.pdf', '1811.01194v1.pdf']",1808.09050v2-Figure4-1.png,Fig. 4. The anomaly detection results with different z−sampling distribution.,"For exploring the effect of z−sampling distribution on the performance of the proposed approach, the statistical indices with z sampled from uniform distribution (i.e., z ∼ U(0, 1)), gaussian distribution (i.e., z ∼ N(0, 1)) and exponential distribution (i.e., z ∼ E(1)) were respectively calculated and the corresponding Nφ − t curves were plotted in Figure 4. It can be observed that the assumed anomaly signal can be detected when z is sampled from any distribution. Meanwhile, for the Nφ − t curves, the p values of N̂φ","Adversarial Feature Learning of Online Monitoring Data for Operational
  Risk Assessment in Distribution Networks","With the deployment of online monitoring systems in distribution networks,
massive amounts of data collected through them contains rich information on the
operating states of the networks. By leveraging the data, an unsupervised
approach based on bidirectional generative adversarial networks (BiGANs) is
proposed for operational risk assessment in distribution networks in this
paper. The approach includes two stages: (1) adversarial feature learning. The
most representative features are extracted from the online monitoring data and
a statistical index $\mathcal{N}_{\phi}$ is calculated for the features, during
which we make no assumptions or simplifications on the real data. (2)
operational risk assessment. The confidence level $1-\alpha$ for the population
mean of the standardized $\mathcal{N}_{\phi}$ is combined with the operational
risk levels which are divided into emergency, high risk, preventive and normal,
and the p value for each data point is calculated and compared with
$\frac{\alpha}{2}$ to determine the risk levels. The proposed approach is
capable of discovering the latent structure of the real data and providing more
accurate assessment result. The synthetic data is employed to illustrate the
selection of parameters involved in the proposed approach. Case studies on the
real-world online monitoring data validate the effectiveness and advantages of
the proposed approach in risk assessment."
scgqa_320,1606.01062v1,"According to the results in Figure 2, how does an increase in p relate to series approximation difficulty?","The parameter p is a measure of the degree of nonlinearity in the problem. As p increases, the problem becomes more nonlinear and the number of terms in the series increases. This is because the series is more difficult to approximate with a linear function when p is large.",1606.01062v1.pdf,"['1606.01062v1.pdf', '1608.08469v1.pdf', '1603.08983v6.pdf', '2007.15404v1.pdf', '2010.12427v3.pdf', '1701.00365v2.pdf', '1803.01118v2.pdf', '1612.07141v3.pdf']",1606.01062v1-Figure2-1.png,Figure 2: The number of terms as a function of p,"Now, for fixed ε and δ Figure 2 illustrates the behaviour of the number of terms n as a function of the parameter p ∈ [1, 2]. The plot was produced using the values T = B(0) = ω = 1, Λ = 3/4, and ε = δ = 0.1.","Whittaker-Kotel'nikov-Shannon approximation of $\varphi$-sub-Gaussian
  random processes","The article starts with generalizations of some classical results and new
truncation error upper bounds in the sampling theorem for bandlimited
stochastic processes. Then, it investigates $L_p([0,T])$ and uniform
approximations of $\varphi$-sub-Gaussian random processes by finite time
sampling sums. Explicit truncation error upper bounds are established. Some
specifications of the general results for which the assumptions can be easily
verified are given. Direct analytical methods are employed to obtain the
results."
scgqa_321,1805.01358v2,What insights about detector performance does Figure 5 provide regarding succinctness on KITTI and EuRoC datasets?,"The graph shows that our detector achieves the highest succinctness on both the KITTI and EuRoC datasets. This is likely due to the fact that our detector is designed specifically for use in robotics applications, where it is important to extract as few interest points as possible while still achieving a high inlier count.",1805.01358v2.pdf,"['1805.01358v2.pdf', '1706.03112v1.pdf', '1501.07107v1.pdf']",1805.01358v2-Figure5-1.png,"Figure 5. Succinctness and AUC-200 values on KITTI (top) and EuRoC (bottom) with k = 10. At low point counts, the least amount of points need to be extracted with our detector. AUC could not be evaluated for LF-NET, as only n ∈ {50, 100, 150} were sampled.","Succinctness answers the question: how many interest points nk need to be extracted by a detector to achieve k inlier correspondences? Hence, succinctness will not only depend on the detector, but also the descriptor, matching, potentially other parts of a feature matching pipeline, and whatever method is used to distinguish inlier correspondences from outlier correspondences. Specifically, while this distinction can be made based on ground truth information, we also allow for it to be achieved using another method, such as geometric verification using P3P [13] with RANSAC [11]. Furthermore, just as for other metrics, succinctness will depend on the data on which it is evaluated. In particular, we need a mechanisms to aggregate nk across multiple image pairs, since nk will be different for every evaluated image pair. For this, we could use histograms, but histograms introduce a dependency on the selected bins. Instead, we propose to use a cumulative aggregation. Specifically, we propose the succinctness curve: the x-axis represents a specific amount of extracted points n, and the value on the y axis is s(n), the fraction of pairs whose nk is lower than n. s(n) can also be thought of as the fraction of image pairs that achieve at least k inliers if n points are extracted – we assume that inlier count increases monotonically with the amount of extracted points. An example of such curves can be seen in Figure 5.","SIPs: Succinct Interest Points from Unsupervised Inlierness Probability
  Learning","A wide range of computer vision algorithms rely on identifying sparse
interest points in images and establishing correspondences between them.
However, only a subset of the initially identified interest points results in
true correspondences (inliers). In this paper, we seek a detector that finds
the minimum number of points that are likely to result in an
application-dependent ""sufficient"" number of inliers k. To quantify this goal,
we introduce the ""k-succinctness"" metric. Extracting a minimum number of
interest points is attractive for many applications, because it can reduce
computational load, memory, and data transmission. Alongside succinctness, we
introduce an unsupervised training methodology for interest point detectors
that is based on predicting the probability of a given pixel being an inlier.
In comparison to previous learned detectors, our method requires the least
amount of data pre-processing. Our detector and other state-of-the-art
detectors are extensively evaluated with respect to succinctness on popular
public datasets covering both indoor and outdoor scenes, and both wide and
narrow baselines. In certain cases, our detector is able to obtain an
equivalent amount of inliers with as little as 60% of the amount of points of
other detectors. The code and trained networks are provided at
https://github.com/uzh-rpg/sips2_open ."
scgqa_322,1706.03112v1,What does Figure 6 indicate about the effectiveness of the proposed method for re-identification in dynamic scenarios?,The results shown in the graph suggest that the proposed adaptation technique is a promising approach for improving the performance of person re-identification systems. This technique could be used to improve the performance of existing systems or to develop new systems that are more robust to changes in the environment.,1706.03112v1.pdf,"['1706.03112v1.pdf', '1701.06190v1.pdf', '1409.2897v1.pdf', '1206.5265v1.pdf', '1805.01892v1.pdf']",1706.03112v1-Figure6-1.png,"Figure 6. Re-id performance with LDML as initial setup. Plots (a,b) show CMC curves averaged over all target camera combinations, introduced one at a time, on WARD and RAiD respectively.",Results. Fig. 6 shows results on WARD and RAiD respectively. Following are the analysis of the figures: (i) Our approach outperforms all compared methods in both datasets which suggests that the proposed adaptation technique works significantly well irrespective of the metric,"Unsupervised Adaptive Re-identification in Open World Dynamic Camera
  Networks","Person re-identification is an open and challenging problem in computer
vision. Existing approaches have concentrated on either designing the best
feature representation or learning optimal matching metrics in a static setting
where the number of cameras are fixed in a network. Most approaches have
neglected the dynamic and open world nature of the re-identification problem,
where a new camera may be temporarily inserted into an existing system to get
additional information. To address such a novel and very practical problem, we
propose an unsupervised adaptation scheme for re-identification models in a
dynamic camera network. First, we formulate a domain perceptive
re-identification method based on geodesic flow kernel that can effectively
find the best source camera (already installed) to adapt with a newly
introduced target camera, without requiring a very expensive training phase.
Second, we introduce a transitive inference algorithm for re-identification
that can exploit the information from best source camera to improve the
accuracy across other camera pairs in a network of multiple cameras. Extensive
experiments on four benchmark datasets demonstrate that the proposed approach
significantly outperforms the state-of-the-art unsupervised learning based
alternatives whilst being extremely efficient to compute."
scgqa_323,1805.05887v1,What relationship between memory and rules does Figure 6 illustrate for the LUCON engine?,"The graph shows that the memory consumption of the LUCON engine during a policy decision scales linearly with the number of rules and labels. This means that the engine can handle a large number of rules and labels without consuming too much memory. This is important for IoT gateway devices, which typically have limited memory resources.",1805.05887v1.pdf,"['1805.05887v1.pdf', '1701.00365v2.pdf', '2006.09358v2.pdf', '1808.00136v2.pdf', '1912.02074v1.pdf']",1805.05887v1-Figure6-1.png,"Figure 6. Memory for evaluating a policy decision. 1-5,00 rules/services/labels","The second metric of our performance evaluation is memory consumption. Here, we are especially interested if the framework is suited to run on typical IoT gateway devices or if the Prolog-based implementation it too memory-intensive for such applications. Figure 6 shows the memory consumption of the LUCON engine during a policy decision. Again, the blue line illustrates how memory consumption scales with an increasing number of rules and the red line indicates behavior with an increasing number of labels.",LUCON: Data Flow Control for Message-Based IoT Systems,"Today's emerging Industrial Internet of Things (IIoT) scenarios are
characterized by the exchange of data between services across enterprises.
Traditional access and usage control mechanisms are only able to determine if
data may be used by a subject, but lack an understanding of how it may be used.
The ability to control the way how data is processed is however crucial for
enterprises to guarantee (and provide evidence of) compliant processing of
critical data, as well as for users who need to control if their private data
may be analyzed or linked with additional information - a major concern in IoT
applications processing personal information. In this paper, we introduce
LUCON, a data-centric security policy framework for distributed systems that
considers data flows by controlling how messages may be routed across services
and how they are combined and processed. LUCON policies prevent information
leaks, bind data usage to obligations, and enforce data flows across services.
Policy enforcement is based on a dynamic taint analysis at runtime and an
upfront static verification of message routes against policies. We discuss the
semantics of these two complementing enforcement models and illustrate how
LUCON policies are compiled from a simple policy language into a first-order
logic representation. We demonstrate the practical application of LUCON in a
real-world IoT middleware and discuss its integration into Apache Camel.
Finally, we evaluate the runtime impact of LUCON and discuss performance and
scalability aspects."
scgqa_324,1407.7736v1,How can the insights from Figure 5's lift chart assist in enhancing editor retention strategies discussed in this paper?,"The implications of the lift chart are that the proposed churn-prediction model can be used to identify potential churners early on. This can help to prevent them from leaving the platform, which can save the company money and improve customer satisfaction.",1407.7736v1.pdf,"['1407.7736v1.pdf', '2008.02777v1.pdf', '1405.5329v4.pdf', '2010.12427v3.pdf', '1603.01793v2.pdf', '1707.04849v1.pdf', '2010.13691v1.pdf', '1509.02054v1.pdf', '1710.10571v5.pdf']",1407.7736v1-Figure5-1.png,Fig. 5: Lift chart obtained by the proposed churn-prediction model. Different curves represent the lift curves for different sliding windows.,"Cumulative gains for churn prediction. The lift factors are widely used by researchers to evaluate the performance of churn-prediction models (e.g. [12]). The lift factors achieved by our model are shown in Figure 5. In lift chart, the diagonal line represents a baseline which randomly selects a subset of editors as potential churners, i.e., it selects s% of the editors that will contain s% of the true churners, resulting in a lift factor of 1. In Figure 5, on average, our model was capable of identifying 10% of editors that contained 21.2% of true churners (i.e. a lift factor of 2.12), 20% of editors that contained 39.3% of true churners (i.e. a lift factor of 1.97), and 30% of editors that contained 54.7% of true churners (i.e. a lift factor of 1.82). Evidently, our model achieved higher lift factors than the baseline. Thus if the objective of the lift analysis is to identify a",A Latent Space Analysis of Editor Lifecycles in Wikipedia,"Collaborations such as Wikipedia are a key part of the value of the modern
Internet. At the same time there is concern that these collaborations are
threatened by high levels of member turnover. In this paper we borrow ideas
from topic analysis to editor activity on Wikipedia over time into a latent
space that offers an insight into the evolving patterns of editor behavior.
This latent space representation reveals a number of different categories of
editor (e.g. content experts, social networkers) and we show that it does
provide a signal that predicts an editor's departure from the community. We
also show that long term editors gradually diversify their participation by
shifting edit preference from one or two namespaces to multiple namespaces and
experience relatively soft evolution in their editor profiles, while short term
editors generally distribute their contribution randomly among the namespaces
and experience considerably fluctuated evolution in their editor profiles."
scgqa_325,1201.3056v1,"In the context of the three-user network depicted, what relationship does Figure 5 show for fi variance and power sold?","The graph shows that as the variance of fi increases, the actual relay power sold also increases. This is because as the demand increases, the users are willing to pay more for relay power, so the relay is able to sell more power.",1201.3056v1.pdf,"['1201.3056v1.pdf', '1301.5201v1.pdf', '1907.11771v1.pdf', '2008.06431v1.pdf', '1807.09483v2.pdf', '1908.09034v2.pdf', '1403.5801v2.pdf', '1708.09328v1.pdf']",1201.3056v1-Figure5-1.png,Fig. 5. Three-user relay network with Rayleigh fading channels and different variances offi.,"Next, we examine the trend of the optimal relay price with an increasing demand. From Lemma 2, P Ii (λ) is a non-decreasing function of |fi|2. So, we can use an increasing |fi|2 to simulate the increasing user demand. In this numerical experiment, we again consider a threeuser network and model all channels as independent circularly symmetric complex Gaussian random variables with zero-mean, that is, they are independent Rayleigh flat-fading channels. The variances of all gi’s and hi’s are 1, while the variance of all fi’s ranges from 1 to 20. A larger variance means a higher average value of |fi|2, which on average means a higher power demand from the users. The transmit power of the users is set to be 10 dB and relay power is set to be 20 dB. Figure 5 shows the optimal relay power price, the actual relay power sold, and the maximum relay revenue with different variances of fi. We can see that as the variance of fi increases, the optimal relay price increases, more relay power is sold, and the maximum relay revenue increases. This fits one of the laws of supply and demand, which says, if the supply is unchanged and demand increases, it leads to higher equilibrium price and quantity.","Power Allocation and Pricing in Multi-User Relay Networks Using
  Stackelberg and Bargaining Games","This paper considers a multi-user single-relay wireless network, where the
relay gets paid for helping the users forward signals, and the users pay to
receive the relay service. We study the relay power allocation and pricing
problems, and model the interaction between the users and the relay as a
two-level Stackelberg game. In this game, the relay, modeled as the service
provider and the leader of the game, sets the relay price to maximize its
revenue; while the users are modeled as customers and the followers who buy
power from the relay for higher transmission rates. We use a bargaining game to
model the negotiation among users to achieve a fair allocation of the relay
power. Based on the proposed fair relay power allocation rule, the optimal
relay power price that maximizes the relay revenue is derived analytically.
Simulation shows that the proposed power allocation scheme achieves higher
network sum-rate and relay revenue than the even power allocation. Furthermore,
compared with the sum-rate-optimal solution, simulation shows that the proposed
scheme achieves better fairness with comparable network sum-rate for a wide
range of network scenarios. The proposed pricing and power allocation solutions
are also shown to be consistent with the laws of supply and demand."
scgqa_326,1911.07924v1,"In the context of the Logo-2K+ dataset, how do ROC curves illustrate the performance differences between logo models?","ROC curves are a common way to visualize the performance of a binary classifier. They plot the true positive rate (TPR) against the false positive rate (FPR) at different thresholds. The TPR is the proportion of positive examples that are correctly classified, while the FPR is the proportion of negative examples that are incorrectly classified. A perfect classifier would have a TPR of 1 and an FPR of 0.

The graph shows that the DRNA-Net model has a higher TPR than the NTS-Net model for all thresholds. This means that the DRNA-Net model is better at correctly classifying positive examples. The DRNA-Net model also has a lower FPR than the NTS-Net model, which means that it is less likely to incorrectly classify negative examples.

Overall, the graph shows that the DRNA-Net model is a better logo classification model than the NTS-Net model.",1911.07924v1.pdf,"['1911.07924v1.pdf', '1905.00569v2.pdf', '1905.07512v3.pdf', '1208.4662v2.pdf']",1911.07924v1-Figure5-1.png,Figure 5: ROC curves of different logo classification models on Logo-2K+.,"Considering image samples from different logo classes are unbalanced, to further comprehensively evaluate the performance of DRNA-Net, we further draw the ROC curves of all the methods in Figure 5, where the dotted green and red curve represent the performance of our method and the purple curve refers to the NTS-Net method. We can see that the true positive rate remains high on the DRNA-Net compared to NTS-Net for test logo classes, and DRNA-Net obtains the best performance in terms of overall quality for logo classi-",Logo-2K+: A Large-Scale Logo Dataset for Scalable Logo Classification,"Logo classification has gained increasing attention for its various
applications, such as copyright infringement detection, product recommendation
and contextual advertising. Compared with other types of object images, the
real-world logo images have larger variety in logo appearance and more
complexity in their background. Therefore, recognizing the logo from images is
challenging. To support efforts towards scalable logo classification task, we
have curated a dataset, Logo-2K+, a new large-scale publicly available
real-world logo dataset with 2,341 categories and 167,140 images. Compared with
existing popular logo datasets, such as FlickrLogos-32 and LOGO-Net, Logo-2K+
has more comprehensive coverage of logo categories and larger quantity of logo
images. Moreover, we propose a Discriminative Region Navigation and
Augmentation Network (DRNA-Net), which is capable of discovering more
informative logo regions and augmenting these image regions for logo
classification. DRNA-Net consists of four sub-networks: the navigator
sub-network first selected informative logo-relevant regions guided by the
teacher sub-network, which can evaluate its confidence belonging to the
ground-truth logo class. The data augmentation sub-network then augments the
selected regions via both region cropping and region dropping. Finally, the
scrutinizer sub-network fuses features from augmented regions and the whole
image for logo classification. Comprehensive experiments on Logo-2K+ and other
three existing benchmark datasets demonstrate the effectiveness of proposed
method. Logo-2K+ and the proposed strong baseline DRNA-Net are expected to
further the development of scalable logo image recognition, and the Logo-2K+
dataset can be found at https://github.com/msn199959/Logo-2k-plus-Dataset."
scgqa_327,1710.10733v4,"According to Figure 5, what can be said about the visual perceptibility of PGD and I-FGM's adversarial examples?","The graph shows that PGD and I-FGM are both effective in generating adversarial examples, but they are less effective than EAD. This is because PGD and I-FGM are less transferable and the perturbations are more visually perceptible.",1710.10733v4.pdf,"['1710.10733v4.pdf', '1407.7736v1.pdf', '2001.07829v1.pdf', '1903.10464v3.pdf']",1710.10733v4-Figure5-1.png,Figure 5: Attack transferability of adversarial examples generated by PGD and I-FGM with various values when attacking a single model.,"In Figure 4, the results of tuning when generating adversarial examples with PGD and I-FGM attacking an ensemble of models are shown. In Figure 5, the results of tuning when generating adversarial examples with PGD and I-FGM attacking a single model are shown. The results simply show that by adding large amounts of visual distortion to the original image, an attacker can cause a target defense model to misclassify. However, when compared to EAD, the adversarial examples of PGD and I-FGM are less transferable and, when successful, the perturbations are more visually perceptible, as indicated by the drastic increase in the L1 and L2 distortion at high .",Attacking the Madry Defense Model with $L_1$-based Adversarial Examples,"The Madry Lab recently hosted a competition designed to test the robustness
of their adversarially trained MNIST model. Attacks were constrained to perturb
each pixel of the input image by a scaled maximal $L_\infty$ distortion
$\epsilon$ = 0.3. This discourages the use of attacks which are not optimized
on the $L_\infty$ distortion metric. Our experimental results demonstrate that
by relaxing the $L_\infty$ constraint of the competition, the elastic-net
attack to deep neural networks (EAD) can generate transferable adversarial
examples which, despite their high average $L_\infty$ distortion, have minimal
visual distortion. These results call into question the use of $L_\infty$ as a
sole measure for visual distortion, and further demonstrate the power of EAD at
generating robust adversarial examples."
scgqa_328,1409.3924v1,What implications does Fig. 8 have for choosing ELM or EELM in machine learning applications?,"The findings in the graph have implications for the design of machine learning algorithms. For example, if a designer is working with a small dataset, they may want to consider using ELM. However, if a designer is working with a large dataset, they may want to consider using EELM.",1409.3924v1.pdf,"['1409.3924v1.pdf', '1509.00374v2.pdf', '1807.09483v2.pdf', '1611.02955v1.pdf']",1409.3924v1-Figure8-1.png,Fig. 8. Learning time of two algorithms for Diabetes,"Fig. 8, which show that in the simulation of the mid size classification application, ELM can reach a higher testing rate than EELM with same number of nodes. Whereas, the time spent by ELM increases much faster than that spent by EELM with the increasing of the number of nodes.",A study on effectiveness of extreme learning machine,"Extreme learning machine (ELM), proposed by Huang et al., has been shown a
promising learning algorithm for single-hidden layer feedforward neural
networks (SLFNs). Nevertheless, because of the random choice of input weights
and biases, the ELM algorithm sometimes makes the hidden layer output matrix H
of SLFN not full column rank, which lowers the effectiveness of ELM. This paper
discusses the effectiveness of ELM and proposes an improved algorithm called
EELM that makes a proper selection of the input weights and bias before
calculating the output weights, which ensures the full column rank of H in
theory. This improves to some extend the learning rate (testing accuracy,
prediction accuracy, learning time) and the robustness property of the
networks. The experimental results based on both the benchmark function
approximation and real-world problems including classification and regression
applications show the good performances of EELM."
scgqa_329,1405.5364v2,"According to Figure 14 from the research, how does fairness change with the number of existing flows in modified FAST?","The graph shows that the modified FAST algorithm manages to stay fair irrespectively of the number of preexisting flows. This is in contrast to the rate reduction approach, which deviates from fairness and approximates original FAST behavior as the number of flows increases.",1405.5364v2.pdf,"['1405.5364v2.pdf', '1807.09483v2.pdf', '1803.09990v2.pdf', '1911.07924v1.pdf', '1912.00088v1.pdf']",1405.5364v2-Figure14-1.png,Figure 14. Impact of the number of preexisting flows.,"Fig. 14 shows that, while our solution manages to stay fair irrespectively of the number of flows, the rate reduction approach deviates from fairness and approximates original FAST behavior as the number of flows increases.",The persistent congestion problem of FAST-TCP: analysis and solutions,"FAST-TCP achieves better performance than traditional TCP-Reno schemes, but
unfortunately it is inherently unfair to older connections due to wrong
estimations of the round-trip propagation delay.
  This paper presents a model for this anomalous behavior of FAST flows, known
as the persistent congestion problem. We first develop an elementary analysis
for a scenario with just two flows, and then build up the general case with an
arbitrary number of flows. The model correctly quantifies how much unfairness
shows up among the different connections, confirming experimental observations
made by several previous studies.
  We built on this model to develop an algorithm to obtain a good estimate of
the propagation delay for FAST-TCP that enables to achieve fairness between
aged and new connections while preserving the high throughput and low buffer
occupancy of the original protocol. Furthermore, our proposal only requires a
modification of the sender host, avoiding the need to upgrade the intermediate
routers in any way."
scgqa_330,1905.07512v3,What conclusions can be drawn about SplitNet Transfer's capability in the Exploration and Flee tasks of IndoorEnv?,"The graph shows that SplitNet Transfer outperforms the other methods on both the Exploration and Flee tasks. This is because SplitNet Transfer is able to reuse its understanding of depth to quickly learn to approach walls, then turn at the last second and head off in a new direction. For the Flee task, SplitNet Transfer identifies long empty hallways and navigates down those away from the start location. None of the other methods learn robust obstacle-avoidance behavior or geometric scene understanding. Instead, they latch on to simple dataset biases such as ""repeatedly move forward then rotate.""",1905.07512v3.pdf,"['1905.07512v3.pdf', '1809.01628v1.pdf', '2004.04276v1.pdf', '1908.09034v2.pdf', '2001.07829v1.pdf', '1607.05970v2.pdf', '1208.4662v2.pdf']",1905.07512v3-Figure7-1.png,"Figure 7. IndoorEnv Task2Task performance as a function of target training episodes. SplitNet Transfer and E2E Transfer are first trained on IndoorEnv Point-Nav, but SplitNet only updates the policy layers whereas E2E updates the entire network. E2E from scratch is randomly initialized a episode 0. The Blind method only receives its previous action as input and is randomly initialized. Oracle agents perform at 33.5 and 19.5 respectively.","Figure 7 shows that SplitNet immediately begins to learn effective new policies, outperforming the other methods almost right away. In Exploration, our method is able to reuse its understanding of depth to quickly learn to approach walls, then turn at the last second and head off in a new direction. For the Flee task, our method identifies long empty hallways and navigates down those away from the start location. None of the other methods learn robust obstacleavoidance behavior or geometric scene understanding. Instead they latch on to simple dataset biases such as “repeatedly move forward then rotate.” Oracle agents perform at 33.5 and 19.5 respectively, but are not directly comparable in that they benefit from knowing the environment layout before beginning the episode.",SplitNet: Sim2Sim and Task2Task Transfer for Embodied Visual Navigation,"We propose SplitNet, a method for decoupling visual perception and policy
learning. By incorporating auxiliary tasks and selective learning of portions
of the model, we explicitly decompose the learning objectives for visual
navigation into perceiving the world and acting on that perception. We show
dramatic improvements over baseline models on transferring between simulators,
an encouraging step towards Sim2Real. Additionally, SplitNet generalizes better
to unseen environments from the same simulator and transfers faster and more
effectively to novel embodied navigation tasks. Further, given only a small
sample from a target domain, SplitNet can match the performance of traditional
end-to-end pipelines which receive the entire dataset. Code is available
https://github.com/facebookresearch/splitnet"
scgqa_331,1908.04647v1,In what way does Figure 5 illustrate the relationship between inf-sup constants and geometric refinements in DG methods?,"The results of the graph have important implications for the design of finite element methods. They suggest that the inf-sup constant is relatively insensitive to the approximation degree, and that all three canonical geometric edge, corner, and corner-edge refinements are effective in ensuring stability. This means that finite element methods can be designed with relatively little concern for the inf-sup constant, and that all three canonical geometric refinements are viable options.",1908.04647v1.pdf,"['1908.04647v1.pdf', '1805.06370v2.pdf', '1808.08442v1.pdf', '1803.01118v2.pdf']",1908.04647v1-Figure5-1.png,Figure 5. Stabilized values from Figure 4 (on the right with logarithmic scaling; the dashed line shows a slope of −3).,"We focus on the canonical geometric edge, corner, and corner-edge refinements from Section 3.1. As before, we first fix the approximation degree k, and refine the meshes step by step in order to monitor the inf-sup constant; see Figure 4 for the resulting plots with different approximation degrees. Again, we display the stabilized values of γa for increasing k in Figure 5. The results are qualitatively similar to the inf-sup constant γB discussed earlier. There is a k-dependence of the inf-sup constant γa which is again much weaker than k","Stability and Convergence of Spectral Mixed Discontinuous Galerkin
  Methods for 3D Linear Elasticity on Anisotropic Geometric Meshes","We consider spectral mixed discontinuous Galerkin finite element
discretizations of the Lam\'e system of linear elasticity in polyhedral domains
in $\mathbb{R}^3$. In order to resolve possible corner, edge, and corner-edge
singularities, anisotropic geometric edge meshes consisting of hexahedral
elements are applied. We perform a computational study on the discrete inf-sup
stability of these methods, and especially focus on the robustness with respect
to the Poisson ratio close to the incompressible limit (i.e. the Stokes
system). Furthermore, under certain realistic assumptions (for analytic data)
on the regularity of the exact solution, we illustrate numerically that the
proposed mixed DG schemes converge exponentially in a natural DG norm."
scgqa_332,1911.09804v2,"How do the test loss, error rate, and ECE relate to MC samples in the study's experiments?","The graph shows that the test loss, test error rate, and test ECE all decrease as the number of MC samples increases. This suggests that ensembling the predictions from models with various sampled network structures enhances the final predictive performance and calibration significantly. This is in contrast to the situation of classic variational BNNs, where using more MC samples does not necessarily bring improvement over using the most likely sample.",1911.09804v2.pdf,"['1911.09804v2.pdf', '1810.04915v1.pdf', '2010.00502v1.pdf', '1303.1635v1.pdf', '1701.08947v1.pdf', '1805.01772v1.pdf']",1911.09804v2-Figure6-1.png,"Figure 6: Test loss (left), test error rate (middle), and test ECE (right) of DBSN vary w.r.t. the number of MC samples used in estimating Eq. (7). (CIFAR-100)","We draw the change of test loss, test error rate and test ECE with respect to the number of MC samples used for testing DBSN in Fig. 5 (CIFAR-10) and Fig. 6 (CIFAR-100). It is clear that ensembling the predictions from models with various sampled network structures enhances the final predictive performance and calibration significantly. This is in marked contrast to the situation of classic variational BNNs, where using more MC samples does not necessarily bring improvement over using the most likely sample. As shown in the plots, we would better utilize 20+ MC samples to predict the unseen data, for adequately exploiting the learned structure distribution. Indeed, we use 100 MC samples in all the experiments, except for the adversarial attack experiments where we use 30 MC samples for attacking and evaluation.","Measuring Uncertainty through Bayesian Learning of Deep Neural Network
  Structure","Bayesian neural networks (BNNs) augment deep networks with uncertainty
quantification by Bayesian treatment of the network weights. However, such
models face the challenge of Bayesian inference in a high-dimensional and
usually over-parameterized space. This paper investigates a new line of
Bayesian deep learning by performing Bayesian inference on network structure.
Instead of building structure from scratch inefficiently, we draw inspirations
from neural architecture search to represent the network structure. We then
develop an efficient stochastic variational inference approach which unifies
the learning of both network structure and weights. Empirically, our method
exhibits competitive predictive performance while preserving the benefits of
Bayesian principles across challenging scenarios. We also provide convincing
experimental justification for our modeling choice."
scgqa_333,1611.02955v1,"According to the graph in Figure 17 for GER, how do the MPA approximation and exact values of influence behave?","The graph shows that the MPA approximation of the harmonic influence of the nodes is consistently higher than the exact value. This is because the computation tree, which has more nodes than the original graph, overcomes the fact that the limit messages W i→j(∞) are smaller.",1611.02955v1.pdf,"['1611.02955v1.pdf', '2002.10790v1.pdf', '2008.01961v3.pdf']",1611.02955v1-Figure17-1.png,"Figure 17: The elements of the vector h(∞) against the corresponding exact values of the harmonic influence, collected in the vector h∗, for the graph GER. All the points are above the 45◦ line.","Also on the graph GER the MPA is not exact, but the nodes’ rankings are nearly preserved. Figure 17 represents the elements of the vector h(∞) against the corresponding elements in the vector h∗. All crosses are above the 45◦ line and nearly aligned in a sort of parabola. The largest value of h`(∞) is about 5 times bigger than the corresponding h∗` . The Spearman’s coefficient for the two vectors h∗ and h(∞) of this simulation is 0.9939. The magenta crosses in Figure 18 compare the elements of ~w(∞) against the corresponding exact values in ~w∗. All the points are below or on the 45◦ line, further than it was for GFE . On graphs containing loops, the asymptotic values of the messagesW i→j(t), i.e. the limits W i→j(∞), are smaller (or equal) than the corresponding exact values, computed using the electrical interpretation. The limit W i→j(∞) is exact if the graph is a tree, else represents the value that would be computed by i on its computation tree (i.e. the infinite “unwrapped” graph obtained exploring the neighborhood of i in a breadth first manner [6]). From an electrical point of view, this tree contains more paths to the reference nodes than the original graph and this fact makes the MPA compute a smaller resistance from node i to the reference. At the same time, in graphs with loops, the MPA approximation of the harmonic influence of the nodes is consistently higher than the exact value. This is again due to the properties of the computation tree, which has more nodes than the original graph. This effect overcomes the fact that the limit messages W i→j(∞) are smaller.","The harmonic influence in social networks and its distributed
  computation by message passing","In this paper we elaborate upon a measure of node influence in social
networks, which was recently proposed by Vassio et al., IEEE Trans. Control
Netw. Syst., 2014. This measure quantifies the ability of the node to sway the
average opinion of the network. Following the approach by Vassio et al., we
describe and study a distributed message passing algorithm that aims to compute
the nodes' influence. The algorithm is inspired by an analogy between
potentials in electrical networks and opinions in social networks. If the graph
is a tree, then the algorithm computes the nodes' influence in a number of
steps equal to the diameter of the graph. On general graphs, the algorithm
converges asymptotically to a meaningful approximation of the nodes' influence.
In this paper we detail the proof of convergence, which greatly extends
previous results in the literature, and we provide simulations that illustrate
the usefulness of the returned approximation."
scgqa_334,1608.08469v1,What trade-offs are depicted in the Pareto front regarding social welfare and fairness in Figure 4?,"The Pareto front is a curve that shows the best possible trade-offs between two or more objectives. In this case, the two objectives are normalized social welfare and normalized fairness measure. The Pareto front shows that the router-assisted controller outperforms the baseline controller in terms of social welfare, while the centralized controller outperforms both the router-assisted and baseline controllers.",1608.08469v1.pdf,"['1608.08469v1.pdf', '2004.05448v1.pdf', '2007.11446v1.pdf', '1902.05312v2.pdf', '1801.06867v1.pdf', '1710.09234v1.pdf', '1703.01827v3.pdf', '1812.09355v1.pdf']",1608.08469v1-Figure4-1.png,Fig. 4: Social welfare vs fairness tradeoff,"Efficiency-vs-fairness tradeoff: We first evaluate the algorithms in terms of normalized social welfare (sum of QoE) and normalized fairness measure (Jain’s index). We change α in α-fairness in order to get different points on the curve. Figure 4 shows the pareto front of the algorithms. There are three observations: First, router-assisted control outperforms baseline controller by 5-7% in terms of social welfare given the same normalized Jain’s index. For example, if we let normalized Jain’s index to be 0.8, router assisted controller achieves 56% of optimal, while baseline controller only achieves 50% of optimal. Second, centralized controller significantly outperforms both router-assisted and baseline controller with 15+% advantage. This is because centralized","On the Efficiency and Fairness of Multiplayer HTTP-based Adaptive Video
  Streaming","User-perceived quality-of-experience (QoE) is critical in internet video
delivery systems. Extensive prior work has studied the design of client-side
bitrate adaptation algorithms to maximize single-player QoE. However,
multiplayer QoE fairness becomes critical as the growth of video traffic makes
it more likely that multiple players share a bottleneck in the network. Despite
several recent proposals, there is still a series of open questions. In this
paper, we bring the problem space to light from a control theory perspective by
formalizing the multiplayer QoE fairness problem and addressing two key
questions in the broader problem space. First, we derive the sufficient
conditions of convergence to steady state QoE fairness under TCP-based
bandwidth sharing scheme. Based on the insight from this analysis that
in-network active bandwidth allocation is needed, we propose a non-linear
MPC-based, router-assisted bandwidth allocation algorithm that regards each
player as closed-loop systems. We use trace-driven simulation to show the
improvement over existing approaches. We identify several research directions
enabled by the control theoretic modeling and envision that control theory can
play an important role on guiding real system design in adaptive video
streaming."
scgqa_335,1611.03254v1,"According to the experiments in this paper, how does the maximum size of (k,r)-cores respond to parameter changes?","The graph shows that the number of (k,r)-cores and the maximum size of (k,r)-cores are much more sensitive to the change of r or k on the two datasets, compared to the average size. This suggests that the number of (k,r)-cores and the maximum size of (k,r)-cores are more likely to be affected by the change of r or k, while the average size is less likely to be affected. This is likely because the number of (k,r)-cores and the maximum size of (k,r)-cores are more directly related to the number of nodes in the graph, while the average size is not as directly related.",1611.03254v1.pdf,"['1611.03254v1.pdf', '2004.05579v1.pdf', '1706.03019v1.pdf']",1611.03254v1-Figure7-1.png,"Figure 7: (k,r)-core Statistics","We also report the number of (k,r)-cores, the average size and maximum size of (k,r)-cores on Gowalla and DBLP. Figure 7(a) and (b) show that both maximum size of (k,r)-cores and the number of (k,r)-cores are much more sensitive to the change of r or k on the two datasets, compared to the average size.","When Engagement Meets Similarity: Efficient (k,r)-Core Computation on
  Social Networks","In this paper, we investigate the problem of (k,r)-core which intends to find
cohesive subgraphs on social networks considering both user engagement and
similarity perspectives. In particular, we adopt the popular concept of k-core
to guarantee the engagement of the users (vertices) in a group (subgraph) where
each vertex in a (k,r)-core connects to at least k other vertices. Meanwhile,
we also consider the pairwise similarity between users based on their profiles.
For a given similarity metric and a similarity threshold r, the similarity
between any two vertices in a (k,r)-core is ensured not less than r. Efficient
algorithms are proposed to enumerate all maximal (k,r)-cores and find the
maximum (k,r)-core, where both problems are shown to be NP-hard. Effective
pruning techniques significantly reduce the search space of two algorithms and
a novel (k,k')-core based (k,r)-core size upper bound enhances performance of
the maximum (k,r)-core computation. We also devise effective search orders to
accommodate the different nature of two mining algorithms. Comprehensive
experiments on real-life data demonstrate that the maximal/maximum (k,r)-cores
enable us to find interesting cohesive subgraphs, and performance of two mining
algorithms is significantly improved by proposed techniques."
scgqa_336,1906.09756v1,What insights does Fig. 2 provide regarding the thresholding effect on classifier performance in Cascade R-CNN?,"The graph shows that the performance of the object detector is affected by the IoU threshold. Specifically, the detector trained with a lower IoU threshold performs better for examples with lower IoUs, while the detector trained with a higher IoU threshold performs better for examples with higher IoUs. This is because the IoU threshold determines the classification boundary where the classifier is most discriminative, i.e. has largest margin.",1906.09756v1.pdf,"['1906.09756v1.pdf', '1006.3688v1.pdf', '1603.04153v1.pdf', '1901.10423v1.pdf', '1007.0328v1.pdf', '2010.13032v1.pdf', '1805.07914v3.pdf', '1207.3107v3.pdf']",1906.09756v1-Figure2-1.png,"Fig. 2: Bounding box localization, classification loss and detection performance of object detectors of increasing IoU threshold u.","Some evidence in support of this premise is given in Fig. 2, which presents the bounding box localization performance, classification loss and detection performance, respectively, of three detectors trained with IoU thresholds of u = 0.5, 0.6, 0.7. Localization and classification are evaluated as a function of the detection hypothesis IoU. Detection is evaluated as a function of the IoU threshold, as in COCO [36]. Fig. 2 (a) shows that the three bounding box regressors tend to achieve the best performance for examples of IoU in the vicinity of the threshold used for detector training. Fig. 2 (c) shows a similar effect for detection, up to some overfitting for the highest thresholds. The detector trained with u = 0.5 outperforms the detector trained with u = 0.6 for low IoUs, underperforming it at higher IoUs. In general, a detector optimized for a single IoU value is not optimal for other values. This is also confirmed by the classification loss, shown in Fig. 2 (b), whose peaks are near the thresholds used for detector training. In general, the threshold determines the classification boundary where the classifier is most discriminative, i.e. has largest margin [6], [15].",Cascade R-CNN: High Quality Object Detection and Instance Segmentation,"In object detection, the intersection over union (IoU) threshold is
frequently used to define positives/negatives. The threshold used to train a
detector defines its \textit{quality}. While the commonly used threshold of 0.5
leads to noisy (low-quality) detections, detection performance frequently
degrades for larger thresholds. This paradox of high-quality detection has two
causes: 1) overfitting, due to vanishing positive samples for large thresholds,
and 2) inference-time quality mismatch between detector and test hypotheses. A
multi-stage object detection architecture, the Cascade R-CNN, composed of a
sequence of detectors trained with increasing IoU thresholds, is proposed to
address these problems. The detectors are trained sequentially, using the
output of a detector as training set for the next. This resampling
progressively improves hypotheses quality, guaranteeing a positive training set
of equivalent size for all detectors and minimizing overfitting. The same
cascade is applied at inference, to eliminate quality mismatches between
hypotheses and detectors. An implementation of the Cascade R-CNN without bells
or whistles achieves state-of-the-art performance on the COCO dataset, and
significantly improves high-quality detection on generic and specific object
detection datasets, including VOC, KITTI, CityPerson, and WiderFace. Finally,
the Cascade R-CNN is generalized to instance segmentation, with nontrivial
improvements over the Mask R-CNN. To facilitate future research, two
implementations are made available at
\url{https://github.com/zhaoweicai/cascade-rcnn} (Caffe) and
\url{https://github.com/zhaoweicai/Detectron-Cascade-RCNN} (Detectron)."
scgqa_337,2007.15176v2,"In the context of Fig. 6, what combinations of λc and λCadv maximize the effectiveness of the proposed framework?",The graph shows that the model's performance is maximized when λc is between 0.005 and 0.1 and λCadv is between 0.0005 and 0.005. This suggests that the model needs a moderate amount of focus on both classification and alignment in order to achieve optimal performance.,2007.15176v2.pdf,"['2007.15176v2.pdf', '1207.3107v3.pdf', '1910.09823v3.pdf', '1701.08947v1.pdf', '1205.4213v2.pdf']",2007.15176v2-Figure6-1.png,Fig. 6: Plots presenting the hyper-parameter analysis of the parameters λc on the classification loss using pseudo-weak labels and λCadv on the category-wise alignment loss.,"Fig. 6 presents two plots for the parameter analysis when using pseudo-weak labels. In Fig. 6(a), we fix λCadv = 0.001 and show that our model achieves the mIoU larger than 47.5% under a range of λc = [0.005, 0.1]. When fixing λc = 0.01, Fig. 6(b) shows that the model performs well under a range of λCadv = [0.0005, 0.005]. However, when we increase λ C adv to be larger than 0.01, the adversarial training process may become unstable and decreases the performance to 46.2%. In addition, decreasing λCadv would give less focus on alignment and gradually degrades the performance, which shows the importance of our alignment process.",Domain Adaptive Semantic Segmentation Using Weak Labels,"Learning semantic segmentation models requires a huge amount of pixel-wise
labeling. However, labeled data may only be available abundantly in a domain
different from the desired target domain, which only has minimal or no
annotations. In this work, we propose a novel framework for domain adaptation
in semantic segmentation with image-level weak labels in the target domain. The
weak labels may be obtained based on a model prediction for unsupervised domain
adaptation (UDA), or from a human annotator in a new weakly-supervised domain
adaptation (WDA) paradigm for semantic segmentation. Using weak labels is both
practical and useful, since (i) collecting image-level target annotations is
comparably cheap in WDA and incurs no cost in UDA, and (ii) it opens the
opportunity for category-wise domain alignment. Our framework uses weak labels
to enable the interplay between feature alignment and pseudo-labeling,
improving both in the process of domain adaptation. Specifically, we develop a
weak-label classification module to enforce the network to attend to certain
categories, and then use such training signals to guide the proposed
category-wise alignment method. In experiments, we show considerable
improvements with respect to the existing state-of-the-arts in UDA and present
a new benchmark in the WDA setting. Project page is at
http://www.nec-labs.com/~mas/WeakSegDA."
scgqa_338,1005.0416v1,What is the average performance of RRT versus RRT* in the obstacle-laden environment depicted in the paper?,"The graph shows that the RRT* algorithm converges to the optimum solution, while the RRT algorithm is about 1.5 of the optimal solution on average. This is because the RRT* algorithm is able to explore the state space more efficiently and find a path in a different homotopy class, which reduces the cost of reaching the target considerably.",1005.0416v1.pdf,"['1005.0416v1.pdf', '1602.07579v1.pdf', '1808.06304v2.pdf']",1005.0416v1-Figure5-1.png,Fig. 5. An environment cluttered with obstacles is considered. The cost of the best paths in the RRT (shown in red) and the RRT∗ (shown in blue) plotted against iterations averaged over 500 trials in (a). The optimal cost is shown in black. The variance of the trials is shown in (b).,"In the second scenario, both algorithms are run in an environment in presence of obstacles. In Figure 3, the trees maintained by the algorithms are shown after 20,000 iterations. The tree maintained by the RRT∗ algorithm is also shown in Figure 4 in different stages. It can be observed that the RRT∗ first rapidly explores the state space just like the RRT. Moreover, as the number of samples increase, the RRT∗ improves its tree to include paths with smaller cost and eventually discovers a path in a different homotopy class, which reduces the cost of reaching the target considerably. Results of a Monte-Carlo study for this scenario is presented in Figure 5. Both algorithms were run alongside up until 20,000 iterations 500 times and cost of the best path in the trees were averaged for each iteration. The figures illustrate that all runs of the RRT∗ algorithm converges to the optimum, whereas the RRT algorithm is about 1.5 of the optimal solution on average. The high variance in solutions returned by the RRT algorithm stems from the fact that there are two different homotopy classes of paths that reach the goal. If the RRT luckily converges to a path of the homotopy class that contains an optimum solution, then the resulting path is relatively closer to the optimum than it is on average. If, on the other hand, the RRT first explores a path of the second homotopy class, which is often the case for this particular scenario, then the solution that RRT converges is generally around twice the optimum.",Incremental Sampling-based Algorithms for Optimal Motion Planning,"During the last decade, incremental sampling-based motion planning
algorithms, such as the Rapidly-exploring Random Trees (RRTs) have been shown
to work well in practice and to possess theoretical guarantees such as
probabilistic completeness. However, no theoretical bounds on the quality of
the solution obtained by these algorithms have been established so far. The
first contribution of this paper is a negative result: it is proven that, under
mild technical conditions, the cost of the best path in the RRT converges
almost surely to a non-optimal value. Second, a new algorithm is considered,
called the Rapidly-exploring Random Graph (RRG), and it is shown that the cost
of the best path in the RRG converges to the optimum almost surely. Third, a
tree version of RRG is introduced, called the RRT$^*$ algorithm, which
preserves the asymptotic optimality of RRG while maintaining a tree structure
like RRT. The analysis of the new algorithms hinges on novel connections
between sampling-based motion planning algorithms and the theory of random
geometric graphs. In terms of computational complexity, it is shown that the
number of simple operations required by both the RRG and RRT$^*$ algorithms is
asymptotically within a constant factor of that required by RRT."
scgqa_339,1006.4386v1,"In the context of the paper, how does increasing total transmit power influence the secrecy rate in Figure 3?","The graph suggests that there is a positive relationship between the total transmit power and the secrecy rate. This means that as the total transmit power increases, the secrecy rate also increases. This is because with more transmit power, the CRB system can better overcome the interference from the eavesdropper and achieve a higher level of secrecy.",1006.4386v1.pdf,"['1006.4386v1.pdf', '1007.0328v1.pdf', '1805.01892v1.pdf', '1307.3687v1.pdf', '2006.03632v1.pdf']",1006.4386v1-Figure3-1.png,Fig. 3. DF Second-hop secrecy rate vs. the total relay transmit powerPT for different cases. Eavesdropper has a stronger channel.,"In Figures 2 and 3, we plot the second-hop secrecy rate, which is the maximum secrecy rate that our collaborative relay beamforming system can support under both total and individual relay power constraints. For the case of individual relay power constraints, we assume that the relays have the same power budgets: pi = PT M . Specifically, in Fig. 2, we have σh = 3, σz = 1, N0 = 1 and M = 5. In this case, the legitimate user has a stronger channel. In Fig. 3, the only changes are σh = 1 and σz = 2, which imply that the eavesdropper has a stronger channel. Our CRB system can achieve secure transmission even when the eavesdropper has more favorable channel conditions. As can be seen from the figures, the highest secrecy rate is achieved, as expected, under a total transmit power constraint. On the other hand, we observe that only a relatively small rate loss is experienced under individual relay power constraints. Moreover, we note that our two different optimization approaches give nearly the same result. It also can be seen that under individual power constraint, the simple suboptimal method suffers a constant loss as compared to SDR or SOCP based optimal value.",Collaborative Relay Beamforming for Secrecy,"In this paper, collaborative use of relays to form a beamforming system and
provide physical-layer security is investigated. In particular,
decode-and-forward (DF) and amplify-and-forward (AF) relay beamforming designs
under total and individual relay power constraints are studied with the goal of
maximizing the secrecy rates when perfect channel state information (CSI) is
available. In the DF scheme, the total power constraint leads to a closed-form
solution, and in this case, the optimal beamforming structure is identified in
the low and high signal-to-noise ratio (SNR) regimes. The beamforming design
under individual relay power constraints is formulated as an optimization
problem which is shown to be easily solved using two different approaches,
namely semidefinite programming and second-order cone programming. A simplified
and suboptimal technique which reduces the computation complexity under
individual power constraints is also presented. In the AF scheme, not having
analytical solutions for the optimal beamforming design under both total and
individual power constraints, an iterative algorithm is proposed to numerically
obtain the optimal beamforming structure and maximize the secrecy rates.
Finally, robust beamforming designs in the presence of imperfect CSI are
investigated for DF-based relay beamforming, and optimization frameworks are
provided"
scgqa_340,2011.08042v1,What does the convexity of the surface in Eq.28 imply about the optimizers' behavior in the paper?,"The surface defined in Eq.28 is a function of two variables, z and y. It is a convex function, which means that it has a single global minimum. The three optimizers are all trying to find this minimum value.",2011.08042v1.pdf,"['2011.08042v1.pdf', '1912.08775v1.pdf', '2002.01322v1.pdf', '1804.10488v2.pdf', '1803.06598v1.pdf', '1809.02337v2.pdf', '1906.03859v1.pdf', '1306.4036v2.pdf']",2011.08042v1-Figure3-1.png,"Figure 3: Behavior of the three optimizers MAS, ADAM and SGD on the surface defined in Eq.28. For better visualization the SGD was shifted on X axis of 0.1.","We set β1 = 0.9, β2 = 0.999, = 10−8, amsgrad = False, dampening d = 0, nesterov = False and µ = 0. As we can see in Fig. 3 our MAS optimizer goes faster towards the minimum value after only two epochs, SGD is fast at the first epoch, however, it decreases its speed soon after and comes close to the minimum after 100 epochs, ADAM instead reaches its minimum after 25 epochs. Our approach can be fast when it gets a large vk from SGD and a large ηa from ADAM.",Mixing ADAM and SGD: a Combined Optimization Method,"Optimization methods (optimizers) get special attention for the efficient
training of neural networks in the field of deep learning. In literature there
are many papers that compare neural models trained with the use of different
optimizers. Each paper demonstrates that for a particular problem an optimizer
is better than the others but as the problem changes this type of result is no
longer valid and we have to start from scratch. In our paper we propose to use
the combination of two very different optimizers but when used simultaneously
they can overcome the performances of the single optimizers in very different
problems. We propose a new optimizer called MAS (Mixing ADAM and SGD) that
integrates SGD and ADAM simultaneously by weighing the contributions of both
through the assignment of constant weights. Rather than trying to improve SGD
or ADAM we exploit both at the same time by taking the best of both. We have
conducted several experiments on images and text document classification, using
various CNNs, and we demonstrated by experiments that the proposed MAS
optimizer produces better performance than the single SGD or ADAM optimizers.
The source code and all the results of the experiments are available online at
the following link https://gitlab.com/nicolalandro/multi\_optimizer"
scgqa_341,1101.0235v1,"Based on the analysis in the paper, which rendering system is highlighted for fastest delta movement times in Figure 7?","The graph shows that the Canvas layer rendering system has the best performance in terms of delta movement times. This is because Canvas is a native web graphics technology that is directly supported by the browser, while VML and SVG are both interpreted technologies that require the browser to convert them into a format that can be rendered. This conversion process can add additional overhead, which can lead to slower performance.",1101.0235v1.pdf,"['1101.0235v1.pdf', '2010.13032v1.pdf', '2008.07524v3.pdf', '1910.11127v1.pdf', '1402.0635v3.pdf']",1101.0235v1-Figure7-1.png,Figure 7. Measured delta movement times against a scripted mouse move of a photo,"LIST OF FIGURES Figure 1. Impression of Flickr, PhotoBucket and Picasa websites showing the grid-like layout . 16 Figure 2. Mock-up of the main web page web describing the overall interface .......................... 19 Figure 3. The MVC architecture as used with the prototype ...................................................... 19 Figure 4. A UML model of the relations between the data objects used within the application .. 20 Figure 5. Representation of the internals of our JavaScript web graphics rendering engine ....... 22 Figure 6. Representation of the Canvas layer rendering system ................................................. 25 Figure 7. Measured delta movement times against a scripted mouse move of a photo ............... 38 Figure 8. Average CPU utilization during a scripted mouse move of a photo. ........................... 39 Figure 9. Delta movement times of a scripted photo mouse move with the colours inverted. ..... 40 Figure 10. Average CPU utilization during a mouse moveof a photo with the colours inverted. 41 Figure 11. Fluctuations in memory usage during the move experiment ..................................... 42 Figure 12. Memory fluctuations during mouse move with colours inverted effect applied......... 43 Figure 13. Rotation times of a single photo when multiple photos on the page. ......................... 44 LIST OF TABLES Table 1. Comparison of web graphics on image manipulation operations ................................. 28 Table 2. Average application times in milliseconds of a crop operation (50, 50, 300, 300) ........ 36 Table 3. Average application times in milliseconds of applying a 70° rotation to a photo. ......... 37 Table 4. Average application times of applying a grayscale effect to a photo. ........................... 37 Table 5. Average application times of inverting the colours within a photo. .............................. 38","Analysis of Using Browser-native Technology to Build Rich Internet
  Applications for Image Manipulation","In this work we investigate whether browser-native technologies can be used
to perform photo manipulation tasks e.g cropping, resizing or rotating an image
within the current mainstream browser. By the use of a case study we will
analyze problems that have occurred during the implementation of a prototype
web application that utilizes browser-native web technology in order to create
an online version of a real world photo scrapbook. Implementation of a
prototype will allows us to analyze the strengths and weaknesses of current web
technology when it comes to browser-based image manipulation. Furthermore we
explore the possibilities of the Ajax in combination Canvas, SVG and VML to
provide a more interactive graphical user interface to perform image
manipulation tasks on the web."
scgqa_342,1802.05945v1,How does the output of rare diseases research in the UK compare across different funding categories as per Figure 7a?,"The graph shows that the output numbers of rare diseases research in the United Kingdom have been increasing over time, with the exception of the ""no funding"" category, which has been decreasing. The largest increase in output has been seen in the ""national"" category, which has more than doubled in size since 2009-2010. The ""other funding"" category has also seen a significant increase, while the ""Europe"" and ""Europe-national"" categories have remained relatively stable.",1802.05945v1.pdf,"['1802.05945v1.pdf', '1302.2824v2.pdf', '2007.11446v1.pdf', '1905.07512v3.pdf', '1905.11471v1.pdf', '2009.08716v1.pdf', '1505.05173v6.pdf', '1209.3394v5.pdf', '1906.02003v1.pdf']",1802.05945v1-Figure7-1.png,"Figure 7a: Output numbers, United Kingdom Figure 7b: Impact scores, United Kingdom","In Figures 4-7 we show parts a and b. In part a the output numbers per funding source are shown, while the b parts display the impact scores related to the output numbers shown in part a. In Figures 4a and 4b the situation for France is depicted. In terms of output development, the none funded is decreasing, while the other types of funding sources show slow increases in numbers. The ‘gap’ between no funding, and the funded share of the French output is relatively large. When we shift our attention to the impact scores, the no funded part has the lowest impact, and is stable on that low level. The other four types of funding sources show higher impact level (fluctuating between 20% above worldwide average impact level to 60% above worldwide average impact level, for the Europe-national funding source). Figures 5a and 5b display the output and impact for Great Britain. Contrary to France, we now observe some funding source types with higher numbers of publications (national and other funding). Europe and Europe-national show the lowest number of publications from Great Britain related to rare diseases research. Focusing on impact scores related to the output from Great Britain on rare diseases, we notice that the non-funded part has a low impact, similar to the situation found for France on this type, albeit it at a somewhat higher level. For Europe, Other funding, and nation, we observe impact level that fluctuate around 50% above average impact level for worldwide, while finally the output in Europe-national context displays a very high impact level, that is increasing to twice worldwide average impact level in 2013-2014/15. In figures 6a and 6b, output and impact for the Netherlands are shown, in which we find some similarity to the situation described for Great Britain: decrease in output for no funding, increasing output for other funding and national (with some stabilization for the latter in the final stages of the period). Like for Great Britain, Europe-national and Europe have small output numbers. Impact wise, the no funding part has an average impact level, while we observe some strong fluctuation for the impact of in particular Europe-national and Europe (which is partially explained by the small numbers involved, which tends to occur more frequently in bibliometric analyses). Other funding and national shows more stable impact levels around 50% above worldwide average impact level. Finally, in Figure 7a and 7b, the situation for Spain is displayed. In Figure 7a, the output is shown. Here we observe a somewhat different pattern as found for the other three countries, as here no funding is covering the largest part of the output, decreasing in numbers, while national as the second largest type of funding source is increasing in numbers. Other funding","How does undone science get funded? A bibliometric analysis linking rare
  diseases publications to national and European funding sources","One of the notable features of undone science debates is how formation of new
interest groups becomes pivotal in mobilizing and championing emerging research
on undone topics. Clearly money is one of the most important mediums through
which different types of actors can support and steer scientists to work on
undone topics. Yet which actors are more visible in their support for
scientific research is something which has seldom been measured. This study
delves into research funding in the context of rare diseases research, a topic
which has evolved from the margins of medical research into a priority area
articulated by many contemporary funding agencies. Rare diseases refer to
conditions affecting relatively few people in a population. Given low
incidences, interest groups have articulated a lack of attention within medical
research compared to more common conditions. The rise to prominence of rare
diseases in research funding policies is often explained in the science studies
literature in terms of effective lobbying by social movements Likewise,
innovative fundraising initiatives, infrastructure building, and close
partnerships with research groups are other means through which interested
actors have sought to build capacity for research into rare medical conditions.
To date however systematic empirical evidence to compare the relative
importance of different actors in funding rare disease research has not been
produced. Building on interest in undone science in STS and science policy
studies, our study hopes to map-out different kinds of funding actors and their
influence on leading scientific research on rare diseases, by use of
bibliometric tools. The approach we are developing relies on the use of Funding
Acknowledgement data provided in Web of Science database."
scgqa_343,1708.01249v1,"According to Fig. 8 in the research paper, how do L1-PCA and L2-PCA perform under different corruption variances?","The graph shows that for weak corruption of variance σ2 < 0dB, L1-PCA and L2-PCA exhibit similar performance. However, as the corruption variance increases, L1-PCA is able to better preserve the subspace proximity than L2-PCA. This is because L1-PCA is more robust to outliers, which are more likely to occur in the presence of strong corruption.",1708.01249v1.pdf,"['1708.01249v1.pdf', '1509.02054v1.pdf', '1910.05107v2.pdf', '1509.08992v2.pdf', '1802.02193v1.pdf', '1603.04153v1.pdf', '1904.01542v3.pdf']",1708.01249v1-Figure8-1.png,"Fig. 8. Subspace proximity versus corruption variance σ2, for L2-PCA and L1-PCA (10 000 realizations; D = 5, N = 10, K = 2).","In Fig. 8 we plot the average SP (over 10 000 independent corruption realizations) for L2-PCA and L1-PCA versus the corruption variance σ2. We observe that for weak corruption of variance σ2 < 0dB, L1-PCA and L2-PCA exhibit",L1-norm Principal-Component Analysis of Complex Data,"L1-norm Principal-Component Analysis (L1-PCA) of real-valued data has
attracted significant research interest over the past decade. However, L1-PCA
of complex-valued data remains to date unexplored despite the many possible
applications (e.g., in communication systems). In this work, we establish
theoretical and algorithmic foundations of L1-PCA of complex-valued data
matrices. Specifically, we first show that, in contrast to the real-valued case
for which an optimal polynomial-cost algorithm was recently reported by
Markopoulos et al., complex L1-PCA is formally NP-hard in the number of data
points. Then, casting complex L1-PCA as a unimodular optimization problem, we
present the first two suboptimal algorithms in the literature for its solution.
Our experimental studies illustrate the sturdy resistance of complex L1-PCA
against faulty measurements/outliers in the processed data."
scgqa_344,1802.05945v1,How are impact scores for UK rare diseases research by funding type represented in Figure 7 of the study?,"The graph shows that the impact scores of rare diseases research in the United Kingdom have been relatively high, with the exception of the ""no funding"" category, which has had a low impact score throughout the entire period. The largest impact score has been seen in the ""Europe-national"" category, which has been more than twice the worldwide average impact score since 2013-2014. The ""national"" category has also had a high impact score, while the ""other funding"" and ""Europe"" categories have had slightly lower impact scores.",1802.05945v1.pdf,"['1802.05945v1.pdf', '1512.00843v3.pdf', '1205.4213v2.pdf', '2001.07829v1.pdf', '2004.04276v1.pdf']",1802.05945v1-Figure7-1.png,"Figure 7a: Output numbers, United Kingdom Figure 7b: Impact scores, United Kingdom","In Figures 4-7 we show parts a and b. In part a the output numbers per funding source are shown, while the b parts display the impact scores related to the output numbers shown in part a. In Figures 4a and 4b the situation for France is depicted. In terms of output development, the none funded is decreasing, while the other types of funding sources show slow increases in numbers. The ‘gap’ between no funding, and the funded share of the French output is relatively large. When we shift our attention to the impact scores, the no funded part has the lowest impact, and is stable on that low level. The other four types of funding sources show higher impact level (fluctuating between 20% above worldwide average impact level to 60% above worldwide average impact level, for the Europe-national funding source). Figures 5a and 5b display the output and impact for Great Britain. Contrary to France, we now observe some funding source types with higher numbers of publications (national and other funding). Europe and Europe-national show the lowest number of publications from Great Britain related to rare diseases research. Focusing on impact scores related to the output from Great Britain on rare diseases, we notice that the non-funded part has a low impact, similar to the situation found for France on this type, albeit it at a somewhat higher level. For Europe, Other funding, and nation, we observe impact level that fluctuate around 50% above average impact level for worldwide, while finally the output in Europe-national context displays a very high impact level, that is increasing to twice worldwide average impact level in 2013-2014/15. In figures 6a and 6b, output and impact for the Netherlands are shown, in which we find some similarity to the situation described for Great Britain: decrease in output for no funding, increasing output for other funding and national (with some stabilization for the latter in the final stages of the period). Like for Great Britain, Europe-national and Europe have small output numbers. Impact wise, the no funding part has an average impact level, while we observe some strong fluctuation for the impact of in particular Europe-national and Europe (which is partially explained by the small numbers involved, which tends to occur more frequently in bibliometric analyses). Other funding and national shows more stable impact levels around 50% above worldwide average impact level. Finally, in Figure 7a and 7b, the situation for Spain is displayed. In Figure 7a, the output is shown. Here we observe a somewhat different pattern as found for the other three countries, as here no funding is covering the largest part of the output, decreasing in numbers, while national as the second largest type of funding source is increasing in numbers. Other funding","How does undone science get funded? A bibliometric analysis linking rare
  diseases publications to national and European funding sources","One of the notable features of undone science debates is how formation of new
interest groups becomes pivotal in mobilizing and championing emerging research
on undone topics. Clearly money is one of the most important mediums through
which different types of actors can support and steer scientists to work on
undone topics. Yet which actors are more visible in their support for
scientific research is something which has seldom been measured. This study
delves into research funding in the context of rare diseases research, a topic
which has evolved from the margins of medical research into a priority area
articulated by many contemporary funding agencies. Rare diseases refer to
conditions affecting relatively few people in a population. Given low
incidences, interest groups have articulated a lack of attention within medical
research compared to more common conditions. The rise to prominence of rare
diseases in research funding policies is often explained in the science studies
literature in terms of effective lobbying by social movements Likewise,
innovative fundraising initiatives, infrastructure building, and close
partnerships with research groups are other means through which interested
actors have sought to build capacity for research into rare medical conditions.
To date however systematic empirical evidence to compare the relative
importance of different actors in funding rare disease research has not been
produced. Building on interest in undone science in STS and science policy
studies, our study hopes to map-out different kinds of funding actors and their
influence on leading scientific research on rare diseases, by use of
bibliometric tools. The approach we are developing relies on the use of Funding
Acknowledgement data provided in Web of Science database."
scgqa_345,1808.08442v1,"According to Fig. 2 in the paper, how does changing parameter A affect MFKF1's performance in under-modeling?",The graph shows that the MFKF1 algorithm with different values of A performs significantly better than the standard FKF in the under-modeling situation. This is because the MFKF1 algorithm is able to track the true state more accurately and converge to a lower steady-state misalignment.,1808.08442v1.pdf,"['1808.08442v1.pdf', '1905.11471v1.pdf', '2003.09700v4.pdf', '2011.03519v1.pdf', '2003.13216v1.pdf', '1303.1635v1.pdf', '1912.08775v1.pdf']",1808.08442v1-Figure2-1.png,Fig. 2. Misalignments of FKF and MFKF1 with different transition parameters in the under-modeling example.,"Fig. 2 shows the misalignment curves of the MFKF1 with different values of A in the under-modeling situation whose setup is the same as the above example. It has been pointed out that the parameter A has influence on the convergence rate, the tracking ability and the steady-state misalignment [3][7]. It can be seen from Fig. 2 that the steady-state misalignment of the MFKF1 increases as the parameter A decreases, but overall the MFKF1 with different values of A performs significantly better than the standard FKF in this situation.",Efficient improvement of frequency-domain Kalman filter,"The frequency-domain Kalman filter (FKF) has been utilized in many audio
signal processing applications due to its fast convergence speed and
robustness. However, the performance of the FKF in under-modeling situations
has not been investigated. This paper presents an analysis of the steady-state
behavior of the commonly used diagonalized FKF and reveals that it suffers from
a biased solution in under-modeling scenarios. Two efficient improvements of
the FKF are proposed, both having the benefits of the guaranteed optimal
steady-state behavior at the cost of a very limited increase of the
computational burden. The convergence behavior of the proposed algorithms is
also compared analytically. Computer simulations are conducted to validate the
improved performance of the proposed methods."
scgqa_346,1509.00374v2,What implications for energy efficiency design can be drawn from the total energy consumption shown in Fig. 8?,The results in this graph show that the joint energy minimization optimization can achieve the lowest total energy consumption. This means that the joint optimization can be used to design energy-efficient communication systems.,1509.00374v2.pdf,"['1509.00374v2.pdf', '1803.01118v2.pdf', '1405.6408v2.pdf', '1610.04213v4.pdf', '1403.2732v1.pdf', '1909.05034v1.pdf']",1509.00374v2-Figure8-1.png,"Fig. 8. Total energy consumption vs. data size under different T Tri,max with Fi = 1500.","In Fig. 7 and Fig. 8, we compare the proposed joint energy minimization optimization with","Joint Energy Minimization and Resource Allocation in C-RAN with Mobile
  Cloud","Cloud radio access network (C-RAN) has emerged as a potential candidate of
the next generation access network technology to address the increasing mobile
traffic, while mobile cloud computing (MCC) offers a prospective solution to
the resource-limited mobile user in executing computation intensive tasks.
Taking full advantages of above two cloud-based techniques, C-RAN with MCC are
presented in this paper to enhance both performance and energy efficiencies. In
particular, this paper studies the joint energy minimization and resource
allocation in C-RAN with MCC under the time constraints of the given tasks. We
first review the energy and time model of the computation and communication.
Then, we formulate the joint energy minimization into a non-convex optimization
with the constraints of task executing time, transmitting power, computation
capacity and fronthaul data rates. This non-convex optimization is then
reformulated into an equivalent convex problem based on weighted minimum mean
square error (WMMSE). The iterative algorithm is finally given to deal with the
joint resource allocation in C-RAN with mobile cloud. Simulation results
confirm that the proposed energy minimization and resource allocation solution
can improve the system performance and save energy."
scgqa_347,1907.10906v1,"In the context of the paper, how do p and q relate to subspace clustering performance as per Figure 1?","The graph shows that the performance of SC improves as p and q both increase. This is because as p and q increase, the amount of information that is leaked to the eavesdropper and the legitimate receiver decreases. This results in a more secure and reliable communication system.",1907.10906v1.pdf,"['1907.10906v1.pdf', '2004.05579v1.pdf', '1708.09328v1.pdf', '1904.03292v2.pdf', '1509.08992v2.pdf', '1903.10464v3.pdf', '1607.00675v1.pdf']",1907.10906v1-Figure1-1.png,"Figure 1: The relation between p and q, when d = 100, n = 5000, κ = 1 − √ 1/2 (s = d/2), ρ = 1.","It is obvious that p will decrease simultaneously if q decreases by increasing τ , which is also demonstrated in Figure 1. Combining the result of the second experiment (c.f. Figure 2), we can find that it is better to make p, q both large than to choose q = 0, although q = 0 is suggested by SDP, which is consistent with our result, while shows that SDP is somewhat inadequate for SC.","Theory of Spectral Method for Union of Subspaces-Based Random Geometry
  Graph","Spectral Method is a commonly used scheme to cluster data points lying close
to Union of Subspaces by first constructing a Random Geometry Graph, called
Subspace Clustering. This paper establishes a theory to analyze this method.
Based on this theory, we demonstrate the efficiency of Subspace Clustering in
fairly broad conditions. The insights and analysis techniques developed in this
paper might also have implications for other random graph problems. Numerical
experiments demonstrate the effectiveness of our theoretical study."
scgqa_348,1909.03961v2,"Referring to Figure 7 in the DCAD paper, how does the number of agents impact trajectory calculation time?","The graph shows that the time required to compute a collision-free trajectory increases with the number of agents. This is because as the number of agents increases, the number of possible collisions also increases. Therefore, the algorithm must spend more time checking for collisions and computing the optimal trajectory.",1909.03961v2.pdf,"['1909.03961v2.pdf', '2002.06090v1.pdf', '1708.09328v1.pdf', '1707.02327v1.pdf', '1404.7045v3.pdf', '2010.07597v2.pdf', '1706.03112v1.pdf', '1402.0808v1.pdf', '1407.5358v1.pdf']",1909.03961v2-Figure7-1.png,Fig. 7: The time (ms) required to compute a collision-free trajectory for a single agent per timestep considering 1 to 50 nearest neighbors.,Figure 7 illustrates the average time taken by our algorithm for computing control input for all agents in the scenario presented in Fig. 3. The number of agents in the scenario is varied from 5 to 50 agents. Each agent considers only the nearest 10 agents as obstacles.,"DCAD: Decentralized Collision Avoidance with Dynamics Constraints for
  Agile Quadrotor Swarms","We present a novel, decentralized collision avoidance algorithm for
navigating a swarm of quadrotors in dense environments populated with static
and dynamic obstacles. Our algorithm relies on the concept of Optimal
Reciprocal CollisionAvoidance (ORCA) and utilizes a flatness-based Model
Predictive Control (MPC) to generate local collision-free trajectories for each
quadrotor. We feedforward linearize the non-linear dynamics of the quadrotor
and subsequently use this linearized model in our MPC framework. Our method is
downwash conscious and computes safe trajectories that avoid quadrotors from
entering each other's downwash regions during close proximity maneuvers. In
addition, we account for the uncertainty in sensed position and velocity data
using Kalman filtering. We evaluate the performance of our algorithm with other
state-of-the-art methods and demonstrate its superior performance in terms of
smoothness of generated trajectories and lower probability of collision during
high velocity maneuvers."
scgqa_349,1006.4386v1,What does Fig. 3 indicate about the effect of individual relay power on the secrecy rate in the studied system?,"The graph suggests that there is a negative relationship between the individual relay power and the secrecy rate. This means that as the individual relay power increases, the secrecy rate decreases. This is because with more individual relay power, the CRB system is more likely to cause interference with itself and reduce the secrecy rate.",1006.4386v1.pdf,"['1006.4386v1.pdf', '1807.09483v2.pdf', '1809.01628v1.pdf', '1608.06005v1.pdf', '2007.11391v1.pdf']",1006.4386v1-Figure3-1.png,Fig. 3. DF Second-hop secrecy rate vs. the total relay transmit powerPT for different cases. Eavesdropper has a stronger channel.,"In Figures 2 and 3, we plot the second-hop secrecy rate, which is the maximum secrecy rate that our collaborative relay beamforming system can support under both total and individual relay power constraints. For the case of individual relay power constraints, we assume that the relays have the same power budgets: pi = PT M . Specifically, in Fig. 2, we have σh = 3, σz = 1, N0 = 1 and M = 5. In this case, the legitimate user has a stronger channel. In Fig. 3, the only changes are σh = 1 and σz = 2, which imply that the eavesdropper has a stronger channel. Our CRB system can achieve secure transmission even when the eavesdropper has more favorable channel conditions. As can be seen from the figures, the highest secrecy rate is achieved, as expected, under a total transmit power constraint. On the other hand, we observe that only a relatively small rate loss is experienced under individual relay power constraints. Moreover, we note that our two different optimization approaches give nearly the same result. It also can be seen that under individual power constraint, the simple suboptimal method suffers a constant loss as compared to SDR or SOCP based optimal value.",Collaborative Relay Beamforming for Secrecy,"In this paper, collaborative use of relays to form a beamforming system and
provide physical-layer security is investigated. In particular,
decode-and-forward (DF) and amplify-and-forward (AF) relay beamforming designs
under total and individual relay power constraints are studied with the goal of
maximizing the secrecy rates when perfect channel state information (CSI) is
available. In the DF scheme, the total power constraint leads to a closed-form
solution, and in this case, the optimal beamforming structure is identified in
the low and high signal-to-noise ratio (SNR) regimes. The beamforming design
under individual relay power constraints is formulated as an optimization
problem which is shown to be easily solved using two different approaches,
namely semidefinite programming and second-order cone programming. A simplified
and suboptimal technique which reduces the computation complexity under
individual power constraints is also presented. In the AF scheme, not having
analytical solutions for the optimal beamforming design under both total and
individual power constraints, an iterative algorithm is proposed to numerically
obtain the optimal beamforming structure and maximize the secrecy rates.
Finally, robust beamforming designs in the presence of imperfect CSI are
investigated for DF-based relay beamforming, and optimization frameworks are
provided"
scgqa_350,1606.01062v1,What does Figure 2 reveal about the dependence of the number of terms on p for $arphi$-sub-Gaussian random processes?,The graph shows that the number of terms increases as p increases. This is because the problem becomes more nonlinear and the series is more difficult to approximate with a linear function when p is large.,1606.01062v1.pdf,"['1606.01062v1.pdf', '1703.03892v5.pdf', '1809.09034v1.pdf', '1908.05243v1.pdf', '1509.00374v2.pdf', '1509.02054v1.pdf']",1606.01062v1-Figure2-1.png,Figure 2: The number of terms as a function of p,"Now, for fixed ε and δ Figure 2 illustrates the behaviour of the number of terms n as a function of the parameter p ∈ [1, 2]. The plot was produced using the values T = B(0) = ω = 1, Λ = 3/4, and ε = δ = 0.1.","Whittaker-Kotel'nikov-Shannon approximation of $\varphi$-sub-Gaussian
  random processes","The article starts with generalizations of some classical results and new
truncation error upper bounds in the sampling theorem for bandlimited
stochastic processes. Then, it investigates $L_p([0,T])$ and uniform
approximations of $\varphi$-sub-Gaussian random processes by finite time
sampling sums. Explicit truncation error upper bounds are established. Some
specifications of the general results for which the assumptions can be easily
verified are given. Direct analytical methods are employed to obtain the
results."
scgqa_351,1808.06304v2,"In the context of the WikiSQL dataset, what essential limitations are shown in Figure 2 regarding model performance?","The graph does not provide any information about the time it takes for the model to learn from the training data. This is an important factor to consider, as the model may not be able to learn from a large number of examples in a reasonable amount of time. Additionally, the graph does not provide any information about the model's performance on different types of questions. This is an important factor to consider, as the model may not perform well on questions that are not included in the training data.",1808.06304v2.pdf,"['1808.06304v2.pdf', '1709.03329v1.pdf', '1607.05970v2.pdf']",1808.06304v2-Figure2-1.png,"Figure 2: Semantic parsing accuracies of the STAMP model on WikiSQL. The x-axis is the training data size in log-scale, and the y-axis includes two evaluation metrics Acclf and Accex.","In this experiment, we randomly sample 20 subsets of examples from the WikiSQL training data, incrementally increased by 3K examples (about 1/20 of the full WikiSQL training data). We use the same training protocol and report the accuracy of the STAMP model on the dev set. Results are given in Figure 2. It is not surprising that more training examples bring higher accuracy. Interestingly, we observe that both accuracies of the neural network based semantic parser grow logarith-",Question Generation from SQL Queries Improves Neural Semantic Parsing,"We study how to learn a semantic parser of state-of-the-art accuracy with
less supervised training data. We conduct our study on WikiSQL, the largest
hand-annotated semantic parsing dataset to date. First, we demonstrate that
question generation is an effective method that empowers us to learn a
state-of-the-art neural network based semantic parser with thirty percent of
the supervised training data. Second, we show that applying question generation
to the full supervised training data further improves the state-of-the-art
model. In addition, we observe that there is a logarithmic relationship between
the accuracy of a semantic parser and the amount of training data."
scgqa_352,1908.04647v1,What insights does Figure 5 offer regarding the relationship between the approximation degree and the inf-sup constant?,"The results of the graph suggest that there is a k-dependence of the inf-sup constant, but that this dependence is much weaker than k. This means that the inf-sup constant is relatively insensitive to the approximation degree, which is an important property for ensuring stability of the finite element method.",1908.04647v1.pdf,"['1908.04647v1.pdf', '2006.16705v1.pdf', '1807.09483v2.pdf', '2008.06134v1.pdf', '1808.08442v1.pdf', '1906.03859v1.pdf']",1908.04647v1-Figure5-1.png,Figure 5. Stabilized values from Figure 4 (on the right with logarithmic scaling; the dashed line shows a slope of −3).,"We focus on the canonical geometric edge, corner, and corner-edge refinements from Section 3.1. As before, we first fix the approximation degree k, and refine the meshes step by step in order to monitor the inf-sup constant; see Figure 4 for the resulting plots with different approximation degrees. Again, we display the stabilized values of γa for increasing k in Figure 5. The results are qualitatively similar to the inf-sup constant γB discussed earlier. There is a k-dependence of the inf-sup constant γa which is again much weaker than k","Stability and Convergence of Spectral Mixed Discontinuous Galerkin
  Methods for 3D Linear Elasticity on Anisotropic Geometric Meshes","We consider spectral mixed discontinuous Galerkin finite element
discretizations of the Lam\'e system of linear elasticity in polyhedral domains
in $\mathbb{R}^3$. In order to resolve possible corner, edge, and corner-edge
singularities, anisotropic geometric edge meshes consisting of hexahedral
elements are applied. We perform a computational study on the discrete inf-sup
stability of these methods, and especially focus on the robustness with respect
to the Poisson ratio close to the incompressible limit (i.e. the Stokes
system). Furthermore, under certain realistic assumptions (for analytic data)
on the regularity of the exact solution, we illustrate numerically that the
proposed mixed DG schemes converge exponentially in a natural DG norm."
scgqa_353,1706.03019v1,What do the subgraph properties reveal about connectivity for varying activity levels in Hurricane Sandy's social media data?,"The results of the study suggest that the larger the size of the subgraph, the more nodes it includes from a lower activity level. This implies that the connectivity between nodes does not follow the rate at which the network grows for larger subgraphs.",1706.03019v1.pdf,"['1706.03019v1.pdf', '1604.06979v1.pdf', '1205.4213v2.pdf', '1911.07924v1.pdf', '1610.01283v4.pdf', '1902.05312v2.pdf', '1804.04290v1.pdf']",1706.03019v1-Figure2-1.png,"Figure 2 Subgraph properties at different activity levels (where activity level is defined by the number of tweets made by a node) (a) Number of nodes and links, (b) Network densities, (c) Number of isolates and connected components,","Fig. 2 shows the variation of the subgraph network properties at various activity levels. It is important to note here that the larger the size of the subgraphs, the more nodes it include from a lower activity level. We observe that the number of nodes and links generated in these subgraphs (both directed and undirected) grow exponentially for larger subgraphs. A similar pattern is observed for the nodes and links that exist in the largest connected component. There exists almost equal number of connected components and isolates for all levels of activity. Network densities of the subgraphs (both directed and undirected) tend to zero for larger subgraphs, having slightly higher densities in the largest connected component in each case. This implies that the connectivity between nodes do not follow the rate at which the network grows for larger subgraphs.","Understanding Information Spreading in Social Media during Hurricane
  Sandy: User Activity and Network Properties","Many people use social media to seek information during disasters while
lacking access to traditional information sources. In this study, we analyze
Twitter data to understand information spreading activities of social media
users during hurricane Sandy. We create multiple subgraphs of Twitter users
based on activity levels and analyze network properties of the subgraphs. We
observe that user information sharing activity follows a power-law distribution
suggesting the existence of few highly active nodes in disseminating
information and many other nodes being less active. We also observe close
enough connected components and isolates at all levels of activity, and
networks become less transitive, but more assortative for larger subgraphs. We
also analyze the association between user activities and characteristics that
may influence user behavior to spread information during a crisis. Users become
more active in spreading information if they are centrally placed in the
network, less eccentric, and have higher degrees. Our analysis provides
insights on how to exploit user characteristics and network properties to
spread information or limit the spreading of misinformation during a crisis
event."
scgqa_354,1706.01341v1,What finding regarding average absolute error does Figure 6.9 present compared to earlier figures in this research?,"The average absolute error in Figure 6.9 is 9.47%, which is lower than the average absolute error in the previous figure. This suggests that the modification to the performance prediction algorithm was successful in improving the accuracy of the predictions.",1706.01341v1.pdf,"['1706.01341v1.pdf', '2001.11086v3.pdf', '1509.08992v2.pdf', '1409.2897v1.pdf', '1607.05970v2.pdf', '1705.00891v1.pdf']",1706.01341v1-Figure6.9-1.png,"Figure 6.9: Final performance predictions for Cabc := AaiBibc. (i = 8, Harpertown E5450, 1 thread, OpenBLAS, median of 10 repetitions)","benchmark are performed just as before. As before, the prediction for the total runtime is obtained from weighting all relevant benchmark timings with the corresponding number of occurrences within the algorithm. In Figure 6.9a, we present the improved performance predictions obtained from this modification. The performance of all algorithms is now predicted with satisfying accuracy—the average absolute error is 9.47 %.",Performance Modeling and Prediction for Dense Linear Algebra,"This dissertation introduces measurement-based performance modeling and
prediction techniques for dense linear algebra algorithms. As a core principle,
these techniques avoid executions of such algorithms entirely, and instead
predict their performance through runtime estimates for the underlying compute
kernels. For a variety of operations, these predictions allow to quickly select
the fastest algorithm configurations from available alternatives. We consider
two scenarios that cover a wide range of computations:
  To predict the performance of blocked algorithms, we design
algorithm-independent performance models for kernel operations that are
generated automatically once per platform. For various matrix operations,
instantaneous predictions based on such models both accurately identify the
fastest algorithm, and select a near-optimal block size.
  For performance predictions of BLAS-based tensor contractions, we propose
cache-aware micro-benchmarks that take advantage of the highly regular
structure inherent to contraction algorithms. At merely a fraction of a
contraction's runtime, predictions based on such micro-benchmarks identify the
fastest combination of tensor traversal and compute kernel."
scgqa_355,1708.07888v3,What performance differences between AES and NV do the figures show regarding label noise in the Branin example?,"The graph shows that AES and NV both perform well on the Branin example when the labels are not noisy. However, when noise is added to the labels, the performance of both methods decreases. AES is more robust to noise than NV, as it is able to exploit rogue points to try to find new feasible regions. In contrast, NV has high error mostly along the input space boundaries, where it cannot query samples outside to further investigate those apparent feasible regions.",1708.07888v3.pdf,"['1708.07888v3.pdf', '2008.07011v1.pdf', '1904.03292v2.pdf', '1608.00887v1.pdf', '2011.03519v1.pdf', '1906.03859v1.pdf', '1611.03254v1.pdf', '1909.01868v3.pdf', '2010.11594v1.pdf']",1708.07888v3-Figure10-1.png,Fig. 10: AES and NV on the Branin example using noisy labels.,"In each case we compare the performance of AES ( = 0.3, η = 1.3) and NV (with tight bounds) under two noise levels. As expected, adding noise to the labels decreases the accuracy of both methods (Fig. 10a and 10b). However, in both cases (Bernoulli noise and Gaussian noise), the noise appears to influence NV more than AES. As shown in Fig. 11, when adding noise to the labels, NV has high error mostly along the input space boundaries, where it cannot query samples outside to further investigate those apparent feasible regions. In contrast, AES tries to exploit those rogue points to try to find new feasible regions, realizing after a few new samples that they are noise.","Active Expansion Sampling for Learning Feasible Domains in an Unbounded
  Input Space","Many engineering problems require identifying feasible domains under implicit
constraints. One example is finding acceptable car body styling designs based
on constraints like aesthetics and functionality. Current active-learning based
methods learn feasible domains for bounded input spaces. However, we usually
lack prior knowledge about how to set those input variable bounds. Bounds that
are too small will fail to cover all feasible domains; while bounds that are
too large will waste query budget. To avoid this problem, we introduce Active
Expansion Sampling (AES), a method that identifies (possibly disconnected)
feasible domains over an unbounded input space. AES progressively expands our
knowledge of the input space, and uses successive exploitation and exploration
stages to switch between learning the decision boundary and searching for new
feasible domains. We show that AES has a misclassification loss guarantee
within the explored region, independent of the number of iterations or labeled
samples. Thus it can be used for real-time prediction of samples' feasibility
within the explored region. We evaluate AES on three test examples and compare
AES with two adaptive sampling methods -- the Neighborhood-Voronoi algorithm
and the straddle heuristic -- that operate over fixed input variable bounds."
scgqa_356,1404.7045v3,"In the research paper, what trend is illustrated regarding Google Scholar Citations and Microsoft Academic Search in Figure 7a?","The graph shows that Google Scholar Citations is more popular than Microsoft Academic Search. This is evident from the fact that the user queries for GSC have not stopped growing since its birth, while the user queries for MAS have shown a progressive decline.",1404.7045v3.pdf,"['1404.7045v3.pdf', '2004.03870v1.pdf', '1910.09823v3.pdf', '2005.09634v1.pdf', '1912.00035v1.pdf', '1906.11938v3.pdf']",1404.7045v3-Figure7-1.png,Figure 7a. “Google Scholar Citations” & “Microsoft Academic Search” search queries *,"Finally, we used the Google Trends service in order to identify which is the most popular academic search engine in search queries made by users. We compared GSC and MAS (Figure 7a), and MAS and GS (Figure 7b). The differences in favor of Google products are awesome. For GSC, as can be seen in Figure 5a, the user queries have not stopped growing since its birth, rapidly beating MAS, which shows a progressive decline.","Empirical Evidences in Citation-Based Search Engines: Is Microsoft
  Academic Search dead?","The goal of this working paper is to summarize the main empirical evidences
provided by the scientific community as regards the comparison between the two
main citation based academic search engines: Google Scholar and Microsoft
Academic Search, paying special attention to the following issues: coverage,
correlations between journal rankings, and usage of these academic search
engines. Additionally, selfelaborated data is offered, which are intended to
provide current evidence about the popularity of these tools on the Web, by
measuring the number of rich files PDF, PPT and DOC in which these tools are
mentioned, the amount of external links that both products receive, and the
search queries frequency from Google Trends. The poor results obtained by MAS
led us to an unexpected and unnoticed discovery: Microsoft Academic Search is
outdated since 2013. Therefore, the second part of the working paper aims at
advancing some data demonstrating this lack of update. For this purpose we
gathered the number of total records indexed by Microsoft Academic Search since
2000. The data shows an abrupt drop in the number of documents indexed from
2,346,228 in 2010 to 8,147 in 2013 and 802 in 2014. This decrease is offered
according to 15 thematic areas as well. In view of these problems it seems
logical not only that Microsoft Academic Searchwas poorly used to search for
articles by academics and students, who mostly use Google or Google Scholar,
but virtually ignored by bibliometricians"
scgqa_357,1906.11938v3,What advantage does QFlip have over Greedy in predicting opponent moves according to the study?,"The main difference between the two strategies, QFlip and Greedy, is that QFlip uses an opponent-learning model (oppLM) to predict the opponent's next move, while Greedy does not. This allows QFlip to play more optimally, as it can take into account the opponent's previous moves and adapt its own strategy accordingly.",1906.11938v3.pdf,"['1906.11938v3.pdf', '1808.07801v3.pdf', '2008.07011v1.pdf', '1710.09234v1.pdf', '1801.08825v1.pdf']",1906.11938v3-Figure7-1.png,"Fig. 7: Player 1 and Player 0’s average benefit for QFlip and Greedy across Player 1 costs. QFlip with oppLM playing against Pδ with fixed k0 = 1 and δ = 50 for 250,000 ticks, averaged over 100 runs.","Comparison to Greedy. Assuming the Greedy strategy against Pδ plays first at time δ, it will play optimally with probability 1− k1/δ. However, with probability k1/δ, Greedy will drop out after its first adaptive move [3]. We compare QFlip and Greedy against Pδ for δ = 50 across many costs in Figure 7. QFlip consistently achieves better average benefit across runs, playing close to optimally on average. Additionally, Player 0 with k0 = 1 attains more benefit on average against a Greedy opponent as a result of these erroneous drop-outs. For k1 < 45, QFlip attains benefit between 5% and 50% better than Greedy on average.","QFlip: An Adaptive Reinforcement Learning Strategy for the FlipIt
  Security Game","A rise in Advanced Persistent Threats (APTs) has introduced a need for
robustness against long-running, stealthy attacks which circumvent existing
cryptographic security guarantees. FlipIt is a security game that models
attacker-defender interactions in advanced scenarios such as APTs. Previous
work analyzed extensively non-adaptive strategies in FlipIt, but adaptive
strategies rise naturally in practical interactions as players receive feedback
during the game. We model the FlipIt game as a Markov Decision Process and
introduce QFlip, an adaptive strategy for FlipIt based on temporal difference
reinforcement learning. We prove theoretical results on the convergence of our
new strategy against an opponent playing with a Periodic strategy. We confirm
our analysis experimentally by extensive evaluation of QFlip against specific
opponents. QFlip converges to the optimal adaptive strategy for Periodic and
Exponential opponents using associated state spaces. Finally, we introduce a
generalized QFlip strategy with composite state space that outperforms a Greedy
strategy for several distributions including Periodic and Uniform, without
prior knowledge of the opponent's strategy. We also release an OpenAI Gym
environment for FlipIt to facilitate future research."
scgqa_358,1808.06818v1,"According to the findings illustrated in Figure 5 of this paper, how does CTS usage relate to user success in searches?","The graph shows that searches with CTS usage lead much more frequently to positive signals than searches without. This is statistically significant for window size≥5 with Chi-Squared-Test, p<0.001. About 14% of the searches lead to positive signals after four interactions, independently of having CTS used before or not. Beginning with five interactions, the percentage of searches with positive signals is higher for searches with CTS usage. This indicates that CTS usage can help users find relevant information more quickly and easily.",1808.06818v1.pdf,"['1808.06818v1.pdf', '1106.3826v2.pdf', '1610.08534v1.pdf']",1808.06818v1-Figure5-1.png,Fig 5. The global usefulness of searches with vs. without use of a combined term suggestion service (CTS),"To analyze the global usefulness of the CTS we follow the evaluation methodology from Section 3. Figure 3 gives an overview of extracted patterns for CTS_select followed by at least one positive signal within an event window of seven succeeding actions (4,569 sessions). Positive signals are color-coded in green. The main pattern which leads to a positive signal is: “CTS_select>>CTS_search>>(view_record)+>>{positive_signal} ”. Figure 4 then shows the analog diagram for searches without the use of CTS_select in advance and positive signals within an event window of seven actions (21,712 sessions). Here, the main starting point is CTS_search (one event less than for CTS_select). Figure 3 and 4 show that the main path patterns differ not very much between the two search variants. However in Figure 5 it can be seen that searches with CTS usage lead much more frequently to positive signals than searches without (statistically significant for window size≥5 with Chi-Squared-Test, p<0.001). About 14% of the searches lead to positive signals after four interactions, independently of having CTS used before or not. Beginning with","A Usefulness-based Approach for Measuring the Local and Global Effect of
  IIR Services","In Interactive Information Retrieval (IIR) different services such as search
term suggestion can support users in their search process. The applicability
and performance of such services is either measured with different
user-centered studies (like usability tests or laboratory experiments) or, in
the context of IR, with their contribution to measures like precision and
recall. However, each evaluation methodology has its certain disadvantages. For
example, user-centered experiments are often costly and small-scaled; IR
experiments rely on relevance assessments and measure only relevance of
documents. In this work we operationalize the usefulness model of Cole et al.
(2009) on the level of system support to measure not only the local effect of
an IR service, but the impact it has on the whole search process. We therefore
use a log-based evaluation approach which models user interactions within
sessions with positive signals and apply it for the case of a search term
suggestion service. We found that the usage of the service significantly often
implicates the occurrence of positive signals during the following session
steps."
scgqa_359,2004.03870v1,"According to Figure 11 in the paper, how does coupling strength relate to the excitation probability of DQD qubits?","The graph shows that the excitation probability of the 1st DQD qubit is a monotonically increasing function of the coupling strength of the first DQD qubit to the cavity. This is because the stronger the coupling, the more likely the qubit is to be excited by the single photon.",2004.03870v1.pdf,"['2004.03870v1.pdf', '1502.03556v1.pdf', '1911.09804v2.pdf', '2007.11391v1.pdf', '1005.0416v1.pdf', '1710.09234v1.pdf', '1603.08981v2.pdf', '1206.5265v1.pdf', '1907.06845v5.pdf']",2004.03870v1-Figure11-1.png,"Figure 11: The excitation probability of the 1st DQD qubit, we fix the terminal time t = 4 and choose µ = 0.2 and κ = 7.5× 10−3ω0.","In other words, the two DQD qubits form a superposition state and are decoupled from the rest of the coherent feedback network. The excitation probability of the 1st DQD qubit with the single-photon input state (76) is shown in Fig. 11. In Fig. 11, it can be observed that the maximum value of the excitation probability is only 0.5 when the two DQD qubits are equally coupled to the cavity. However, the excitation probability can approximately attains 1 if Γ1 is sufficiently larger than Γ2, as predicted by Theorem 3.","On the dynamics of a quantum coherent feedback network of
  cavity-mediated double quantum dot qubits","The purpose of this paper is to present a comprehensive study of a coherent
feedback network where the main component consists of two distant double
quantum dot (DQD) qubits which are directly coupled to a cavity. This main
component has recently been physically realized (van Woerkom, {\it et al.},
Microwave photon-mediated interactions between semiconductor qubits, Physical
Review X, 8(4):041018, 2018). The feedback loop is closed by cascading this
main component with a beamsplitter. The dynamics of this coherent feedback
network is studied from three perspectives. First, an analytic form of the
output single-photon state of the network driven by a single-photon state is
derived; in particular, it is observed that coherent feedback elongates
considerably the interaction between the input single photon and the network.
Second, excitation probabilities of DQD qubits are computed when the network is
driven by a single-photon input state. Moreover, if the input is vacuum but one
of the two DQD qubits is initialized in its excited state, the explicit
expression of the state of the network is derived, in particular, it is shown
that the output field and the two DQD qubits can form an entangled state if the
transition frequencies of two DQD qubits are equal. Finally, the exact form of
the pulse shape is obtained by which the single-photon input can fully excite
one of these two DQD qubits at any controllable time, which may be useful in
the construction of $2$-qubit quantum gates."
scgqa_360,2010.08182v3,What unique challenges are highlighted in the paper regarding post volume and demographic data in urban analysis?,"There are a number of challenges in quantifying the relationship between the number of posts and demographic factors. One challenge is that the number of posts is not always a reliable indicator of population. For example, in some cities, there may be a large number of people who do not use social media, while in other cities, social media may be more popular. Another challenge is that the relationship between the number of posts and demographic factors may change over time. For example, the number of posts may increase or decrease as the population of a city changes.",2010.08182v3.pdf,"['2010.08182v3.pdf', '1907.05050v3.pdf', '1603.08981v2.pdf', '1505.02851v1.pdf']",2010.08182v3-Figure2-1.png,Figure 2. Number of posts of “JW” cities and “WG” cities,"employed to normalize the index. A drawback of this method is that the number of posts is not always significantly correlated with population. It is difficult to quantify the relationship between the number of posts and demographic factors. As shown in Figure 2 in section 4.1, the relationship between the number of posts and population varies in different cities.","Measuring the Dynamic Impact of High-Speed Railways on Urban
  Interactions in China","High-speed rail (HSR) has become an important mode of inter-city
transportation between large cities. Inter-city interaction facilitated by HSR
tends to play a more prominent role in promoting urban and regional economic
integration and development. Quantifying the impact of HSR's interaction on
cities and people is therefore crucial for long-term urban and regional
development planning and policy making. We develop an evaluation framework
using toponym information from social media as a proxy to estimate the dynamics
of such interactions. This paper adopts two types of spatial information:
toponyms from social media posts, and the geographical location information
embedded in social media posts. The framework highlights the asymmetric nature
of social interaction among cities, and proposes a series of metrics to
quantify such impact from multiple perspectives, including interaction
strength, spatial decay, and channel effect. The results show that HSRs not
only greatly expand the uneven distribution of inter-city connections, but also
significantly reshape the interactions that occur along HSR routes through the
channel effect."
scgqa_361,2011.07119v1,What conclusion can be drawn about the relationship between graph topology and iterative process performance in tvopt?,"The fact that the random graph has the best results implies that connectivity is an important factor in the performance of the iterative process. This is because a more connected graph has more paths between nodes, which means that the error in the iterative process is more likely to be corrected.",2011.07119v1.pdf,"['2011.07119v1.pdf', '1805.07914v3.pdf', '1311.6183v1.pdf', '1106.3242v2.pdf', '2011.09375v1.pdf', '1708.07888v3.pdf', '1906.02003v1.pdf', '1907.04002v1.pdf', '1802.02193v1.pdf']",2011.07119v1-Figure4-1.png,Fig. 4. Fixed point residual {‖xk − xk−1‖}k∈N for different graph topologies.,"In Figure 4 we report the fixed point residual (defined as {‖xk − xk−1‖}k∈N) for different graph topologies. We remark that the random graph has ∼ 225 edges and thus is the more connected of the four topologies, which explains the fact that it achieves the better results.",tvopt: A Python Framework for Time-Varying Optimization,"This paper introduces tvopt, a Python framework for prototyping and
benchmarking time-varying (or online) optimization algorithms. The paper first
describes the theoretical approach that informed the development of tvopt. Then
it discusses the different components of the framework and their use for
modeling and solving time-varying optimization problems. In particular, tvopt
provides functionalities for defining both centralized and distributed online
problems, and a collection of built-in algorithms to solve them, for example
gradient-based methods, ADMM and other splitting methods. Moreover, the
framework implements prediction strategies to improve the accuracy of the
online solvers. The paper then proposes some numerical results on a benchmark
problem and discusses their implementation using tvopt. The code for tvopt is
available at https://github.com/nicola-bastianello/tvopt."
scgqa_362,1409.2897v1,"According to Figure 4, what factor most influences user adaptation in the handwriting recognition study?",The graph shows that the major contribution of user adaptation comes from the fact that the users write faster in the last 5 sessions compared to the first 5 sessions. This is likely because the users have become more familiar with the system and are therefore able to write more quickly.,1409.2897v1.pdf,"['1409.2897v1.pdf', '1909.03961v2.pdf', '1704.03458v1.pdf', '2005.11699v2.pdf']",1409.2897v1-Figure4-1.png,Figure 4: The average writing time per session and the average mutual information per session under the condition Rfixed.,"Furthermore, Figure 4a and Figure 4b reveal that the major contribution of user adaptation comes from the fact that the users write faster in the last 5 sessions compared to the first 5 sessions (p < 0.0001), and not because of the system received more information from the user (p = 0.9723). This result is as expected according to the law of practice [12].",Co-adaptation in a Handwriting Recognition System,"Handwriting is a natural and versatile method for human-computer interaction,
especially on small mobile devices such as smart phones. However, as
handwriting varies significantly from person to person, it is difficult to
design handwriting recognizers that perform well for all users. A natural
solution is to use machine learning to adapt the recognizer to the user. One
complicating factor is that, as the computer adapts to the user, the user also
adapts to the computer and probably changes their handwriting. This paper
investigates the dynamics of co-adaptation, a process in which both the
computer and the user are adapting their behaviors in order to improve the
speed and accuracy of the communication through handwriting. We devised an
information-theoretic framework for quantifying the efficiency of a handwriting
system where the system includes both the user and the computer. Using this
framework, we analyzed data collected from an adaptive handwriting recognition
system and characterized the impact of machine adaptation and of human
adaptation. We found that both machine adaptation and human adaptation have
significant impact on the input rate and must be considered together in order
to improve the efficiency of the system as a whole."
scgqa_363,2002.06090v1,"According to the results presented in Figure 3, what trend does cache capability exhibit on throughput in the paper?","The graph shows that the average system throughput increases with increasing cache capability. This is because a larger cache allows more data to be stored locally, which reduces the amount of data that needs to be transmitted over the network. This results in lower latency and higher throughput.",2002.06090v1.pdf,"['2002.06090v1.pdf', '1811.00416v5.pdf', '2011.08042v1.pdf', '1912.00088v1.pdf', '1804.04290v1.pdf', '2007.11391v1.pdf', '2008.06431v1.pdf', '2007.15176v2.pdf']",2002.06090v1-Figure3-1.png,"Figure 3: Impact of cache capability capability. 2.5 3 3.5 4 Computation Capability, g(GHz)",Fig. 3 and Fig. 4 illustrate the impact of cache size and computing capability on the average,"Mobile Communications, Computing and Caching Resources Optimization for
  Coded Caching with Device Computing","Edge caching and computing have been regarded as an efficient approach to
tackle the wireless spectrum crunch problem. In this paper, we design a general
coded caching with device computing strategy for content computation, e.g.,
virtual reality (VR) rendering, to minimize the average transmission bandwidth
with the caching capacity and the energy constraints of each mobile device, and
the maximum tolerable delay constraint of each task. The key enabler is that
because both coded data and stored data can be the data before or after
computing, the proposed scheme has numerous edge computing and caching paths
corresponding to different bandwidth requirement. We thus formulate a joint
coded caching and computing optimization problem to decide whether the mobile
devices cache the input data or the output data, which tasks to be coded cached
and which tasks to compute locally. The optimization problem is shown to be 0-1
nonconvex nonsmooth programming and can be decomposed into the computation
programming and the coded caching programming. We prove the convergence of the
computation programming problem by utilizing the alternating direction method
of multipliers (ADMM), and a stationary point can be obtained. For the coded
cache programming, we design a low complexity algorithm to obtain an acceptable
solution. Numerical results demonstrate that the proposed scheme provides a
significant bandwidth saving by taking full advantage of the caching and
computing capability of mobile devices."
scgqa_364,1804.06161v2,What does the analysis of boost pressure and EGR trajectories over WLTP-medium cycle reveal about baseline calibration factors?,"The graph shows the boost pressure and EGR rate trajectories over the WLTP-medium cycle for the baseline calibration parameters. This is important because it allows us to see how the engine responds to different inputs, and to identify any areas where the performance could be improved.",1804.06161v2.pdf,"['1804.06161v2.pdf', '2004.01867v1.pdf', '1804.03842v1.pdf', '1607.08438v1.pdf', '1703.01827v3.pdf', '1805.00184v1.pdf', '2011.09375v1.pdf']",1804.06161v2-Figure14-1.png,Fig. 14. Boost pressure and EGR rate trajectories over WLTP-medium cycle for baseline calibration parameters.,"2) Calibration over WLTP-Medium Phase: The proposed diesel airpath control framework is tested when the engine is running over the medium phase of WLTP. The closedloop response obtained with the baseline parameters (τgboost = 0.5, τ g EGR = 0.5, w g = 0.5, gboost = 0 and g EGR = 0) is shown in Fig. 14. Five regions are identified to improve the transient response in one or both output channels. Oscillations are observed in both output channels in region Aw while overshoots and an undershooting behaviour in the EGR rate channel are observed in regions Bw, Cw, Ew and Dw,","Fast Calibration of a Robust Model Predictive Controller for Diesel
  Engine Airpath","A significant challenge in the development of control systems for diesel
airpath applications is to tune the controller parameters to achieve
satisfactory output performance, especially whilst adhering to input and safety
constraints in the presence of unknown system disturbances. Model-based control
techniques, such as model predictive control (MPC), have been successfully
applied to multivariable and highly nonlinear systems, such as diesel engines,
while considering operational constraints. However, efficient calibration of
typical implementations of MPC is hindered by the high number of tuning
parameters and their non-intuitive correlation with the output response. In
this paper, the number of effective tuning parameters is reduced through
suitable structural modifications to the controller formulation and an
appropriate redesign of the MPC cost function to aid rapid calibration.
Furthermore, a constraint tightening-like approach is augmented to the control
architecture to provide robustness guarantees in the face of uncertainties. A
switched linear time-varying MPC strategy with recursive feasibility guarantees
during controller switching is proposed to handle transient operation of the
engine. The robust controller is first implemented on a high fidelity
simulation environment, with a comprehensive investigation of its calibration
to achieve desired transient response under step changes in the fuelling rate.
An experimental study then validates and highlights the performance of the
proposed controller architecture for selected tunings of the calibration
parameters for fuelling steps and over drive cycles."
scgqa_365,2002.06199v1,"Referring to Figure 5 in the AER object classification study, what temporal aspect does the x-axis indicate?","The x-axis of the graph represents the time (in milliseconds) it takes to perform inference with incomplete information. The y-axis represents the accuracy of the inference, which is the percentage of correctly classified images.",2002.06199v1.pdf,"['2002.06199v1.pdf', '1511.04338v2.pdf', '1106.3826v2.pdf', '1403.5617v1.pdf', '1706.03019v1.pdf', '1806.05387v1.pdf', '1910.09592v1.pdf']",2002.06199v1-Figure5-1.png,Figure 5: Performance of the inference with incomplete information on MNIST-DVS dataset.,nontemporal classifier SVM (with the same feature extraction procedure). The results are averaged over 10 runs and shown in Figure 5.,"Effective AER Object Classification Using Segmented
  Probability-Maximization Learning in Spiking Neural Networks","Address event representation (AER) cameras have recently attracted more
attention due to the advantages of high temporal resolution and low power
consumption, compared with traditional frame-based cameras. Since AER cameras
record the visual input as asynchronous discrete events, they are inherently
suitable to coordinate with the spiking neural network (SNN), which is
biologically plausible and energy-efficient on neuromorphic hardware. However,
using SNN to perform the AER object classification is still challenging, due to
the lack of effective learning algorithms for this new representation. To
tackle this issue, we propose an AER object classification model using a novel
segmented probability-maximization (SPA) learning algorithm. Technically, 1)
the SPA learning algorithm iteratively maximizes the probability of the classes
that samples belong to, in order to improve the reliability of neuron responses
and effectiveness of learning; 2) a peak detection (PD) mechanism is introduced
in SPA to locate informative time points segment by segment, based on which
information within the whole event stream can be fully utilized by the
learning. Extensive experimental results show that, compared to
state-of-the-art methods, not only our model is more effective, but also it
requires less information to reach a certain level of accuracy."
scgqa_366,1811.01194v1,What does Fig. 5 imply about the performance of audiovisual networks for noisy speech recognition?,"The results in the graph suggest that audiovisual architectures are a promising approach for developing more robust and accurate speech recognition systems. By combining information from both the visual and audio modalities, audiovisual architectures are able to better handle noisy speech and achieve higher MCRs than audio-only and visual-only architectures.",1811.01194v1.pdf,"['1811.01194v1.pdf', '1911.11395v2.pdf', '1703.01827v3.pdf', '2005.09634v1.pdf', '1707.04849v1.pdf', '1706.01341v1.pdf', '1410.7867v1.pdf', '1405.7705v1.pdf']",1811.01194v1-Figure5-1.png,Fig. 5. MCR for audio and three audiovisual networks using DEMAND noise database (25dB correspond to speech without additive noise).,"In Section 7.2 we reported results averaged over 7 different noise levels and 6 different categories. We provide here a more detailed analysis, by breaking down the error rates by noise category and noise level. In Fig. 5 we present MCR attained by 5 architectures, where each MCR is derived as the average over all noise categories (i.e. 6 × 25000 = 150000 videos). Although the relative improvement attained by audiovisual architectures is larger under extreme noisy conditions (-10dB to -5dB), notable improvements are attained even in higher SNRs (0dB to 20dB) as well as on speech without additive noise. Moreover, by comparing the MCR of our best visual-only network (V15) with our best audio-only network (A4) we conclude that the performance of the former is equivalent to that of the latter with SNR ≈ −4.5 additive noise.","Pushing the boundaries of audiovisual word recognition using Residual
  Networks and LSTMs","Visual and audiovisual speech recognition are witnessing a renaissance which
is largely due to the advent of deep learning methods. In this paper, we
present a deep learning architecture for lipreading and audiovisual word
recognition, which combines Residual Networks equipped with spatiotemporal
input layers and Bidirectional LSTMs. The lipreading architecture attains
11.92% misclassification rate on the challenging Lipreading-In-The-Wild
database, which is composed of excerpts from BBC-TV, each containing one of the
500 target words. Audiovisual experiments are performed using both intermediate
and late integration, as well as several types and levels of environmental
noise, and notable improvements over the audio-only network are reported, even
in the case of clean speech. A further analysis on the utility of target word
boundaries is provided, as well as on the capacity of the network in modeling
the linguistic context of the target word. Finally, we examine difficult word
pairs and discuss how visual information helps towards attaining higher
recognition accuracy."
scgqa_367,1905.05538v1,"In the research paper 'Assessing the Difficulty of Classifying ConceptNet Relations', what does Figure 4 illustrate about concept characteristics?","The graph shows that there is a positive correlation between concept characteristics and model performance. This means that as the concept characteristics increase, the model performance also increases. This is likely because the concept characteristics provide more information about the relationship between the two concepts, which helps the model to learn the correct label.",1905.05538v1.pdf,"['1905.05538v1.pdf', '1706.03112v1.pdf', '1703.07626v1.pdf', '1809.09034v1.pdf', '1703.07020v4.pdf', '1804.06674v1.pdf', '1701.08947v1.pdf', '2009.07756v1.pdf']",1905.05538v1-Figure4-1.png,"Figure 4: Interrelations between concept characteristics and model performance, based on FFNN+RNN+individually tuned thresholds, OW-1, scaled to range 0 to 15.","In this section we will discuss the hypotheses derived from our analysis of CONCEPTNET properties (§3.3), and based on that, to determine which approaches and representations are best suited for CONCEPTNET-based commonsense relation classification. To aid the discussion we produced Figures 2, 3, 4, and Table 5. Fig. 2 plots differences in performance for each relation for the setting we wish to compare: concept encoding using centroids (FFNN) vs. RNNs (FFNN+RNN) (blue), global vs. relation-specific prediction threshold (orange), and OW-1 vs. CW setting (grey). Fig. 3 visualizes ambiguous – that means co-occurring – relations in our dataset in a symmetric heatmap. Fig. 4 displays interrelations between concept characteristics and model performance, based on our best performing system (FFNN+RNN+ind. tuned thresholds, OW-1). To observe correlations between clas-","Assessing the Difficulty of Classifying ConceptNet Relations in a
  Multi-Label Classification Setting","Commonsense knowledge relations are crucial for advanced NLU tasks. We
examine the learnability of such relations as represented in CONCEPTNET, taking
into account their specific properties, which can make relation classification
difficult: a given concept pair can be linked by multiple relation types, and
relations can have multi-word arguments of diverse semantic types. We explore a
neural open world multi-label classification approach that focuses on the
evaluation of classification accuracy for individual relations. Based on an
in-depth study of the specific properties of the CONCEPTNET resource, we
investigate the impact of different relation representations and model
variations. Our analysis reveals that the complexity of argument types and
relation ambiguity are the most important challenges to address. We design a
customized evaluation method to address the incompleteness of the resource that
can be expanded in future work."
scgqa_368,1303.1635v1,"According to the findings in your research, how does the packet delivery ratio change for AOMDV and IZM-DSR with pause time?","The graph shows that the packet delivery ratio of ZD-AOMDV is the highest, followed by AOMDV, AOMDV-IZM and IZM-DSR. This is because ZD-AOMDV uses a more efficient routing algorithm that takes into account the pause time of nodes. As the pause time increases, the packet delivery ratio of all protocols decreases, but the decrease is more pronounced for IZM-DSR. This is because IZM-DSR uses a less efficient routing algorithm that does not take into account the pause time of nodes.",1303.1635v1.pdf,"['1303.1635v1.pdf', '2003.06259v1.pdf', '1909.01868v3.pdf', '2008.02777v1.pdf']",1303.1635v1-Figure10-1.png,"Figure 10. Packet delivery ratio vs. Pause time (Max speed=40Km/h, No of Src=1, Traffic=35Kbps)","Figure 9 shows packet delivery ratio versus max speed of nodes in random way point model, figure 10 shows packet delivery ratio versus pause time of nodes in random way point model, figure 11 shows packet delivery ratio versus number of data sources (number of connections) and figure 12 shows packet delivery ratio versus offered traffic.",Improving Energy Efficiency in MANETs by Multi-Path Routing,"Some multi-path routing algorithm in MANET, simultaneously send information
to the destination through several directions to reduce end-to-end delay. In
all these algorithms, the sent traffic through a path affects the adjacent path
and unintentionally increases the delay due to the use of adjacent paths.
Because, there are repetitive competitions among neighboring nodes, in order to
obtain the joint channel in adjacent paths. The represented algorithm in this
study tries to discover the distinct paths between source and destination nodes
with using Omni directional antennas, to send information through these
simultaneously. For this purpose, the number of active neighbors is counted in
each direction with using a strategy. These criterions are effectively used to
select routes. Proposed algorithm is based on AODV routing algorithm, and in
the end it is compared with AOMDV, AODVM, and IZM-DSR algorithms which are
multi-path routing algorithms based on AODV and DSR. Simulation results show
that using the proposed algorithm creates a significant improvement in energy
efficiency and reducing end-to-end delay."
scgqa_369,1407.5358v1,"In the context of the helicopter hovering task, what do the results in Figure 10 indicate about SARSA and KBSF?","The graph shows that both SARSA and KBSF are able to learn to hover the helicopter stably, with SARSA achieving slightly better performance. The performance of both agents stabilizes after around 70000 episodes, probably because at this point there is almost no exploration taking place anymore.",1407.5358v1.pdf,"['1407.5358v1.pdf', '1910.10700v1.pdf', '1809.08207v1.pdf']",1407.5358v1-Figure10-1.png,Figure 10: Results on the helicopter hovering task averaged over 50 runs. The learned controllers were tested from a fixed state (see text for details). The shadowed regions represent 99% confidence intervals.,"Figure 10 shows the results obtained by SARSA and KBSF on the helicopter hovering task. Note in Figure 10a how the average episode length increases abruptly at the points in which the value of is decreased. This is true for both SARSA and KBSF. Also, since the number of steps executed per episode increases over time, the interval in between such abrupt changes decreases in length, as expected. Finally, observe how the performance of both agents stabilizes after around 70000 episodes, probably because at this point there is almost no exploration taking place anymore.",Practical Kernel-Based Reinforcement Learning,"Kernel-based reinforcement learning (KBRL) stands out among reinforcement
learning algorithms for its strong theoretical guarantees. By casting the
learning problem as a local kernel approximation, KBRL provides a way of
computing a decision policy which is statistically consistent and converges to
a unique solution. Unfortunately, the model constructed by KBRL grows with the
number of sample transitions, resulting in a computational cost that precludes
its application to large-scale or on-line domains. In this paper we introduce
an algorithm that turns KBRL into a practical reinforcement learning tool.
Kernel-based stochastic factorization (KBSF) builds on a simple idea: when a
transition matrix is represented as the product of two stochastic matrices, one
can swap the factors of the multiplication to obtain another transition matrix,
potentially much smaller, which retains some fundamental properties of its
precursor. KBSF exploits such an insight to compress the information contained
in KBRL's model into an approximator of fixed size. This makes it possible to
build an approximation that takes into account both the difficulty of the
problem and the associated computational cost. KBSF's computational complexity
is linear in the number of sample transitions, which is the best one can do
without discarding data. Moreover, the algorithm's simple mechanics allow for a
fully incremental implementation that makes the amount of memory used
independent of the number of sample transitions. The result is a kernel-based
reinforcement learning algorithm that can be applied to large-scale problems in
both off-line and on-line regimes. We derive upper bounds for the distance
between the value functions computed by KBRL and KBSF using the same data. We
also illustrate the potential of our algorithm in an extensive empirical study
in which KBSF is applied to difficult tasks based on real-world data."
scgqa_370,1703.10422v2,"According to the experiments highlighted in the paper, what role do receive antennas play in MRC-ZF design?","The results in this figure suggest that the number of receive antennas is an important factor in the design of multi-user MIMO systems. In order to achieve the best performance, it is important to have a large number of receive antennas. Additionally, the use of MRC-ZF receivers can help to improve performance, even with imperfect CSI.",1703.10422v2.pdf,"['1703.10422v2.pdf', '1206.5265v1.pdf', '1708.09328v1.pdf', '1611.04706v2.pdf', '1808.06304v2.pdf']",1703.10422v2-Figure3-1.png,"Fig. 3: Performance of the MRC and MRC-ZF receivers with respect to the number of receive antennas, for 5 users each of them using 20 dB transmit power","In this section, simulation results are presented to verify our theoretical analysis. In the simulations, time delays follow the distribution mentioned in Eq. (3), noise samples and fading coefficients are also distributed as CN(0, 1). The large-scale channel fading is modeled as βk = zk/(rk/rh) v, where zk is a log-normal random variable with standard deviation of σ, v is the path-loss exponent, and rk is the distance between the kth user and the BS which varies in the interval of [rh, R]. We have assumed that v = 1.8, σ = 8, rh = 100 and R = 1000. The large scale fading is assumed to be fixed and 10, 000 different realizations of time delays, fading coefficients and noise samples are simulated. In Fig. 3, the performance of the MRC and MRC-ZF receivers with perfect CSI and imperfect CSI is presented by theoretical approximation in Theorems 1, 2, 3 and 4 and via simulation. The sum rate for 5 users are plotted with respect to the number of receive antennas. The results include rectangular (Rect.) pulse shape and raised cosine (R.C.) pulse shape with roll-off factor of β = 0.5 truncated at 3 side lobes. Our theoretical approximation and simulation results match. It also shows that, unknown time delays limit the performance of MRC receivers, and by increasing M , the sum rate is saturated. However, the performance of the MRC-ZF receiver is not saturated and by increasing M , the sum rate increases.","On the Performance of MRC Receiver with Unknown Timing Mismatch-A Large
  Scale Analysis","There has been extensive research on large scale multi-user multiple-input
multiple-output (MU-MIMO) systems recently. Researchers have shown that there
are great opportunities in this area, however, there are many obstacles in the
way to achieve full potential of using large number of receive antennas. One of
the main issues, which will be investigated thoroughly in this paper, is timing
asynchrony among signals of different users. Most of the works in the
literature, assume that received signals are perfectly aligned which is not
practical. We show that, neglecting the asynchrony can significantly degrade
the performance of existing designs, particularly maximum ratio combining
(MRC). We quantify the uplink achievable rates obtained by MRC receiver with
perfect channel state information (CSI) and imperfect CSI while the system is
impaired by unknown time delays among received signals. We then use these
results to design new algorithms in order to alleviate the effects of timing
mismatch. We also analyze the performance of introduced receiver design, which
is called MRC-ZF, with perfect and imperfect CSI. For performing MRC-ZF, the
only required information is the distribution of timing mismatch which
circumvents the necessity of time delay acquisition or synchronization. To
verify our analytical results, we present extensive simulation results which
thoroughly investigate the performance of the traditional MRC receiver and the
introduced MRC-ZF receiver."
scgqa_371,2004.01867v1,How do the follower trajectories in Fig. 13.1 demonstrate bounded tracking from the leader's dynamics?,"The key features of the graph that support the conclusion that bipartite bounded tracking is achieved are the following:

* The leader's position and velocity trajectories are smooth and converge to the desired values.
* The followers' position and velocity trajectories are also smooth and converge to the desired values.
* The followers' trajectories are bounded, which means that they do not diverge from the desired values.

These features indicate that the agents are able to communicate with each other and share information about their states, which allows them to adjust their own actions in order to track the leader. This is a key feature of bipartite bounded tracking, and it is evident from the graph that the agents are able to achieve this.",2004.01867v1.pdf,"['2004.01867v1.pdf', '1910.04573v3.pdf', '1806.05387v1.pdf', '1603.04812v2.pdf']",2004.01867v1-Figure13.1-1.png,Fig. 13.1. State trajectories of all agents in Example 13.3.,"Example 13.3. Consider second-order MASs with a leader of unmeasurable velocity and acceleration. The agents interact with each other through the signed digraph G̃2 in Fig. 5.1. To satisfy condition (13.6), we select τ = 0.1, φ1 = 3 and φ2 = 1. It is assumed that the leader moves with a time-varying acceleration a(k) = 1 + 0.5 sin(0.4k), and each follower has an approximate estimate a0(k) = 1 for the leader’s acceleration. Finally, we can observe from the position and velocity trajectories in Fig. 13.1 that the bipartite bounded tracking is achieved.","Sub/super-stochastic matrix with applications to bipartite tracking
  control over signed networks","In this contribution, the properties of sub-stochastic matrix and
super-stochastic matrix are applied to analyze the bipartite tracking issues of
multi-agent systems (MASs) over signed networks, in which the edges with
positive weight and negative weight are used to describe the cooperation and
competition among the agents, respectively. For the sake of integrity of the
study, the overall content is divided into two parts. In the first part, we
examine the dynamics of bipartite tracking for first-order MASs, second-order
MASs and general linear MASs in the presence of asynchronous interactions,
respectively. Asynchronous interactions mean that each agent only interacts
with its neighbors at the instants when it wants to update the state rather
than keeping compulsory consistent with other agents. In the second part, we
investigate the problems of bipartite tracing in different practical scenarios,
such as time delays, switching topologies, random networks, lossy links, matrix
disturbance, external noise disturbance, and a leader of unmeasurable velocity
and acceleration. The bipartite tracking problems of MASs under these different
scenario settings can be equivalently converted into the product convergence
problems of infinite sub-stochastic matrices (ISubSM) or infinite
super-stochastic matrices (ISupSM). With the help of nonnegative matrix theory
together with some key results related to the compositions of directed edge
sets, we establish systematic algebraic-graphical methods of dealing with the
product convergence of ISubSM and ISupSM. Finally, the efficiency of the
proposed methods is verified by computer simulations."
scgqa_372,1811.00416v5,How do the actual and hypothetical importance scores differ in relation to the CTCF task in the paper's Figure 2?,"The graph shows the importance of each base in the sequence for the CTCF task. The actual importance is the importance of the base in the original sequence, while the hypothetical importance is the importance of the base if it were present in the sequence. The hypothetical importance reveals the impact of other bases not present in the original sequence.",1811.00416v5.pdf,"['1811.00416v5.pdf', '1501.01582v1.pdf', '1612.01450v1.pdf', '1710.09234v1.pdf', '1708.07888v3.pdf', '1610.01283v4.pdf', '1910.03072v1.pdf', '2010.08182v3.pdf', '1706.03112v1.pdf']",1811.00416v5-Figure2-1.png,Figure 2: Actual and hypothetical importance scores for CTCF task on an example sequence containing the CTCF motif. The hypothetical importance reveals the impact of other bases not present in the original sequence.,"TF-MoDISco takes as its input per-base importance scores for every prediction task. These importance scores can be derived through a variety of methods including the ones described in [8]. A positive importance value for a particular task indicates that the base influenced the output of the network towards a positive prediction for the task, and a negative importance indicates that the base influences the output of the network towards a negative prediction. Scores that are near zero indicate that the particular base is unimportant for the task in question. We found that TF-MoDISco results were better if, in addition to using importance scores on the input sequence, we incorporated information about hypothetical importance if other unobserved bases were present. A hypothetical importance score answers the question “if I were to mutate the sequence at a particular position to a different letter, what would the importance on the newly-introduced letter look like?”. As a specific example, consider a basic importance-scoring method such as gradient × input. When a sequence is one-hot encoded (i.e. ‘input’ can be either 1 or 0), the value of gradient × input would be zero on all bases that are absent from the sequence and equal to the gradient on the bases that are present (here, ‘base’ refers to a specific letter at a specific position; at every position, only one of ACGT can be present). If a single position were mutated to a different base, one might anticipate that the value of gradient × input at the newly-introduced base would be close to the current value of the gradient (assuming that the gradient doesn’t change dramatically as a result of the mutation). Thus, the gradients give a readout of what the contributions to the network would be if different bases were present at particular positions; if (gradient × input) is used as the importance scoring method, then the gradients alone would be a good choice for the “hypothetical importance”. In practice, the “hypothetical importance” behaves like an autocomplete of the sequence, giving insight into what patterns the network was looking for at a given region (Fig. 2). What is a good choice of hypothetical importance when DeepLIFT scores are used? DeepLIFT defines quantities known as the multipliers m∆x∆t such that:","Technical Note on Transcription Factor Motif Discovery from Importance
  Scores (TF-MoDISco) version 0.5.6.5","TF-MoDISco (Transcription Factor Motif Discovery from Importance Scores) is
an algorithm for identifying motifs from basepair-level importance scores
computed on genomic sequence data. This technical note focuses on version
v0.5.6.5. The implementation is available at
https://github.com/kundajelab/tfmodisco/tree/v0.5.6.5"
scgqa_373,1712.03538v1,"In the context of the study, what does Figure 5's precision-recall curve indicate about the balance of precision and recall?","The precision-recall curves show how well a model can predict a particular condition, given a certain level of recall. In other words, the curves show the trade-off between precision and recall. Precision is the proportion of true positives to all predicted positives, while recall is the proportion of true positives to all actual positives. A model with a high precision-recall curve will have a high level of both precision and recall, while a model with a low precision-recall curve will have a low level of both precision and recall.",1712.03538v1.pdf,"['1712.03538v1.pdf', '1402.7063v1.pdf', '1207.3107v3.pdf', '1907.11314v1.pdf', '1603.01185v2.pdf', '1203.1203v2.pdf', '1504.07495v1.pdf']",1712.03538v1-Figure5-1.png,Figure 5: Precision-recall curves for predicting each condition.,"Figure 2 shows the AUC-score of each model for each task separately, and Figure 3 the true positive rate at a low false positive rate of 0.1. Precisionrecall curves for model/task are in Figure 5. STL is a multilayer perceptron with two hidden layers (with a similar number of parameters as the proposed MTL model). The MTL +gender and MTL models predict all tasks simultaneously, but are only evaluated on the main respective task.",Multi-Task Learning for Mental Health using Social Media Text,"We introduce initial groundwork for estimating suicide risk and mental health
in a deep learning framework. By modeling multiple conditions, the system
learns to make predictions about suicide risk and mental health at a low false
positive rate. Conditions are modeled as tasks in a multi-task learning (MTL)
framework, with gender prediction as an additional auxiliary task. We
demonstrate the effectiveness of multi-task learning by comparison to a
well-tuned single-task baseline with the same number of parameters. Our best
MTL model predicts potential suicide attempt, as well as the presence of
atypical mental health, with AUC > 0.8. We also find additional large
improvements using multi-task learning on mental health tasks with limited
training data."
scgqa_374,2009.06124v1,What relationship does the graph in UltraFuzz establish between computation resources and the path coverage of the fuzzing tools?,"The graph shows that the path coverage reached by each tool increases as the computation resources increase. For example, for the baseline AFLsingle-core, it found 2,538, 2,786, 3,270, 4,173, 6,257 and 8,044 paths in 4, 8, 16, 32, 64 and 128 units of computation resources in freetype. This suggests that fuzzing tools are more effective when they have more computational resources available.",2009.06124v1.pdf,"['2009.06124v1.pdf', '1603.08981v2.pdf', '2005.13300v1.pdf', '1707.02439v2.pdf', '1608.06005v1.pdf', '1708.01249v1.pdf', '1808.06304v2.pdf']",2009.06124v1-Figure3-1.png,Fig. 3. Comparison of path coverage reached by different tools with the same computation resources.,"We use path coverage as the main criterion to measure the performance of these fuzzing tools. Fig. 3 plots the average number of paths discovered by these tools throughout 3 runs in different computation resources. From Fig. 3, the path coverage reached by each tool is rising as the increase of computation resources. For example, for the baseline AFLsingle-core, it found 2,538, 2,786, 3,270, 4,173, 6,257 and 8,044 paths in 4, 8, 16, 32, 64 and 128 units of computation resources in freetype.",UltraFuzz: Towards Resource-saving in Distributed Fuzzing,"Recent research has sought to improve fuzzing performance via parallel
computing. However, researchers focus on improving efficiency while ignoring
the increasing cost of testing resources. Parallel fuzzing in the distributed
environment amplifies the resource-wasting problem caused by the random nature
of fuzzing. In the parallel mode, owing to the lack of an appropriate task
dispatching scheme and timely fuzzing status synchronization among different
fuzzing instances, task conflicts and workload imbalance occur, making the
resource-wasting problem severe. In this paper, we design UltraFuzz, a fuzzer
for resource-saving in distributed fuzzing. Based on centralized dynamic
scheduling, UltraFuzz can dispatch tasks and schedule power globally and
reasonably to avoid resource-wasting. Besides, UltraFuzz can elastically
allocate computing power for fuzzing and seed evaluation, thereby avoiding the
potential bottleneck of seed evaluation that blocks the fuzzing process.
UltraFuzz was evaluated using real-world programs, and the results show that
with the same testing resource, UltraFuzz outperforms state-of-the-art tools,
such as AFL, AFL-P, PAFL, and EnFuzz. Most importantly, the experiment reveals
certain results that seem counter-intuitive, namely that parallel fuzzing can
achieve ``super-linear acceleration'' when compared with single-core fuzzing.
We conduct additional experiments to reveal the deep reasons behind this
phenomenon and dig deep into the inherent advantages of parallel fuzzing over
serial fuzzing, including the global optimization of seed energy scheduling and
the escape of local optimal seed. Additionally, 24 real-world vulnerabilities
were discovered using UltraFuzz."
scgqa_375,2002.10790v1,What insights does Figure 1 provide about BSGD's effectiveness for invariant logistic regression across various noise levels and minibatch sizes?,"The graph shows that BSGD performs well for the invariant logistic regression problem under different inner minibatch sizes and different noise levels. When the noise level is low, a small inner batch size can be used to achieve good performance. However, as the noise level increases, a larger inner batch size is needed to control the bias incurred by the biased gradient estimator of BSGD. This is consistent with the theoretical findings of the delicate trade-off between the inner batch size and the number of iterations.",2002.10790v1.pdf,"['2002.10790v1.pdf', '1504.01124v3.pdf', '1804.06161v2.pdf', '1903.10464v3.pdf', '1404.7045v3.pdf', '1807.06736v1.pdf', '1902.05312v2.pdf']",2002.10790v1-Figure1-1.png,"Figure 1: Performance of BSGD for the invariant Logistic regression problem under different inner minibatch sizes and different noise levels: (a) σ22 = 1, (b) σ 2 2 = 10, (c) σ 2 2 = 100.","For a given total number of samples Q = 106, the performance of BSGD with different inner batch sizes and under different perturbation levels is summarized in Figure 1. When increasing σ22 from 1 to 100, larger inner batch sizes are needed to control the bias incurred by the biased gradient estimator of BSGD. This numerical observation supports our theoretical findings of the delicate trade-off between the inner batch size and the number of iterations. We also compare the performance achieved by BSGD and SAA, in terms of F (x) − F ∗1, by selecting the best inner batch sizes for a given σ22 ∈ {1, 10, 100}. The results are summarized in Table 2. A detailed result can be found in Table 5 in Appendix C.1. We observe that given the same budget of samples, BSGD outperforms SAA and requires a much smaller inner batch size in practice.","Biased Stochastic Gradient Descent for Conditional Stochastic
  Optimization","Conditional Stochastic Optimization (CSO) covers a variety of applications
ranging from meta-learning and causal inference to invariant learning. However,
constructing unbiased gradient estimates in CSO is challenging due to the
composition structure. As an alternative, we propose a biased stochastic
gradient descent (BSGD) algorithm and study the bias-variance tradeoff under
different structural assumptions. We establish the sample complexities of BSGD
for strongly convex, convex, and weakly convex objectives, under smooth and
non-smooth conditions. We also provide matching lower bounds of BSGD for convex
CSO objectives. Extensive numerical experiments are conducted to illustrate the
performance of BSGD on robust logistic regression, model-agnostic meta-learning
(MAML), and instrumental variable regression (IV)."
scgqa_376,1410.7867v1,"According to Fig. 6 in the research paper, how does the learning algorithm's convergence manifest over iterations?","The graph shows that the online per-queue post-decision value functions learning algorithm converges to the optimal value as the number of iterations increases. This is evident from the fact that the post-decision value functions of the traffic queue maintained for UE 1 approach a constant value as the iteration step increases. This convergence property is important for ensuring that the learning algorithm is able to find the optimal value for the post-decision value functions, which is necessary for achieving optimal performance.",1410.7867v1.pdf,"['1410.7867v1.pdf', '1509.02054v1.pdf', '1808.10082v4.pdf', '1909.03961v2.pdf']",1410.7867v1-Figure6-1.png,Fig. 6. Per-queue Post-decision Value Functions,"Fig. 6 shows the convergence property of the online perqueue post-decision value functions(w.r.t Rn, size of which is equal to mean packet size N̄i) learning algorithm. For viewing convenience, the post-decision value functions of the traffic queue maintained for UE 1 is plotted with the increasing of iteration step. It is significant that the learning converges","Resource Allocation Optimization for Delay-Sensitive Traffic in
  Fronthaul Constrained Cloud Radio Access Networks","The cloud radio access network (C-RAN) provides high spectral and energy
efficiency performances, low expenditures and intelligent centralized system
structures to operators, which has attracted intense interests in both academia
and industry. In this paper, a hybrid coordinated multi-point transmission
(H-CoMP) scheme is designed for the downlink transmission in C-RANs, which
fulfills the flexible tradeoff between cooperation gain and fronthaul
consumption. The queue-aware power and rate allocation with constraints of
average fronthaul consumption for the delay-sensitive traffic are formulated as
an infinite horizon constrained partially observed Markov decision process
(POMDP), which takes both the urgent queue state information (QSI) and the
imperfect channel state information at transmitters (CSIT) into account. To
deal with the curse of dimensionality involved with the equivalent Bellman
equation, the linear approximation of post-decision value functions is
utilized. A stochastic gradient algorithm is presented to allocate the
queue-aware power and transmission rate with H-CoMP, which is robust against
unpredicted traffic arrivals and uncertainties caused by the imperfect CSIT.
Furthermore, to substantially reduce the computing complexity, an online
learning algorithm is proposed to estimate the per-queue post-decision value
functions and update the Lagrange multipliers. The simulation results
demonstrate performance gains of the proposed stochastic gradient algorithms,
and confirm the asymptotical convergence of the proposed online learning
algorithm."
scgqa_377,1407.6074v1,What does Fig 1 reveal about the consistency of cattle interactions in the study's high-resolution dataset?,"The graph shows that there is neither temporal stationarity nor spatial homogeneity in this high-resolution cattle social network (number of contacts). This means that the network density changes significantly within a day, and that there are no consistent patterns of contact between cattle across the study area.",1407.6074v1.pdf,"['1407.6074v1.pdf', '1911.09804v2.pdf', '1908.05243v1.pdf', '1505.05173v6.pdf']",1407.6074v1-Figure1-1.png,"Fig 1. Time Series of Indirect Contact (Grain, Water, Hay), Periodogram, and Coherency Plot","The time series of cattle social network density (proportional to number of contacts) for the complete observation period (192h in total) is plotted against the time series of indirect contact between cattle and grain, water, and hay, respectively (Fig. 1). The time series of direct contact show substantial diurnal cycle, and the time series between direct and indirect contacts (grain and hay) are significantly coupled, which is supported by the coherency plot (since the solid black coherency line is usually above the red significance level). However the time series between direct and indirect contact with water are not significantly correlated (the coherency line hardly exceeds the significance level). These results show that cattle social network density changes significantly within a day, and feeding activity promotes clustering (thus more dense social networks) around grain and hay, while drinking is not a key factor for network changes. These results indicate that there is neither temporal stationarity nor spatial homogeneity in this high-resolution cattle social network (number of contacts).","Spatial-Temporal Dynamics of High-Resolution Animal Social Networks:
  What Can We Learn from Domestic Animals?","Recent studies of animal social networks have significantly increased our
understanding of animal behavior, social interactions, and many important
ecological and epidemiological processes. However, most of the studies are at
low temporal and spatial resolution due to the difficulty in recording accurate
contact information. Domestic animals such as cattle have social behavior and
serve as an excellent study system because their position can be explicitly and
continuously tracked, allowing their social networks to be accurately
constructed. We used radio-frequency tags to accurately track cattle position
and analyze high-resolution cattle social networks. We tested the hypothesis of
temporal stationarity and spatial homogeneity in these high-resolution networks
and demonstrated substantial spatial-temporal heterogeneity during different
daily time periods (feeding and non-feeding) and in different areas of the pen
(grain bunk, water trough, hay bunk, and other general pen area). The social
network structure is analyzed using global network characteristics (network
density, exponential random graph model structure), subgroup clustering
(modularity), triadic property (transitivity), and dyadic interactions
(correlation coefficient from a quadratic assignment procedure). Cattle tend to
have the strongest and most consistent contacts with others around the hay bunk
during the feeding time. These results cannot be determined from data at lower
spatial (aggregated at entire pen level) or temporal (aggregated at daily
level) resolution. These results reveal new insights for real-time animal
social network structure dynamics, providing more accurate descriptions that
allow more accurate modeling of multiple (both direct and indirect) disease
transmission pathways."
scgqa_378,1703.01827v3,How do the second-order moment ratios behave across iterations in the 44-layer network shown in the paper?,The graph shows that the ratio of second-order moment of output errors of each layer to the second-order moment of input errors at each iteration of training a 44-layer network tends to converge to a certain stable evolution pattern. This suggests that there may be a potential evolution pattern in training deep networks.,1703.01827v3.pdf,"['1703.01827v3.pdf', '1804.03842v1.pdf', '1809.01628v1.pdf']",1703.01827v3-Figure4-1.png,"Figure 4. Evolution of backward signal propagation of a 44-layer plain network. X axis denotes layer index and Y axis denotes ratio of second-order moment of current layer to highest layer. We only present 200th, 2000th, 20000th and 30000th iteration, respectively. About after 2000 to 3000 iterations, the ratio trend to converge to a certain of stable evolution pattern shown in the 20000th and 30000th iterations.","In this paper the value of n is somewhat heuristic, which is derived from our observation to the evolution of ratios of second-order moment of output errors of each layer to the second-order moment of input errors at each iteration of training a 44-layer network. Fig. 4 reveals that it probably exists a potential evolution pattern in training deep networks. Actually we just shrink the degradation gap instead of eliminating it in training genuinely deep networks and one of our future work will focus on the methodology of modulation.","All You Need is Beyond a Good Init: Exploring Better Solution for
  Training Extremely Deep Convolutional Neural Networks with Orthonormality and
  Modulation","Deep neural network is difficult to train and this predicament becomes worse
as the depth increases. The essence of this problem exists in the magnitude of
backpropagated errors that will result in gradient vanishing or exploding
phenomenon. We show that a variant of regularizer which utilizes orthonormality
among different filter banks can alleviate this problem. Moreover, we design a
backward error modulation mechanism based on the quasi-isometry assumption
between two consecutive parametric layers. Equipped with these two ingredients,
we propose several novel optimization solutions that can be utilized for
training a specific-structured (repetitively triple modules of Conv-BNReLU)
extremely deep convolutional neural network (CNN) WITHOUT any shortcuts/
identity mappings from scratch. Experiments show that our proposed solutions
can achieve distinct improvements for a 44-layer and a 110-layer plain networks
on both the CIFAR-10 and ImageNet datasets. Moreover, we can successfully train
plain CNNs to match the performance of the residual counterparts.
  Besides, we propose new principles for designing network structure from the
insights evoked by orthonormality. Combined with residual structure, we achieve
comparative performance on the ImageNet dataset."
scgqa_379,2008.07524v3,What does Figure 5 reveal about the efficiency of QVC models versus neural networks in achieving rewards during the CartPole experiment?,The graph shows that the QVC models achieve a better policy and arrive at this policy faster than the neural network models. This is evident from the fact that the QVC models reach a higher average reward at a lower number of iterations.,2008.07524v3.pdf,"['2008.07524v3.pdf', '1905.05538v1.pdf', '1710.09234v1.pdf', '1405.5329v4.pdf']",2008.07524v3-Figure5-1.png,Figure 5: Comparison of NN and QVC DDQN on CartPole,"The first environment is CartPole, which we use to compare QVC based DQN/DDQN with traditional deep neural network DQN/DDQN. The Directional encoding scheme is applied to both the neural network and the QVC. Specifically, this means that just as the qubits are encoded with 0s and 1s, so too does the neural network receive a binary array. All graphs begin at 50 iterations because the average reward is calculated by averaging over the previous 50 iterations. Figure 4 shows a comparison between the traditional neural network DQN and the two types (pure, hybrid) of QVC used. All shaded areas represent the 95% confidence interval over 6 runs. This figure demonstrates that both hybrid and pure QVC models achieve a better policy and arrive at this policy faster than traditional neural networks. Figure 5 demonstrates the same comparison, using the Double DQN algorithm. This experiment demonstrates that the QVC models perform at least as well, if not better, than the neural network based models.",Reinforcement Learning with Quantum Variational Circuits,"The development of quantum computational techniques has advanced greatly in
recent years, parallel to the advancements in techniques for deep reinforcement
learning. This work explores the potential for quantum computing to facilitate
reinforcement learning problems. Quantum computing approaches offer important
potential improvements in time and space complexity over traditional algorithms
because of its ability to exploit the quantum phenomena of superposition and
entanglement. Specifically, we investigate the use of quantum variational
circuits, a form of quantum machine learning. We present our techniques for
encoding classical data for a quantum variational circuit, we further explore
pure and hybrid quantum algorithms for DQN and Double DQN. Our results indicate
both hybrid and pure quantum variational circuit have the ability to solve
reinforcement learning tasks with a smaller parameter space. These comparison
are conducted with two OpenAI Gym environments: CartPole and Blackjack, The
success of this work is indicative of a strong future relationship between
quantum machine learning and deep reinforcement learning."
scgqa_380,1803.10225v1,"What conclusion about latent variable similarity can be drawn from the peak in C(z,r) in this research?","The peak of the cross-correlation C(z,r) is a measure of the similarity between the latent variable z and the reconstruction r. The higher the peak of the cross-correlation C(z,r), the more similar the latent variable z and the reconstruction r are. This means that the generated images are more realistic.",1803.10225v1.pdf,"['1803.10225v1.pdf', '2010.07597v2.pdf', '1810.03742v1.pdf', '1509.08992v2.pdf']",1803.10225v1-Figure3-1.png,"Fig. 3: Evolution of the peak of the cross-correlation C(z, r) over various training epochs.","It would be of interest to examine the evolution of this correlation over the epochs. With this regard, Fig. 3 reports",Light Gated Recurrent Units for Speech Recognition,"A field that has directly benefited from the recent advances in deep learning
is Automatic Speech Recognition (ASR). Despite the great achievements of the
past decades, however, a natural and robust human-machine speech interaction
still appears to be out of reach, especially in challenging environments
characterized by significant noise and reverberation. To improve robustness,
modern speech recognizers often employ acoustic models based on Recurrent
Neural Networks (RNNs), that are naturally able to exploit large time contexts
and long-term speech modulations. It is thus of great interest to continue the
study of proper techniques for improving the effectiveness of RNNs in
processing speech signals.
  In this paper, we revise one of the most popular RNN models, namely Gated
Recurrent Units (GRUs), and propose a simplified architecture that turned out
to be very effective for ASR. The contribution of this work is two-fold: First,
we analyze the role played by the reset gate, showing that a significant
redundancy with the update gate occurs. As a result, we propose to remove the
former from the GRU design, leading to a more efficient and compact single-gate
model. Second, we propose to replace hyperbolic tangent with ReLU activations.
This variation couples well with batch normalization and could help the model
learn long-term dependencies without numerical issues.
  Results show that the proposed architecture, called Light GRU (Li-GRU), not
only reduces the per-epoch training time by more than 30% over a standard GRU,
but also consistently improves the recognition accuracy across different tasks,
input features, noisy conditions, as well as across different ASR paradigms,
ranging from standard DNN-HMM speech recognizers to end-to-end CTC models."
scgqa_381,1202.4232v2,What key difference between designed and actual slopes is illustrated in Figure 13 of the study?,"The graph shows that the actual slopes of y = kp(vr−vo)−iL are greater than the designed slopes. This is because the ESR Rc contributes to the output voltage ripple, which increases the actual slopes.",1202.4232v2.pdf,"['1202.4232v2.pdf', '1405.7705v1.pdf', '1910.03072v1.pdf', '1208.4662v2.pdf', '1505.05173v6.pdf']",1202.4232v2-Figure13-1.png,"Figure 13: Unstable orbit y0(t) (solid line) and h(t) (dashed line), Rc = 5 mΩ and kp = 237","The ESR Rc contributes to the output voltage ripple, and hence affects the current loop. Two cases are simulated: Rc = 5 mΩ and Rc = 0. Simulations show that with Rc = 5 mΩ, the critical gain k∗p is 237, and with Rc = 0, k ∗ p = 452, which are also confirmed by the exact sampled-data analysis [21]. Take Rc = 5 mΩ and kp = 237, for example. The signal waveforms indicating (weak) subharmonic oscillation are shown in Fig. 12. The steady states are 2T -periodic orbits. The unstable T -periodic orbit y0(t) = Cx0(t) + Du = kp(vr − vo(t)) − iL(t) (with iL inverted) still exists and is shown in Fig. 13, along with h(t). If the T -periodic orbit x0(t) is perturbed, it will go to the stable 2T -periodic orbit shown in Fig. 12. The designed slopes of (on-time inductor current slope) m1 = (1−D)vs/L and (off-time inductor current slope) m2 = Dvs/L are 2.48×106 and 3.63×106, respectively. However, ESR Rc contributes to the output voltage ripple, and the actual slopes of y = kp(vr−vo)−iL shown in Fig. 13 are 5.39×106 and 7.88×106, respectively. Even with increased ripples, (ẏ0(d+) +ma)/(ẏ","Boundary Conditions of Subharmonic Oscillations in
  Fixed-Switching-Frequency DC-DC Converters","Design-oriented boundary conditions for subharmonic oscillations are of great
interest recently. Based on a subharmonic oscillation boundary condition
reported in a PhD thesis more than a decade ago, extended new boundary
conditions are derived in closed forms for general switching DC-DC converters.
Sampled-data and harmonic balance analyses are applied and generate equivalent
results. It is shown that equivalent series resistance causes the boundary
conditions for voltage/current mode control to have similar forms. Some
recently reported boundary conditions become special cases in view of the
general boundary conditions derived. New Nyquist-like design-oriented plots are
proposed to predict or prevent the occurrence of the subharmonic oscillation.
The relation between the crossover frequency and the subharmonic oscillation is
also analyzed."
scgqa_382,1908.04655v1,What insights does the RMSE comparison in the left panel of Figure 5(a) provide about autoPR performance?,"The left panel of the graph shows the (logarithm of the) root mean squared error (RMSE) of the estimate θ̂ over 10 realisations of the data for each value of θ∗, for both the standard NS approach and the autoPR method. The autoPR method generally achieves lower RMSE values than the standard NS approach, indicating that it is more accurate. This is likely due to the fact that the autoPR method uses a more sophisticated optimization algorithm that is better able to find the global minimum of the likelihood function.",1908.04655v1.pdf,"['1908.04655v1.pdf', '2005.14165v4.pdf', '1812.09355v1.pdf', '1911.07924v1.pdf', '1608.00887v1.pdf', '1908.09034v2.pdf', '1005.0416v1.pdf']",1908.04655v1-Figure5-1.png,"Fig. 5 Algorithm performance comparison in the univariate example over 10 realisations of the data for each value of θ∗. The left panel shows Log10(RMSE) of the estimate θ̂, and the right panel shows the mean number of likelihood evaluations, both of which are obtained using the standard NS approach (blue star points) and the autoPR method (red square points).","is given in Figure 5(a), which shows the (logarithm of the) root mean squared error (RMSE) of the estimate θ̂ over 10 realisations of the data for each value of θ∗, for both the standard NS approach and the autoPR method.",Bayesian posterior repartitioning for nested sampling,"Priors in Bayesian analyses often encode informative domain knowledge that
can be useful in making the inference process more efficient. Occasionally,
however, priors may be unrepresentative of the parameter values for a given
dataset, which can result in inefficient parameter space exploration, or even
incorrect inferences, particularly for nested sampling (NS) algorithms. Simply
broadening the prior in such cases may be inappropriate or impossible in some
applications. Hence our previous solution to this problem, known as posterior
repartitioning (PR), redefines the prior and likelihood while keeping their
product fixed, so that the posterior inferences and evidence estimates remain
unchanged, but the efficiency of the NS process is significantly increased. In
its most practical form, PR raises the prior to some power beta, which is
introduced as an auxiliary variable that must be determined on a case-by-case
basis, usually by lowering beta from unity according to some pre-defined
`annealing schedule' until the resulting inferences converge to a consistent
solution. Here we present a very simple yet powerful alternative Bayesian
approach, in which beta is instead treated as a hyperparameter that is inferred
from the data alongside the original parameters of the problem, and then
marginalised over to obtain the final inference. We show through numerical
examples that this Bayesian PR (BPR) method provides a very robust,
self-adapting and computationally efficient `hands-off' solution to the problem
of unrepresentative priors in Bayesian inference using NS. Moreover, unlike the
original PR method, we show that even for representative priors BPR has a
negligible computational overhead relative to standard nesting sampling, which
suggests that it should be used as the default in all NS analyses."
scgqa_383,1804.03842v1,"According to the results shown in Figure 10, how does OLCPM's stability compare to CPM's in the SocioPatterns network?","The graph shows that OLCPM outperforms CPM in terms of NMI values for both k = 3 and k = 4. This suggests that OLCPM is a more effective algorithm for community detection in collaboration networks. Additionally, the graph shows that OLCPM is more stable than CPM, as the NMI values for OLCPM do not vary as much across different days and hours. This suggests that OLCPM is less sensitive to noise and outliers, making it a more reliable algorithm for community detection.",1804.03842v1.pdf,"['1804.03842v1.pdf', '1805.05887v1.pdf', '1801.09097v2.pdf']",1804.03842v1-Figure10-1.png,Figure 10: NMI values of OLCPM and CPM [13] for k = 3 and k = 4 in on SocioPatterns collaboration networks [31].,"(Communities yielded by DyCPM and OCPM are identical). Then, for each snapshot, we compute the NMI according to [30]. Results are displayed in Figure 10. We show results for k=3 and k=4, which yield the best results.","OLCPM: An Online Framework for Detecting Overlapping Communities in
  Dynamic Social Networks","Community structure is one of the most prominent features of complex
networks. Community structure detection is of great importance to provide
insights into the network structure and functionalities. Most proposals focus
on static networks. However, finding communities in a dynamic network is even
more challenging, especially when communities overlap with each other. In this
article , we present an online algorithm, called OLCPM, based on clique
percolation and label propagation methods. OLCPM can detect overlapping
communities and works on temporal networks with a fine granularity. By locally
updating the community structure, OLCPM delivers significant improvement in
running time compared with previous clique percolation techniques. The
experimental results on both synthetic and real-world networks illustrate the
effectiveness of the method."
scgqa_384,1209.5833v2,"In the context of the LabelMe dataset, how is the relationship between bit count and algorithm performance depicted in Figure 3?","The graph shows that the performance of the different algorithms varies with the number of bits. S-LSH performs well with a small number of bits, but its performance degrades as the number of bits increases. MLH shows no learning performance improvements, and SH performs poorer as the number of bits increase.",1209.5833v2.pdf,"['1209.5833v2.pdf', '1804.00243v2.pdf', '1805.05887v1.pdf', '1603.08981v2.pdf', '1704.03458v1.pdf', '1912.02074v1.pdf']",1209.5833v2-Figure3-1.png,"Figure 3: LabelMe: Curves of precision (left) and recall (right) versus the number of bits for L2, S-LSH, LSH, SH, and MLH.","Fig. 3 shows graphs of dependency of the precision and the recall on the number of bits with the acquisition fixed at 0.01. S-LSH is observed performing well. MLH shows no learning performance improvements, and SH performs poorer as the number of bits increase.",Locality-Sensitive Hashing with Margin Based Feature Selection,"We propose a learning method with feature selection for Locality-Sensitive
Hashing. Locality-Sensitive Hashing converts feature vectors into bit arrays.
These bit arrays can be used to perform similarity searches and personal
authentication. The proposed method uses bit arrays longer than those used in
the end for similarity and other searches and by learning selects the bits that
will be used. We demonstrated this method can effectively perform optimization
for cases such as fingerprint images with a large number of labels and
extremely few data that share the same labels, as well as verifying that it is
also effective for natural images, handwritten digits, and speech features."
scgqa_385,1610.00017v2,What does Fig. 2 illustrate about the correlation between information block size and latency in optimal communication?,"The graph shows that the achievable latency decreases as the information block size message increases. This is because a larger information block size message requires more time to transmit, which in turn increases the latency. However, the graph also shows that the achievable latency decreases as the channel condition improves. This is because a better channel condition means that the transmission can be completed more quickly, which in turn reduces the latency.",1610.00017v2.pdf,"['1610.00017v2.pdf', '1402.1892v2.pdf', '1905.08337v1.pdf', '1208.4662v2.pdf', '1512.02567v1.pdf']",1610.00017v2-Figure2-1.png,"Fig. 2. Achievable latency as a function of the information block size message for different channel conditions, probability of block errorǫ = 10−7","For a fixed duration transmission, the minimal latency is simply obtained by using equation (14). For a single hop wireless link, and assuming a known constant channel gain where P is the received power, the minimal latency is depicted in Fig. 2 for an error rate of ǫ = 10−7. For example, let us consider that we have PT = 2.5 or 4 dB, and an information block size message of k = 103 bits needs to be sent. If all the bandwidth is used, then a blocklength of n = 186 symbols or channel use are necessary and for a bandwidth of W = 50 MHz, the latency would be L = 1.86 µs. However, to our knowledge, we don’t know of such a good (186,2103,ǫ) code.",On Optimal Latency of Communications,"In this paper we investigate the optimal latency of communications. Focusing
on fixed rate communication without any feedback channel, this paper
encompasses low-latency strategies with which one hop and multi-hop
communication issues are treated from an information theoretic perspective. By
defining the latency as the time required to make decisions, we prove that if
short messages can be transmitted in parallel Gaussian channels, for example,
via orthogonal frequency-division multiplexing (OFDM)-like signals, there
exists an optimal low-latency strategy for every code. This can be achieved via
early-detection schemes or asynchronous detections. We first provide the
optimal achievable latency in additive white Gaussian noise (AWGN) channels for
every channel code given a probability block error $\epsilon$. This can be
obtained via sequential ratio tests or a ""genie"" aided, \textit{e.g}.
error-detecting codes. Results demonstrate the effectiveness of the approach.
Next, we show how early-detection can be effective with OFDM signals while
maintaining its spectral efficiency via random coding or pre-coding random
matrices. Finally, we explore the optimal low-latency strategy in multi-hop
relaying schemes. For amplify-and-forward (AF) and decode-and-forward (DF)
relaying schemes there exist an optimal achievable latency. In particular, we
first show that there exist a better low-latency strategy, for which AF relays
could transmit while receiving. This can be achieved by using amplify and
forward combined with early detection."
scgqa_386,1908.09653v1,How does the graph in Figure 4.7 illustrate the balance between accuracy and generalization for γ values?,"The graph shows that the minimum RMSE(Θ) and minimum CV error change in a similar way with respect to the values of γ. This suggests that there is a trade-off between the accuracy and generalization ability of the model, and that the optimal value of γ is the one that strikes the best balance between these two factors.",1908.09653v1.pdf,"['1908.09653v1.pdf', '1412.4318v1.pdf', '1403.5801v2.pdf', '2007.11446v1.pdf', '1910.09592v1.pdf', '2001.11086v3.pdf']",1908.09653v1-Figure4.7-1.png,Figure 4.7: Minimum RMSE(Θ) (left) and minimum CV error (right) for different values of γ from the GSCA model with GDP penalty. One standard error bars are added to the CV error plot.,"−5 is used as the stopping criteria for all the following experiments to save time. The values of λ and γ are selected in the same way as was described in Section 4.4. Fig. 4.7 shows the minimum RMSE(Θ) and minimum CV error achieved for different values of the hyper-parameter γ. The minimum CV error changes in a similar way as the minimum RMSE(Θ) with respect to the values of γ. However, taking into account the uncertainty of estimated CV errors, the difference of the minimum CV errors for different γ is very small. Thus, we recommend to fix γ to be 1, rather than using cross validation to select it. Furthermore, setting γ = 1 as the default value for the GDP penalty has a probabilistic interpretation, see in [27].",Fusing heterogeneous data sets,"In systems biology, it is common to measure biochemical entities at different
levels of the same biological system. One of the central problems for the data
fusion of such data sets is the heterogeneity of the data. This thesis
discusses two types of heterogeneity. The first one is the type of data, such
as metabolomics, proteomics and RNAseq data in genomics. These different omics
data reflect the properties of the studied biological system from different
perspectives. The second one is the type of scale, which indicates the
measurements obtained at different scales, such as binary, ordinal, interval
and ratio-scaled variables. In this thesis, we developed several statistical
methods capable to fuse data sets of these two types of heterogeneity. The
advantages of the proposed methods in comparison with other approaches are
assessed using comprehensive simulations as well as the analysis of real
biological data sets."
scgqa_387,1909.00392v1,What trend does Figure 4 illustrate about reprojection error and mean relative distance in noncooperative spacecraft pose estimation?,"The graph shows that the reprojection error increases with the mean relative distance. This is to be expected, as the further away the object is from the camera, the more difficult it is to accurately reconstruct its 3D keypoint coordinates. However, even at a mean relative distance of 30 m, the reprojection error is still relatively low, at around 20 pixels. This suggests that the CNN trained with labels from recovered keypoints is able to learn the offsets from the ground-truth coordinates, even at large distances.",1909.00392v1.pdf,"['1909.00392v1.pdf', '1606.06377v1.pdf', '1608.00887v1.pdf', '2002.06199v1.pdf', '1703.07020v4.pdf', '2008.01961v3.pdf']",1909.00392v1-Figure4-1.png,"Figure 4. Average of reprojection error of recovered 3D keypoints plotted against mean relative distance, ||tBC ||2, for the SPEED synthetic training images.","Overall, the reconstructed 3D keypoint coordinates have an average distance error of 5.7 mm compared to those in the wireframe model, with the maximum error around 9.0 mm. Figure 4 plots the reprojection error of the recovered keypoints against the ground-truth keypoints from the wireframe model. While the maximum average error is around 40 pixels, the CNN trained with labels from recovered keypoints implicitly learns the offsets from the ground-truth coordinates.","Towards Robust Learning-Based Pose Estimation of Noncooperative
  Spacecraft","This work presents a novel Convolutional Neural Network (CNN) architecture
and a training procedure to enable robust and accurate pose estimation of a
noncooperative spacecraft. First, a new CNN architecture is introduced that has
scored a fourth place in the recent Pose Estimation Challenge hosted by
Stanford's Space Rendezvous Laboratory (SLAB) and the Advanced Concepts Team
(ACT) of the European Space Agency (ESA). The proposed architecture first
detects the object by regressing a 2D bounding box, then a separate network
regresses the 2D locations of the known surface keypoints from an image of the
target cropped around the detected Region-of-Interest (RoI). In a single-image
pose estimation problem, the extracted 2D keypoints can be used in conjunction
with corresponding 3D model coordinates to compute relative pose via the
Perspective-n-Point (PnP) problem. These keypoint locations have known
correspondences to those in the 3D model, since the CNN is trained to predict
the corners in a pre-defined order, allowing for bypassing the computationally
expensive feature matching processes. This work also introduces and explores
the texture randomization to train a CNN for spaceborne applications.
Specifically, Neural Style Transfer (NST) is applied to randomize the texture
of the spacecraft in synthetically rendered images. It is shown that using the
texture-randomized images of spacecraft for training improves the network's
performance on spaceborne images without exposure to them during training. It
is also shown that when using the texture-randomized spacecraft images during
training, regressing 3D bounding box corners leads to better performance on
spaceborne images than regressing surface keypoints, as NST inevitably distorts
the spacecraft's geometric features to which the surface keypoints have closer
relation."
scgqa_388,1506.06213v1,What insights does Figure 6 provide regarding the relationship between PNR ratios and the decision variable's detection capability?,"The graph shows that the decision variable can distinguish between no primary user case and primary user presence based on the PNR. This is because the conditional PDF under H1 is more peaked when the PNR ratio is high, indicating that the decision variable is more likely to be in the region that corresponds to the presence of a primary user.",1506.06213v1.pdf,"['1506.06213v1.pdf', '1710.10571v5.pdf', '1302.3123v1.pdf']",1506.06213v1-Figure6-1.png,"Figure 6. Conditional PDF underH0 and conditional PDF underH1 for PNR=-2, 0, 2, and 4 dB","Next, the hypothesis test is to be verified by exploring the conditional PDF under both H0 and H1. In fact, when there is no primary user in band, the decision variable follows only one unique PDF that is shown in Fig. 6. Under H1, the conditional PDF depends on the PNR ratio. Four additional curves are also shown in Fig. 6 for the conditional PDF under H1 with four different PNR values (-2, 0, 2, and 4 dB). It is clear that the decision variable can distinguish between no primary user case and primary user presence based on the PNR.","Spectrum Monitoring Using Energy Ratio Algorithm For OFDM-Based
  Cognitive Radio Networks","This paper presents a spectrum monitoring algorithm for Orthogonal Frequency
Division Multiplexing (OFDM) based cognitive radios by which the primary user
reappearance can be detected during the secondary user transmission. The
proposed technique reduces the frequency with which spectrum sensing must be
performed and greatly decreases the elapsed time between the start of a primary
transmission and its detection by the secondary network. This is done by
sensing the change in signal strength over a number of reserved OFDM
sub-carriers so that the reappearance of the primary user is quickly detected.
Moreover, the OFDM impairments such as power leakage, Narrow Band Interference
(NBI), and Inter-Carrier Interference (ICI) are investigated and their impact
on the proposed technique is studied. Both analysis and simulation show that
the \emph{energy ratio} algorithm can effectively and accurately detect the
appearance of the primary user. Furthermore, our method achieves high immunity
to frequency-selective fading channels for both single and multiple receive
antenna systems, with a complexity that is approximately twice that of a
conventional energy detector."
scgqa_389,2011.09375v1,What conclusion can be drawn from Figure 4 regarding the runtime differences among solvers for random 3-regular graphs?,"The graph shows that the deterministic solvers, namely, color refinement and choco, have quadratic runtime on random regular graphs. This is because these graphs have n leaves immediately attached to the root, and are asymmetric. Traces, on the other hand, has a special strategy called the trace invariant, which enables it to abort computation for most of the leaves very early, resulting in quite modest quadratic runtime. In particular, it is still able to outperform dejavu on the isomorphic instances of this benchmark set. On the nonisomorphic instances, dejavu does however also exploit the trace invariant, and thus achieves comparable performance to traces.",2011.09375v1.pdf,"['2011.09375v1.pdf', '2004.05448v1.pdf', '1809.09034v1.pdf', '1209.5833v2.pdf', '1906.11938v3.pdf', '1703.07020v4.pdf', '1910.11127v1.pdf', '2001.11086v3.pdf']",2011.09375v1-Figure4-1.png,Figure 4: Isomorphic pairs random 3-regular graphs (left) and non-isomorphic pairs (right).,"A very interesting case arises for random regular graphs (see Figure 4). These graphs with n nodes, say, result in search trees that almost surely have n leaves immediately attached to the root. Furthermore, they are asymmetric. For the sake of argument, assume that color refinement runs in O(n) on these graphs. For the deterministic solvers, this in turn results in quadratic runtime, while dejavu runs in O(n √ n). Traces however has a special strategy which is very effective on this set, namely the trace invariant. This enables it to abort computation for most of the leaves very early, resulting in quite modest quadratic runtime. In particular, it is still able to outperform dejavu on the isomorphic instances of this benchmark set. On the nonisomorphic instances, dejavu does however also exploit",Engineering a Fast Probabilistic Isomorphism Test,"We engineer a new probabilistic Monte-Carlo algorithm for isomorphism
testing. Most notably, as opposed to all other solvers, it implicitly exploits
the presence of symmetries without explicitly computing them.
  We provide extensive benchmarks, showing that the algorithm outperforms all
state-of-the-art solutions for isomorphism testing on most inputs from the de
facto standard benchmark library for isomorphism testing. On many input types,
our data not only show improved running times by an order of magnitude, but
also reflect a better asymptotic behavior.
  Our results demonstrate that, with current algorithms, isomorphism testing is
in practice easier than the related problems of computing the automorphism
group or canonically labeling a graph. The results also show that probabilistic
algorithms for isomorphism testing can be engineered to outperform
deterministic approaches, even asymptotically."
scgqa_390,1803.11512v1,What maximum throughput value does the distributed optimization control algorithm achieve in this paper's experiments?,"The results of the graph suggest that the distributed optimization control algorithm is a promising approach for improving network throughput. The algorithm is able to achieve a maximum throughput of 22.48 Mbps, which is significantly higher than the throughput achieved by the Douglas-Rachford splitting method. This suggests that the distributed optimization control algorithm is more efficient in terms of network throughput.",1803.11512v1.pdf,"['1803.11512v1.pdf', '1708.05355v1.pdf', '1907.11314v1.pdf']",1803.11512v1-Figure6-1.png,Figure 6: Network throughput within collaboration space.,"In terms of network throughput, Fig. 6 shows that the throughput increases up to 22.48 Mbps. In this figure, the coordinate selection rules (Cyclic, Gauss-Southwell, Randomized) in our distributed optimization control algorithm and the Douglas-Rachford splitting method have almost the same performance.","Joint Communication, Computation, Caching, and Control in Big Data
  Multi-access Edge Computing","The concept of multi-access edge computing (MEC) has been recently introduced
to supplement cloud computing by deploying MEC servers to the network edge so
as to reduce the network delay and alleviate the load on cloud data centers.
However, compared to a resourceful cloud, an MEC server has limited resources.
When each MEC server operates independently, it cannot handle all of the
computational and big data demands stemming from the users devices.
Consequently, the MEC server cannot provide significant gains in overhead
reduction due to data exchange between users devices and remote cloud.
Therefore, joint computing, caching, communication, and control (4C) at the
edge with MEC server collaboration is strongly needed for big data
applications. In order to address these challenges, in this paper, the problem
of joint 4C in big data MEC is formulated as an optimization problem whose goal
is to maximize the bandwidth saving while minimizing delay, subject to the
local computation capability of user devices, computation deadline, and MEC
resource constraints. However, the formulated problem is shown to be
non-convex. To make this problem convex, a proximal upper bound problem of the
original formulated problem that guarantees descent to the original problem is
proposed. To solve the proximal upper bound problem, a block successive upper
bound minimization (BSUM) method is applied. Simulation results show that the
proposed approach increases bandwidth-saving and minimizes delay while
satisfying the computation deadlines."
scgqa_391,1405.5329v4,How does single branch uniform sampling behave according to the distortion-rate function in this paper?,"The caption of the figure is referring to the fact that single branch uniform sampling does not always achieve the maximum achievable sampling rate for a given sampling rate and oversampling factor. This is because the sampling process introduces distortion to the spectrum, which can limit the maximum achievable sampling rate.",1405.5329v4.pdf,"['1405.5329v4.pdf', '1305.1657v1.pdf', '1612.03449v3.pdf']",1405.5329v4-Figure16-1.png,"Fig. 16: The function D( fs,R) at two values of R for the process with spectrum given in the small frame. Unlike in this example, single branch uniform sampling in general does not achieve D(R) for fs ≤ fNyq.",which is depicted in Fig. 16 for two different values of R.,Distortion-Rate Function of Sub-Nyquist Sampled Gaussian Sources,"The amount of information lost in sub-Nyquist sampling of a continuous-time
Gaussian stationary process is quantified. We consider a combined source coding
and sub-Nyquist reconstruction problem in which the input to the encoder is a
noisy sub-Nyquist sampled version of the analog source. We first derive an
expression for the mean squared error in the reconstruction of the process from
a noisy and information rate-limited version of its samples. This expression is
a function of the sampling frequency and the average number of bits describing
each sample. It is given as the sum of two terms: Minimum mean square error in
estimating the source from its noisy but otherwise fully observed sub-Nyquist
samples, and a second term obtained by reverse waterfilling over an average of
spectral densities associated with the polyphase components of the source. We
extend this result to multi-branch uniform sampling, where the samples are
available through a set of parallel channels with a uniform sampler and a
pre-sampling filter in each branch. Further optimization to reduce distortion
is then performed over the pre-sampling filters, and an optimal set of
pre-sampling filters associated with the statistics of the input signal and the
sampling frequency is found. This results in an expression for the minimal
possible distortion achievable under any analog to digital conversion scheme
involving uniform sampling and linear filtering. These results thus unify the
Shannon-Whittaker-Kotelnikov sampling theorem and Shannon rate-distortion
theory for Gaussian sources."
scgqa_392,1501.07107v1,"Considering the findings in your paper, what are the key algorithmic distinctions between IPVSS and hard-threshold as shown in the figure?","IPVSS and hard-threshold are two different algorithms for solving the weighted least squares problem. IPVSS is an iterative algorithm that uses a step-size that decreases as the iteration time increases. This allows IPVSS to achieve a fast convergence speed while also avoiding overshooting the optimal solution. Hard-threshold, on the other hand, is a non-iterative algorithm that uses a fixed step-size. This results in a slower convergence speed, but it also reduces the risk of overshooting the optimal solution.

The graph shows the performance of IPVSS and hard-threshold on a weighted least squares problem. The x-axis of the graph shows the number of iterations, and the y-axis shows the error between the estimated and true solutions. As can be seen from the graph, IPVSS converges to the optimal solution faster than hard-threshold. However, hard-threshold does not overshoot the optimal solution, while IPVSS does.

Overall, IPVSS is a better choice for problems where fast convergence speed is important. However, hard-threshold is a better choice for problems where overshooting the optimal solution is not acceptable.",1501.07107v1.pdf,"['1501.07107v1.pdf', '2010.00502v1.pdf', '1804.00243v2.pdf']",1501.07107v1-Figure2-1.png,Fig. 2 IPVSS v.s. hard-threshold.,"In (14), the initial step-size ( )n is the same as ISS-LMS while it decreases as the iteration time (n ), as shown in Fig. 2. In the first stage, the main demand is fast convergence speed which is decide by the","Iterative-Promoting Variable Step Size Least Mean Square Algorithm for
  Accelerating Adaptive Channel Estimation","Invariable step size based least-mean-square error (ISS-LMS) was considered
as a very simple adaptive filtering algorithm and hence it has been widely
utilized in many applications, such as adaptive channel estimation. It is well
known that the convergence speed of ISS-LMS is fixed by the initial step-size.
In the channel estimation scenarios, it is very hard to make tradeoff between
convergence speed and estimation performance. In this paper, we propose an
iterative-promoting variable step size based least-mean-square error (VSS-LMS)
algorithm to control the convergence speed as well as to improve the estimation
performance. Simulation results show that the proposed algorithm can achieve
better estimation performance than previous ISS-LMS while without sacrificing
convergence speed."
scgqa_393,2008.02777v1,"In the study's evaluation of network models, which two line heights were used in the experiments depicted in Figure 2?",The two line heights are 48 and 64 pixels.,2008.02777v1.pdf,"['2008.02777v1.pdf', '1802.02193v1.pdf', '1404.7045v3.pdf', '1809.07412v2.pdf', '1803.04037v1.pdf', '1911.02623v1.pdf']",2008.02777v1-Figure2-1.png,Figure 2: Minimum and maximum CER of trained models by line height used. Lower CERs are better.,"We first present two central findings on binarization and the number of pooling layers in the network architecture, which we termed P earlier (see Section 4.2). Figure 2 shows the CERs of a wide range of network models trained on our investigated two line heights. For each K (on the x-axis), we see the best and worst CERs obtained (best and worst in all investigated N , R, and P as described in Section 4.2.2). We found that","On the Accuracy of CRNNs for Line-Based OCR: A Multi-Parameter
  Evaluation","We investigate how to train a high quality optical character recognition
(OCR) model for difficult historical typefaces on degraded paper. Through
extensive grid searches, we obtain a neural network architecture and a set of
optimal data augmentation settings. We discuss the influence of factors such as
binarization, input line height, network width, network depth, and other
network training parameters such as dropout. Implementing these findings into a
practical model, we are able to obtain a 0.44% character error rate (CER) model
from only 10,000 lines of training data, outperforming currently available
pretrained models that were trained on more than 20 times the amount of data.
We show ablations for all components of our training pipeline, which relies on
the open source framework Calamari."
scgqa_394,1803.04037v1,"In the context of the Kaggle sales forecasting competition, how does the logarithmic loss perform against MSE as shown in the graph?","The study found that the new metric, called the logarithmic loss, is more effective than the traditional mean squared error (MSE) metric in predicting values across a large range of orders of magnitudes. This is because the logarithmic loss avoids penalizing large differences in prediction when both the predicted and the true number are large. For example, predicting 5 when the true value is 50 is penalized more than predicting 500 when the true value is 545. This makes the logarithmic loss a more accurate and reliable metric for evaluating machine learning models.",1803.04037v1.pdf,"['1803.04037v1.pdf', '1908.04647v1.pdf', '1407.7736v1.pdf', '1303.1635v1.pdf']",1803.04037v1-Figure1-1.png,Figure 1: Metric visualization.,"suitable when predicting values across a large range of orders of magnitudes. It avoids penalizing large differences in prediction when both the predicted and the true number are large: predicting 5 when the true value is 50 is penalized more than predicting 500 when the true value is 545. It can be seen on Figure 1, whereby the left side is true positive values and number of iterations is in the bottom.","Sales forecasting using WaveNet within the framework of the Kaggle
  competition","We took part in the Corporacion Favorita Grocery Sales Forecasting
competition hosted on Kaggle and achieved the 2nd place. In this abstract
paper, we present an overall analysis and solution to the underlying
machine-learning problem based on time series data, where major challenges are
identified and corresponding preliminary methods are proposed. Our approach is
based on the adaptation of dilated convolutional neural network for time series
forecasting. By applying this technique iteratively to batches of n examples, a
big amount of time series data can be eventually processed with a decent speed
and accuracy. We hope this paper could serve, to some extent, as a review and
guideline of the time series forecasting benchmark, inspiring further attempts
and researches."
scgqa_395,1006.3688v1,"In the context of the P300 speller results, what difference does Figure 2 highlight for the target versus non-target ERPs?","The graph shows that the target character (red curve) has a significantly different ERP than the eight non–target characters (grey curves). This difference is evident in the P300 component, which reaches its peak 400ms after stimlus onset with an amplitude of approximately 4µV at electrode Pz. The P300 waveform has a rather broad shape, starting from 300ms and lasting until 600ms.",1006.3688v1.pdf,"['1006.3688v1.pdf', '1709.08441v4.pdf', '1608.00887v1.pdf', '1909.01868v3.pdf', '1906.07255v3.pdf', '1804.00243v2.pdf', '1809.01628v1.pdf', '1404.7045v3.pdf']",1006.3688v1-Figure2-1.png,"Figure 2: ERP waveforms for combined attention and fixation. Grand average ERPs for attended/fixated character (red), and the eight remaining ones (grey). Time points where t–tests revealed significant differences between red and each grey curve were shaded red (α = 0.1%).","In the first part of the experiment subjects had to fixate and count the target character “E” while neglecting all others. Figure 2 shows the grand average ERPs for all trials presenting the target character (red curve), and trials presenting the eight non–target ones (grey curves). To examine significant differences among them, we performed t–tests for equality of target and each non–target ERP. Time points where all eight null hypotheses were rejected at 0.1% level were shaded red. The ERP of the target character contained a P300 component, reaching its peak 400ms after stimlus onset with an amplitude of approximately 4µV at electrode Pz. P300 waveform had a rather broad shape, starting from 300ms and lasting until 600ms. Figure 4 shows the topographic map of the ERP for the target character at 400ms. It shows the typical spatial distribution with a positivity at parieto– central electrodes and declining to the periphery [7, 2]. Besides this, the data also contained discriminantive information at posterior electrodes before",Is the P300 Speller Independent?,"The P300 speller is being considered as an independent brain-computer
interface. That means it measures the user's intent, and does not require the
user to move any muscles. In particular it should not require eye fixation of
the desired character. However, it has been shown that posterior electrodes
provide significant discriminative information, which is likely related to
visual processing. These findings imply the need for studies controlling the
effect of eye movements. In experiments with a 3x3 character matrix, attention
and eye fixation was directed to different characters. In the event-related
potentials, a P300 occurred for the attended character, and N200 was seen for
the trials showing the focussed character. It occurred at posterior sites,
reaching its peak at 200ms after stimulus onset. The results suggest that gaze
direction plays an important role in P300 speller paradigm. By controlling gaze
direction it is possible to separate voluntary and involuntary EEG responses to
the highlighting of characters."
scgqa_396,1903.10464v3,"According to the findings displayed in Figure 8, which method shows superior performance for features in dimension 10?","The main findings of the graph are that the Gaussian method generally shows the best performance, and the combined empirical and Gaussian/copula approaches also work well. For the piecewise constant model, the TreeSHAP method behaves similarly to the Gaussian method, while the other methods perform worse.",1903.10464v3.pdf,"['1903.10464v3.pdf', '1811.00416v5.pdf', '1106.3242v2.pdf', '1804.00243v2.pdf', '1704.03458v1.pdf']",1903.10464v3-Figure8-1.png,Figure 8. Dimension 10: MAE and skill score for linear model with Gaussian distributed features.,"The results from the simulation experiments are shown in Figures 8 and 9 and Table 1. While numerical integration was used to compute the exact Shapley values in the 3 dimensional experiments, we have to turn to Monte Carlo integration for the 10 dimensional case. From the figures, we see that the results with Gaussian distributed features in dimension 10 are mostly the same as for the 3 dimensional counterparts in Figures 2 and 3, with the Gaussian method generally showing the best performance. The combined empirical and Gaussian/copula approaches also works well. For the piecewise constant model, the TreeSHAP method behaves","Explaining individual predictions when features are dependent: More
  accurate approximations to Shapley values","Explaining complex or seemingly simple machine learning models is an
important practical problem. We want to explain individual predictions from a
complex machine learning model by learning simple, interpretable explanations.
Shapley values is a game theoretic concept that can be used for this purpose.
The Shapley value framework has a series of desirable theoretical properties,
and can in principle handle any predictive model. Kernel SHAP is a
computationally efficient approximation to Shapley values in higher dimensions.
Like several other existing methods, this approach assumes that the features
are independent, which may give very wrong explanations. This is the case even
if a simple linear model is used for predictions. In this paper, we extend the
Kernel SHAP method to handle dependent features. We provide several examples of
linear and non-linear models with various degrees of feature dependence, where
our method gives more accurate approximations to the true Shapley values. We
also propose a method for aggregating individual Shapley values, such that the
prediction can be explained by groups of dependent variables."
scgqa_397,2005.13754v1,"According to the paper's findings in Fig. 9, how does the moving average filter impact distance estimation?","A moving average filter is a type of low-pass filter that smooths out the data by averaging it over a certain number of samples. This can help to reduce noise and outliers, and to make the data more consistent. In the case of RSS data, a moving average filter can help to improve the accuracy of distance estimation. This is because the raw RSS data can be noisy and inconsistent, especially in indoor environments where there are multiple reflections and diffractions of the signal. By averaging the data over a certain number of samples, the moving average filter can help to remove these noise and outliers, and to provide a more accurate estimate of the distance.",2005.13754v1.pdf,"['2005.13754v1.pdf', '1808.10082v4.pdf', '1805.06370v2.pdf', '2008.06431v1.pdf', '1710.10733v4.pdf', '2004.04276v1.pdf']",2005.13754v1-Figure9-1.png,Fig. 9: The raw RSS data vs. the filtered RSS data computed through moving average.,"To mitigate the possible outlier, we apply a moving average filter to the raw data. A comparison between the raw RSS data with the filtered data, for d ={0.2, 1, 2, 3, 5} m is shown in Fig. 9. It is clear that the filtered data provides a much smoother signal. However, there are still variations in signal strength even though the distance is the same. In practice, it is hard to obtain only line-of-sight (LOS) signal in indoor environments due to the reflection and diffraction of the signal. Extracting multipath profile features to achieve better distance estimation could be a possible future direction.",COVID-19 and Your Smartphone: BLE-based Smart Contact Tracing,"Contact tracing is of paramount importance when it comes to preventing the
spreading of infectious diseases. Contact tracing is usually performed manually
by authorized personnel. Manual contact tracing is an inefficient, error-prone,
time-consuming process of limited utility to the population at large as those
in close contact with infected individuals are informed hours, if not days,
later. This paper introduces an alternative way to manual contact tracing. The
proposed Smart Contact Tracing (SCT) system utilizes the smartphone's Bluetooth
Low Energy (BLE) signals and machine learning classifier to accurately and
quickly determined the contact profile. SCT's contribution is two-fold: a)
classification of the user's contact as high/low-risk using precise proximity
sensing, and b) user anonymity using a privacy-preserving communications
protocol. SCT leverages BLE's non-connectable advertising feature to broadcast
a signature packet when the user is in the public space. Both broadcasted and
observed signatures are stored in the user's smartphone and they are only
uploaded to a secure signature database when a user is confirmed by public
health authorities to be infected. Using received signal strength (RSS) each
smartphone estimates its distance from other user's phones and issues real-time
alerts when social distancing rules are violated. The paper includes extensive
experimentation utilizing real-life smartphone positions and a comparative
evaluation of five machine learning classifiers. Reported results indicate that
a decision tree classifier outperforms other states of the art classification
methods in terms of accuracy. Lastly, to facilitate research in this area, and
to contribute to the timely development of advanced solutions the entire data
set of six experiments with about 123,000 data points is made publicly
available."
scgqa_398,1703.07020v4,What relationship between probability p and MSE performance is illustrated in Fig. 8 of the research?,"The graph shows that the performance of the different algorithms varies as the probability p changes. The ""Separate"" algorithm, which does not exploit the SCS property, has a fixed performance regardless of p. The ""GivenCluster"" algorithm, which exploits the SCS property by grouping antennas into clusters, exhibits better performance with larger p, since larger p indicates less clusters and larger cluster size. The ""Dirichlet-VB"" and the proposed ""Dirichlet-MP"" algorithms also show better performance with larger p, but their performance deteriorates with the decrease of p, even becoming slightly worse than ""Separate"" when p ≤ 0.5. This is because small p indicates more clusters and fewer antennas within each cluster, which can lead to errors in the grouping of antennas for the Dirichlet-based algorithms.",1703.07020v4.pdf,"['1703.07020v4.pdf', '1402.1892v2.pdf', '2005.09634v1.pdf', '1909.03961v2.pdf', '2008.02777v1.pdf', '1912.00035v1.pdf']",1703.07020v4-Figure8-1.png,"Fig. 8. MSE performance versus probability p, with SNR=8dB and 24 pilots.","In Fig. 8 shows MSE performance versus different probability p, with SNR=8dB and N = 24. It shows that the performance of “Separate” is fixed with different p, since no SCS property is utilized, “GivenCluster” exhibits better performance with the increasement of p, since larger p indicates less clusters and larger cluster size. The “Dirichlet-VB” and the proposed “Dirichlet-MP” also show better performance with larger p, but their performance deteriorate with the decrease of p, even may slightly worse than “Separate” when p ≤ 0.5. We can explain such interesting result as follows. Small p indicates more clusters and fewer antennas within each cluster. As shown in Fig 4 (b), the antenna array is grouped into 28 clusters with p = 0.5, and most of the clusters only has 1- 3 elements. Such dense clusters may lead to errors in the grouping of antennas for the Dirichlet-based algorithms, i.e., channels have no SCS property be grouped into one cluster, and such errors will lead to performance loss. Note that, we add a new curve in Fig. 8, denotes the estimator which roughly assume the whole array have the SCS property, and is denoted as “SCS-Array”. It shows that, the “SCS-Array” have the same performance with “GivenCluster” with p = 1, but deteriorate rapidly when p < 1. In the other hand, compared to “SCSArray” the robustness can be significantly improved with the SCS-exploiting algorithm proposed in this paper.","Low-Complexity Message Passing Based Massive MIMO Channel Estimation by
  Exploiting Unknown Sparse Common Support with Dirichlet Process","This paper investigates the problem of estimating sparse channels in massive
MIMO systems. Most wireless channels are sparse with large delay spread, while
some channels can be observed having sparse common support (SCS) within a
certain area of the antenna array, i.e., the antenna array can be grouped into
several clusters according to the sparse supports of channels. The SCS property
is attractive when it comes to the estimation of large number of channels in
massive MIMO systems. Using the SCS of channels, one expects better
performance, but the number of clusters and the elements for each cluster are
always unknown in the receiver. In this paper, {the Dirichlet process} is
exploited to model such sparse channels where those in each cluster have SCS.
We proposed a low complexity message passing based sparse Bayesian learning to
perform channel estimation in massive MIMO systems by using combined BP with MF
on a factor graph. Simulation results demonstrate that the proposed massive
MIMO sparse channel estimation outperforms the state-of-the-art algorithms.
Especially, it even shows better performance than the variational Bayesian
method applied for massive MIMO channel estimation."
scgqa_399,1907.11771v1,What is observed about accuracy and function evaluations in the experiments shown in Fig. 7?,"The graph shows that the various methods converge to the analytic solution as the time step is normalized by the number of function evaluations per step. This means that the methods are able to achieve the same accuracy with fewer function evaluations, which can lead to significant time savings.",1907.11771v1.pdf,"['1907.11771v1.pdf', '1911.11395v2.pdf', '2008.02777v1.pdf', '2010.08182v3.pdf', '1610.08332v1.pdf', '1905.05284v1.pdf', '1301.5201v1.pdf', '1309.3959v1.pdf']",1907.11771v1-Figure7-1.png,Fig. 7 Convection error vs. normalized time step for various methods,"Convergence to the analytic solution for the various methods is demonstrated in Fig. 7. For fair comparison across methods, the horizontal axis is time step normalized by the number of function evaluations per step. Thus vertical slices correspond to equal time-to-solution, neglecting the overhead of sharing data across cores. We use a high precision floating point library [1] for computation since machine precision is",A parallel-in-time approach for wave-type PDEs,"Numerical solutions to wave-type PDEs utilizing method-of-lines require the
ODE solver's stability domain to include a large stretch of the imaginary axis
surrounding the origin. We show here that extrapolation based solvers of
Gragg-Bulirsch-Stoer (GBS) type can meet this requirement. Extrapolation
methods utilize several independent time stepping sequences, making them highly
suited for parallel execution. Traditional extrapolation schemes use all time
stepping sequences to maximize the method's order of accuracy. The present
method instead maintains a desired order of accuracy while employing additional
time stepping sequences to shape the resulting stability domain. We optimize
the extrapolation coefficients to maximize the stability domain's imaginary
axis coverage. This yields a family of explicit schemes that approaches maximal
time step size for wave propagation problems. On a computer with several cores
we achieve both high order and fast time to solution compared with traditional
ODE integrators."
scgqa_400,2005.11699v2,How does the TM-PNN initialized from ODE perform in predicting physical pendulum movement as shown in the paper?,"The graph suggests that the TM-PNN has the potential to be used in physical pendulum applications. This is because the TM-PNN is able to make accurate predictions about the pendulum's dynamics, even when the initial angle is not known. This makes the TM-PNN a valuable tool for predicting the behavior of physical pendulums.",2005.11699v2.pdf,"['2005.11699v2.pdf', '1506.06213v1.pdf', '1804.06161v2.pdf', '1512.00843v3.pdf', '2004.05448v1.pdf', '1805.07914v3.pdf', '1606.06377v1.pdf', '1302.2824v2.pdf', '2008.01961v3.pdf']",2005.11699v2-Figure11-1.png,Figure 11: Predictions of initialized from ODE (9) TM-PNN for different initial angles. The TM-PNN initially represents a rough assumtion about the pendulum dynamics.,"Fig. 11 shows that map (10) with weights (13) represents a theoretical oscillation of the mathematical pendulum with L = 0.30 m. Though the initialized with (13) PNN only roughly approximates the real pendulum, it can be used for the physical prediction of the dynamics starting with arbitrary angles.","Physics-based polynomial neural networks for one-shot learning of
  dynamical systems from one or a few samples","This paper discusses an approach for incorporating prior physical knowledge
into the neural network to improve data efficiency and the generalization of
predictive models. If the dynamics of a system approximately follows a given
differential equation, the Taylor mapping method can be used to initialize the
weights of a polynomial neural network. This allows the fine-tuning of the
model from one training sample of real system dynamics. The paper describes
practical results on real experiments with both a simple pendulum and one of
the largest worldwide X-ray source. It is demonstrated in practice that the
proposed approach allows recovering complex physics from noisy, limited, and
partial observations and provides meaningful predictions for previously unseen
inputs. The approach mainly targets the learning of physical systems when
state-of-the-art models are difficult to apply given the lack of training data."
scgqa_401,1203.1203v2,What distinct subwords are represented by the complexity function fw(n) for the trapezoidal word in this study?,"The complexity function of a word w is a function that describes the number of distinct subwords of w of length n. In the case of the trapezoidal word w = aaababa, the complexity function is fw(n) = 4 for n ≥ 3, and fw(n) = 3 for n < 3. This means that the number of distinct subwords of length n in w is 4 for n ≥ 3, and 3 for n < 3.",1203.1203v2.pdf,"['1203.1203v2.pdf', '1802.05945v1.pdf', '1706.01341v1.pdf', '1906.07255v3.pdf', '1501.06137v1.pdf']",1203.1203v2-Figure1-1.png,"Figure 1: The graph of the complexity function of the trapezoidal word w = aaababa. One has min{Rw,Kw} = 3 and max{Rw,Kw} = 4.",Example 2.7. The word w = aaababa considered in Example 2.3 is trapezoidal. See Figure 1.,Enumeration and Structure of Trapezoidal Words,"Trapezoidal words are words having at most $n+1$ distinct factors of length
$n$ for every $n\ge 0$. They therefore encompass finite Sturmian words. We give
combinatorial characterizations of trapezoidal words and exhibit a formula for
their enumeration. We then separate trapezoidal words into two disjoint
classes: open and closed. A trapezoidal word is closed if it has a factor that
occurs only as a prefix and as a suffix; otherwise it is open. We investigate
open and closed trapezoidal words, in relation with their special factors. We
prove that Sturmian palindromes are closed trapezoidal words and that a closed
trapezoidal word is a Sturmian palindrome if and only if its longest repeated
prefix is a palindrome. We also define a new class of words, \emph{semicentral
words}, and show that they are characterized by the property that they can be
written as $uxyu$, for a central word $u$ and two different letters $x,y$.
Finally, we investigate the prefixes of the Fibonacci word with respect to the
property of being open or closed trapezoidal words, and show that the sequence
of open and closed prefixes of the Fibonacci word follows the Fibonacci
sequence."
scgqa_402,1206.5265v1,"In the context of the experiments in the paper, what does Figure 1(b) reveal about greedy CSS heuristics?","The graph shows that the greedy CSS heuristics perform well in terms of cost. The cost of the greedy CSS heuristics is a fraction of the BF-CSS cost, which is in effect the exact BF algorithm for n ≤ 14. This shows that the greedy CSS heuristics are able to find good solutions with a relatively small number of nodes expanded.",1206.5265v1.pdf,"['1206.5265v1.pdf', '2002.10790v1.pdf', '1711.06964v1.pdf', '2010.13691v1.pdf', '1202.4232v2.pdf', '2004.01867v1.pdf']",1206.5265v1-Figure1-1.png,Figure 1: (a) The average number of nodes expanded by the SearchPi with heuristic A = 0 for various values of n and θ. The error bars mark the minimum and maximum values over niter = 10 replications. (b) The cost of the greedy CSS heuristics as a fraction of the BF-CSS cost. The BF-CSS heuristic is in effect the exact BF algorithm for n ≤ 14. The data are Q matrices with independent random entries. The boxplots are over niter = 10 replications.,"We have confirmed this experimentally, on samples withN = 5000 from distribution Pθ,π0 with random π0 and with θ ≡ 1, 1.5, 2, 3. Each experiment was replicated niter = 10 times. In all cases, all the heuristics returned the optimal permutation. For this experiment, Figure 1,a shows the number of nodes expanded by the BF algorithm as a function of θ and n.",Consensus ranking under the exponential model,"We analyze the generalized Mallows model, a popular exponential model over
rankings. Estimating the central (or consensus) ranking from data is NP-hard.
We obtain the following new results: (1) We show that search methods can
estimate both the central ranking pi0 and the model parameters theta exactly.
The search is n! in the worst case, but is tractable when the true distribution
is concentrated around its mode; (2) We show that the generalized Mallows model
is jointly exponential in (pi0; theta), and introduce the conjugate prior for
this model class; (3) The sufficient statistics are the pairwise marginal
probabilities that item i is preferred to item j. Preliminary experiments
confirm the theoretical predictions and compare the new algorithm and existing
heuristics."
scgqa_403,1907.10906v1,"In the context of the theory presented, what is the effect of decreasing q on p as illustrated in Figure 1?","The graph shows that as q decreases, p also decreases. This is because the two parameters are inversely related, meaning that as one increases, the other decreases. This relationship is consistent with the results of the second experiment, which showed that it is better to make p and q both large than to choose q = 0. This is because q = 0 is suggested by SDP, which is somewhat inadequate for SC.",1907.10906v1.pdf,"['1907.10906v1.pdf', '1911.04231v2.pdf', '1808.06304v2.pdf', '1707.02342v1.pdf', '1904.01542v3.pdf', '1909.01868v3.pdf']",1907.10906v1-Figure1-1.png,"Figure 1: The relation between p and q, when d = 100, n = 5000, κ = 1 − √ 1/2 (s = d/2), ρ = 1.","It is obvious that p will decrease simultaneously if q decreases by increasing τ , which is also demonstrated in Figure 1. Combining the result of the second experiment (c.f. Figure 2), we can find that it is better to make p, q both large than to choose q = 0, although q = 0 is suggested by SDP, which is consistent with our result, while shows that SDP is somewhat inadequate for SC.","Theory of Spectral Method for Union of Subspaces-Based Random Geometry
  Graph","Spectral Method is a commonly used scheme to cluster data points lying close
to Union of Subspaces by first constructing a Random Geometry Graph, called
Subspace Clustering. This paper establishes a theory to analyze this method.
Based on this theory, we demonstrate the efficiency of Subspace Clustering in
fairly broad conditions. The insights and analysis techniques developed in this
paper might also have implications for other random graph problems. Numerical
experiments demonstrate the effectiveness of our theoretical study."
scgqa_404,2005.14165v4,"In the experiments depicted in Figure 1.2, how does descriptive context influence GPT-3's performance on few-shot learning?",The graph shows that the addition of a natural language task description improves model performance. This is likely because the task description provides additional context that helps the model to understand the task.,2005.14165v4.pdf,"['2005.14165v4.pdf', '1905.05284v1.pdf', '1811.01194v1.pdf', '1803.04037v1.pdf', '2006.11769v1.pdf', '1710.10571v5.pdf', '2006.09358v2.pdf', '1811.00912v4.pdf']",2005.14165v4-Figure1.2-1.png,"Figure 1.2: Larger models make increasingly efficient use of in-context information. We show in-context learning performance on a simple task requiring the model to remove random symbols from a word, both with and without a natural language task description (see Sec. 3.9.2). The steeper “in-context learning curves” for large models demonstrate improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range of tasks.","Figure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to remove extraneous symbols from a word. Model performance improves with the addition of a natural language task description, and with the number of examples in the model’s context, K. Few-shot learning also improves dramatically with model size. Though the results in this case are particularly striking, the general trends with both model size and number of examples in-context hold for most tasks we study. We emphasize that these “learning” curves involve no gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning.",Language Models are Few-Shot Learners,"Recent work has demonstrated substantial gains on many NLP tasks and
benchmarks by pre-training on a large corpus of text followed by fine-tuning on
a specific task. While typically task-agnostic in architecture, this method
still requires task-specific fine-tuning datasets of thousands or tens of
thousands of examples. By contrast, humans can generally perform a new language
task from only a few examples or from simple instructions - something which
current NLP systems still largely struggle to do. Here we show that scaling up
language models greatly improves task-agnostic, few-shot performance, sometimes
even reaching competitiveness with prior state-of-the-art fine-tuning
approaches. Specifically, we train GPT-3, an autoregressive language model with
175 billion parameters, 10x more than any previous non-sparse language model,
and test its performance in the few-shot setting. For all tasks, GPT-3 is
applied without any gradient updates or fine-tuning, with tasks and few-shot
demonstrations specified purely via text interaction with the model. GPT-3
achieves strong performance on many NLP datasets, including translation,
question-answering, and cloze tasks, as well as several tasks that require
on-the-fly reasoning or domain adaptation, such as unscrambling words, using a
novel word in a sentence, or performing 3-digit arithmetic. At the same time,
we also identify some datasets where GPT-3's few-shot learning still struggles,
as well as some datasets where GPT-3 faces methodological issues related to
training on large web corpora. Finally, we find that GPT-3 can generate samples
of news articles which human evaluators have difficulty distinguishing from
articles written by humans. We discuss broader societal impacts of this finding
and of GPT-3 in general."
scgqa_405,1604.04026v1,What trend is observed in the graph regarding SRCD's running time when using multiple threads for 100 iterations?,"The graph shows that the running time of the proposed algorithm SRCD for 100 iterations with different number of latent components using different number of threads significantly decreases when the number of used threads increases. This is because the parallel algorithm can utilize multiple threads to process the data simultaneously, which reduces the overall running time.",1604.04026v1.pdf,"['1604.04026v1.pdf', '1906.07610v2.pdf', '2005.09634v1.pdf', '1906.11938v3.pdf', '1804.03842v1.pdf', '1703.10422v2.pdf', '1501.07107v1.pdf']",1604.04026v1-Figure4-1.png,Fig. 4: Running time of 100 iterations with r = 50 and using different number of threads,"This section investigates running the proposed algorithm on large datasets with different settings. Figure 3 shows the running time of Algorithm SRCD for 100 iterations with different number of latent component using 1 thread. Clearly, the running time linearly increases, which fits the theoretical analyses about fast convergence and linear complexity for large sparse datasets in Section III. Furthermore, concerning the parallel algorithm, the running time of Algorithm SRCD for 100 iterations significantly decreases when the number of used threads increases in Figure 4. In addition, the running time is acceptable for large applications. Hence, these results indicate that the proposed algorithm SRCD is feasible for large scale applications.","Fast Parallel Randomized Algorithm for Nonnegative Matrix Factorization
  with KL Divergence for Large Sparse Datasets","Nonnegative Matrix Factorization (NMF) with Kullback-Leibler Divergence
(NMF-KL) is one of the most significant NMF problems and equivalent to
Probabilistic Latent Semantic Indexing (PLSI), which has been successfully
applied in many applications. For sparse count data, a Poisson distribution and
KL divergence provide sparse models and sparse representation, which describe
the random variation better than a normal distribution and Frobenius norm.
Specially, sparse models provide more concise understanding of the appearance
of attributes over latent components, while sparse representation provides
concise interpretability of the contribution of latent components over
instances. However, minimizing NMF with KL divergence is much more difficult
than minimizing NMF with Frobenius norm; and sparse models, sparse
representation and fast algorithms for large sparse datasets are still
challenges for NMF with KL divergence. In this paper, we propose a fast
parallel randomized coordinate descent algorithm having fast convergence for
large sparse datasets to archive sparse models and sparse representation. The
proposed algorithm's experimental results overperform the current studies' ones
in this problem."
scgqa_406,2008.13170v1,How does the compact SIAC filter perform in comparison to the original SIAC filter as shown in Figure 4.7?,"The results shown in the graph suggest that SIAC filtering is an effective technique for improving the accuracy and smoothness of DG methods. This is especially true for higher-order DG methods, where the compact SIAC filter can be used to achieve significant improvements in accuracy without significantly increasing the computational cost.",2008.13170v1.pdf,"['2008.13170v1.pdf', '2008.07524v3.pdf', '1302.2824v2.pdf', '1906.07610v2.pdf', '1611.04706v2.pdf', '2009.08716v1.pdf', '1902.07084v2.pdf', '1802.02193v1.pdf', '1603.08981v2.pdf']",2008.13170v1-Figure4.7-1.png,"Figure 4.7: The point-wise error plots for advection equation (3.20) for the DG method with the filtering techniques with polynomial P2. Here, the final time T = 1. We observe that the compared to the DG solution, the solutions after filtering with both the original and compact SIAC filtering have recovered the smoothness in the approximation and reduced the error.","The L2 norm errors and respective accuracy orders are given in Table 4.4. Compared to the DG solutions with the regular accuracy order of k+ 1, the filtered solutions (with both the original and compact SIAC filtering) have the superconvergence order of 2k+ 1. Furthermore, we observe that compared to the original SIAC filter, the compact filter has a much smaller support size while shows noticeable better accuracy for higher-order cases. For example, in the k = 3 case, the compact SIAC filter has only half the original SIAC filter’s support size, while the filtered errors are more than 10 times smaller. In Figure 4.7, we show the point-wise error plots before and after applying the original and compact SIAC filtering. We note that both filters recover smoothness in the DG solution as well as reduce the error.","How to Design A Generic Accuracy-Enhancing Filter for Discontinuous
  Galerkin Methods","Higher-order accuracy (order of $k+1$ in the $L^2$ norm) is one of the well
known beneficial properties of the discontinuous Galerkin (DG) method.
Furthermore, many studies have demonstrated the superconvergence property
(order of $2k+1$ in the negative norm) of the semi-discrete DG method. One can
take advantage of this superconvergence property by post-processing techniques
to enhance the accuracy of the DG solution. A popular class of post-processing
techniques to raise the convergence rate from order $k+1$ to order $2k+1$ in
the $L^2$ norm is the Smoothness-Increasing Accuracy-Conserving (SIAC)
filtering. In addition to enhancing the accuracy, the SIAC filtering also
increases the inter-element smoothness of the DG solution. The SIAC filtering
was introduced for the DG method of the linear hyperbolic equation by Cockburn
et al. in 2003. Since then, there are many generalizations of the SIAC
filtering have been proposed. However, the development of SIAC filtering has
never gone beyond the framework of using spline functions (mostly B-splines) to
construct the filter function. In this paper, we first investigate the general
basis function (beyond the spline functions) that can be used to construct the
SIAC filter. The studies of the general basis function relax the SIAC filter
structure and provide more specific properties, such as extra smoothness, etc.
Secondly, we study the basis functions' distribution and propose a new SIAC
filter called compact SIAC filter that significantly reduces the original SIAC
filter's support size while preserving (or even improving) its ability to
enhance the accuracy of the DG solution. We show that the proofs of the new
SIAC filters' ability to extract the superconvergence and provide numerical
results to confirm the theoretical results and demonstrate the new finding's
good numerical performance."
